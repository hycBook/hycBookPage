<?xml version="1.0" encoding="utf-8"?>
<search>
	<entry>
		<title>pytorch学习_进阶知识</title>
		<url>/article/53040.html</url>
		<content>pytorch中文文档Tensortorch.Tensor是一种包含单一数据类型元素的多维矩阵Torch定义了10种CPU tensor类型和GPU tensor类型：Data typedtypeCPU tensorGPU tensor32-bit floating pointtorch.float32 or torch.floattorch.FloatTensortorch.cuda.FloatTensor64-bit floating pointtorch.float64 or torch.doubletorch.DoubleTensortorch.cuda.DoubleTensor16-bit floating point [1]torch.float16 or torch.halftorch.HalfTensortorch.cuda.HalfTensor16-bit floating point [2]torch.bfloat16torch.BFloat16Tensortorch.cuda.BFloat16Tensor32-bit complextorch.complex32 or torch.chalf64-bit complextorch.complex64 or torch.cfloat128-bit complextorch.complex128 or torch.cdouble8-bit integer (unsigned)torch.uint8torch.ByteTensortorch.cuda.ByteTensor8-bit integer (signed)torch.int8torch.CharTensortorch.cuda.CharTensor16-bit integer (signed)torch.int16 or torch.shorttorch.ShortTensortorch.cuda.ShortTensor32-bit integer (signed)torch.int32 or torch.inttorch.IntTensortorch.cuda.IntTensor64-bit integer (signed)torch.int64 or torch.longtorch.LongTensortorch.cuda.LongTensorBooleantorch.booltorch.BoolTensortorch.cuda.BoolTensorquantized 8-bit integer (unsigned)torch.quint8torch.ByteTensor/quantized 8-bit integer (signed)torch.qint8torch.CharTensor/quantized 32-bit integer (signed)torch.qint32torch.IntTensor/quantized 4-bit integer (unsigned) [3]torch.quint4x2torch.ByteTensor/创建一个张量tensor可以从Python的list或序列构建12345torch.FloatTensor([[1, 2, 3], [4, 5, 6]])Out[0]: tensor([[1., 2., 3.],        [4., 5., 6.]])根据可选择的大小和数据新建一个tensor。 如果没有提供参数，将会返回一个空的零维张量。如果提供了numpy.ndarray,torch.Tensor或torch.Storage，将会返回一个有同样参数的tensor.如果提供了python序列，将会从序列的副本创建一个tensor1234567891011# 接口 一个空张量tensor可以通过规定其大小来构建class torch.Tensorclass torch.Tensor(*sizes)class torch.Tensor(size)class torch.Tensor(sequence)class torch.Tensor(ndarray)class torch.Tensor(tensor)class torch.Tensor(storage)# 实例化torch.IntTensor(2, 4).zero_()可以用python的索引和切片来获取和修改一个张量tensor中的内容123456789x = torch.FloatTensor([[1, 2, 3], [4, 5, 6]])x[1][2]Out[0]: tensor(6.)    x[0][1] = 8xOut[1]: tensor([[1., 8., 3.],        [4., 5., 6.]])每一个张量tensor都有一个相应的torch.Storage用来保存其数据。类tensor提供了一个存储的多维的、横向视图，并且定义了在数值运算会改变tensor的函数操作会用一个下划线后缀来标示。比如，torch.FloatTensor.abs_()会在原地计算绝对值，并返回改变后的tensor，而tensor.FloatTensor.abs()将会在一个新的tensor中计算结果关键属性和方法Tensor.new_tensorReturns a new Tensor with data as the tensor data.Tensor.new_fullReturns a Tensor of size size filled with fill_value.Tensor.new_emptyReturns a Tensor of size size filled with uninitialized data.Tensor.new_onesReturns a Tensor of size size filled with 1.Tensor.new_zerosReturns a Tensor of size size filled with 0.Tensor.is_cudaIs True if the Tensor is stored on the GPU, False otherwise.Tensor.is_quantizedIs True if the Tensor is quantized, False otherwise.Tensor.is_metaIs True if the Tensor is a meta tensor, False otherwise.Tensor.deviceIs the torch.device where this Tensor is.Tensor.gradThis attribute is None by default and becomes a Tensor the first time a call to backward() computes gradients for self.Tensor.ndimAlias for dim()Tensor.realReturns a new tensor containing real values of the self tensor for a complex-valued input tensor.Tensor.imagReturns a new tensor containing imaginary values of the self tensor.Tensor.absSee torch.abs()Tensor.abs_In-place version of abs()Tensor.absoluteAlias for abs()Tensor.absolute_In-place version of absolute() Alias for abs_()Tensor.acosSee torch.acos()Tensor.acos_In-place version of acos()Tensor.arccosSee torch.arccos()Tensor.arccos_In-place version of arccos()Tensor.addAdd a scalar or tensor to self tensor.Tensor.add_In-place version of add()Tensor.addbmmSee torch.addbmm()Tensor.addbmm_In-place version of addbmm()Tensor.addcdivSee torch.addcdiv()Tensor.addcdiv_In-place version of addcdiv()Tensor.addcmulSee torch.addcmul()Tensor.addcmul_In-place version of addcmul()Tensor.addmmSee torch.addmm()Tensor.addmm_In-place version of addmm()Tensor.sspaddmmSee torch.sspaddmm()Tensor.addmvSee torch.addmv()Tensor.addmv_In-place version of addmv()Tensor.addrSee torch.addr()Tensor.addr_In-place version of addr()Tensor.adjointAlias for adjoint()Tensor.allcloseSee torch.allclose()Tensor.amaxSee torch.amax()Tensor.aminSee torch.amin()Tensor.aminmaxSee torch.aminmax()Tensor.angleSee torch.angle()Tensor.apply_Applies the function callable to each element in the tensor, replacing each element with the value returned by callable.Tensor.argmaxSee torch.argmax()Tensor.argminSee torch.argmin()Tensor.argsortSee torch.argsort()Tensor.argwhereSee torch.argwhere()Tensor.asinSee torch.asin()Tensor.asin_In-place version of asin()Tensor.arcsinSee torch.arcsin()Tensor.arcsin_In-place version of arcsin()Tensor.as_stridedSee torch.as_strided()Tensor.atanSee torch.atan()Tensor.atan_In-place version of atan()Tensor.arctanSee torch.arctan()Tensor.arctan_In-place version of arctan()Tensor.atan2See torch.atan2()Tensor.atan2_In-place version of atan2()Tensor.arctan2See torch.arctan2()Tensor.arctan2_atan2_(other) -&gt; TensorTensor.allSee torch.all()Tensor.anySee torch.any()Tensor.backwardComputes the gradient of current tensor w.r.t.Tensor.baddbmmSee torch.baddbmm()Tensor.baddbmm_In-place version of baddbmm()Tensor.bernoulliReturns a result tensor where each \texttt{result[i]}result[i] is independently sampled from \text{Bernoulli}(\texttt{self[i]})Bernoulli(self[i]).Tensor.bernoulli_Fills each location of self with an independent sample from \text{Bernoulli}(\texttt{p})Bernoulli(p).Tensor.bfloat16self.bfloat16() is equivalent to self.to(torch.bfloat16).Tensor.bincountSee torch.bincount()Tensor.bitwise_notSee torch.bitwise_not()Tensor.bitwise_not_In-place version of bitwise_not()Tensor.bitwise_andSee torch.bitwise_and()Tensor.bitwise_and_In-place version of bitwise_and()Tensor.bitwise_orSee torch.bitwise_or()Tensor.bitwise_or_In-place version of bitwise_or()Tensor.bitwise_xorSee torch.bitwise_xor()Tensor.bitwise_xor_In-place version of bitwise_xor()Tensor.bitwise_left_shiftSee torch.bitwise_left_shift()Tensor.bitwise_left_shift_In-place version of bitwise_left_shift()Tensor.bitwise_right_shiftSee torch.bitwise_right_shift()Tensor.bitwise_right_shift_In-place version of bitwise_right_shift()Tensor.bmmSee torch.bmm()Tensor.boolself.bool() is equivalent to self.to(torch.bool).Tensor.byteself.byte() is equivalent to self.to(torch.uint8).Tensor.broadcast_toSee torch.broadcast_to().Tensor.cauchy_Fills the tensor with numbers drawn from the Cauchy distribution:Tensor.ceilSee torch.ceil()Tensor.ceil_In-place version of ceil()Tensor.charself.char() is equivalent to self.to(torch.int8).Tensor.choleskySee torch.cholesky()Tensor.cholesky_inverseSee torch.cholesky_inverse()Tensor.cholesky_solveSee torch.cholesky_solve()Tensor.chunkSee torch.chunk()Tensor.clampSee torch.clamp()Tensor.clamp_In-place version of clamp()Tensor.clipAlias for clamp().Tensor.clip_Alias for clamp_().Tensor.cloneSee torch.clone()Tensor.contiguousReturns a contiguous in memory tensor containing the same data as self tensor.Tensor.copy_Copies the elements from src into self tensor and returns self.Tensor.conjSee torch.conj()Tensor.conj_physicalSee torch.conj_physical()Tensor.conj_physical_In-place version of conj_physical()Tensor.resolve_conjSee torch.resolve_conj()Tensor.resolve_negSee torch.resolve_neg()Tensor.copysignSee torch.copysign()Tensor.copysign_In-place version of copysign()Tensor.cosSee torch.cos()Tensor.cos_In-place version of cos()Tensor.coshSee torch.cosh()Tensor.cosh_In-place version of cosh()Tensor.corrcoefSee torch.corrcoef()Tensor.count_nonzeroSee torch.count_nonzero()Tensor.covSee torch.cov()Tensor.acoshSee torch.acosh()Tensor.acosh_In-place version of acosh()Tensor.arccoshacosh() -&gt; TensorTensor.arccosh_acosh_() -&gt; TensorTensor.cpuReturns a copy of this object in CPU memory.Tensor.crossSee torch.cross()Tensor.cudaReturns a copy of this object in CUDA memory.Tensor.logcumsumexpSee torch.logcumsumexp()Tensor.cummaxSee torch.cummax()Tensor.cumminSee torch.cummin()Tensor.cumprodSee torch.cumprod()Tensor.cumprod_In-place version of cumprod()Tensor.cumsumSee torch.cumsum()Tensor.cumsum_In-place version of cumsum()Tensor.chalfself.chalf() is equivalent to self.to(torch.complex32).Tensor.cfloatself.cfloat() is equivalent to self.to(torch.complex64).Tensor.cdoubleself.cdouble() is equivalent to self.to(torch.complex128).Tensor.data_ptrReturns the address of the first element of self tensor.Tensor.deg2radSee torch.deg2rad()Tensor.dequantizeGiven a quantized Tensor, dequantize it and return the dequantized float Tensor.Tensor.detSee torch.det()Tensor.dense_dimReturn the number of dense dimensions in a sparse tensor self.Tensor.detachReturns a new Tensor, detached from the current graph.Tensor.detach_Detaches the Tensor from the graph that created it, making it a leaf.Tensor.diagSee torch.diag()Tensor.diag_embedSee torch.diag_embed()Tensor.diagflatSee torch.diagflat()Tensor.diagonalSee torch.diagonal()Tensor.diagonal_scatterSee torch.diagonal_scatter()Tensor.fill_diagonal_Fill the main diagonal of a tensor that has at least 2-dimensions.Tensor.fmaxSee torch.fmax()Tensor.fminSee torch.fmin()Tensor.diffSee torch.diff()Tensor.digammaSee torch.digamma()Tensor.digamma_In-place version of digamma()Tensor.dimReturns the number of dimensions of self tensor.Tensor.distSee torch.dist()Tensor.divSee torch.div()Tensor.div_In-place version of div()Tensor.divideSee torch.divide()Tensor.divide_In-place version of divide()Tensor.dotSee torch.dot()Tensor.doubleself.double() is equivalent to self.to(torch.float64).Tensor.dsplitSee torch.dsplit()Tensor.element_sizeReturns the size in bytes of an individual element.Tensor.eqSee torch.eq()Tensor.eq_In-place version of eq()Tensor.equalSee torch.equal()Tensor.erfSee torch.erf()Tensor.erf_In-place version of erf()Tensor.erfcSee torch.erfc()Tensor.erfc_In-place version of erfc()Tensor.erfinvSee torch.erfinv()Tensor.erfinv_In-place version of erfinv()Tensor.expSee torch.exp()Tensor.exp_In-place version of exp()Tensor.expm1See torch.expm1()Tensor.expm1_In-place version of expm1()Tensor.expandReturns a new view of the self tensor with singleton dimensions expanded to a larger size.Tensor.expand_asExpand this tensor to the same size as other.Tensor.exponential_Fills self tensor with elements drawn from the exponential distribution:Tensor.fixSee torch.fix().Tensor.fix_In-place version of fix()Tensor.fill_Fills self tensor with the specified value.Tensor.flattenSee torch.flatten()Tensor.flipSee torch.flip()Tensor.fliplrSee torch.fliplr()Tensor.flipudSee torch.flipud()Tensor.floatself.float() is equivalent to self.to(torch.float32).Tensor.float_powerSee torch.float_power()Tensor.float_power_In-place version of float_power()Tensor.floorSee torch.floor()Tensor.floor_In-place version of floor()Tensor.floor_divideSee torch.floor_divide()Tensor.floor_divide_In-place version of floor_divide()Tensor.fmodSee torch.fmod()Tensor.fmod_In-place version of fmod()Tensor.fracSee torch.frac()Tensor.frac_In-place version of frac()Tensor.frexpSee torch.frexp()Tensor.gatherSee torch.gather()Tensor.gcdSee torch.gcd()Tensor.gcd_In-place version of gcd()Tensor.geSee torch.ge().Tensor.ge_In-place version of ge().Tensor.greater_equalSee torch.greater_equal().Tensor.greater_equal_In-place version of greater_equal().Tensor.geometric_Fills self tensor with elements drawn from the geometric distribution:Tensor.geqrfSee torch.geqrf()Tensor.gerSee torch.ger()Tensor.get_deviceFor CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.Tensor.gtSee torch.gt().Tensor.gt_In-place version of gt().Tensor.greaterSee torch.greater().Tensor.greater_In-place version of greater().Tensor.halfself.half() is equivalent to self.to(torch.float16).Tensor.hardshrinkSee torch.nn.functional.hardshrink()Tensor.heavisideSee torch.heaviside()Tensor.histcSee torch.histc()Tensor.histogramSee torch.histogram()Tensor.hsplitSee torch.hsplit()Tensor.hypotSee torch.hypot()Tensor.hypot_In-place version of hypot()Tensor.i0See torch.i0()Tensor.i0_In-place version of i0()Tensor.igammaSee torch.igamma()Tensor.igamma_In-place version of igamma()Tensor.igammacSee torch.igammac()Tensor.igammac_In-place version of igammac()Tensor.index_add_Accumulate the elements of alpha times source into the self tensor by adding to the indices in the order given in index.Tensor.index_addOut-of-place version of torch.Tensor.index_add_().Tensor.index_copy_Copies the elements of tensor into the self tensor by selecting the indices in the order given in index.Tensor.index_copyOut-of-place version of torch.Tensor.index_copy_().Tensor.index_fill_Fills the elements of the self tensor with value value by selecting the indices in the order given in index.Tensor.index_fillOut-of-place version of torch.Tensor.index_fill_().Tensor.index_put_Puts values from the tensor values into the tensor self using the indices specified in indices (which is a tuple of Tensors).Tensor.index_putOut-place version of index_put_().Tensor.index_reduce_Accumulate the elements of source into the self tensor by accumulating to the indices in the order given in index using the reduction given by the reduce argument.Tensor.index_reduceTensor.index_selectSee torch.index_select()Tensor.indicesReturn the indices tensor of a sparse COO tensor.Tensor.innerSee torch.inner().Tensor.intself.int() is equivalent to self.to(torch.int32).Tensor.int_reprGiven a quantized Tensor, self.int_repr() returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.Tensor.inverseSee torch.inverse()Tensor.iscloseSee torch.isclose()Tensor.isfiniteSee torch.isfinite()Tensor.isinfSee torch.isinf()Tensor.isposinfSee torch.isposinf()Tensor.isneginfSee torch.isneginf()Tensor.isnanSee torch.isnan()Tensor.is_contiguousReturns True if self tensor is contiguous in memory in the order specified by memory format.Tensor.is_complexReturns True if the data type of self is a complex data type.Tensor.is_conjReturns True if the conjugate bit of self is set to true.Tensor.is_floating_pointReturns True if the data type of self is a floating point data type.Tensor.is_inferenceSee torch.is_inference()Tensor.is_leafAll Tensors that have requires_grad which is False will be leaf Tensors by convention.Tensor.is_pinnedReturns true if this tensor resides in pinned memory.Tensor.is_set_toReturns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).Tensor.is_sharedChecks if tensor is in shared memory.Tensor.is_signedReturns True if the data type of self is a signed data type.Tensor.is_sparseIs True if the Tensor uses sparse storage layout, False otherwise.Tensor.istftSee torch.istft()Tensor.isrealSee torch.isreal()Tensor.itemReturns the value of this tensor as a standard Python number.Tensor.kthvalueSee torch.kthvalue()Tensor.lcmSee torch.lcm()Tensor.lcm_In-place version of lcm()Tensor.ldexpSee torch.ldexp()Tensor.ldexp_In-place version of ldexp()Tensor.leSee torch.le().Tensor.le_In-place version of le().Tensor.less_equalSee torch.less_equal().Tensor.less_equal_In-place version of less_equal().Tensor.lerpSee torch.lerp()Tensor.lerp_In-place version of lerp()Tensor.lgammaSee torch.lgamma()Tensor.lgamma_In-place version of lgamma()Tensor.logSee torch.log()Tensor.log_In-place version of log()Tensor.logdetSee torch.logdet()Tensor.log10See torch.log10()Tensor.log10_In-place version of log10()Tensor.log1pSee torch.log1p()Tensor.log1p_In-place version of log1p()Tensor.log2See torch.log2()Tensor.log2_In-place version of log2()Tensor.log_normal_Fills self tensor with numbers samples from the log-normal distribution parameterized by the given mean \muμ and standard deviation \sigmaσ.Tensor.logaddexpSee torch.logaddexp()Tensor.logaddexp2See torch.logaddexp2()Tensor.logsumexpSee torch.logsumexp()Tensor.logical_andSee torch.logical_and()Tensor.logical_and_In-place version of logical_and()Tensor.logical_notSee torch.logical_not()Tensor.logical_not_In-place version of logical_not()Tensor.logical_orSee torch.logical_or()Tensor.logical_or_In-place version of logical_or()Tensor.logical_xorSee torch.logical_xor()Tensor.logical_xor_In-place version of logical_xor()Tensor.logitSee torch.logit()Tensor.logit_In-place version of logit()Tensor.longself.long() is equivalent to self.to(torch.int64).Tensor.ltSee torch.lt().Tensor.lt_In-place version of lt().Tensor.lesslt(other) -&gt; TensorTensor.less_In-place version of less().Tensor.luSee torch.lu()Tensor.lu_solveSee torch.lu_solve()Tensor.as_subclassMakes a cls instance with the same data pointer as self.Tensor.map_Applies callable for each element in self tensor and the given tensor and stores the results in self tensor.Tensor.masked_scatter_Copies elements from source into self tensor at positions where the mask is True.Tensor.masked_scatterOut-of-place version of torch.Tensor.masked_scatter_()Tensor.masked_fill_Fills elements of self tensor with value where mask is True.Tensor.masked_fillOut-of-place version of torch.Tensor.masked_fill_()Tensor.masked_selectSee torch.masked_select()Tensor.matmulSee torch.matmul()Tensor.matrix_powerNOTEmatrix_power() is deprecated, use torch.linalg.matrix_power() instead.Tensor.matrix_expSee torch.matrix_exp()Tensor.maxSee torch.max()Tensor.maximumSee torch.maximum()Tensor.meanSee torch.mean()Tensor.nanmeanSee torch.nanmean()Tensor.medianSee torch.median()Tensor.nanmedianSee torch.nanmedian()Tensor.minSee torch.min()Tensor.minimumSee torch.minimum()Tensor.mmSee torch.mm()Tensor.smmSee torch.smm()Tensor.modeSee torch.mode()Tensor.movedimSee torch.movedim()Tensor.moveaxisSee torch.moveaxis()Tensor.msortSee torch.msort()Tensor.mulSee torch.mul().Tensor.mul_In-place version of mul().Tensor.multiplySee torch.multiply().Tensor.multiply_In-place version of multiply().Tensor.multinomialSee torch.multinomial()Tensor.mvSee torch.mv()Tensor.mvlgammaSee torch.mvlgamma()Tensor.mvlgamma_In-place version of mvlgamma()Tensor.nansumSee torch.nansum()Tensor.narrowSee torch.narrow()Tensor.narrow_copySee torch.narrow_copy().Tensor.ndimensionAlias for dim()Tensor.nan_to_numSee torch.nan_to_num().Tensor.nan_to_num_In-place version of nan_to_num().Tensor.neSee torch.ne().Tensor.ne_In-place version of ne().Tensor.not_equalSee torch.not_equal().Tensor.not_equal_In-place version of not_equal().Tensor.negSee torch.neg()Tensor.neg_In-place version of neg()Tensor.negativeSee torch.negative()Tensor.negative_In-place version of negative()Tensor.nelementAlias for numel()Tensor.nextafterSee torch.nextafter()Tensor.nextafter_In-place version of nextafter()Tensor.nonzeroSee torch.nonzero()Tensor.normSee torch.norm()Tensor.normal_Fills self tensor with elements samples from the normal distribution parameterized by mean and std.Tensor.numelSee torch.numel()Tensor.numpyReturns the tensor as a NumPy ndarray.Tensor.orgqrSee torch.orgqr()Tensor.ormqrSee torch.ormqr()Tensor.outerSee torch.outer().Tensor.permuteSee torch.permute()Tensor.pin_memoryCopies the tensor to pinned memory, if it’s not already pinned.Tensor.pinverseSee torch.pinverse()Tensor.polygammaSee torch.polygamma()Tensor.polygamma_In-place version of polygamma()Tensor.positiveSee torch.positive()Tensor.powSee torch.pow()Tensor.pow_In-place version of pow()Tensor.prodSee torch.prod()Tensor.put_Copies the elements from source into the positions specified by index.Tensor.qrSee torch.qr()Tensor.qschemeReturns the quantization scheme of a given QTensor.Tensor.quantileSee torch.quantile()Tensor.nanquantileSee torch.nanquantile()Tensor.q_scaleGiven a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().Tensor.q_zero_pointGiven a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().Tensor.q_per_channel_scalesGiven a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.Tensor.q_per_channel_zero_pointsGiven a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.Tensor.q_per_channel_axisGiven a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.Tensor.rad2degSee torch.rad2deg()Tensor.random_Fills self tensor with numbers sampled from the discrete uniform distribution over [from, to - 1].Tensor.ravelsee torch.ravel()Tensor.reciprocalSee torch.reciprocal()Tensor.reciprocal_In-place version of reciprocal()Tensor.record_streamEnsures that the tensor memory is not reused for another tensor until all current work queued on stream are complete.Tensor.register_hookRegisters a backward hook.Tensor.remainderSee torch.remainder()Tensor.remainder_In-place version of remainder()Tensor.renormSee torch.renorm()Tensor.renorm_In-place version of renorm()Tensor.repeatRepeats this tensor along the specified dimensions.Tensor.repeat_interleaveSee torch.repeat_interleave().Tensor.requires_gradIs True if gradients need to be computed for this Tensor, False otherwise.Tensor.requires_grad_Change if autograd should record operations on this tensor: sets this tensor’s requires_grad attribute in-place.Tensor.reshapeReturns a tensor with the same data and number of elements as self but with the specified shape.Tensor.reshape_asReturns this tensor as the same shape as other.Tensor.resize_Resizes self tensor to the specified size.Tensor.resize_as_Resizes the self tensor to be the same size as the specified tensor.Tensor.retain_gradEnables this Tensor to have their grad populated during backward().Tensor.retains_gradIs True if this Tensor is non-leaf and its grad is enabled to be populated during backward(), False otherwise.Tensor.rollSee torch.roll()Tensor.rot90See torch.rot90()Tensor.roundSee torch.round()Tensor.round_In-place version of round()Tensor.rsqrtSee torch.rsqrt()Tensor.rsqrt_In-place version of rsqrt()Tensor.scatterOut-of-place version of torch.Tensor.scatter_()Tensor.scatter_Writes all values from the tensor src into self at the indices specified in the index tensor.Tensor.scatter_add_Adds all values from the tensor src into self at the indices specified in the index tensor in a similar fashion as scatter_().Tensor.scatter_addOut-of-place version of torch.Tensor.scatter_add_()Tensor.scatter_reduce_Reduces all values from the src tensor to the indices specified in the index tensor in the self tensor using the applied reduction defined via the reduce argument (&quot;sum&quot;, &quot;prod&quot;, &quot;mean&quot;, &quot;amax&quot;, &quot;amin&quot;).Tensor.scatter_reduceOut-of-place version of torch.Tensor.scatter_reduce_()Tensor.selectSee torch.select()Tensor.select_scatterSee torch.select_scatter()Tensor.set_Sets the underlying storage, size, and strides.Tensor.share_memory_Moves the underlying storage to shared memory.Tensor.shortself.short() is equivalent to self.to(torch.int16).Tensor.sigmoidSee torch.sigmoid()Tensor.sigmoid_In-place version of sigmoid()Tensor.signSee torch.sign()Tensor.sign_In-place version of sign()Tensor.signbitSee torch.signbit()Tensor.sgnSee torch.sgn()Tensor.sgn_In-place version of sgn()Tensor.sinSee torch.sin()Tensor.sin_In-place version of sin()Tensor.sincSee torch.sinc()Tensor.sinc_In-place version of sinc()Tensor.sinhSee torch.sinh()Tensor.sinh_In-place version of sinh()Tensor.asinhSee torch.asinh()Tensor.asinh_In-place version of asinh()Tensor.arcsinhSee torch.arcsinh()Tensor.arcsinh_In-place version of arcsinh()Tensor.sizeReturns the size of the self tensor.Tensor.slogdetSee torch.slogdet()Tensor.slice_scatterSee torch.slice_scatter()Tensor.sortSee torch.sort()Tensor.splitSee torch.split()Tensor.sparse_maskReturns a new sparse tensor with values from a strided tensor self filtered by the indices of the sparse tensor mask.Tensor.sparse_dimReturn the number of sparse dimensions in a sparse tensor self.Tensor.sqrtSee torch.sqrt()Tensor.sqrt_In-place version of sqrt()Tensor.squareSee torch.square()Tensor.square_In-place version of square()Tensor.squeezeSee torch.squeeze()Tensor.squeeze_In-place version of squeeze()Tensor.stdSee torch.std()Tensor.stftSee torch.stft()Tensor.storageReturns the underlying storage.Tensor.storage_offsetReturns self tensor’s offset in the underlying storage in terms of number of storage elements (not bytes).Tensor.storage_typeReturns the type of the underlying storage.Tensor.strideReturns the stride of self tensor.Tensor.subSee torch.sub().Tensor.sub_In-place version of sub()Tensor.subtractSee torch.subtract().Tensor.subtract_In-place version of subtract().Tensor.sumSee torch.sum()Tensor.sum_to_sizeSum this tensor to size.Tensor.svdSee torch.svd()Tensor.swapaxesSee torch.swapaxes()Tensor.swapdimsSee torch.swapdims()Tensor.symeigSee torch.symeig()Tensor.tSee torch.t()Tensor.t_In-place version of t()Tensor.tensor_splitSee torch.tensor_split()Tensor.tileSee torch.tile()Tensor.toPerforms Tensor dtype and/or device conversion.Tensor.to_mkldnnReturns a copy of the tensor in torch.mkldnn layout.Tensor.takeSee torch.take()Tensor.take_along_dimSee torch.take_along_dim()Tensor.tanSee torch.tan()Tensor.tan_In-place version of tan()Tensor.tanhSee torch.tanh()Tensor.tanh_In-place version of tanh()Tensor.atanhSee torch.atanh()Tensor.atanh_In-place version of atanh()Tensor.arctanhSee torch.arctanh()Tensor.arctanh_In-place version of arctanh()Tensor.tolistReturns the tensor as a (nested) list.Tensor.topkSee torch.topk()Tensor.to_denseCreates a strided copy of self if self is not a strided tensor, otherwise returns self.Tensor.to_sparseReturns a sparse copy of the tensor.Tensor.to_sparse_csrConvert a tensor to compressed row storage format (CSR).Tensor.to_sparse_cscConvert a tensor to compressed column storage (CSC) format.Tensor.to_sparse_bsrConvert a CSR tensor to a block sparse row (BSR) storage format of given blocksize.Tensor.to_sparse_bscConvert a CSR tensor to a block sparse column (BSC) storage format of given blocksize.Tensor.traceSee torch.trace()Tensor.transposeSee torch.transpose()Tensor.transpose_In-place version of transpose()Tensor.triangular_solveSee torch.triangular_solve()Tensor.trilSee torch.tril()Tensor.tril_In-place version of tril()Tensor.triuSee torch.triu()Tensor.triu_In-place version of triu()Tensor.true_divideSee torch.true_divide()Tensor.true_divide_In-place version of true_divide_()Tensor.truncSee torch.trunc()Tensor.trunc_In-place version of trunc()Tensor.typeReturns the type if dtype is not provided, else casts this object to the specified type.Tensor.type_asReturns this tensor cast to the type of the given tensor.Tensor.unbindSee torch.unbind()Tensor.unflattenSee torch.unflatten().Tensor.unfoldReturns a view of the original tensor which contains all slices of size size from self tensor in the dimension dimension.Tensor.uniform_Fills self tensor with numbers sampled from the continuous uniform distribution:Tensor.uniqueReturns the unique elements of the input tensor.Tensor.unique_consecutiveEliminates all but the first element from every consecutive group of equivalent elements.Tensor.unsqueezeSee torch.unsqueeze()Tensor.unsqueeze_In-place version of unsqueeze()Tensor.valuesReturn the values tensor of a sparse COO tensor.Tensor.varSee torch.var()Tensor.vdotSee torch.vdot()Tensor.viewReturns a new tensor with the same data as the self tensor but of a different shape.Tensor.view_asView this tensor as the same size as other.Tensor.vsplitSee torch.vsplit()Tensor.whereself.where(condition, y) is equivalent to torch.where(condition, self, y).Tensor.xlogySee torch.xlogy()Tensor.xlogy_In-place version of xlogy()Tensor.zero_Fills self tensor with zeros.storagetensor的数据结构、storage()、stride()、storage_offset()pytorch中一个tensor对象分为头信息区（Tensor）和存储区（Storage）两部分头信息区主要保存tensor的形状（size）、步长（stride）、数据类型（type）等信息；而真正的data（数据)则以连续一维数组的形式放在存储区，由torch.Storage实例管理着注意：storage永远是一维数组，任何维度的tensor的实际数据都存储在一维的storage中获取tensor的storage123456789101112131415a = torch.tensor([[1.0, 4.0],[2.0, 1.0],[3.0, 5.0]])a.storage()Out[0]:  1.0 4.0 2.0 1.0 3.0 5.0[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]a.storage()[2] = 9id(a.storage())Out[1]: 1343354913168实例图片分类小土堆+李沐课程笔记PyTorch深度学习快速入门教程（绝对通俗易懂！）【小土堆】Pytorch加载数据Pytorch中加载数据需要Dataset、Dataloader。Dataset提供一种方式去获取每个数据及其对应的label，告诉我们总共有多少个数据。Dataloader为后面的网络提供不同的数据形式，它将一批一批数据进行一个打包。Tensorboard1234567891011121314151617import torchvisionfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriter# 准备的测试数据集test_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor())               # batch_size=4 使得 img0, target0 = dataset[0]、img1, target1 = dataset[1]、img2, target2 = dataset[2]、img3, target3 = dataset[3]，然后这四个数据作为Dataloader的一个返回      test_loader = DataLoader(dataset=test_data,batch_size=64,shuffle=True,num_workers=0,drop_last=False)      # 用for循环取出DataLoader打包好的四个数据writer = SummaryWriter(&quot;logs&quot;)step = 0for data in test_loader:    imgs, targets = data # 每个data都是由4张图片组成，imgs.size 为 [4,3,32,32]，四张32×32图片三通道，targets由四个标签组成                 writer.add_images(&quot;test_data&quot;,imgs,step)    step = step + 1    writer.close()Transforms① Transforms当成工具箱的话，里面的class就是不同的工具。例如像totensor、resize这些工具。② Transforms拿一些特定格式的图片，经过Transforms里面的工具，获得我们想要的结果。123456789from torchvision import transformsfrom PIL import Imageimg_path = &quot;Data/FirstTypeData/val/bees/10870992_eebeeb3a12.jpg&quot;img = Image.open(img_path)  tensor_trans = transforms.ToTensor()  # 创建 transforms.ToTensor类 的实例化对象tensor_img = tensor_trans(img)  # 调用 transforms.ToTensor类 的__call__的魔术方法   print(tensor_img)torchvision数据集① torchvision中有很多数据集，当我们写代码时指定相应的数据集指定一些参数，它就可以自行下载。② CIFAR-10数据集包含60000张32×32的彩色图片，一共10个类别，其中50000张训练图片，10000张测试图片。12345678910111213import torchvisiontrain_set = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;,train=True,download=True) # root为存放数据集的相对路线test_set = torchvision.datasets.CIFAR10(root=&quot;./dataset&quot;,train=False,download=True) # train=True是训练集，train=False是测试集  print(test_set[0])       # 输出的3是target print(test_set.classes)  # 测试数据集中有多少种img, target = test_set[0] # 分别获得图片、targetprint(img)print(target)print(test_set.classes[target]) # 3号target对应的种类img.show()损失函数① Loss损失函数一方面计算实际输出和目标之间的差距。② Loss损失函数另一方面为我们更新输出提供一定的依据L1loss损失函数123456789import torchfrom torch.nn import L1Lossinputs = torch.tensor([1,2,3],dtype=torch.float32)targets = torch.tensor([1,2,5],dtype=torch.float32)inputs = torch.reshape(inputs,(1,1,1,3))targets = torch.reshape(targets,(1,1,1,3))loss = L1Loss()  # 默认为 maenresult = loss(inputs,targets)print(result)MSE损失函数12345678910import torchfrom torch.nn import L1Lossfrom torch import nninputs = torch.tensor([1,2,3],dtype=torch.float32)targets = torch.tensor([1,2,5],dtype=torch.float32)inputs = torch.reshape(inputs,(1,1,1,3))targets = torch.reshape(targets,(1,1,1,3))loss_mse = nn.MSELoss()result_mse = loss_mse(inputs,targets)print(result_mse)交叉熵损失函数12345678910import torchfrom torch.nn import L1Lossfrom torch import nnx = torch.tensor([0.1,0.2,0.3])y = torch.tensor([1])x = torch.reshape(x,(1,3)) # 1的 batch_size，有三类loss_cross = nn.CrossEntropyLoss()result_cross = loss_cross(x,y)print(result_cross)优化器① 损失函数调用backward方法，就可以调用损失函数的反向传播方法，就可以求出我们需要调节的梯度，我们就可以利用我们的优化器就可以根据梯度对参数进行调整，达到整体误差降低的目的。② 梯度要清零，如果梯度不清零会导致梯度累加1234567891011loss = nn.CrossEntropyLoss() # 交叉熵    tudui = Tudui()optim = torch.optim.SGD(tudui.parameters(),lr=0.01)   # 随机梯度下降优化器for data in dataloader:    imgs, targets = data    outputs = tudui(imgs)    result_loss = loss(outputs, targets) # 计算实际输出与目标输出的差距    optim.zero_grad()  # 梯度清零    result_loss.backward() # 反向传播，计算损失函数的梯度    optim.step()   # 根据梯度，对网络的参数进行调优    print(result_loss) # 对数据只看了一遍，只看了一轮，所以loss下降不大神经网络学习率优化123456789101112131415161718192021222324252627282930313233343536373839404142434445import torchimport torchvisionfrom torch import nn from torch.nn import Conv2d, MaxPool2d, Flatten, Linear, Sequentialfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriterdataset = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),download=True)       dataloader = DataLoader(dataset, batch_size=64,drop_last=True)class Tudui(nn.Module):    def __init__(self):        super(Tudui, self).__init__()                self.model1 = Sequential(            Conv2d(3,32,5,padding=2),            MaxPool2d(2),            Conv2d(32,32,5,padding=2),            MaxPool2d(2),            Conv2d(32,64,5,padding=2),            MaxPool2d(2),            Flatten(),            Linear(1024,64),            Linear(64,10)        )            def forward(self, x):        x = self.model1(x)        return x    loss = nn.CrossEntropyLoss() # 交叉熵    tudui = Tudui()optim = torch.optim.SGD(tudui.parameters(),lr=0.01)   # 随机梯度下降优化器scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=5, gamma=0.1) # 每过 step_size 更新一次优化器，更新是学习率为原来的学习率的 0.1 倍    for epoch in range(20):    running_loss = 0.0    for data in dataloader:        imgs, targets = data        outputs = tudui(imgs)        result_loss = loss(outputs, targets) # 计算实际输出与目标输出的差距        optim.zero_grad()  # 梯度清零        result_loss.backward() # 反向传播，计算损失函数的梯度        optim.step()   # 根据梯度，对网络的参数进行调优        scheduler.step() # 学习率太小了，所以20个轮次后，相当于没走多少        running_loss = running_loss + result_loss    print(running_loss) # 对这一轮所有误差的总和网络模型使用及修改网络模型添加12345678import torchvisionfrom torch import nndataset = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=True,transform=torchvision.transforms.ToTensor(),download=True)       vgg16_true = torchvision.models.vgg16(pretrained=True) # 下载卷积层对应的参数是多少、池化层对应的参数时多少，这些参数时ImageNet训练好了的vgg16_true.add_module('add_linear',nn.Linear(1000,10)) # 在VGG16后面添加一个线性层，使得输出为适应CIFAR10的输出，CIFAR10需要输出10个种类print(vgg16_true)网络模型修改1234567import torchvisionfrom torch import nnvgg16_false = torchvision.models.vgg16(pretrained=False) # 没有预训练的参数     print(vgg16_false)vgg16_false.classifier[6] = nn.Linear(4096,10)print(vgg16_false)网络模型保存与读取模型结构 + 模型参数12345678import torchvisionimport torchvgg16 = torchvision.models.vgg16(pretrained=False)torch.save(vgg16,&quot;./model/vgg16_method1.pth&quot;) # 保存方式一：模型结构 + 模型参数      print(vgg16)model = torch.load(&quot;./model/vgg16_method1.pth&quot;) # 保存方式一对应的加载模型    print(model)模型参数（官方推荐），不保存网络模型结构12345678import torchvisionimport torchvgg16 = torchvision.models.vgg16(pretrained=False)torch.save(vgg16.state_dict(),&quot;./model/vgg16_method2.pth&quot;) # 保存方式二：模型参数（官方推荐）,不再保存网络模型结构  print(vgg16)model = torch.load(&quot;./model/vgg16_method2.pth&quot;) # 导入模型参数   print(model)固定模型参数在训练过程中可能需要固定一部分模型的参数，只更新另一部分参数，有两种思路实现这个目标一个是设置不要更新参数的网络层为false另一个就是在定义优化器时只传入要更新的参数当然最优的做法是，优化器中只传入requires_grad=True的参数，这样占用的内存会更小一点，效率也会更高12345678910111213141516171819202122232425262728293031323334353637383940414243import torchimport torch.nn as nnimport torch.optim as optim# 定义一个简单的网络class net(nn.Module):    def __init__(self, num_class=3):        super(net, self).__init__()        self.fc1 = nn.Linear(8, 4)        self.fc2 = nn.Linear(4, num_class)    def forward(self, x):        return self.fc2(self.fc1(x))model = net()# 冻结fc1层的参数for name, param in model.named_parameters():    if &quot;fc1&quot; in name:        param.requires_grad = Falseloss_fn = nn.CrossEntropyLoss()# 只传入requires_grad = True的参数optimizer = optim.SGD(filter(lambda p: p.requires_grad, net.parameters(), lr=1e-2)print(&quot;model.fc1.weight&quot;, model.fc1.weight)print(&quot;model.fc2.weight&quot;, model.fc2.weight)model.train()for epoch in range(10):    x = torch.randn((3, 8))    label = torch.randint(0, 3, [3]).long()    output = model(x)    loss = loss_fn(output, label)    optimizer.zero_grad()    loss.backward()    optimizer.step()print(&quot;model.fc1.weight&quot;, model.fc1.weight)print(&quot;model.fc2.weight&quot;, model.fc2.weight)训练流程DataLoader加载数据集123456789101112131415161718import torchvisionfrom torch import nnfrom torch.utils.data import DataLoader# 准备数据集train_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=True,transform=torchvision.transforms.ToTensor(),download=True)       test_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),download=True)       # length 长度train_data_size = len(train_data)test_data_size = len(test_data)# 如果train_data_size=10，则打印：训练数据集的长度为：10print(&quot;训练数据集的长度：{}&quot;.format(train_data_size))print(&quot;测试数据集的长度：{}&quot;.format(test_data_size))# 利用 Dataloader 来加载数据集train_dataloader = DataLoader(train_data_size, batch_size=64)        test_dataloader = DataLoader(test_data_size, batch_size=64)测试网络正确12345678910111213141516171819202122232425262728import torchfrom torch import nn# 搭建神经网络class Tudui(nn.Module):    def __init__(self):        super(Tudui, self).__init__()                self.model1 = nn.Sequential(            nn.Conv2d(3,32,5,1,2),  # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2                nn.MaxPool2d(2),            nn.Conv2d(32,32,5,1,2),            nn.MaxPool2d(2),            nn.Conv2d(32,64,5,1,2),            nn.MaxPool2d(2),            nn.Flatten(),  # 展平后变成 64*4*4 了            nn.Linear(64*4*4,64),            nn.Linear(64,10)        )            def forward(self, x):        x = self.model1(x)        return x    if __name__ == '__main__':    tudui = Tudui()    input = torch.ones((64,3,32,32))    output = tudui(input)    print(output.shape)  # 测试输出的尺寸是不是我们想要的网络训练数据① model.train()和model.eval()的区别主要在于Batch Normalization和Dropout两层。② 如果模型中有BN层(Batch Normalization）和 Dropout，需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。③ 不启用 Batch Normalization 和 Dropout。 如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。④ 训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的性质。⑤ 在做one classification的时候，训练集和测试集的样本分布是不一样的，尤其需要注意这一点123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107import torchvisionimport torchfrom torch import nnfrom torch.utils.data import DataLoaderfrom torch.utils.tensorboard import SummaryWriter# from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里class Tudui(nn.Module):    def __init__(self):        super(Tudui, self).__init__()                self.model1 = nn.Sequential(            nn.Conv2d(3,32,5,1,2),  # 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2                nn.MaxPool2d(2),            nn.Conv2d(32,32,5,1,2),            nn.MaxPool2d(2),            nn.Conv2d(32,64,5,1,2),            nn.MaxPool2d(2),            nn.Flatten(),  # 展平后变成 64*4*4 了            nn.Linear(64*4*4,64),            nn.Linear(64,10)        )            def forward(self, x):        x = self.model1(x)        return x# 准备数据集train_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=True,transform=torchvision.transforms.ToTensor(),download=True)       test_data = torchvision.datasets.CIFAR10(&quot;./dataset&quot;,train=False,transform=torchvision.transforms.ToTensor(),download=True)       # length 长度train_data_size = len(train_data)test_data_size = len(test_data)# 如果train_data_size=10，则打印：训练数据集的长度为：10print(&quot;训练数据集的长度：{}&quot;.format(train_data_size))print(&quot;测试数据集的长度：{}&quot;.format(test_data_size))# 利用 Dataloader 来加载数据集train_dataloader = DataLoader(train_data, batch_size=64)        test_dataloader = DataLoader(test_data, batch_size=64)# 创建网络模型tudui = Tudui() # 损失函数loss_fn = nn.CrossEntropyLoss() # 交叉熵，fn 是 fuction 的缩写# 优化器learning = 0.01  # 1e-2 就是 0.01 的意思optimizer = torch.optim.SGD(tudui.parameters(),learning)   # 随机梯度下降优化器  # 设置网络的一些参数# 记录训练的次数total_train_step = 0# 记录测试的次数total_test_step = 0# 训练的轮次epoch = 10# 添加 tensorboardwriter = SummaryWriter(&quot;logs&quot;)for i in range(epoch):    print(&quot;-----第 {} 轮训练开始-----&quot;.format(i+1))        # 训练步骤开始    tudui.train() # 当网络中有dropout层、batchnorm层时，这些层能起作用    for data in train_dataloader:        imgs, targets = data        outputs = tudui(imgs)        loss = loss_fn(outputs, targets) # 计算实际输出与目标输出的差距                # 优化器对模型调优        optimizer.zero_grad()  # 梯度清零        loss.backward() # 反向传播，计算损失函数的梯度        optimizer.step()   # 根据梯度，对网络的参数进行调优                total_train_step = total_train_step + 1        if total_train_step % 100 == 0:            print(&quot;训练次数：{}，Loss：{}&quot;.format(total_train_step,loss.item()))  # 方式二：获得loss值            writer.add_scalar(&quot;train_loss&quot;,loss.item(),total_train_step)        # 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）    tudui.eval()  # 当网络中有dropout层、batchnorm层时，这些层不能起作用    total_test_loss = 0    total_accuracy = 0    with torch.no_grad():  # 没有梯度了        for data in test_dataloader: # 测试数据集提取数据            imgs, targets = data            outputs = tudui(imgs)            loss = loss_fn(outputs, targets) # 仅data数据在网络模型上的损失            total_test_loss = total_test_loss + loss.item() # 所有loss            accuracy = (outputs.argmax(1) == targets).sum()            total_accuracy = total_accuracy + accuracy                print(&quot;整体测试集上的Loss：{}&quot;.format(total_test_loss))    print(&quot;整体测试集上的正确率：{}&quot;.format(total_accuracy/test_data_size))    writer.add_scalar(&quot;test_loss&quot;,total_test_loss,total_test_step)    writer.add_scalar(&quot;test_accuracy&quot;,total_accuracy/test_data_size,total_test_step)      total_test_step = total_test_step + 1        torch.save(tudui, &quot;./model/tudui_{}.pth&quot;.format(i)) # 保存每一轮训练后的结果    #torch.save(tudui.state_dict(),&quot;tudui_{}.path&quot;.format(i)) # 保存方式二             print(&quot;模型已保存&quot;)    writer.close()迁移学习迁移学习 ｜ 模型查看&amp;参数查看 ｜ 预训练模型加载 ｜ 模型修改 ｜ 参数冻结模型|参数查看123456789101112131415161718192021222324252627282930313233343536import torchclass MyModel(torch.nn.Module):    def __init__(self):        super().__init__()        self.layer1 = torch.nn.Sequential(            torch.nn.Linear(3, 4),            torch.nn.Linear(4, 3),        )        self.layer2 = torch.nn.Linear(3, 6)        self.layer3 = torch.nn.Sequential(            torch.nn.Linear(6, 7),            torch.nn.Linear(7, 5),        )    def forward(self, x):        x = self.layer1(x)        x = self.layer2(x)        x = self.layer3(x)        return xnet = MyModel()print(net)MyModel(  (layer1): Sequential(    (0): Linear(in_features=3, out_features=4, bias=True)    (1): Linear(in_features=4, out_features=3, bias=True)  )  (layer2): Linear(in_features=3, out_features=6, bias=True)  (layer3): Sequential(    (0): Linear(in_features=6, out_features=7, bias=True)    (1): Linear(in_features=7, out_features=5, bias=True)  ))查看参数123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051for layer in net.modules():    print(type(layer))  # 查看每一层的类型    # print(layer)&lt;class '__main__.MyModel'&gt;&lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.container.Sequential'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;&lt;class 'torch.nn.modules.linear.Linear'&gt;for param in net.parameters():    print(param.shape)  # 打印每一层的参数torch.Size([4, 3])torch.Size([4])torch.Size([3, 4])torch.Size([3])torch.Size([6, 3])torch.Size([6])torch.Size([7, 6])torch.Size([7])torch.Size([5, 7])torch.Size([5])for name, param in net.named_parameters():    print(name, param.shape)  # 看的更细layer1.0.weight torch.Size([4, 3])layer1.0.bias torch.Size([4])layer1.1.weight torch.Size([3, 4])layer1.1.bias torch.Size([3])layer2.weight torch.Size([6, 3])layer2.bias torch.Size([6])layer3.0.weight torch.Size([7, 6])layer3.0.bias torch.Size([7])layer3.1.weight torch.Size([5, 7])layer3.1.bias torch.Size([5])for key, value in net.state_dict().items():  # 参数名以及参数    print(key, value.shape)layer1.0.weight torch.Size([4, 3])layer1.0.bias torch.Size([4])layer1.1.weight torch.Size([3, 4])layer1.1.bias torch.Size([3])layer2.weight torch.Size([6, 3])layer2.bias torch.Size([6])layer3.0.weight torch.Size([7, 6])layer3.0.bias torch.Size([7])layer3.1.weight torch.Size([5, 7])layer3.1.bias torch.Size([5])模型保存|加载123456# 1、加载模型+参数net = torch.load(&quot;resnet50.pth&quot;)# 2、已有模型,加载预训练参数resnet50 = models.resnet50(weights=None)  resnet50.load_state_dict(torch.load(&quot;resnet58_weight.pth&quot;))网络的修改123456789101112131415161718192021222324252627282930313233from torch import nnfrom torchvision import modelsalexnet = models.alexnet(weights=models.AlexNet_Weights.DEFAULT)print(alexnet)AlexNet(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))    (1): ReLU(inplace=True)    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))    (4): ReLU(inplace=True)    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (7): ReLU(inplace=True)    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (9): ReLU(inplace=True)    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU(inplace=True)    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))  (classifier): Sequential(    (0): Dropout(p=0.5, inplace=False)    (1): Linear(in_features=9216, out_features=4096, bias=True)    (2): ReLU(inplace=True)    (3): Dropout(p=0.5, inplace=False)    (4): Linear(in_features=4096, out_features=4096, bias=True)    (5): ReLU(inplace=True)    (6): Linear(in_features=4096, out_features=1000, bias=True)  ))修改网络结构123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120# 1、-----删除网络的最后一层-----# del alexnet.classifierdel alexnet.classifier[6]print(alexnet)AlexNet(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))    (1): ReLU(inplace=True)    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))    (4): ReLU(inplace=True)    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (7): ReLU(inplace=True)    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (9): ReLU(inplace=True)    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU(inplace=True)    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))  (classifier): Sequential(    (0): Dropout(p=0.5, inplace=False)    (1): Linear(in_features=9216, out_features=4096, bias=True)    (2): ReLU(inplace=True)    (3): Dropout(p=0.5, inplace=False)    (4): Linear(in_features=4096, out_features=4096, bias=True)    (5): ReLU(inplace=True)  ))# 2、-----删除网络的最后多层-----alexnet.classifier = alexnet.classifier[:-2]print(alexnet)AlexNet(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))    (1): ReLU(inplace=True)    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))    (4): ReLU(inplace=True)    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (7): ReLU(inplace=True)    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (9): ReLU(inplace=True)    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU(inplace=True)    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))  (classifier): Sequential(    (0): Dropout(p=0.5, inplace=False)    (1): Linear(in_features=9216, out_features=4096, bias=True)    (2): ReLU(inplace=True)    (3): Dropout(p=0.5, inplace=False)  ))# 3、-----修改网络的某一层-----alexnet.classifier[6] = nn.Linear(in_features=4096, out_features=1024)print(alexnet)AlexNet(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))    (1): ReLU(inplace=True)    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))    (4): ReLU(inplace=True)    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (7): ReLU(inplace=True)    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (9): ReLU(inplace=True)    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU(inplace=True)    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))  (classifier): Sequential(    (0): Dropout(p=0.5, inplace=False)    (1): Linear(in_features=9216, out_features=4096, bias=True)    (2): ReLU(inplace=True)    (3): Linear(in_features=4096, out_features=1024, bias=True)  ))# 4、-----网络添加层，每次添加一层-----alexnet.classifier.add_module('7', nn.ReLU(inplace=True))alexnet.classifier.add_module('8', nn.Linear(in_features=1024, out_features=20))print(alexnet)AlexNet(  (features): Sequential(    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))    (1): ReLU(inplace=True)    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))    (4): ReLU(inplace=True)    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (7): ReLU(inplace=True)    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (9): ReLU(inplace=True)    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))    (11): ReLU(inplace=True)    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)  )  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))  (classifier): Sequential(    (0): Dropout(p=0.5, inplace=False)    (1): Linear(in_features=9216, out_features=4096, bias=True)    (2): ReLU(inplace=True)    (3): Linear(in_features=4096, out_features=1024, bias=True)    (4): ReLU(inplace=True)    (5): Linear(in_features=1024, out_features=20, bias=True)  ))参数冻结123456789101112131415161718192021222324252627# 任务一∶# 1、将模型A作为backbone，修改为模型B# 2、模型A的预训练参数加载到模型B上resnet_modified = resnet50()new_weights_dict = resnet_modified.state_dict()resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT)weights_dict = resnet.state_dict()for k in weights_dict.keys():    if k in new_weights_dict.keys() and not k.startswith('fc'):        new_weights_dict[k] = weights_dict[k]resnet_modified.load_state_dict(new_weights_dict)# resnet_modified.load_state_dict(new_weights_dict,strict=False)# 任务二:# 冻结与训练好的参数params = []train_layer = ['layer5', 'conv_end', 'bn_end']for name, param in resnet_modified.named_parameters():    if any(name.startswith(prefix) for prefix in train_layer):        print(name)        params.append(param)    else:        param.requires_grad = Falseoptimizer = torch.optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=5e-4)</content>
	</entry>
	<entry>
		<title>LLM Tokenizer分词系列</title>
		<url>/article/57252.html</url>
		<content>tokenizerhugging face Tokenizer文档huggingface的分词器的摘要【LLM系列之Tokenizer】如何科学地训练一个LLM分词器概述文本分词的过程涉及将文本拆分成多个单词或子单词。接着，这些单词或子单词会被映射到特定的ID，转换过程涉及一个查找表，这是一种简单的对应关系因此，我们的主要关注点在于解析文本为一系列的单词或子单词更具体地说，我们将探讨🤗 Transformers库中常用的三种主要分词器类型：Byte-Pair Encoding (BPE)、WordPiece和SentencePiece，并且我们将提供实例说明哪种模型采用了哪种分词器要了解特定预训练模型使用了哪种分词器，你可以参考每个模型主页上的文档说明，例如BertTokenizer，你会发现模型采用的是WordPiece分词器分词例子将一段文本分词到小块是一个比它看起来更加困难的任务，并且有很多方式来实现分词，举个例子，让我们看看这个句子1&quot;Don't you love 🤗 Transformers? We sure do.&quot;对这段文本分词的一个简单方式，就是使用空格来分词，得到的结果是：1[&quot;Don't&quot;, &quot;you&quot;, &quot;love&quot;, &quot;🤗&quot;, &quot;Transformers?&quot;, &quot;We&quot;, &quot;sure&quot;, &quot;do.&quot;]上面的分词是一个明智的开始，但是如果我们查看token &quot;Transformers?&quot; 和 &quot;do.&quot;，我们可以观察到标点符号附在单词&quot;Transformer&quot; 和 &quot;do&quot;的后面，这并不是最理想的情况我们应该将标点符号考虑进来，这样一个模型就没必要学习一个单词和每个可能跟在后面的 标点符号的不同的组合，这么组合的话，模型需要学习的组合的数量会急剧上升。将标点符号也考虑进来，对范例文本进行分词的结果就是：1[&quot;Don&quot;, &quot;'&quot;, &quot;t&quot;, &quot;you&quot;, &quot;love&quot;, &quot;🤗&quot;, &quot;Transformers&quot;, &quot;?&quot;, &quot;We&quot;, &quot;sure&quot;, &quot;do&quot;, &quot;.&quot;]分词的结果更好了，然而，这么做也是不好的，分词怎么处理单词&quot;Don't&quot;，&quot;Don't&quot;的含义是&quot;do not&quot;，所以这么分词[&quot;Do&quot;, &quot;n't&quot;] 会更好现在开始事情就开始变得复杂起来了，部分的原因是每个模型都有它自己的分词类型依赖于我们应用在文本分词上的规则， 相同的文本会产生不同的分词输出用在训练数据上的分词规则，被用来对输入做分词操作，一个预训练模型才会正确的执行spaCy and Moses 是两个受欢迎的基于规则的分词器，将这两个分词器应用在示例文本上，spaCy 和 Moses会输出类似下面的结果：1[&quot;Do&quot;, &quot;n't&quot;, &quot;you&quot;, &quot;love&quot;, &quot;🤗&quot;, &quot;Transformers&quot;, &quot;?&quot;, &quot;We&quot;, &quot;sure&quot;, &quot;do&quot;, &quot;.&quot;]可见上面的分词使用到了空格和标点符号的分词方式，以及基于规则的分词方式空格和标点符号分词以及基于规则的分词都是单词分词的例子，不那么严格的来说，单词分词的定义就是将句子分割到很多单词然而将文本分割到更小的块是符合直觉的，当处理大型文本语料库时，上面的 分词方法会导致很多问题在这种情况下，空格和标点符号分词通常会产生一个非常大的词典（使用到的所有不重复的单词和tokens的集合）像：Transformer XL使用空格和标点符号分词，结果会产生一个大小是267,735的词典这么大的一个词典容量，迫使模型有着一个巨大的embedding矩阵，以及巨大的输入和输出层，这会增加内存使用量，也会提高时间复杂度通常情况下，transformers模型几乎没有词典容量大于50,000的，特别是只在一种语言上预训练的模型所以如果简单的空格和标点符号分词让人不满意，为什么不简单的对字符分词尽管字符分词是非常简单的，并且能极大的减少内存使用，降低时间复杂度，但是这样做会让模型很难学到有意义的输入表达像： 比起学到单词&quot;today&quot;的一个有意义的上下文独立的表达，学到字母&quot;t&quot;的一个有意义的上下文独立的表达是相当困难的因此，字符分词经常会伴随着性能的下降。所以为了获得最好的结果，transformers模型在单词级别分词和字符级别分词之间使用了一个折中的方案被称作子词分词分词粒度NLP中Tokenizers总结（BPE、WordPiece、Unigram和SentencePiece）在NLP中，模型如Bert、GPT）的输入通常需要先进行tokenize，其目的是将输入的文本流，切分为一个个子串，每个子串都有完整的语义，便于学习embedding表达和后续模型的使用。tokenize有三种粒度：word/subword/charword/词：词是最自然的语言单元，对于英文来说其天然存在空格进行，切分相对容易，常用的分词器有spaCy和Moses 中文不具备这样的分割符，所以相对困难一些，不过目前也有Jieba、HanLP、LTP等分词器，这些分词器基于规则与模型，可以取得良好的分词效果使用词时会有2个问题，通常情况下词表大小不超过5w：词表通常是基于语料进行分词获得，但遇到新的语料时可能会出现OOV的情况词表过于庞大，对于模型来说大部分参数都集中在输入输出层，不利于模型学习，且容易爆内存（显存）char/字符：字符是一种语言最基本的组成单元，如英文中的’a’、’b’、’c’或中文中的‘你’、‘我’、‘他’等，使用字符有如下问题：字符数量是有限的通常数量较少，这样在学习每个字符的embedding向量时，每个字符中包含非常多的语义，学习起来比较困难以字符分割，会造成序列长度过长，对后续应用造成较大限制subword/子词：它介于char和word之间，可以很好的平衡词汇量和语义独立性，它的切分准则是常用的词不被切分，而不常见的词切分为子词子词分词子词分词原则子词分词算法依赖这样的原则：频繁使用的单词不应该被分割成更小的子词很少使用的单词应该被分解到有意义的子词举个例子： &quot;annoyingly&quot;能被看作一个很少使用的单词，能被分解成&quot;annoying&quot;和`”ly”```&quot;annoying&quot;和&quot;ly&quot;作为独立地子词，出现的次数都很频繁，而且与此同时单词&quot;annoyingly&quot;的含义可以通过组合&quot;annoying&quot;和&quot;ly&quot;的含义来获得在粘合和胶水语言上，像Turkish语言，这么做是相当有用的，在这样的语言里，通过线性组合子词，大多数情况下你能形成任意长的复杂的单词子词分词允许模型有一个合理的词典大小，而且能学到有意义的上下文独立地表达除此以外，子词分词可以让模型处理以前从来没见过的单词， 方式是通过分解这些单词到已知的子词，举个例子：BertTokenizer对句子&quot;I have a new GPU!&quot;分词的结果如下：12345&gt;&gt;&gt; from transformers import BertTokenizer&gt;&gt;&gt; tokenizer = BertTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)&gt;&gt;&gt; tokenizer.tokenize(&quot;I have a new GPU!&quot;)[&quot;i&quot;, &quot;have&quot;, &quot;a&quot;, &quot;new&quot;, &quot;gp&quot;, &quot;##u&quot;, &quot;!&quot;]因为我们正在考虑不区分大小写的模型，句子首先被转换成小写字母形式我们可以见到单词[&quot;i&quot;, &quot;have&quot;, &quot;a&quot;, &quot;new&quot;]在分词器的词典内，但是这个单词&quot;gpu&quot;不在词典内所以，分词器将&quot;gpu&quot;分割成已知的子词[&quot;gp&quot; and &quot;##u&quot;]&quot;##&quot;意味着剩下的 token应该附着在前面那个token的后面，不带空格的附着（分词的解码或者反向）另外一个例子，XLNetTokenizer对前面的文本例子分词结果如下：12345from transformers import XLNetTokenizertokenizer = XLNetTokenizer.from_pretrained(&quot;xlnet-base-cased&quot;)tokenizer.tokenize(&quot;Don't you love 🤗 Transformers? We sure do.&quot;)&gt;&gt;&gt; [&quot;▁Don&quot;, &quot;'&quot;, &quot;t&quot;, &quot;▁you&quot;, &quot;▁love&quot;, &quot;▁&quot;, &quot;🤗&quot;, &quot;▁&quot;, &quot;Transform&quot;, &quot;ers&quot;, &quot;?&quot;, &quot;▁We&quot;, &quot;▁sure&quot;, &quot;▁do&quot;, &quot;.&quot;]当我们查看SentencePiece时会回过头来解释这些&quot;▁&quot;符号的含义。正如你能见到的，很少使用的单词 &quot;Transformers&quot;能被分割到更加频繁使用的子词&quot;Transform&quot;和&quot;ers&quot;现在让我们来看看不同的子词分割算法是怎么工作的，注意到所有的这些分词算法依赖于某些训练的方式，这些训练通常在语料库上完成， 相应的模型也是在这个语料库上训练的Byte-Pair Encoding (BPE)BPE-Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)Byte Pair EncodingBPE依赖于一个预分词器，这个预分词器会将训练数据分割成单词。预分词可以是简单的 空格分词，像：GPT-2，RoBERTa更加先进的预分词方式包括了基于规则的分词，像：XLM，FlauBERT，FlauBERT在大多数语言使用了Moses，或者GPT，GPT使用了Spacy和ftfy，统计了训练语料库中每个单词的频次在预分词以后，生成了单词的集合，也确定了训练数据中每个单词出现的频次下一步，BPE产生了一个基础词典，包含了集合中所有的符号，BPE学习融合的规则-组合基础词典中的两个符号来形成一个新的符号BPE会一直学习直到词典的大小满足了期望的词典大小的要求。注意到 期望的词典大小是一个超参数，在训练这个分词器以前就需要人为指定举个例子，让我们假设在预分词以后，下面的单词集合以及他们的频次都已经确定好了：1(&quot;hug&quot;, 10), (&quot;pug&quot;, 5), (&quot;pun&quot;, 12), (&quot;bun&quot;, 4), (&quot;hugs&quot;, 5)所以，基础的词典是[&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;]，将所有单词分割成基础词典内的符号，就可以获得：1(&quot;h&quot; &quot;u&quot; &quot;g&quot;, 10), (&quot;p&quot; &quot;u&quot; &quot;g&quot;, 5), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, 12), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, 4), (&quot;h&quot; &quot;u&quot; &quot;g&quot; &quot;s&quot;, 5)BPE接着会统计每个可能的符号对的频次，然后挑出出现最频繁的的符号对，在上面的例子中，&quot;h&quot;跟了&quot;u&quot;出现了10 + 5 = 15次 （10次是出现了10次&quot;hug&quot;，5次是出现了5次&quot;hugs&quot;）然而，最频繁的符号对是&quot;u&quot;后面跟了个&quot;g&quot;，总共出现了10 + 5 + 5 = 20次因此，分词器学到的第一个融合规则是组合所有的&quot;u&quot;后面跟了个&quot;g&quot;符号下一步，&quot;ug&quot;被加入到了词典内。单词的集合就变成了：1(&quot;h&quot; &quot;ug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, 12), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, 4), (&quot;h&quot; &quot;ug&quot; &quot;s&quot;, 5)BPE接着会统计出下一个最普遍的出现频次最大的符号对，也就是&quot;u&quot;后面跟了个&quot;n&quot;，出现了16次，&quot;u&quot;，&quot;n&quot;被融合成了&quot;un&quot;。也被加入到了词典中，再下一个出现频次最大的符号对是&quot;h&quot;后面跟了个&quot;ug&quot;，出现了15次又一次这个符号对被融合成了&quot;hug&quot;， 也被加入到了词典中在当前这步，词典是[&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;, &quot;hug&quot;]，我们的单词集合则是：1(&quot;hug&quot;, 10), (&quot;p&quot; &quot;ug&quot;, 5), (&quot;p&quot; &quot;un&quot;, 12), (&quot;b&quot; &quot;un&quot;, 4), (&quot;hug&quot; &quot;s&quot;, 5)假设，the Byte-Pair Encoding在这个时候停止训练，学到的融合规则并应用到其他新的单词上（只要这些新单词不包括不在基础词典内的符号 就行）举个例子，单词&quot;bug&quot;会被分词到[&quot;b&quot;, &quot;ug&quot;]，但是&quot;mug&quot;会被分词到[&quot;&lt;unk&gt;&quot;, &quot;ug&quot;]，因为符号&quot;m&quot;不在基础词典内通常来看的话，单个字母像&quot;m&quot;不会被&quot;&lt;unk&gt;&quot;符号替换掉，因为训练数据通常包括了每个字母，每个字母至少出现了一次，但是在特殊的符号 中也可能发生像emojis就像之前提到的那样，词典的大小，举个例子，基础词典的大小 + 融合的数量，是一个需要配置的超参数举个例子：GPT 的词典大小是40,478，因为GPT有着478个基础词典内的字符，在40,000次融合以后选择了停止训练Byte-level BPE一个包含了所有可能的基础字符的基础字典可能会非常大，如果考虑将所有的unicode字符作为基础字符为了拥有一个更好的基础词典，GPT-2使用了字节 作为基础词典，这是一个非常聪明的技巧，迫使基础词典是256大小，而且确保了所有基础字符包含在这个词典内。使用了其他的规则来处理标点符号，这个GPT2的分词器能对每个文本进行分词，不需要使用到符号。GPT-2有一个大小是50,257 的词典，对应到256字节的基础tokens，一个特殊的文本结束token，这些符号经过了50,000次融合学习WordPieceJapanese and Korean Voice Search (Schuster et al., 2012)WordPiece是子词分词算法，被用在BERT，DistilBERT，和Electra，和BPE非常相似WordPiece首先初始化一个词典，这个词典包含了出现在训练数据中的每个字符，然后递进的学习一个给定数量的融合规则和BPE相比较， WordPiece不会选择出现频次最大的符号对，而是选择了加入到字典以后能最大化训练数据似然值的符号对所以这到底意味着什么？参考前面的例子，最大化训练数据的似然值，等价于找到一个符号对，它们的概率除以这个符号对中第一个符号的概率，接着除以第二个符号的概率，在所有的符号对中商最大像：如果&quot;ug&quot;的概率除以&quot;u&quot;除以&quot;g&quot;的概率的商，比其他任何符号对更大， 这个时候才能融合&quot;u&quot;和&quot;g&quot;直觉上，WordPiece，和BPE有点点不同，WordPiece是评估融合两个符号会失去的量，来确保这么做是值得的UnigramSubword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, 2018)Unigram是一个子词分词器算法，和BPE或者WordPiece相比较 ，Unigram使用大量的符号来初始化它的基础字典，然后逐渐的精简每个符号来获得一个更小的词典。举例来看基础词典能够对应所有的预分词 的单词以及最常见的子字符串。Unigram没有直接用在任何transformers的任何模型中，但是和SentencePiece一起联合使用。在每个训练的步骤，Unigram算法在当前词典的训练数据上定义了一个损失函数（经常定义为log似然函数的），还定义了一个unigram语言模型。 然后，对词典内的每个符号，算法会计算如果这个符号从词典内移除，总的损失会升高多少Unigram然后会移除百分之p的符号，这些符号的loss 升高是最低的（p通常是10%或者20%），像：这些在训练数据上对总的损失影响最小的符号重复这个过程，直到词典已经达到了期望的大小。 为了任何单词都能被分词，Unigram算法总是保留基础的字符因为Unigram不是基于融合规则（和BPE以及WordPiece相比较），在训练以后算法有几种方式来分词，如果一个训练好的Unigram分词器 的词典是这个：1[&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;, &quot;hug&quot;],&quot;hugs&quot;可以被分词成[&quot;hug&quot;, &quot;s&quot;], [&quot;h&quot;, &quot;ug&quot;, &quot;s&quot;]或者[&quot;h&quot;, &quot;u&quot;, &quot;g&quot;, &quot;s&quot;]所以选择哪一个呢？Unigram在保存词典的时候还会保存训练语料库内每个token的概率，所以在训练以后可以计算每个可能的分词结果的概率实际上算法简单的选择概率最大的那个分词结果，但是也会提供概率来根据分词结果的概率来采样一个可能的分词结果分词器在损失函数上训练，这些损失函数定义了这些概率假设训练数据包含了这些单词 ，一个单词的所有可能的分词结果的集合定义为，然后总的损失就可以定义为：SentencePieceSentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018)目前为止描述的所有分词算法都有相同的问题：它们都假设输入的文本使用空格来分开单词，然而，不是所有的语言都使用空格来分开单词一个可能的解决方案是使用某种语言特定的预分词器。像：XLM使用了一个特定的中文、日语和Thai的预分词器为了更加广泛的解决这个问题，SentencePiece将输入文本看作一个原始的输入流，因此使用的符合集合中也包括了空格SentencePiece然后会使用BPE或者unigram算法来产生合适的词典举例来说，XLNetTokenizer使用了SentencePiece，这也是为什么上面的例子中&quot;▁&quot;符号包含在词典内SentencePiece解码是非常容易的，因为所有的tokens能被concatenate起来，然后将&quot;▁&quot;替换成空格库内所有使用了SentencePiece的transformers模型，会和unigram组合起来使用，像：使用了SentencePiece的模型是ALBERT, XLNet，Marian，和T5训练分词器大模型基础知识系列：从头训练一个自己的Tokenizer当前，预训练语言模型已成为NLP算法工程师的工具箱中的常客。在实际应用中，几乎所有的NLP模型都依赖于分词器（Tokenizer）来处理文本数据虽然通常我们会倾向于使用现成的分词器，但有时候创建一个定制化的分词器也是必要的对于分词器的构建，通常可以选择使用sentencepiece或者huggingface的tokenizers库，我们可以采用tokenizers库来训练我们自己的分词器，确保tokenizers库已经安装在你的系统上1pip install tokenizers有了tokenizers库，我们可以开始构建我们的Tokenizer。这个过程包括配置多个组件以自定Tokenizer的行为，包括但不限于：模型（Models）：这是Tokenizer的核心，负责实际的分词操作。可选的模型包括WordLevel、BPE、Unigram和WordPiece规范化器（Normalizers）：规范化器用于预处理输入文本，将其转换为标准化的格式，如进行Unicode规范化或转换为小写，同时跟踪与原始文本的对齐关系预分词器（PreTokenizers）：预分词器按照一定规则拆分输入文本，以确保底层模型按照这些预设边界构建令牌后处理器（PostProcessors）：在分词流程完成后，后处理器负责在标记化后的字符串中插入特殊标记，比如说为模型提供标准格式的字符串解码器（Decoders）：解码器能够将分词器生成的ID转换回人类可读的文本这些组件的组合使得Tokenizer不仅能够执行基本的分词任务，还能为特定的NLP问题提供定制化的解决方案在调用 Tokenizer.encode 或 Tokenizer.encode_batch 时，输入文本将经过以下流程：规范化预分词模型处理后处理123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v1.0@author: huangyc@file: train_tokenizer.py@Description: @time: 2024/2/15 9:42&quot;&quot;&quot;from datasets import load_datasetfrom tokenizers.models import BPEfrom tokenizers.normalizers import NFD, StripAccentsfrom tokenizers.pre_tokenizers import Whitespace, Punctuation, Digits, ByteLevelfrom tokenizers.trainers import BpeTrainerfrom tokenizers import Tokenizer, normalizers, pre_tokenizers, decodersfrom tokenizers import tokenizersfrom tokenizers.processors import TemplateProcessingdef batch_iterator(batch_size=10000):    dataset = load_dataset(&quot;TurboPascal/tokenizers_example_zh_en&quot;, cache_dir='./cache/')    print(dataset)    for i in range(0, len(dataset), batch_size):        yield dataset['train'][i: i + batch_size][&quot;text&quot;]def train_tokenizer():    # 自定数据集    data_files = [r&quot;.\data\dataset_hyc.txt&quot;]    # 定义tokenizer    tokenizer = Tokenizer(BPE(unk_token=&quot;[UNK]&quot;))    # 定义一个归一化对象    normalizer = normalizers.Sequence([NFD(), StripAccents()])    tokenizer.normalizer = normalizer    # 我们主要使用四类分割，空白、标点符号、数字、Bytelevel    # pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(), Digits(individual_digits=True), ByteLevel()])    # tokenizer.pre_tokenizer = pre_tokenizer    # 使用空白分割    tokenizer.pre_tokenizer = Whitespace()    # 解码器    tokenizer.decoder = decoders.ByteLevel(add_prefix_space=True, use_regex=True)    # 字节级 BPE 可能在生成的令牌中包括空白。如果您不希望偏移量包含这些空格，那么必须使用这个 PostProcessor。    tokenizer.post_processor = tokenizers.processors.ByteLevel()    # 定义一个BpeTrainer    trainer = BpeTrainer(special_tokens=[&quot;[UNK]&quot;, &quot;[CLS]&quot;, &quot;[SEP]&quot;, &quot;[PAD]&quot;, &quot;[MASK]&quot;])    # 开始训练    # 方式一    tokenizer.train(data_files, trainer)    # 方式二    # tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset['train']))    # tokenizer保存    tokenizer.save(&quot;data/tokenizer-wiki.json&quot;)    # tokenizer加载    tokenizer = Tokenizer.from_file(&quot;data/tokenizer-wiki.json&quot;)    sentence = &quot;我尝试了很多的方法&quot;    output = tokenizer.encode(sentence)    print(output.tokens)    # ['我', '[UNK]', '[UNK]', '了', '很', '多的', '方', '法']    print(output.ids)    # [376, 0, 0, 44, 339, 1561, 438, 524]    print(output.offsets[5])    # (5, 7)    print(sentence[output.offsets[5][0]:output.offsets[5][1]])    # '多的'    tokenizer.token_to_id(&quot;[SEP]&quot;)    # 2    # 后续处理    tokenizer.post_processor = TemplateProcessing(single=&quot;[CLS] $A [SEP]&quot;, pair=&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;,                                                  special_tokens=[(&quot;[CLS]&quot;, tokenizer.token_to_id(&quot;[CLS]&quot;)),                                                                  (&quot;[SEP]&quot;, tokenizer.token_to_id(&quot;[SEP]&quot;)), ], )    output = tokenizer.encode_batch([sentence])    print(output)    output = tokenizer.encode_batch([[&quot;我尝试了许多的方法&quot;, &quot;却始终没有成功&quot;], [&quot;自己说过的话&quot;, &quot;就必须要努力去践行&quot;]])    print(output)    tokenizer.enable_padding(pad_id=3, pad_token=&quot;[PAD]&quot;)    output = tokenizer.encode_batch([&quot;我尝试了许多的方法&quot;, &quot;却始终没有成功&quot;])    print(output[0].tokens, output[1].tokens)    # ['[CLS]', '我', '[UNK]', '[UNK]', '了', '许', '多的', '方', '法', '[SEP]'] ['[CLS]', '[UNK]', '[UNK]', '[UNK]', '没有', '成', '功', '[SEP]', '[PAD]', '[PAD]']    print(output[1].attention_mask)    # [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]if __name__ == '__main__':    train_tokenizer()</content>
	</entry>
	<entry>
		<title>咕呱锻炼随笔</title>
		<url>/article/61183.html</url>
		<content>      本文需要输入密码才能访问哦   </content>
	</entry>
	<entry>
		<title>机器学习_一元函数微积分(2)</title>
		<url>/article/8171.html</url>
		<content>      本文需要输入密码才能访问哦   </content>
	</entry>
	<entry>
		<title>机器学习_最优化方法(2)</title>
		<url>/article/14910.html</url>
		<content>      本文需要输入密码才能访问哦   </content>
	</entry>
	<entry>
		<title>机器学习_概率论(2)</title>
		<url>/article/41340.html</url>
		<content>      本文需要输入密码才能访问哦   </content>
	</entry>
	<entry>
		<title>机器学习_线性代数与矩阵论(3)</title>
		<url>/article/29825.html</url>
		<content>      本文需要输入密码才能访问哦   </content>
	</entry>
	<entry>
		<title>机器学习_线性代数与矩阵论(2)</title>
		<url>/article/58496.html</url>
		<content>      本文需要输入密码才能访问哦   </content>
	</entry>
	<entry>
		<title>豚妞成长记录</title>
		<url>/article/51068.html</url>
		<content>      本文需要输入密码才能访问哦   </content>
	</entry>
	<entry>
		<title>图神经网络</title>
		<url>/article/47450.html</url>
		<content>概述PyG DocumentationPyG(PyTorch Geometric)是一个建立在 PyTorch 基础上的库，用于轻松编写和训练图神经网络(GNN)，用于与结构化数据相关的广泛应用任务形态GNN中常见任务：Graph级别任务、Node级别任务、Edge级别任务包括：节点分类，节点连接预测，图相似度检测，异常检测，图分类多层GNN不会改变图的拓扑结构，但是会改变点的特征，多层GNN处理可以扩大感受野GCN和CNN有什么不同和相同点不同点：图卷积中每个点的邻居数量是不确定的，图卷积中的数据输入格式不确定相同点：GCN和CNN本质都需要对输入数据做特征提取，且都是根据某个点周围点的情况提取特征GCN 可以处理 semi-supervised learning，即只有部分节点有标签的情况，计算损失时只用有标签的计算GCN 的层数一般3~5层比较好，层数叠加可能导致反效果</content>
	</entry>
	<entry>
		<title>nlp关键词和摘要提取技术整理</title>
		<url>/article/42898.html</url>
		<content>切词等待…关键词提取自动关键词抽取研究综述2017特征驱动的关键词提取算法综述2018关键词提取研究综述2021概述概念关键词提取技术是一种自然语言处理技术，旨在从给定的文本中自动识别出最具代表性和重要性的关键词或短语关键词通常是文本中具有特殊含义、能够概括文本主题或内容的词语或短语使用场景关键词提取技术的目标是对文本进行语义分析和内容抽取，从而提取出最能代表文本主题和内容的关键词这些关键词可以用于文本分类、信息检索、文本摘要、主题建模、信息过滤等自然语言处理任务经典方法关键词提取技术通常结合了文本的语言统计特征、词频分布、词性、上下文关系、语义相似度等多种信息源，以识别并提取出最相关和具有区分度的关键词常见的关键词提取方法包括基于词频、TF-IDF、文本图结构、语言模型、图模型、深度学习等多种技术手段关键词提取技术在信息处理、文本挖掘、自动化文档处理等领域具有重要应用价值，能够帮助人们更快速、准确地理解和处理大量文本信息分类NLP中关键字提取方法总结和概述基于统计统计方法是最简单的。他们计算关键字的统计数据并使用这些统计数据对它们进行评分。一些最简单的统计方法是词频、词搭配和共现也有一些更复杂的，例如TF-IDF和YAKE!tf-idf概述TF-IDF或term frequency–inverse document frequency，会计算文档中单词相对于整个语料库(更多文档集)的重要性它计算文档中每个词的频率，并通过词在整个语料库中的频率的倒数对其进行加权。最后，选择得分最高的词作为关键词，TF-IDF有两层意思词频: Term Frequency，缩写为TF，通常来说，一个分词出现的次数越多，代表越重要逆文档频率: Inverse Document Frequency，缩写为IDF，在现实生活中，出现次数最多的词是一些无意义的词，比如停用词，对搜索结果毫无帮助，必须通过分词器提前过滤掉的词公式 - TF-IDF from scratch in python on a real-world dataset.术语tdNcorpus释义term(word)document(set of words)一篇文档中的词集合count of corpus语料数量the total document set总体文档集合一个分词Term的相关性由tf*idf公式简化表达。tf-idf模型包含了二个简单事实：某个term分词在一个文档中出现次数(tf)越多，这个词与文档越相关某个索引中包含某个term分词的文档数量越少(idf)，这个term分词越重要例子考虑一个包含100个单词的文档，其中Leon这个分词出现了10次。这个时候TF=(10/100)=0.1并且假设Lucene索引中有1000W份文档数量，其中有1000份文档中出现了Leon这个分词，此时逆文档频率(IDF)计算为IDF=log(10,000,000/1,000)=4因此，TD-IDF计算为TF*IDF=0.1 * 4=0.412345678910111213import jiebaimport jieba.analysesentence = '中华蜜蜂原产于中国，是中国的土著蜂，适应中国各地的气候和蜜源条件，适于定地饲养且稳产，尤其是在南方山区，有着其他蜂种不可替代的地位'seg_list = jieba.cut(sentence, cut_all=True)print(&quot;, &quot;.join(seg_list))keywords = jieba.analyse.extract_tags(sentence, topK=20, withWeight=True, allowPOS=('n', 'nr', 'ns'))print(keywords)&gt;&gt;&gt;中华, 蜜蜂, 原产, 产于, 中国, ，, 是, 中国, 的, 土著, 蜂, ，, 适应, 中国, 各地, 的, 气候, 和, 蜜源, 条件, ，, 适于, 定, 地, 饲养, 且, 稳产, ，, 尤其, 是, 在, 南方, 方山, 山区, ，, 有着, 其他, 蜂, 种, 不可, 替代, 的, 地位[('定地', 0.7969), ('蜂种', 0.7969), ('稳产', 0.7340), ('蜜源', 0.66725), ('中国', 0.60546), ('蜜蜂', 0.5859), ('土著', 0.55968), ('原产', 0.544705), ('替代', 0.484315), ('山区', 0.44390), ('气候', 0.38804), ('地位', 0.34710), ('条件', 0.32636)]优缺点速度快: TF-IDF的优点是速度快语言无关: TF-IDF与语言无关需要语料: 需要至少几十个文档的语料库不够全面: 缺点是单纯于词频来判断一个分词的重要性，不够全面无法捕获语义: 缺点是不能捕捉分词Term在文档中的位置变种TF-IDF 的4大常见变种变种1: 对数函数变换 TF，解决TF现行增长问题变种2: 对 TF 进行标准化，解决长短文档问题变种3: 对数函数变换 IDF，解决IDF 现行增长问题变种4: 查询词及文档向量标准化，解决长短文档问题YAKE2018ECIR的最佳短论文奖 A Text Feature Based Automatic Keyword Extraction Method for Single Documentsgithub Yet Another Keyword Extractor (Yake)概述YAKE(Yet Another Keyword Extractor)是一种关键字提取方法，它利用单个文档的统计特征来提取关键字它通过五个步骤提取关键字，旨在从文本中自动提取最相关的关键词和关键短语YAKE算法的工作原理如下：文本预处理：将输入文本进行预处理，包括分词、去除停用词等特征提取：使用词频、位置权重、长度等特征来衡量单词和短语的重要性关键词候选生成：根据特征权重，生成候选关键词和关键短语关键词权重计算：根据候选关键词的特征权重，计算它们的最终权重关键词筛选：根据设定的阈值或排序方法，筛选出具有高权重的关键词和关键短语作为最终结果YAKE算法与传统的基于统计和语言模型的关键词提取方法不同，它采用了基于特征权重的方法，使得算法更加灵活和可定制此外，YAKE还支持多语言关键词提取，并能够处理领域特定的文本总体而言，YAKE算法通过综合考虑单词和短语的特征权重，以及它们在文本中的频率和位置等信息，来提取与文本内容相关的关键词和关键短语官方实现仅支持英文基于图技术TextRankTextRank算法介绍及实现TextRank是一种基于图的算法，用于关键词提取和文本摘要。它基于PageRank算法的思想，将文本表示为一个图，其中节点表示文本中的单词或短语，边表示它们之间的关系通过计算节点之间的权重和连接关系，TextRank可以确定文本中最重要的单词或短语。TextRank的步骤如下：分割文本：将文本分割成句子或单词构建图：根据文本中的句子或单词构建一个图，其中每个句子或单词作为一个节点，边表示它们之间的关系。常见的关系可以是共现关系或语义关系计算权重：为图中的每个节点计算权重。通常使用词频或TF-IDF作为初始权重迭代计算：通过迭代计算节点之间的权重，更新每个节点的权重值。迭代过程中，节点的权重将考虑其相邻节点的权重值排序节点：根据节点的权重值对节点进行排序，得到关键词或摘要TextRank算法在关键词提取和文本摘要等任务中表现良好，它不需要依赖预训练模型，可以直接应用于各种领域的文本处理任务PageRankThe PageRank Citation Ranking: Bringing Order to the Web 1999关键词提取和摘要算法TextRank详解与实战PageRank算法通过计算网页链接的数量和质量来粗略估计网页的重要性，算法创立之初即应用在谷歌的搜索引擎中，对网页进行排名。PageRank算法的核心思想如下：链接数量：如果一个网页被越多的其他网页链接，说明这个网页越重要，即该网页的PR值(PageRank值)会相对较高链接质量：如果一个网页被一个越高权值的网页链接，也能表明这个网页越重要，即一个PR值很高的网页链接到一个其他网页，那么被链接到的网页的PR值会相应地因此而提高PageRank算法计算公式其中，是网页的重要性(PR值)，是阻尼系数，一般为0.85，是整个互联网中所存在的有指同网页的链接的网页集合，是网页中存在的指向所有外部网页的链辖的集合，是该集合中元素的个数例子等待…PageRank算法与TextRank算法的区别PageRank算法根据网页之间的链接关系构造网络，TextRank算法根据词之间的共现关系构造网络PageRank算法构造的网络中的边是有向无权边，TextRank算法构造的网络中的边是无向有权边实现基于Textrank4zh的TextRank算法实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182from textrank4zh import TextRank4Keyword, TextRank4Sentenceimport jieba.analysefrom snownlp import SnowNLPimport pandas as pdimport numpy as np #关键词抽取def keywords_extraction(text):    tr4w = TextRank4Keyword(allow_speech_tags=['n', 'nr', 'nrfg', 'ns', 'nt', 'nz'])    # allow_speech_tags   --词性列表，用于过滤某些词性的词    tr4w.analyze(text=text, window=2, lower=True, vertex_source='all_filters', edge_source='no_stop_words',                 pagerank_config={'alpha': 0.85, })    # text    --  文本内容，字符串    # window  --  窗口大小，int，用来构造单词之间的边。默认值为2    # lower   --  是否将英文文本转换为小写，默认值为False    # vertex_source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来构造pagerank对应的图中的节点    #                -- 默认值为`'all_filters'`，可选值为`'no_filter', 'no_stop_words', 'all_filters'    # edge_source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来构造pagerank对应的图中的节点之间的边    #              -- 默认值为`'no_stop_words'`，可选值为`'no_filter', 'no_stop_words', 'all_filters'`。边的构造要结合`window`参数     # pagerank_config  -- pagerank算法参数配置，阻尼系数为0.85    keywords = tr4w.get_keywords(num=6, word_min_len=2)    # num           --  返回关键词数量    # word_min_len  --  词的最小长度，默认值为1    return keywords #关键短语抽取def keyphrases_extraction(text):    tr4w = TextRank4Keyword()    tr4w.analyze(text=text, window=2, lower=True, vertex_source='all_filters', edge_source='no_stop_words',                 pagerank_config={'alpha': 0.85, })    keyphrases = tr4w.get_keyphrases(keywords_num=6, min_occur_num=1)    # keywords_num    --  抽取的关键词数量    # min_occur_num   --  关键短语在文中的最少出现次数    return keyphrases #关键句抽取def keysentences_extraction(text):    tr4s = TextRank4Sentence()    tr4s.analyze(text, lower=True, source='all_filters')    # text    -- 文本内容，字符串    # lower   -- 是否将英文文本转换为小写，默认值为False    # source  -- 选择使用words_no_filter, words_no_stop_words, words_all_filters中的哪一个来生成句子之间的相似度。    #   -- 默认值为`'all_filters'`，可选值为`'no_filter', 'no_stop_words', 'all_filters'    # sim_func -- 指定计算句子相似度的函数     # 获取最重要的num个长度大于等于sentence_min_len的句子用来生成摘要    keysentences = tr4s.get_key_sentences(num=3, sentence_min_len=6)    return keysentences  def keywords_textrank(text):    keywords = jieba.analyse.textrank(text, topK=6)    return keywords  if __name__ == &quot;__main__&quot;:    text = &quot;来源：中国科学报本报讯（记者肖洁）又有一位中国科学家喜获小行星命名殊荣！4月19日下午，中国科学院国家天文台在京举行“周又元星”颁授仪式，&quot; \           &quot;我国天文学家、中国科学院院士周又元的弟子与后辈在欢声笑语中济济一堂。国家天文台党委书记、&quot; \           &quot;副台长赵刚在致辞一开始更是送上白居易的诗句：“令公桃李满天下，何须堂前更种花。”&quot; \           &quot;据介绍，这颗小行星由国家天文台施密特CCD小行星项目组于1997年9月26日发现于兴隆观测站，&quot; \           &quot;获得国际永久编号第120730号。2018年9月25日，经国家天文台申报，&quot; \           &quot;国际天文学联合会小天体联合会小天体命名委员会批准，国际天文学联合会《小行星通报》通知国际社会，&quot; \           &quot;正式将该小行星命名为“周又元星”。&quot;    #关键词抽取    keywords=keywords_extraction(text)    print(keywords)     #关键短语抽取    keyphrases=keyphrases_extraction(text)    print(keyphrases)     #关键句抽取    keysentences=keysentences_extraction(text)    print(keysentences)        &gt;&gt;&gt;[{'word': '小行星', 'weight': 0.05808}, {'word': '天文台', 'weight': 0.05721}, {'word': '命名', 'weight': 0.048517}, {'word': '中国', 'weight': 0.045716}, {'word': '中国科学院', 'weight': 0.037818}, {'word': '国家', 'weight': 0.03438}]['小行星命名'][{'index': 4, 'sentence': '2018年9月25日，经国家天文台申报，国际天文学联合会小天体联合会小天体命名委员会批准，国际天文学联合会《小行星通报》通知国际社会，正式将该小行星命名为“周又元星”', 'weight': 0.2281}, {'index': 3, 'sentence': '”据介绍，这颗小行星由国家天文台施密特CCD小行星项目组于1997年9月26日发现于兴隆观测站，获得国际永久编号第120730号', 'weight': 0.2106}, {'index': 1, 'sentence': '4月19日下午，中国科学院国家天文台在京举行“周又元星”颁授仪式，我国天文学家、中国科学院院士周又元的弟子与后辈在欢声笑语中济济一堂', 'weight': 0.20209}]基于jieba的TextRank算法实现12345678910111213141516if __name__ == &quot;__main__&quot;:    text = &quot;来源：中国科学报本报讯（记者肖洁）又有一位中国科学家喜获小行星命名殊荣！4月19日下午，中国科学院国家天文台在京举行“周又元星”颁授仪式，&quot; \           &quot;我国天文学家、中国科学院院士周又元的弟子与后辈在欢声笑语中济济一堂。国家天文台党委书记、&quot; \           &quot;副台长赵刚在致辞一开始更是送上白居易的诗句：“令公桃李满天下，何须堂前更种花。”&quot; \           &quot;据介绍，这颗小行星由国家天文台施密特CCD小行星项目组于1997年9月26日发现于兴隆观测站，&quot; \           &quot;获得国际永久编号第120730号。2018年9月25日，经国家天文台申报，&quot; \           &quot;国际天文学联合会小天体联合会小天体命名委员会批准，国际天文学联合会《小行星通报》通知国际社会，&quot; \           &quot;正式将该小行星命名为“周又元星”。&quot;     # 基于jieba的textrank算法实现    keywords=keywords_textrank(text)    print(keywords)&gt;&gt;&gt;['小行星', '命名', '国际', '中国', '国家', '天文学家']RAKERAKE和TextRank的主要区别在于RAKE考虑候选关键字内的共现而不是固定窗口。它使用更简单、更具统计性的评分程序。该算法对每个文档分别进行，因此不需要文档语料库来进行关键词提取基于深度学习深度学习的出现使基于嵌入的方法成为可能。研究人员开发了几种使用文档嵌入的关键字提取方法(例如Bennani等人)这些方法主要查找候选关键字列表(例如，Bennani等人只考虑由名词和形容词组成的关键字)他们将文档和候选关键字嵌入到相同的嵌入空间中，并测量文档和关键字嵌入之间的相似度(例如余弦相似度)。他们根据相似度度量选择与文档文本最相似的关键字SIFRankSIFRank: A New Baseline for Unsupervised Keyphrase Extraction Based on Pre-Trained Language Model 2019论文阅读笔记： SIFRank and BERT-KPEgithub sunyilgdx/SIFRank_zhSIFRank比较适合短文本的关键词抽取，而SIFRank+大幅增加了长文本的关键词抽取效果步骤人工标注：分词+标词性获取候选关键词列表：利用正则表达式确定名词短语(例如：形容词+名词)，将名词短语作为候选关键短语通过预训练语言模型，得到关键词的embedding同样地，得到句子或文档的embedding计算3与4结果的余弦相似度，选取topN作为其最终提取的关键词NP chunker在SIFRank方法中，NP chunker是一种用于识别和提取名词短语(Noun Phrase)的工具或组件。NP chunker的目标是从给定的文本中定位和提取出包含一个或多个名词的短语。名词短语通常由一个名词作为核心词，并可能包含其他修饰词或限定词NP chunker使用一系列语法规则或机器学习模型来识别名词短语的边界，并将它们标记为一个单独的短语单元这个过程有助于更好地理解文本的结构和语义，特别是在文本中涉及到名词短语的关键短语提取任务中。在SIFRank方法中，NP chunker用于提取候选关键短语，并为后续的关键短语排序和评分提供基础SIF模型句子嵌入模型SIF(Smooth Inverse Frequency)是一种用于将句子转换为连续向量表示的方法它旨在捕捉句子的语义信息，并将句子表示为稠密的低维向量。SIF模型的关键思想是结合词频信息来调整词向量的权重，以降低高频词的重要性，同时提高低频词的重要性。这样可以减轻一些常见词对句子表示的影响，使得句子表示更加注重那些在语义上更具区分度的词语。SIF模型通过简单的数学运算，如减法和加权平均，来计算句子的嵌入表示它在自然语言处理任务中被广泛应用，如文本分类、情感分析和句子相似度计算等sentence embedding的假设是：文章是由一个topic生成的，文章中的每个句子亦是如此，因此，句子的embedding应该与文章embedding的期望值(topic embedding)相近Bert-KPECapturing Global Informativeness in Open Domain Keyphrase Extraction 2021BERT-KPE是最近由thunlp提出的方法，在OpenKP和KP20K上都达到了state-of-the-art和良好的鲁棒性有监督的方式KeyBertKeyBERTKeyword Extraction with BERT「关键词」提取都有哪些方案？当我们想要从特定文档中了解关键信息时，通常会使用关键词提取。关键词提取是一种自动化的过程，用于提取与输入文本最相关的单词和短语通过使用Rake和YAKE!等方法，我们已经可以使用易于使用的软件包来提取关键词和关键短语。然而，这些模型通常基于文本的统计特性而不是语义相似性进行工作于是BERT登场。BERT是一个双向变换器模型，可以将短语和文档转化为能够捕捉其意义的向量使用BERT提取文档向量(嵌入)以获取文档级表示。然后，针对N元语法词/短语提取词向量。最后，我们使用余弦相似度来查找与文档最相似的词/短语。然后，可以将最相似的词识定义为最能描述整个文档的词概述什么是KeybertKeybert是一种基于无监督学习的关键词抽取技术，不仅效果好，而且易于使用Keybert主要通过Bert获取文档和候选词的embedding，然后使用余弦相似度计算得到文档中最相似的候选词作为关键词多样性在关键词提取中，多样性问题指的是关键词列表中存在大量相似或重复的关键词，缺乏多样性和代表性。这可能导致关键信息的丢失或重复，并降低关键词提取的效果MSS最大总距离(Max Sum Distance)：通过将文档中最相似的关键词/短语与候选关键词/短语进行组合，找到彼此之间相似性最低的组合，这样可以确保关键词之间的差异性MMR最大边际相关性（Maximal Marginal Relevance，MMR)：通过使用余弦相似度来创建关键词/短语，基于相似性的排序然后，从排序后的结果中选择与文档最相关的关键词/短语，并选择与已选择关键词/短语最不相似的候选关键词/短语，这样可以确保结果具有高度的多样性示例KeyBert Quickstart安装12345678# 默认hugging facepip install keybert# 其他后端pip install keybert[flair]pip install keybert[gensim]pip install keybert[spacy]pip install keybert[use]基础KeyBERT基础用法1234567891011121314151617181920212223242526272829303132333435from keybert import KeyBERTdoc = &quot;&quot;&quot;         Supervised learning is the machine learning task of learning a function that         maps an input to an output based on example input-output pairs.[1] It infers a         function from labeled training data consisting of a set of training examples.[2]         In supervised learning, each example is a pair consisting of an input object         (typically a vector) and a desired output value (also called the supervisory signal).         A supervised learning algorithm analyzes the training data and produces an inferred function,         which can be used for mapping new examples. An optimal scenario will allow for the         algorithm to correctly determine the class labels for unseen instances. This requires         the learning algorithm to generalize from the training data to unseen situations in a         'reasonable' way (see inductive bias).      &quot;&quot;&quot;kw_model = KeyBERT()keywords = kw_model.extract_keywords(doc)# 设置keyphrase_ngram_range来确定生成的关键词/关键短语的长度范围&gt;&gt;&gt; kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 1), stop_words=None)[('learning', 0.4604), ('algorithm', 0.4556), ('training', 0.4487), ('class', 0.4086), ('mapping', 0.3700)]# 要提取关键短语，只需将keyphrase_ngram_range设置为(1, 2)或更高，具体取决于您希望在生成的关键短语中包含的单词数量&gt;&gt;&gt; kw_model.extract_keywords(doc, keyphrase_ngram_range=(1, 2), stop_words=None)[('learning algorithm', 0.6978), ('machine learning', 0.6305), ('supervised learning', 0.5985), ('algorithm analyzes', 0.5860), ('learning function', 0.5850)]# 设置highlight来在文档中突出显示关键词keywords = kw_model.extract_keywords(doc, highlight=True)关键词多样化默认情况下，KeyBERT仅基于余弦相似度比较文档和候选关键词/关键短语。然而，这可能导致非常相似的单词出现在最准确的关键词/关键短语列表中为了确保它们更加多样化，我们可以采取两种方法来微调输出结果，即最大总距离(Max Sum Distance)和最大边际相关性(Maximal Marginal Relevance)最大总距离: 为了使结果多样化，我们选取文档中与前top_n个最相似的单词/短语。然后，我们从这2 x top_n个单词中选取所有top_n个组合，并提取彼此之间最不相似的组合，通过余弦相似度进行比较1234567&gt;&gt;&gt; kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english',                              use_maxsum=True, nr_candidates=20, top_n=5)[('set training examples', 0.7504), ('generalize training data', 0.7727), ('requires learning algorithm', 0.5050), ('supervised learning algorithm', 0.3779), ('learning machine learning', 0.2891)]最大边际相关性: 为了使结果多样化，我们可以使用最大边际相关性（Maximal Marginal Relevance，MMR）来创建关键词/关键短语，它也基于余弦相似度1234567891011121314151617# 具有高多样性的结果kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english',                          use_mmr=True, diversity=0.7)[('algorithm generalize training', 0.7727), ('labels unseen instances', 0.1649), ('new examples optimal', 0.4185), ('determine class labels', 0.4774), ('supervised learning algorithm', 0.7502)]# 具有低多样性的结果&gt;&gt;&gt; kw_model.extract_keywords(doc, keyphrase_ngram_range=(3, 3), stop_words='english',                              use_mmr=True, diversity=0.2)[('algorithm generalize training', 0.7727), ('supervised learning algorithm', 0.7502), ('learning machine learning', 0.7577), ('learning algorithm analyzes', 0.7587), ('learning algorithm generalize', 0.7514)]其他关键词算法生成的候选关键词在某些情况下，您可能希望使用其他关键词算法生成的候选关键词或从可能的关键词/关键短语列表中检索的候选关键词在KeyBERT中，您可以轻松使用这些候选关键词进行关键词提取1234567891011import yakefrom keybert import KeyBERT# Create candidateskw_extractor = yake.KeywordExtractor(top=50)candidates = kw_extractor.extract_keywords(doc)candidates = [candidate[0] for candidate in candidates]# Pass candidates to KeyBERTkw_model = KeyBERT()keywords = kw_model.extract_keywords(doc, candidates=candidates)Guided KeyBERTGuided KeyBERT(引导式KeyBERT)与引导式主题建模类似，它试图将训练引导到一组种子术语上。当应用KeyBERT时，它会自动提取与特定文档最相关的关键词。然而，有时利益相关者和用户正在寻找特定类型的关键词。例如，当通过contentful在您的网站上发布一篇文章时，您通常已经了解与该文章相关的全局关键词。但是，文章中可能存在您希望通过关键词提取出来的特定主题。为了实现这一点，我们只需给KeyBERT提供一组相关的种子关键词(可以是单个关键词)，并搜索与文档和种子关键词都相似的关键词使用这个功能非常简单，只需定义一个种子关键词列表并将其传递给KeyBERT即可123456from keybert import KeyBERTkw_model = KeyBERT()# Define our seeded termseed_keywords = [&quot;information&quot;]keywords = kw_model.extract_keywords(doc, seed_keywords=seed_keywords)当你有一个大型数据集，并且想要微调诸如多样性之类的参数时，每次更改参数时重新计算文档和单词嵌入可能需要很长时间。相反，我们可以预先计算这些嵌入，并将它们传递给.extract_keywords，这样我们只需计算一次即可1234from keybert import KeyBERTkw_model = KeyBERT()doc_embeddings, word_embeddings = kw_model.extract_embeddings(docs)然后，你可以使用这些嵌入并将它们传递给.extract_keywords来加快模型的调整1keywords = kw_model.extract_keywords(docs, doc_embeddings=doc_embeddings, word_embeddings=word_embeddings).extract_embeddings中有几个参数定义了如何生成候选关键词/关键短语的列表：candidateskeyphrase_ngram_rangestop_wordsmin_dfvectorizer这些参数的值在.extract_embeddings和.extract_keywords中需要完全相同，换句话说，以下内容将起作用，因为它们使用相同的参数子集1234567from keybert import KeyBERTkw_model = KeyBERT()doc_embeddings, word_embeddings = kw_model.extract_embeddings(docs, min_df=1, stop_words=&quot;english&quot;)keywords = kw_model.extract_keywords(docs, min_df=1, stop_words=&quot;english&quot;,                                      doc_embeddings=doc_embeddings,                                      word_embeddings=word_embeddings)然而，以下内容将抛出错误，因为我们没有为min_df和stop_words使用相同的值1234567from keybert import KeyBERTkw_model = KeyBERT()doc_embeddings, word_embeddings = kw_model.extract_embeddings(docs, min_df=3, stop_words=&quot;dutch&quot;)keywords = kw_model.extract_keywords(docs, min_df=1, stop_words=&quot;english&quot;,                                      doc_embeddings=doc_embeddings,                                      word_embeddings=word_embeddings)摘要提取等待…生成式 + 抽取式</content>
	</entry>
	<entry>
		<title>LLM模型微调系列</title>
		<url>/article/59381.html</url>
		<content>LLM模型什么是LLM（大语音模型）概述Large Language Model(LLM)，也称为大型语言模型，是一种基于机器学习和自然语言处理技术的模型，它通过对大量的文本数据进行训练，来学习服务人类语言理解和生成的能力LLM的核心思想是通过大规模的无监督训练来学习自然语言的模式和语言结构，这在一定程度上能够模拟人类的语言认知和生成过程与传统的NLP模型相比，LLM能够更好地理解和生成自然文本，同时还能够表现出一定的逻辑思维和推理能力近年来，LLM得到了广泛的应用，其中最具代表性的是谷歌的BERT和OpenAI的GPT系列。这些模型在多个自然语言处理领域已经取得了显著的成果，包括文本分类、命名实体识别、情感分析、机器翻译、自动问答等然而，在实际应用中，LLM面临着更多的挑战首先，LLM需要大量的计算资源和大规模的数据集来训练，这对于一般的企业和个人来说十分困难其次，由于LLM模型的复杂性和计算量较大，对于实时的语言处理应用来说，LLM在应用效率和响应速度上还存在一定的局限性因此，如何解决模型训练和应用过程中的计算性能和效率问题，是LLM面临的主要挑战之一微调LLM大模型低资源微调p tuning v2和lora区别prefix, p-tuningv2, lora finetune该怎么选择?让天下没有难Tuning的大模型：PEFT技术简介 2023-04大模型高效微调综述上：Adapter Tuning、AdaMix、PET、Prefix-Tuning、Prompt Tuning、P-tuning、P-tuning v2大模型高效微调综述下： DiffPruning、BitFit、LoRa、AdaLoRA、MAM Adapters、UniPELT微调(Fine-tuning)是一种常用的技术，用于将预训练的语言模型适应于特定的任务或领域。微调的目的是通过在特定任务上进行有监督的训练，调整模型参数以提高其性能和适应性以下是微调在适应语言模型中的有效性的几个原因：迁移学习：预训练的语言模型在大规模文本数据上进行了无监督的学习，从中学习到了通用的语言表示。通过微调，我们可以将这些通用的语言表示迁移到特定任务或领域上，因此可以利用模型在预训练阶段学到的知识少样本学习：微调通常只需要在特定任务的相对较小的标注数据集上进行训练，而不是从头开始训练一个全新的模型。这对于许多任务来说是非常有益的，因为获得大规模标注数据可能是昂贵或困难的。通过利用预训练模型的泛化能力，微调可以在少量标注样本上实现较好的性能领域自适应：通过微调，可以将语言模型从通用领域适应到特定领域。通过在特定领域的数据上微调，模型可以学习到该领域的特定语言模式、词汇和上下文，从而提高在该领域任务上的性能模型个性化：微调还可以用于个性化模型，以适应特定用户或特定应用场景的需求。通过微调模型，可以根据个体用户的偏好、行为或数据特点进行定制，提供更准确和个性化的预测和推荐微调语言模型是一种有效的方法，可以通过迁移学习、少样本学习、领域自适应和模型个性化等方式，利用预训练模型的优势和泛化能力，提高模型在特定任务或领域上的性能和适应性为什么需要微调高效训练，减少训练成本共享基础大模型，在上面叠加自己的新模型发展脉络Adapter系列AdapterFusion: Non-Destructive Task Composition for Transfer Learning 2021Lexicon Enhanced Chinese Sequence Labeling Using BERT Adapter 2021LLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention 2023LLaMA-Adapter V2: Parameter-Efficient Visual Instruction Model 2023github LLaMA-Adapter: Efficient Fine-tuning of LLaMAp-tunning系列Prefix-Tuning: Optimizing Continuous Prompts for Generation 2021The Power of Scale for Parameter-Efficient Prompt Tuning 2021P-Tuning - GPT Understands, Too 2021P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks 2022lora系列LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS 2021AdaLoRA Adaptive Budget Allocation for Parameter-Efficient Fine-Tuning 2023QLORA: Efficient Finetuning of Quantized LLM 2023另外huggingface很贴心的把常见的fine-Tuning方法都做了集成，只用几行代码就可添加和修改，十分方便，还有微软提供的加速库huggingface官网实现的fine-Tuning方法microsoft/DeepSpeed 加速解密Prompt系列3. 冻结LM微调Prompt: Prefix-Tuning &amp; Prompt-Tuning &amp; P-Tuning微调LM和全部冻结的prompt模板相比，微调Prompt范式最大的区别就是prompt模板都是连续型(Embedding)，而非和Token对应的离散型模板核心在于我们并不关心prompt本身是否是自然语言，只关心prompt作为探针能否引导出预训练模型在下游任务上的特定能力固定LM微调Prompt的范式有以下几个优点性价比高: 微调参数少，冻结LM只微调prompt部分的参数无人工参与: 无需人工设计prompt模板，依赖模型微调即可多任务共享模型: 因为LM被冻结，只需训练针对不同任务的prompt即可。因此可以固定预训练模型，拔插式加入Prompt用于不同下游任务Adapter Tuning预训练模型微调 | 一文带你了解Adapter TuningAdapter Tuning随着计算机硬件性能的提高，预训练模型参数量越来越多，在训练下游任务时进行全模型微调变得昂贵且耗时，Adapter 的出现缓解了这个问题。Adapter在预训练模型每层中插入用于下游任务的参数，在微调时将模型主体冻结，仅训练特定于任务的参数，减少训练时算力开销Adapter模块设计方法2019年，Houlsby N等人将Adapter引入NLP领域，作为全模型微调的一种替代方案。Adapter主体架构下图所示Prefix/Prompt-Tuninghugging face参数高效微调peft源码解析P系列关系：Prefix-Tuning(软提示/连续提示)Prompt-Tuning(软提示/连续提示)(可看做是Prefix-Tuning的简化版本)P-Tuning(软提示/连续提示)P-Tuning V2(软提示/连续提示)(可看做是Prefix-Tuning的优化版本)Prefix Tuning和PTuning V2在实现上基本上是一样的，其实就是一样的下面是peft作者回复的关于Prefix Tuning和PTuning V2在实现上的关系(How to switch to P-Tuning v2)123Hello, those are implemented together. P-Tuning v2 introduced optional parameterization of prompt tokens which you can specify via prefix_projection of PrefixTuningConfig. The other contribution was the ability of work without verbalizers using the linear classification head for NLU tasks whereas Prefix-Tuning paper which focused on NLG didn't focus on this.So, they are supported via the same PrefixEncoder PEFT method另外在peft/peft_model.p的代码中有这样一段(大概1106行)123456789101112131415161718if peft_config.peft_type == PeftType.PREFIX_TUNING:    # PREFIX_TUNING、P_TUNING_V2    past_key_values = self.get_prompt(batch_size)    return self.base_model(        input_ids=input_ids, inputs_embeds=inputs_embeds, past_key_values=past_key_values, **kwargs    )else:    # PROMPT_TUNING、P_TUNING    if inputs_embeds is None:        inputs_embeds = self.word_embeddings(input_ids)    # concat prompt labels    if labels is not None:        prefix_labels = torch.full((batch_size, peft_config.num_virtual_tokens), -100).to(labels.device)        kwargs[&quot;labels&quot;] = torch.cat((prefix_labels, labels), dim=1)    prompts = self.get_prompt(batch_size=batch_size, task_ids=task_ids)    prompts = prompts.to(inputs_embeds.dtype)    inputs_embeds = torch.cat((prompts, inputs_embeds), dim=1)    return self.base_model(inputs_embeds=inputs_embeds, **kwargs)可以看出PREFIX_TUNING生效是通过past_key_values传播的，下面是通过拼接到inputs_embeds上实现的Prefix-Tuning等待…Prefix-Tuning可以理解是CTRL[1]模型的连续化升级版，为了生成不同领域和话题的文本，CTRL是在预训练阶段在输入文本前加入了control code，例如好评前面加’Reviews Rating:5.0’,差评前面加’Reviews Rating:1.0’, 政治评论前面加‘Politics Title:’，把语言模型的生成概率，优化成了基于文本主题的条件概率Prefix-Tuning进一步把control code优化成了虚拟Token，每个NLP任务对应多个虚拟Token的Embedding（prefix），对于Decoder-Only的GPT，prefix只加在句首，对于Encoder-Decoder的BART，不同的prefix同时加在编码器和解码器的开头。在下游微调时，LM的参数被冻结，只有prefix部分的参数进行更新。不过这里的prefix参数不只包括embedding层而是虚拟token位置对应的每一层的activation都进行更新Prompt-Tuninghttps://github.com/google-research/prompt-tuning等待…Prompt-Tunning是以上prefix-Tunning的简化版本，面向NLU任务，进行了更全面的效果对比，并且在大模型上成功打平了LM微调的效果对比Prefix-Tunning，prompt-tuning的主要差异如下:论文使用100个prefix token作为默认参数，大于以上prefix-tuning默认的10个token，不过差异在于prompt-Tunning只对输入层(Embedding)进行微调，而Prefix是对虚拟Token对应的上游layer全部进行微调。因此Prompt-Tunning的微调参数量级要更小，且不需要修改原始模型结构，这是“简化”的来源。相同的prefix长度，Prompt-Tunning(&lt;0.01%)微调的参数量级要比Prefix-Tunning(0.1%~1%)小10倍以上P-TuningP-Tuning V1github THUDM/P-tuning手动尝试最优的提示无异于大海捞针，于是便有了自动离散提示搜索的方法(左图)，但提示是离散的，神经网络是连续的，所以寻找的最优提示可能是次优的。p-tuning依然是固定LLM参数，利用多层感知机和LSTM对prompt进行编码，编码之后与其他向量进行拼接之后正常输入LLM。注意，训练之后只保留prompt编码之后的向量即可，无需保留编码器动机一个刻板印象是GPT不适合理解类任务，这篇就是去思考这种刻板印象是否正确GPT-3采用人工构造的模版来做in context learning，人工设计的模版的变化特别敏感，加一个词或者少一个词，或者变动位置啥的都会造成比较大的变化（这里作者做了一个简单的验证实验，具体看论文）。近来的自动化搜索模版工作成本也比较高，同时以前这种离散化的token的搜索出来的结果可能并不是最优的和prefix-tuning差不多，反正是基于这两点去设计了一种连续可微的模版相比prefix-tuning，这里加了可微的virtual token，但是仅限于输入，没有在每层加；另外virtual token的位置也不一定是前缀，插入的位置是可选的。这里的出发点实际是把传统人工设计模版中的真实token替换成可微的virtual tokenP-Tuning V2github THUDM/P-tuning-v2P-tuning V2论文和代码实现详解chatGLM的浅薄解析 P-tuning V2大模型参数高效微调技术原理综述（三）-P-Tuning、P-Tuning v2可以简单的将P-Tuning认为是针对Prompt Tuning的改进，P-Tuning v2认为是针对Prefix Tuning的改进概述代码示例PrefixEncoder类，为了获得连续prompt，设计的模块12345678910111213141516171819202122232425262728293031323334import torchclass PrefixEncoder(torch.nn.Module):    r'''    The torch.nn model to encode the prefix    Input shape: (batch-size, prefix-length)    Output shape: (batch-size, prefix-length, 2*layers*hidden)    '''    def __init__(self, config):        super().__init__()        self.prefix_projection = config.prefix_projection        if self.prefix_projection:            # Use a two-layer MLP to encode the prefix            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.hidden_size)            # 初始化重参数化的编码器            self.trans = torch.nn.Sequential(                torch.nn.Linear(config.hidden_size, config.prefix_hidden_size),                torch.nn.Tanh(),                torch.nn.Linear(config.prefix_hidden_size, config.num_hidden_layers * 2 * config.hidden_size)            )        else:            self.embedding = torch.nn.Embedding(config.pre_seq_len, config.num_hidden_layers * 2 * config.hidden_size)    def forward(self, prefix: torch.Tensor):        if self.prefix_projection:            prefix_tokens = self.embedding(prefix)            past_key_values = self.trans(prefix_tokens)        else:            past_key_values = self.embedding(prefix)        return past_key_values源码也可以看到 Prefix Tuning 与 P-Tuning v2 最主要的差别就是是否进行重新参数化编码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192class BertPrefixForTokenClassification(BertPreTrainedModel):    def __init__(self, config):        super().__init__(config)        self.num_labels = config.num_labels        self.bert = BertModel(config, add_pooling_layer=False)        self.dropout = torch.nn.Dropout(config.hidden_dropout_prob)        self.classifier = torch.nn.Linear(config.hidden_size, config.num_labels)        from_pretrained = False        if from_pretrained:            self.classifier.load_state_dict(torch.load('model/checkpoint.pkl'))        for param in self.bert.parameters():            param.requires_grad = False        self.pre_seq_len = config.pre_seq_len        self.n_layer = config.num_hidden_layers        self.n_head = config.num_attention_heads        self.n_embd = config.hidden_size // config.num_attention_heads        self.prefix_tokens = torch.arange(self.pre_seq_len).long()        self.prefix_encoder = PrefixEncoder(config)        bert_param = 0        for name, param in self.bert.named_parameters():            bert_param += param.numel()        all_param = 0        for name, param in self.named_parameters():            all_param += param.numel()        total_param = all_param - bert_param        print('total param is {}'.format(total_param))  # 9860105    def get_prompt(self, batch_size):        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(self.bert.device)        # 得到连续Prompt        past_key_values = self.prefix_encoder(prefix_tokens)        # bsz, seqlen, _ = past_key_values.shape        # 改变形状        past_key_values = past_key_values.view(            batch_size,            self.pre_seq_len,            self.n_layer * 2,            self.n_head,            self.n_embd        )        past_key_values = self.dropout(past_key_values)        # 改变形状，划分成数组。每一个数组元素形状为：(2,batch_size,n_head,seq_len,head_dim)        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)        return past_key_values    def forward(            self,            input_ids=None,            attention_mask=None,            token_type_ids=None,            position_ids=None,            head_mask=None,            inputs_embeds=None,            labels=None,            output_attentions=None,            output_hidden_states=None,            return_dict=None,    ):        return_dict = return_dict if return_dict is not None else self.config.use_return_dict        batch_size = input_ids.shape[0]        past_key_values = self.get_prompt(batch_size=batch_size)        prefix_attention_mask = torch.ones(batch_size, self.pre_seq_len).to(self.bert.device)        attention_mask = torch.cat((prefix_attention_mask, attention_mask), dim=1)        # 开始传递past_key_values        outputs = self.bert(            input_ids,            attention_mask=attention_mask,            token_type_ids=token_type_ids,            position_ids=position_ids,            head_mask=head_mask,            inputs_embeds=inputs_embeds,            output_attentions=output_attentions,            output_hidden_states=output_hidden_states,            return_dict=return_dict,            past_key_values=past_key_values,        )        ...        return TokenClassifierOutput(            loss=loss,            logits=logits,            hidden_states=outputs.hidden_states,            attentions=outputs.attentions,        )一次前向计算中，P-tuning v2会通过self.get_prompt(batch_size=batch_size)得到要连续PromptBertEncoder会执行for循环，把past_key_values拆分到一个个BertLayer12345678910111213self.layer = nn.ModuleList([BertLayer(config) for _ in range(config.num_hidden_layers)])...for i, layer_module in enumerate(self.layer):if output_hidden_states:    all_hidden_states = all_hidden_states + (hidden_states,)    layer_head_mask = head_mask[i] if head_mask is not None else Nonepast_key_value = past_key_values[i] if past_key_values is not None else None...# BertLayerlayer_module(..., past_key_value, ...)巧妙的利用past_key_values参数，将past_key_values数组中每一个元素，拼接到BertSelfAttention中Key和Value代码跟踪链路BertModel -&gt; BertEncoder -&gt; BertLayer -&gt; BertAttention -&gt; BertSelfAttention1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class BertSelfAttention(nn.Module):...    def transpose_for_scores(self, x: torch.Tensor) -&gt; torch.Tensor:        # 将张量转换形状，调换维度。这个代码会在seq_length维度进行拼接，其他维度不可动        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)        x = x.view(new_x_shape)        return x.permute(0, 2, 1, 3)        def forward(        self,        hidden_states: torch.Tensor,        attention_mask: Optional[torch.FloatTensor] = None,        head_mask: Optional[torch.FloatTensor] = None,        encoder_hidden_states: Optional[torch.FloatTensor] = None,        encoder_attention_mask: Optional[torch.FloatTensor] = None,        past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,        output_attentions: Optional[bool] = False,    ) -&gt; Tuple[torch.Tensor]:        mixed_query_layer = self.query(hidden_states)        # If this is instantiated as a cross-attention module, the keys        # and values come from an encoder; the attention mask needs to be        # such that the encoder's padding tokens are not attended to.        is_cross_attention = encoder_hidden_states is not None        if is_cross_attention and past_key_value is not None:            # reuse k,v, cross_attentions            key_layer = past_key_value[0]            value_layer = past_key_value[1]            attention_mask = encoder_attention_mask        elif is_cross_attention:            key_layer = self.transpose_for_scores(self.key(encoder_hidden_states))            value_layer = self.transpose_for_scores(self.value(encoder_hidden_states))            attention_mask = encoder_attention_mask        elif past_key_value is not None:            key_layer = self.transpose_for_scores(self.key(hidden_states))            value_layer = self.transpose_for_scores(self.value(hidden_states))            key_layer = torch.cat([past_key_value[0], key_layer], dim=2)            value_layer = torch.cat([past_key_value[1], value_layer], dim=2)        else:            key_layer = self.transpose_for_scores(self.key(hidden_states))            value_layer = self.transpose_for_scores(self.value(hidden_states))        query_layer = self.transpose_for_scores(mixed_query_layer)                ...这里就会把past_key_value拼接到了原始的k、v上面，这样子就相当于给k、v添加了额外需要学习的参数了，再微调时只更新这部分新的参数即可P-tuning V2连续Prompt代码实现仿真代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v1.0@author: huangyc@file: p_tuning_test.py@Description: @time: 2023/6/6 15:31&quot;&quot;&quot;import torchfrom torch import nndef run():    def transpose_for_scores(x: torch.Tensor) -&gt; torch.Tensor:        new_x_shape = x.size()[:-1] + (12, 64)        x = x.view(new_x_shape)        return x.permute(0, 2, 1, 3)    prompt = torch.rand(32, 128, 48, 12, 64)  # batch_size, seq_len, num_layer*2, num_head, head_size    prompt = prompt.permute([2, 0, 3, 1, 4])    print(f&quot;P-tuningV2构造的trainable continuous embeddings形状：{prompt.shape}&quot;)    past_key_values = prompt.split(2)    num_layers = 24    hidden_dim = 768    n_head = 12    head_dim = hidden_dim // n_head    all_head_size = n_head * head_dim    hidden_states = torch.randn(32, 128, 768)  # batch_size, seq_len, hidden_size    print(f&quot;输入的向量形状：{hidden_states.shape}&quot;)    for i in range(num_layers):        past_key_value = past_key_values[i]        print(f&quot;每一层BertLayer需要加入的prompt形状: {past_key_value.shape}&quot;)        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None        # BertSelfAttention        query = nn.Linear(hidden_dim, all_head_size)        key = nn.Linear(hidden_dim, all_head_size)        value = nn.Linear(hidden_dim, all_head_size)        # 原始kv的大小        key_layer = transpose_for_scores(key(hidden_states))        old_key_layer_shape = key_layer.shape        print(f&quot;经过transpose_for_scores后的key形状：{old_key_layer_shape}&quot;)        value_layer = transpose_for_scores(value(hidden_states))        old_value_layer_shape = value_layer.shape        print(f&quot;经过transpose_for_scores后的value形状：{old_value_layer_shape}\n&quot;)        # 拼接后kv的大小        key_layer = torch.cat([past_key_value[0], key_layer], dim=2)        print(            f&quot;past_key_value[0]的形状：{past_key_value[0].shape} 原始key_layer的形状：{old_key_layer_shape} 经过cat后的key_layer形状：{key_layer.shape}&quot;)        value_layer = torch.cat([past_key_value[1], value_layer], dim=2)        print(            f&quot;past_key_value[1]的形状：{past_key_value[1].shape} 原始value_layer的形状：{old_value_layer_shape} 经过cat后的value_layer形状：{value_layer.shape}\n&quot;)        mixed_query_layer = query(hidden_states)        print(f&quot;hidden_states经过query层后输出的形状：{mixed_query_layer.size()}&quot;)  # batch seq len embed        query_layer = transpose_for_scores(mixed_query_layer)        print(f&quot;经过transpose_for_scores后的query形状{query_layer.size()}&quot;)  # batch        print(&quot;注意力分数开始计算&quot;)        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))        print(f&quot;attention_scores的形状：{attention_scores.size()}&quot;)  # batch head seq_len seq_len        print(&quot;开始注意力汇聚计算&quot;)        context_layer = torch.matmul(attention_scores, value_layer)        print(f&quot;注意力汇聚后输出矩阵context_layer的形状：{context_layer.size()}&quot;)  # batch head seq_len embed/12        print(&quot;最后，将context_layer的形状恢复成输入hidden_states的形状&quot;)        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()        new_context_layer_shape = context_layer.size()[:-2] + (768,)        context_layer = context_layer.view(new_context_layer_shape)        print(f&quot;context_layer的形状恢复完成，其形状为：{context_layer.size()}&quot;)        print(&quot;一次P-tuningV2的BertLayer计算仿真结束&quot;)        breakif __name__ == '__main__':    run()测试输出123456789101112131415161718192021S:\Anaconda3\envs\torch38\python.exe Q:\pyCharmWS\chatgpts\P-tuning-v2\tests\p_tuning_test.py P-tuningV2构造的trainable continuous embeddings形状：torch.Size([48, 32, 12, 128, 64])输入的向量形状：torch.Size([32, 128, 768])每一层BertLayer需要加入的prompt形状: torch.Size([2, 32, 12, 128, 64])经过transpose_for_scores后的key形状：torch.Size([32, 12, 128, 64])经过transpose_for_scores后的value形状：torch.Size([32, 12, 128, 64])====================&gt; 核心past_key_value[0]的形状：torch.Size([32, 12, 128, 64]) 原始key_layer的形状：torch.Size([32, 12, 128, 64]) 经过cat后的key_layer形状：torch.Size([32, 12, 256, 64])past_key_value[1]的形状：torch.Size([32, 12, 128, 64]) 原始value_layer的形状：torch.Size([32, 12, 128, 64]) 经过cat后的value_layer形状：torch.Size([32, 12, 256, 64])====================&gt; 核心hidden_states经过query层后输出的形状：torch.Size([32, 128, 768])经过transpose_for_scores后的query形状torch.Size([32, 12, 128, 64])注意力分数开始计算attention_scores的形状：torch.Size([32, 12, 128, 256])开始注意力汇聚计算注意力汇聚后输出矩阵context_layer的形状：torch.Size([32, 12, 128, 64])最后，将context_layer的形状恢复成输入hidden_states的形状context_layer的形状恢复完成，其形状为：torch.Size([32, 128, 768])一次P-tuningV2的BertLayer计算仿真结束LORA系列点赞👍B站博主 小杨不努力0v0 + 博主相关的文章链接LORA(转)LoRA：训练你的GPT【论文粗读·1】一种通过低秩近似增量矩阵的，经过广泛验证足够Robust的微调方法摘要随着自然语言处理(NLP)模型规模的不断增长，由于成本和资源限制，对其进行完全微调以用于下游任务的挑战日益增加介绍低秩适应(Low-Rank Adaptation)。LoRA通过引入参数矩阵来减少参数，并将GPU内存需求降低了3倍。相比于使用GPT-3进行微调，它将参数减少了10,000倍尽管可训练参数更少，但LoRA在大多数语言模型上表现优于微调，具有更高的训练吞吐量和无推理延迟对语言模型适应中的秩缺失进行的实证研究为LoRA的有效性提供了证据，LoRA是开源的介绍对于下游任务而言，完全微调大型语言模型是具有挑战性的。 受到[内在维度]的研究启发，我们提出了LoRA，具有以下优势：低任务切换开销: 一个预训练模型可以被共享并用于构建多个针对不同任务的小型LoRA模块参数高效: LoRA通过使用自适应优化器，在训练过程中使得训练更高效，并将硬件门槛降低了最多3倍无推理延迟: LoRA的简单线性设计使得可训练矩阵在部署时可以与冻结权重合并正交性: LoRA与许多先前的方法是正交的，并且可以与它们结合使用，比如前缀微调(prefix-tuning)问题描述模型训练时参数量评估，对于LLM，如果模型的参数时的话全量微调使用Adam优化器下的，，并且使用混合精度，一个参数需要16个bytes来存储，这16个bytes分别为权重需要来存储，激活值需要，为了更新权重还需存一个复制需要，优化器需要存两个值，分别是和(方差)，分别需要其中占2bytes，占bytes，总共为bytes因此，个参数就需要bytesLoraLora在训练时，将原来的参数固定下来，只更新新增加的参数，因此Mem Required:  bytes在GPT-3的175B参数下，这里的可以达到原来的为什么会提出Lora呢原来的Adapter方法也是固定模型的参数，只训练MLP参数，但是这样子有几个弊端增加了网络深度，增加了推理的时间添加MLP之后，训练出来的最优方案也只能收敛到MLP层，不一定是全局最好的直接去优化这个Promote并不能保证优化是单调的，也就是不是全局最优，很难优化好减少了可用于处理下游任务的序列长度，因为新加入的Promote会占用输入的token长度具体方法最核心的思路如下公式所示，研究表明通常是一个欠秩的矩阵因此，可以进行低秩分解训练时，初始化为全零矩阵，这样子参数量就从变成了，这里的一般是远小于的优点LoRA是对完全微调的一种推广方法在适应过程中，LoRA不需要对权重矩阵进行完全秩的累积梯度更新，而是可以基于预训练的权重矩阵设置秩当LoRA应用于所有权重矩阵并且偏置进行训练时，这种方法提供了类似于完全微调的表现能力没有额外的推理延迟在生产部署中，LoRA可以计算和存储，其中和属于，当切换到另一个下游任务时，可以通过减去并加上不同的来恢复，这是一个快速操作，几乎没有额外的内存开销(潮汐GPU)为什么低秩矩阵有效当给定参数数量时，应该调整预训练Transformer中的哪些具体权重矩阵子集以实现最佳的下游性能？在给定参数预算的情况下，确定要调整的权重矩阵子集以实现最佳下游性能是一个复杂的问题，并且没有固定的答案通常，可以考虑根据下游任务的特点和需求进行权衡和选择一种常见的方法是通过对不同权重矩阵进行实验性微调，并根据性能评估来确定适合特定任务的权重矩阵子集最优的适应矩阵是否真的是欠秩的吗？如果是，那么在实际情况下推荐的秩是多少？最优的适应矩阵是否真的是欠秩的，这取决于具体情况。秩缺失意味着矩阵的秩(矩阵的线性独立列数或行数的最大数量)较低对于实际目的，建议选择适当的秩以平衡模型性能和计算成本。具体推荐的秩取决于任务的复杂性、数据集的规模以及可用的计算资源等因素和之间的关系是什么？和之间是否存在高相关性？的大小与相比如何？表示适应矩阵，用于调整预训练权重矩阵，和之间的关系取决于具体的适应方法和优化算法。在某些情况下，可以通过对的微小调整来获得，而在其他情况下，可能包含更大的变化和之间的相关性取决于适应方法的设计和优化过程的细节。它们可以存在一定的相关性，但具体情况可能因模型架构、任务要求和数据集特征而异的大小与的大小之间没有固定的比较关系，因为它们的尺度取决于具体的数值范围和调整方法对于attention参数附加到哪个上更有效实验在GPT-3 175B模型上设置了一个参数预算为18M(如果以FP16存储，大约为35MB)。这对应于当我们适应一种注意力权重时r=8，或者当我们适应两种类型时r=4，适用于所有96层需要注意的是，将所有参数放在或中会导致显著降低性能，而注入到和则产生最佳结果。这表明，即使秩为4，中包含了足够的信息，使得与使用具有较大秩的单一类型的权重相比，使用更多的权重矩阵更可取回答：将参数设置到q、v上时，r取多少合适LoRA在非常小的秩(特别是对于，而言)下已经取得了相当的竞争力，这表明更新矩阵可能具有非常小的内在秩，使用低秩矩阵对LLM进行fine tune的时候，可以用一个非常小的低秩矩阵，就可以捕捉到对下游任务的一些特征信息，这为我们提供了一个非常高效的LLM的fine tune的方式，同时提高了下游任务的性能回答：和之间的关系是什么首先对进行奇异值分解，并且把它左奇异值向量和右奇异值向量乘到上，把映射到的子空间，并计算范数，同时还把映射到映射到随机矩阵上以此来证明与有更强的相关性首先，与随机矩阵相比，与具有更强的相关性，表明放大了预训练模型中的中已经存在的某些特征其次，不是重复的奇异值方向，而是只放大在中没有强调的方向第三，放大因子相当巨大：当r=4时，21.5≈6.91/0.32，表明只是放大了中的一些特征，且放大倍数是很大的，相当于是把下游任务需要的特征提取出来并进行放大AdaLoRA(转)AdaLoRA：更强大的LoRAgithub cauyxy/YourGPTLora中的r是一个确定值，但不是对于所有的层，像q、k这样的层，q内在秩比较大，v内在秩比较小，对于不同的矩阵应该使用不同的内在秩AdaLoRA是在对同样的参数量下，对不同的矩阵使用不同的r，通过奇异值分解，判断r的大小，来取得更好的效果提出问题在NLP领域，对于下游任务进行大型预训练语言模型的微调已经成为一种重要的做法。一般而言，我们会采用对原有的预训练模型进行全量微调的方法来适配下游任务，但这种方法存在两个问题训练阶段: 对于预训练模型进行微调的时候，为了更新权重参数，需要大量的显存来存储参数的梯度和优化器信息，在当今预训练模型的参数变得越来越大的情况下，针对下游任务微调门槛变得越来越高推理阶段: 由于我们训练的时候是对于模型参数进行全量的更新，所以多个下游任务需要为每个任务维护一个大型模型的独立副本，这样就导致我们在实际应用的时候浪费了不必要的存储现有方法: 为了解决这些问题，研究者提出了两个主要研究方向，以减少微调参数的数量，同时保持甚至提高预训练语言模型的性能添加小型网络模块将小型网络模块添加到PLMs中，保持基础模型保持不变的情况下仅针对每个任务微调这些模块，可以用于所有任务。这样，只需引入和更新少量任务特定的参数，就可以适配下游的任务，大大提高了预训练模型的实用性，方法示例Adapter tuning：是在基础模型的各层之间插入小型神经模块Prefix tuning：将可训练的前缀标记附加到基础模型的输入或隐藏层上Prompt Tuning: 修改模型的输入，在模型输入的前面加一些特定的前缀可行之处：可以达到与完全微调几乎相当的性能，同时仅更新不到原始模型参数的1％，大大减少了内存消耗。存在问题：Adapter tuning：引入了推理延迟，最终收敛到适配器层Prefix or Prompt tuning：直接优化Prefix和Prompt是非单调的，比较难收敛，并且消耗了输入的token下游任务增量更新对预训练权重的增量更新进行建模，而无需修改模型架构方法示例：Diff pruning：将初始化为与相同的维度，然后根据参数的大小按元素对进行剪枝LoRA：通过两个小得多的矩阵的乘积将参数化为低阶矩阵可行之处：可以达到与完全微调几乎相当的性能存在问题:Diff pruning：需要底层实现来加速非结构化稀疏矩阵的计算，不能直接使用现有的框架训练过程中需要存储完整的矩阵，相比于Full finetune并没有降低计算成本LoRA：预先指定每个增量矩阵的内在秩r相同，忽略了在微调预训练模型时，权重矩阵的重要性在不同模块和层之间存在显著差异只训练了self-attention，没有训练feed-forward networks，事实上FFN更重要问题总结不能预先指定矩阵的秩，需要动态更新增量矩阵的R权重矩阵的重要性在不同模块和层之间存在显著差异需要找到更加重要的矩阵，分配更多的参数，裁剪不重要的矩阵找到重要的矩阵，提升模型效果裁剪不重要的矩阵，降低参数计算量，降低模型效果差的风险解决方案目标：在类似LoRA的微调过程中动态分配参数预算给权重矩阵调整增量矩阵的秩来控制预算分配。AdaLoRA将关键的增量矩阵分配高秩以捕捉更精细和任务特定的信息，而将较不重要的矩阵的秩降低以防止过拟合并节省计算预算采用参数化矩阵来模拟SVD，并舍弃不重要的奇异值，同时保留奇异向量。由于对一个大矩阵进行精确SVD分解的计算消耗非常大，这种方法可以加速计算，同时保留未来恢复的可能性并稳定训练在训练损失中添加了额外的惩罚项，以规范奇异矩阵P和Q的正交性，从而避免SVD的大量计算并稳定训练SVD-BASED ADAPTATION如上所述，我们把增量矩阵做一个奇异值分解的近似，即，对矩阵更新的描述则有如下表示为了保证和的正交性，即，我们提出如下所示的正则损失为什么不直接在原来的BA上进行修剪？当一对奇异向量被认为为不重要时，我们必须修剪它的所有元素。这就导致几乎不可能重新激活修剪过的奇异向量，因为它们的元素都被清零并且不再训练与之对比，AdaLoRA只是Mask了奇异值LoRA的A和B不是正交的，这意味着奇异向量可以相互依赖。 与截断最小的奇异值相比，丢弃奇异向量可能会导致原始矩阵发生更大的变化。因此，在分配完秩的每一步之后，增量矩阵通常会发生更多不可预测的显著变化，这导致训练不稳定，甚至损害模型的效果IMPORTANCE-AWARE RANK ALLOCATION我们将基于SVD的秩调整应用于每个权重矩阵，包括每个transformer层的和。为了控制参数预算，我们在训练期间根据重要性得分迭代修剪奇异值为了更好地表示，我们用来索引增量矩阵 for ，用来表示第个矩阵的奇异值，奇异向量三元组， 来表示这个三元组的重要性而作为参数训练的代价，同时加上正则化项，就得出了如下的目标函数: (是正则化系数)我们在训练的时候就可以通过梯度下降的方式对进行更新，下面是的例子然后我们再基于对进行裁剪其中包含所有三元组的重要性分数。是第步剩余奇异值的预算通过这种方式，我们通过修剪不太重要的奇异值，将更多预算留给优先级较高的增量矩阵Magnitude of singular values这样的话只有最小的奇异值以及最不重要的奇异向量被去弃。它最大限度地减小了与原始矩阵的偏差，进一步稳定了训练。但是这个度量不能正确量化参数(三元组)对模型性能的贡献Sensitivity-based importance之前的工作利用灵敏度来量化单个参数的重要性，并据此对参数进行非结构化修剪。在我们的例子上，我们必须设计一个新的度量标准，因为三元组要被按组丢弃了，所以每一项的敏感性都应该被考虑，并适当地组合起来，以量化三元组对模型性能的整体贡献我们设计了如下所示的函数来计算importance score我们可以采用的灵敏度，定义为梯度权重乘积的大小:本质上近似于参数归零时的损失变化。如果去除一个参数影响较大，则模型对该参数敏感，我们应该保留它但之前的工作指出，直接计算的敏感性还不是一个可靠的重要指标。这样的分数是在抽样的minibatch上估计的。随机采样和复杂的训练动态导致灵敏度估计的变异性大，不确定性大，这样可能会导致对于参数的重要性的错误估计。提出通过灵敏度平滑和不确定性量化，加入累计灵敏度的影响来解决这一问题:接下来，我们把定义为和的乘积这样，我们就得到了一个既考虑了三元组所有元素，又考虑了累计灵敏度足够平滑的一个重要性函数GLOBAL BUDGET SCHEDULER在低秩自适应的情况下，调整秩自然是为了控制参数预算。因此，我们将预算定义为所有增量矩阵的总秩，即总奇异值的数量回想一下，预算分配是在微调期间迭代执行的。为了便于训练，我们提出了一个全局预算调度器。具体来说，我们从略高于目标预算的初始预算算开始(例如，的1.5倍)我们将每个增量矩阵的初始秩设为。我们对步进行warmup，然后按照三次计划减少预算，直到达到最后，我们得到的修正完预算分布，并对步骤的模型进行了微调这使得AdaLoRA可以先探索参数空间，然后再关注最重要的权重实验验证QLORAFineTune -&gt; P_tuning -&gt; P_tuning V2 -&gt; LoRA -&gt; QLoRABERT AdapterRLHF入门】大语言模型常用微调框架介绍｜LoRA&amp;Prefix-Tuning&amp;Prompt-Tuning&amp;P-Tuning v2&amp;RLHF微调原理简介RLHF: Reinforcement Learning from Human。Feedback，即基于人工反馈机制的强化学习。最早与2022年4月，由OpenAI研究团队系统总结并提出.并在GPT模型的对话类任务微调中大放异彩，被称为ChatGPT背后的功臣RLHF也是目前为止常用的、最为复杂的基于强化学习的大语言模型微调方法，目前最好的端到端RLHF实现是DeepSpeedChat库，由微软开源并维护基于强化学习的进阶微调方法RLHF方法论文地址: https://arxiv.org/abs/2203.02155步骤1: 监督微调 (SFT)-一 使用精选的人类回答来微调预训练的语言模型以应对各种查询步骤2:奖励模型微调 — 使用一个包含人类对同一查询的多个答案打分的数据集来训练一个独立的 (通常比 SFT 小的) 奖励模型 (RW) 步骤3: RLHF 训练 —利用 Proximal Policy Optimization (PPO) 算法根据 RW 模型的奖励D九天Hector反馈进一步微调 SFT 模型。Flash_Atten(转)前置知识 GPU Arch:自顶向下分析 + B站 GPU Arch：自顶向下分析【浅谈底层·1】随着人工智能特别是以GPT为代表的生成式AI的迅猛发展，GPU已经成为了一种不可或缺的工具，甚至企业都以拥有多少高端GPU作为抓住风口能力的衡量标准。相比之下，CPU虽然在传统计算领域占据主导地位，但在处理AI任务时却不及GPU出色为什么AI计算通常选择GPU而不是CPU，分析GPU在AI计算中的优势，同时，从底层原理探讨从Volta到最新的Hopper四代NVIDIA GPU架构的演进，展示其不断提升的性能和功能GPU主要由计算单元ALU组成。CPU不仅被Cache占据了大量空间，而且还有有复杂的控制逻辑和诸多优化电路，相比之下，计算能力只是CPU很小的一部分通过上面自顶向下的分析，我们知道，对于GPU中的存储部分访问速度由快到慢，计算部分从大到小排列为NVLink是什么？为什么需要他？大模型通常具有巨大的参数数量和复杂的结构，需要处理大量的数据。分布式训练将这些大型模型分割成多个部分，由多个GPU或计算节点并行处理，每个部分处理自己的数据子集。然后通过全局通信，参数同步等方式进行梯度传播，此时GPU之间的通信带宽就变的越来越重要在NVLink出现之前，GPU与GPU之间的数据交互通过PCIe（Peripheral Component Interconnect Express）总线进行。但PCIe存在两个问题，一是PCIe总线的带宽相对有限，其中PCIe 4.0x16的最大带宽也就64GB/s，二是PCIe总线的延迟相对较高，在GPU之间传输数据时，每次数据传输都需要通过CPU和主机内存来完成。这种传输路径会导致额外的延迟，并降低数据传输的效率。然而，深度学习应用中需要更高的带宽和更低的延迟，PCIe显然是无法满足当下的神经网络训练需求NVLink利用高带宽、低延迟的通信通道，直接将多个GPU连接在一起，实现快速、高效的数据传输和共享。通过NVLink，GPU之间的数据交互可以直接在GPU之间进行，而无需通过CPU和主机内存。这种直接内存访问（DMA）的方式大大减少了数据传输的复制和延迟，提高了数据共享的效率。此外，NVLink还提供了一致的内存空间，使得多个GPU能够共享同一份内存，简化了程序设计和数据管理的复杂性概述FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness Paper 2022FlashAttention: 更快训练更长上下文的GPTTransformer作为GPT类模型的基础架构提供了强大的特征处理能力，但是处理更长上下文仍然是一个挑战，因为核心的自注意力模块在序列长度上具有O(N^2)的时间和内存复杂度😓这篇Flash Attention的工作深入硬件，新提出了一种具有IO感知的，快速的⚡️，节省内存的🧠，精确的🎯注意力算法。目前，Flash Attention已经集成至torch2.0，并且社区也提供了多种实现78s看懂FlashAttention【有点意思·1】 核心要点⚡️为什么加快了计算？Fast降低了耗时的HBM访问次数。采用Tiling技术分块从HBM加载数据到SRAM进行融合计算🧠为什么节省了内存？Memory-Efficient不再对中间矩阵S，P进行存储。在反向的时候通过Recomputation重新计算来计算梯度🎯为什么是精准注意力？Exact Attention算法流程只是分块计算，无近似操作提出问题Transformer结构已成为自然语言处理和图像分类等应用中最常用的架构。尽管Transformer在规模上不断增大和加深，但处理更长上下文仍然是一个挑战，因为核心的自注意力模块在序列长度上具有二次方的时间和内存复杂度。这导致在处理长序列时速度变慢且内存需求巨大。因此，我们需要一些优化算法来提高注意力模块的计算速度和内存利用率解决方案ForwardStandard Attention在注意力的一般实现中，对 三个输入执行以下算法得到输出，其中softmax行级别执行在这个算法中，，矩阵都是很大，需要在HBM中实例化来进行存储，这样就会带来很多HBM的访问次数， 最终体现到算法时间端到端较长的延迟FlashAttention(Tiling)理论基础在传统算法中，一种方式是将Mask和SoftMax部分融合，以减少访存次数。然而，FlashAttention则更加激进，它将从输入到输出的整个过程进行融合，以避免，矩阵的存储开销，实现端到端的延迟缩减。然而，由于输入的长度通常很长，无法完全将完整的及中间计算结果存储在SRAM中。因此，需要依 赖HBM进行访存操作，与原始计算延迟相比没有太大差异，甚至会变慢(没具体测)为了让计算过程的结果完全在SRAM中，摆脱对HBM的依赖，可以采用分片操作，每次进行部分计算，确保这些计算结果能在SRAM内进行交互，待得到对应的结果后再进行输出这个过程中，有一点需要注意的是，之前对于softmax的计算是以行为单位的，如下所示:当我们将输入进行分片后，无法对完整的行数据执行Softmax操作。这是因为Softmax函数在计算时需要考虑整个行的数据然而，我们可以通过如下所示方法来获得与完整行Softmax相同的结果，而无需使用近似操作具体的分块softmax代码演示12345678910111213141516171819202122232425262728293031323334353637383940import torchq = torch.tensor([1,2]).float()v = torch.tensor([1,2]).float()q_sm = torch.softmax(q, 0)print(q_sm)   # tensor([0.2689, 0.7311])torch.dot(q_sm, v)  # tensor(1.7311)m_pre = float(&quot;-inf&quot;)l_pre = 0cur_sum = 0block1 = torch.tensor([1]).float()# get cur max valuem_cur = max(torch.max(block1), m_pre)# scale pre log value by max expl_pre *= torch.exp(m_pre - m_cur)# calculate current log sump = torch.exp(block1 - m_cur)l_cur = torch.sum(p) + l_pre# scale pre result by log sumcur_sum = cur_sum * l_pre / l_curp = p / l_curcur_sum = 1 * p[0]l_pre = l_curm_pre = m_curprint(cur_sum)   # tensor(1.)block2 = torch.tensor([2]).float()m_cur = max(torch.max(block2), m_pre)l_pre *= torch.exp(m_pre - m_cur)p = torch.exp(block2 - m_cur)l_cur = torch.sum(p) + l_precur_sum = cur_sum * l_pre / l_curp = p / l_curcur_sum += 2 * p[0]print(cur_sum)   # tensor(1.7311)代码实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374@triton.jitdef _fwd_kernel(    Q, K, V, sm_scale,    L, M,    Out,    stride_qz, stride_qh, stride_qm, stride_qk,    stride_kz, stride_kh, stride_kn, stride_kk,    stride_vz, stride_vh, stride_vk, stride_vn,    stride_oz, stride_oh, stride_om, stride_on,    Z, H, N_CTX,    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,    BLOCK_N: tl.constexpr,):    start_m = tl.program_id(0)    off_hz = tl.program_id(1)    # initialize offsets    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)    offs_n = tl.arange(0, BLOCK_N)    offs_d = tl.arange(0, BLOCK_DMODEL)    off_q = off_hz * stride_qh + offs_m[:, None] * stride_qm + offs_d[None, :] * stride_qk    off_k = off_hz * stride_qh + offs_n[None, :] * stride_kn + offs_d[:, None] * stride_kk    off_v = off_hz * stride_qh + offs_n[:, None] * stride_qm + offs_d[None, :] * stride_qk    # Initialize pointers to Q, K, V    q_ptrs = Q + off_q    k_ptrs = K + off_k    v_ptrs = V + off_v    # initialize pointer to m and l    m_prev = tl.zeros([BLOCK_M], dtype=tl.float32) - float(&quot;inf&quot;)    l_prev = tl.zeros([BLOCK_M], dtype=tl.float32)    acc = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)    # load q: it will stay in SRAM throughout    q = tl.load(q_ptrs)    # loop over k, v and update accumulator    for start_n in range(0, (start_m + 1) * BLOCK_M, BLOCK_N):        # -- compute qk ----        k = tl.load(k_ptrs)        qk = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32)        qk += tl.dot(q, k)        qk *= sm_scale        qk = tl.where(offs_m[:, None] &gt;= (start_n + offs_n[None, :]), qk, float(&quot;-inf&quot;))        # compute new m        m_curr = tl.maximum(tl.max(qk, 1), m_prev)        # correct old l        l_prev *= tl.exp(m_prev - m_curr)        # attention weights        p = tl.exp(qk - m_curr[:, None])        l_curr = tl.sum(p, 1) + l_prev        # rescale operands of matmuls        l_rcp = 1. / l_curr        p *= l_rcp[:, None]        acc *= (l_prev * l_rcp)[:, None]        # update acc        p = p.to(Q.dtype.element_ty)        v = tl.load(v_ptrs)        acc += tl.dot(p, v)        # update m_i and l_i        l_prev = l_curr        m_prev = m_curr        # update pointers        k_ptrs += BLOCK_N * stride_kn        v_ptrs += BLOCK_N * stride_vk    # rematerialize offsets to save registers    start_m = tl.program_id(0)    offs_m = start_m * BLOCK_M + tl.arange(0, BLOCK_M)    # write back l and m    l_ptrs = L + off_hz * N_CTX + offs_m    m_ptrs = M + off_hz * N_CTX + offs_m    tl.store(l_ptrs, l_prev)    tl.store(m_ptrs, m_prev)    # initialize pointers to output    offs_n = tl.arange(0, BLOCK_DMODEL)    off_o = off_hz * stride_oh + offs_m[:, None] * stride_om + offs_n[None, :] * stride_on    out_ptrs = Out + off_o    tl.store(out_ptrs, acc)IO复杂度分析Standard Attention对于标准注意力实现，初期我们需要把输入 从HBM中读取，并计算完毕后把输出写入到HBM中第一步把读取出来计算出，然后把存回去，内存访问复杂度第二步把读取出来计算出，然后把存回去，内存访问复杂度第三步把读取出来计算出，然后计算出结果，内存访问复杂度综上所述，整体的内存访问复杂度为FlashAttention对于FlashAttention，我们设置一个分块大小来把分成块，对于的每一块都要把部分的全部元素Load一遍，这样则有FlashAttention的内存访问复杂度为在这里，我们需要两个分块大小，的分块大小，的分块大小，我们设定SRAM的大小为，为了能把分块后的放进SRAM，那么则有一下限制:相应的，有如下限制:最终，还有一个中间态需要存储，则有如下限制:综上，限制如下进而推出那么在 的前提下，则有FlashAttention的HBM内存访问复杂度为：在语言建模中，通常有，则有。这样，在前向的过程中，我们采用分块计算的方式，避免了矩阵的存储开销，整体的运算都在SRAM内进行，降低了HBM访问次数，大大提升了计算的速度，减少了对存储的消耗Backward理论基础在上面前向的时候我们为了减少HBM访存次数，降低内存消耗量，我们并没有对矩阵进行存储，而这个在反向传播计算梯度的时候确实需要的一个信息之前有通过Gradient checkpointing的方式来实现梯度实现在前向的时候更加节省内存我们这里则采用重新计算的方式来计算对应的梯度。在上面前向计算的时候我们不会存储矩阵，但是我们会存储对应的指数项之和来进行梯度的计算我们在反向的过程中最重要的事情就是就是Loss函数对对应的梯度其中$\mathbf{O}\mathbf{d} \mathbf{O}=\frac{\partial \phi}{\partial \mathbf{O}}\mathbf{O}$$是现成的而对应的梯度也很好计算，由于，根据链式求导法则和矩阵求导法则则有，更详细如下所示:而对应的梯度算起来就比较复杂一点。这两个经过的计算逻辑步骤更多，我们可以一步一步的来进行计算。我们可以先计算，。由于 ，则有如下表示Fact: 的雅各比矩阵为，具体推导见Derivative of the Softmax Function and the Categorical Cross-Entropy Loss由于， 根据上述定理则有:接下来我们定义如下表示:根据上述定义简化上上式则有如下表示:相应的可表示为如下形式:又因为，结合上述推导利用链式求导法则对应的梯度有如下表示：至此，我们得到了一个完整的包含前向和反向的，降低了HBM访问次数的，新的Attention算子代码实现12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182@triton.jitdef _bwd_kernel(    Q, K, V, sm_scale, Out, DO,    DQ, DK, DV,    L, M,    D,    stride_qz, stride_qh, stride_qm, stride_qk,    stride_kz, stride_kh, stride_kn, stride_kk,    stride_vz, stride_vh, stride_vk, stride_vn,    Z, H, N_CTX,    num_block,    BLOCK_M: tl.constexpr, BLOCK_DMODEL: tl.constexpr,    BLOCK_N: tl.constexpr,):    off_hz = tl.program_id(0)    off_z = off_hz // H    off_h = off_hz % H    # offset pointers for batch/head    Q += off_z * stride_qz + off_h * stride_qh    K += off_z * stride_qz + off_h * stride_qh    V += off_z * stride_qz + off_h * stride_qh    DO += off_z * stride_qz + off_h * stride_qh    DQ += off_z * stride_qz + off_h * stride_qh    DK += off_z * stride_qz + off_h * stride_qh    DV += off_z * stride_qz + off_h * stride_qh    for start_n in range(0, num_block):        lo = start_n * BLOCK_M        # initialize row/col offsets        offs_qm = lo + tl.arange(0, BLOCK_M)        offs_n = start_n * BLOCK_M + tl.arange(0, BLOCK_M)        offs_m = tl.arange(0, BLOCK_N)        offs_k = tl.arange(0, BLOCK_DMODEL)        # initialize pointers to value-like data        q_ptrs = Q + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)        k_ptrs = K + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)        v_ptrs = V + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)        do_ptrs = DO + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)        dq_ptrs = DQ + (offs_qm[:, None] * stride_qm + offs_k[None, :] * stride_qk)        # pointer to row-wise quantities in value-like data        D_ptrs = D + off_hz * N_CTX        m_ptrs = M + off_hz * N_CTX        # initialize dv amd dk        dv = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)        dk = tl.zeros([BLOCK_M, BLOCK_DMODEL], dtype=tl.float32)        # k and v stay in SRAM throughout        k = tl.load(k_ptrs)        v = tl.load(v_ptrs)        # loop over rows        for start_m in range(lo, num_block * BLOCK_M, BLOCK_M):            offs_m_curr = start_m + offs_m            # load q, k, v, do on-chip            q = tl.load(q_ptrs)            # recompute p = softmax(qk, dim=-1).T            # NOTE: `do` is pre-divided by `l`; no normalization here            qk = tl.dot(q, tl.trans(k))            qk = tl.where(offs_m_curr[:, None] &gt;= (offs_n[None, :]), qk, float(&quot;-inf&quot;))            m = tl.load(m_ptrs + offs_m_curr)            p = tl.exp(qk * sm_scale - m[:, None])            # compute dv            do = tl.load(do_ptrs)            dv += tl.dot(tl.trans(p.to(Q.dtype.element_ty)), do)            # compute dp = dot(v, do)            Di = tl.load(D_ptrs + offs_m_curr)            dp = tl.zeros([BLOCK_M, BLOCK_N], dtype=tl.float32) - Di[:, None]            dp += tl.dot(do, tl.trans(v))            # compute ds = p * (dp - delta[:, None])            ds = p * dp * sm_scale            # compute dk = dot(ds.T, q)            dk += tl.dot(tl.trans(ds.to(Q.dtype.element_ty)), q)            # compute dq            dq = tl.load(dq_ptrs)            dq += tl.dot(ds.to(Q.dtype.element_ty), k)            tl.store(dq_ptrs, dq)            # increment pointers            dq_ptrs += BLOCK_M * stride_qm            q_ptrs += BLOCK_M * stride_qm            do_ptrs += BLOCK_M * stride_qm        # write-back        dv_ptrs = DV + (offs_n[:, None] * stride_qm + offs_k[None, :] * stride_qk)        dk_ptrs = DK + (offs_n[:, None] * stride_kn + offs_k[None, :] * stride_kk)        tl.store(dv_ptrs, dv)        tl.store(dk_ptrs, dk)Block-Sparse相比于上面的全量计算，块稀疏的FlashAttention需要额外提供一个Mask矩阵用于将一些元 素置零来保证块稀疏加速计算本章对于块稀疏的一个计算只是一个简单的尝试，没有进行太深入的探索，所以这里我们先一笔带过，后面我们可以讲一篇对FlashAttention进行块稀疏优化的工作SCFA实验验证通过实验验证发现，FlashAttention在速度和内存占用方面都表现出明显的优势，并取得了良好的效果目前，FlashAttention已经经过广泛验证, torch2.0中已提供flashattention的实现正如标题《Fast and Memory-Efficient Exact Attention with IO-Awareness》所示，FlashAttention的优点在于充分考虑了在计算任务中IO的重要性，并通过分块计算的方式开发了一种快速、节省显存、精确无近似的注意力实现方法。这使得我们更便于训练具有更长上下文的Transformer模型，并且为后续注意力算法的优化提供了一个基准</content>
	</entry>
	<entry>
		<title>LLM模型部署调试推理</title>
		<url>/article/24897.html</url>
		<content>LLM模型什么是LLM（大语音模型）概述Large Language Model(LLM)，也称为大型语言模型，是一种基于机器学习和自然语言处理技术的模型，它通过对大量的文本数据进行训练，来学习服务人类语言理解和生成的能力LLM的核心思想是通过大规模的无监督训练来学习自然语言的模式和语言结构，这在一定程度上能够模拟人类的语言认知和生成过程与传统的NLP模型相比，LLM能够更好地理解和生成自然文本，同时还能够表现出一定的逻辑思维和推理能力近年来，LLM得到了广泛的应用，其中最具代表性的是谷歌的BERT和OpenAI的GPT系列。这些模型在多个自然语言处理领域已经取得了显著的成果，包括文本分类、命名实体识别、情感分析、机器翻译、自动问答等然而，在实际应用中，LLM面临着更多的挑战首先，LLM需要大量的计算资源和大规模的数据集来训练，这对于一般的企业和个人来说十分困难其次，由于LLM模型的复杂性和计算量较大，对于实时的语言处理应用来说，LLM在应用效率和响应速度上还存在一定的局限性因此，如何解决模型训练和应用过程中的计算性能和效率问题，是LLM面临的主要挑战之一主要应用LLM在实际应用中有多种形式，以下是一些具体的示例：自然语言理解：通过语法词汇、句法语义、语境等相互作用，使计算机能够理解人类语言。如通过语音合成功能，实现文字转语音的技术。另外，基于LLM技术的英语学习App可以针对不同的用户让机器进行自适应教学，帮助用户更加高效地学习英语机器翻译：LLM可以利用大量的文本数据进行翻译学习，提高翻译准确度和语音翻译度，同时加速翻译速度，例如谷歌翻译等体验都有明显提升情感分析：LLM技术可以通过对大量数据进行分析，实现对人们在社交媒体上对商品、服务和品牌等的评价，从而帮助企业了解消费者的心态，并根据情感信息来优化其营销策略机器学习语音识别：在语音识别领域，LLM技术可以通过深度学习算法等技术改进，从而在分辨率、声音重叠、噪音和复杂语音语言中实现更好的语音识别效果。同时，在语音交互上，也可以进行一些基于LLM的定制，例如智能音箱、语音接待员和客户服务机器人等文本生成：LLM技术在文本生成方面的应用可以在文本摘要、翻译和自动化写作等方面进行适用。例如，LLM模型可以从短篇小说的背景信息中自动生成描述、对话等内容，同时还可以通过对不同风格的文本进行学习，并从中学习创造性的文本生成GPU环境腾讯云GPU价格表阿里云GPU价格表阿里云购买一台自己的GPU服服务器购买阿里云服务器可以通过以下步骤进行：访问阿里云官网并注册一个账户登录您的阿里云账户，选择”产品与服务”，然后选择”云服务器 ECS”在云服务器ECS页面上，您可以选择不同的实例类型和配置，根据您的需求选择适合的服务器规格。这些规格包括CPU、内存、存储空间等配置网络和安全组，您可以设置网络类型、公网带宽、IP地址等根据您的需求选择购买时长，可以选择包年包月或按小时计费，这里如果只是为了测试或玩一玩，可以选择最便宜的方式配置系统和软件，您可以选择操作系统和其他软件预装选项查看订单和价格，确认无误后，点击”立即购买”在支付页面选择合适的支付方式完成购买完成支付后，您将收到购买服务器的确认信息和服务器登录凭证端口开放安装基础的python环境首先复制公网ip，使用ssh工具连接自己的服务器，用户名默认是root，密码为自定义密码，登陆进来首先会系统初始化GPU环境，等待完成后会自动重启自动重启后，可以用nvidia-smi查看nvidia驱动配置安装git，后面很多项目或者模型下载会用到，包括git lfs1234567# 安装gityum install git -y# 安装git lfs（大文件下载）curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bashyum install git-lfsgit lfs install安装anaconda，并创建自己的python环境1234567# 安装anaconda环境wget https://repo.anaconda.com/archive/Anaconda3-2023.03-1-Linux-x86_64.shchmod +x Anaconda3-2023.03-1-Linux-x86_64.sh./Anaconda3-2023.03-1-Linux-x86_64.sh# 创建自己的python环境conda create -n gpt310 python=3.10 anaconda安装pytorch，这里的cuda选cu117，因为上面的服务器的cuda是11.4的，这里的cu117是可以向下兼容的12345# 切换到gpt310source activate gpt310# 安装gpu版的pytorch, 需要在GPU环境下安装，否则安装是cpu版本pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117在Python中导入PyTorch并检查CUDA是否可用1234567import torch# 检查PyTorch是否使用了CUDAif torch.cuda.is_available():    print(&quot;CUDA可用&quot;)else:    print(&quot;CUDA不可用&quot;)还可以使用torch.cuda.get_device_capability()函数来获取计算机上支持的CUDA设备的计算能力。这可以帮助您确定所安装的CUDA版本是否与您的显卡兼容用完机器记得关机，不然会一直扣费AutoDLAutoDL平台简介AutoDL算力平台是指为自动化深度学习(AutoDL)任务提供计算资源和基础设施的平台。由于深度学习任务通常需要大量的计算资源和存储空间，为了有效地执行AutoDL任务，需要具备相应的算力平台一个优秀的AutoDL算力平台通常具备以下特点：高性能计算：提供强大的计算能力，包括高性能的CPU、GPU或专用的AI芯片等。这些计算资源可以支持AutoDL任务的模型训练、超参数搜索和架构搜索等计算密集型任务分布式计算：支持分布式计算和训练，使得AutoDL任务可以在多个计算节点上并行执行，从而加快任务的完成时间。这对于大规模数据和复杂模型的AutoDL任务尤为重要数据存储和管理：提供高效的数据存储和管理系统，以支持大规模数据集的存储和访问。这可以确保AutoDL任务能够快速、可靠地访问所需的训练数据和验证数据任务调度和资源管理：具备任务调度和资源管理功能，可以有效地管理多个AutoDL任务的执行，包括资源分配、优先级管理和任务调度等，以保证任务的顺利进行易用性和灵活性：提供友好的用户界面和工具，使得用户可以方便地配置和管理AutoDL任务。同时，提供灵活的配置选项，以满足不同任务和需求的定制化要求AutoDL算力平台的设计和功能旨在提供高效、可扩展和易用的计算资源，以支持AutoDL任务的快速迭代和大规模应用。它们可以帮助研究人员和开发者更加便捷地进行AutoDL实验和模型优化，从而推动自动化深度学习领域的发展购买算力选择计费方式和显卡选择是否需要扩展数据盘，以及基础镜像，这里选Miniconda环境启动机器，进入终端可以看到12345678910111213141516171819+--------------------------------------------------AutoDL--------------------------------------------------------+目录说明:╔═════════════════╦════════╦════╦═════════════════════════════════════════════════════════════════════════╗║目录             ║名称    ║速度║说明                                                                     ║╠═════════════════╬════════╬════╬═════════════════════════════════════════════════════════════════════════╣║/                ║系 统 盘║一般║实例关机数据不会丢失，可存放代码等。会随保存镜像一起保存。               ║║/root/autodl-tmp ║数 据 盘║ 快 ║实例关机数据不会丢失，可存放读写IO要求高的数据。但不会随保存镜像一起保存 ║╚═════════════════╩════════╩════╩═════════════════════════════════════════════════════════════════════════╝CPU ：0.5 核心内存：2 GBGPU ：No devices were found存储：  系 统 盘/               ：46% 12G/25G  数 据 盘/root/autodl-tmp：65% 33G/50G+----------------------------------------------------------------------------------------------------------------+*注意: 1.系统盘较小请将大的数据存放于数据盘或网盘中，重置系统时数据盘和网盘中的数据不受影响2.清理系统盘请参考：https://www.autodl.com/docs/qa/安装git-lfs以及ssl等依赖123456curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.deb.sh | sudo bashsudo apt-get install git-lfsgit lfs installsudo apt-get install opensslsudo apt-get install libssl-dev安装pytorch环境12345678# 创建自己的python环境conda create -n gpt310 python=3.10 anaconda# 切换到gpt310source activate gpt310# 安装gpu版的pytorch, 需要在GPU环境下安装，否则安装是cpu版本pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117在Python中导入PyTorch并检查CUDA是否可用1234567import torch# 检查PyTorch是否使用了CUDAif torch.cuda.is_available():    print(&quot;CUDA可用&quot;)else:    print(&quot;CUDA不可用&quot;)其它事项控制台下容器实例就是租用的机器，自定义服务是将容器内的6006端口映射到公网/root/autodl-tmp可以放一些数据或模型，开关机不会丢失，但不随镜像一起保存无卡开机模型不带GPU，很便宜，一小时0.01元本地👻没有显卡，🤕伤不起🤕Langchain-ChatGLMgithub imClumsyPanda/langchain-ChatGLM🤖️ 一种利用 langchain 思想实现的基于本地知识库的问答应用，目标期望建立一套对中文场景与开源模型支持友好、可离线运行的知识库问答解决方案💡 受 GanymedeNil 的项目 document.ai 和 AlexZhangji 创建的 ChatGLM-6B Pull Request 启发，建立了全流程可使用开源模型实现的本地知识库问答应用。现已支持使用 ChatGLM-6B 等大语言模型直接接入，或通过 fastchat api 形式接入 Vicuna, Alpaca, LLaMA, Koala, RWKV 等模型✅ 本项目中 Embedding 默认选用的是 GanymedeNil/text2vec-large-chinese，LLM 默认选用的是 ChatGLM-6B。依托上述模型，本项目可实现全部使用开源模型离线私有部署⛓️ 本项目实现原理如下图所示，过程包括加载文件 -&gt; 读取文本 -&gt; 文本分割 -&gt; 文本向量化 -&gt; 问句向量化 -&gt; 在文本向量中匹配出与问句向量最相似的top k个 -&gt; 匹配出的文本作为上下文和问题一起添加到prompt中 -&gt; 提交给LLM生成回答从文档处理角度来看，实现流程如下：ChatGLM-6B模型硬件需求注：如未将模型下载至本地，请执行前检查$HOME/.cache/huggingface/文件夹剩余空间，模型文件下载至本地需要15GB存储空间。注：一些其它的可选启动项见项目启动选项模型下载方法可参考常见问题中Q8量化等级最低 GPU 显存（推理）最低 GPU 显存（高效参数微调）FP16（无量化）13 GB14 GBINT88 GB9 GBINT46 GB7 GB准备工作项目环境准备先在/home目录下创建huangyc文件夹，进入huangyc目录下后，拉取项目代码1git clone https://github.com/imClumsyPanda/langchain-ChatGLM.git安装必要库123456# centos系统下yum install libX11 -yyum install libXext -ypip uninstall detectron2# ubuntu系统不需要进入langchain-ChatGLM文件夹，安装依赖库，AutoDl平台下好像需要加-i参数1pip install -r requirements.txt -i https://pypi.tuna.tsinghua.edu.cn/simple模型准备从huggingface开放模型下找到自己的需要的模型，用git lfs下载，如果在AutoDl平台，可以把模型下载到/root/autodl-tmp1git clone https://huggingface.co/THUDM/chatglm-6b-int4.git可以用ls -lh .命令查看下载的文件大小是否正常下载完的chatglm-6b-int4模型文件夹，放到langchain-ChatGLM项目的model文件夹下，没有model文件夹的话，自己创建下如果下载失败的话，可以试下用wget下载，例如1wget --no-check-certificate https://huggingface.co/THUDM/chatglm-6b-int4/resolve/main/pytorch_model.bin推理修改配置打开langchain-ChatGLM/config/model_config.py，修改如下配置123456789101112131415161718# supported LLM models# llm_model_dict 处理了loader的一些预设行为，如加载位置，模型名称，模型处理器实例llm_model_dict = {......    &quot;chatglm-6b-int4&quot;: {        &quot;name&quot;: &quot;chatglm-6b-int4&quot;,-        &quot;pretrained_model_name&quot;: &quot;THUDM/chatglm-6b-int4&quot;,+        &quot;pretrained_model_name&quot;: &quot;model/chatglm-6b-int4&quot;,               &quot;local_model_path&quot;: None,        &quot;provides&quot;: &quot;ChatGLM&quot;    }    ......}# LLM 名称LLM_MODEL = &quot;chatglm-6b-int4&quot;# 如果你需要加载本地的model，指定这个参数  ` --no-remote-model`，或者下方参数修改为 `True`NO_REMOTE_MODEL = False这样模型会从本地加载修改webui.py，用于启动Web 交互123456789(demo .queue(concurrency_count=3)- .launch(server_name='0.0.0.0',+ .launch(server_name='localhost',                   server_port=7860,  # AutoDl平台用的是6006         show_api=False,-        share=False,+        share=True,                   inbrowser=False))阿里云服务器那边需要开放7860端口启动web服务进入到langchain-ChatGLM项目下，执行webui.py1python webui.py如果发现端口被占用了，可以用netstat -atunlp | grep 7860命令查看并杀死占用该端口的进程(自己判断)打开网页，你的ip:7860，就可以看到如下界面，默认是对话界面可以切换到知识库测试，在这里可以上传自己的文档到知识库后台看下当前显存占用，模型用的是chatglm-6b-int4模型之后测试上传了《流畅的Python》高清官方中文版.pdf，后台一直在更新知识库，等了十几分钟，还没结束，我就切换到普通对话了，又测了十几轮对话，突然爆了显存不够，不知道是不是上传知识库引起的，还是随着对话增加，显存一直没释放pycharm远程配置PyCharm远程配置解释器和项目提供了以下几个重要的用途和优势：远程开发：您可以在本地使用PyCharm编写代码，并将代码部署和运行在远程服务器上。这意味着您不需要在本地配置和安装与远程服务器环境相同的软件和依赖项。您可以利用服务器上的计算资源和环境来进行开发和测试，减轻了本地机器的负担统一开发环境：通过远程配置解释器和项目，您可以在本地使用PyCharm的功能和工具来开发和调试代码。您可以享受PyCharm提供的智能代码补全、调试器、版本控制集成等功能，无需切换到其他编辑器或远程终端协作与团队开发：远程配置解释器和项目使团队成员可以共享相同的开发环境。无论是在本地还是远程服务器上，团队成员可以使用相同的配置和依赖项来开发和测试代码。这有助于确保代码在不同环境下的一致性，并促进团队协作和开发效率远程调试和错误排查：通过配置远程解释器，您可以在本地使用PyCharm的调试器来调试远程服务器上的代码。这样，您可以逐步执行代码、观察变量和监控程序状态，以便更轻松地进行错误排查和修复远程部署和管理：通过远程配置项目，您可以将本地的代码同步到远程服务器上，并在服务器上运行和管理项目。这简化了部署过程，并使您能够直接在远程服务器上操作项目文件和资源PyCharm远程配置解释器和项目提供了一种方便而高效的方式，让您可以在本地使用PyCharm进行开发和调试，同时利用远程服务器的优势来运行和部署代码。这对于需要在远程环境下进行开发和协作的场景非常有用，如远程服务器上的Web开发、数据处理和云计算等任务pycharm远程配置参考本站的Spark集群搭建章节下的配置远程解释器相关技术langchain: langchain、LangChain：为你定制一个专属的GPTLangChain是一个用于开发基于语言模型的应用程序开发框架，用户可以利用LangChain的模块来改善大语言模型的使用，通过输入自己的知识库来定制化自己的大语言模型faiss: Faiss Documentation、[python] 向量检索库Faiss使用指北所谓相似性搜索是指通过比较多维空间中数据之间的相似性来搜索与输入数据最相似的目标数据。例如人脸识别中，通过比较人脸向量之前的距离来识别当前人脸与哪张人脸相似。因此，该技术被广泛应用于信息检索、计算机视觉、数据分析等领域。如果要检索的数据很多时，那么就需要一个向量检索库来加速检索Faiss包含多种相似性搜索方法，并提供cpu和gpu版本支持。Faiss的优势在于通过较小的精度损失提高向量相似度的检索速度和减少内存使用量ChatGLM-6B: chatglm、github THUDM/ChatGLM-6BChatGLM-6B是一个开源的、支持中英双语的对话语言模型，基于General Language Model (GLM)架构，具有62亿参数。结合模型量化技术，用户可以在消费级的显卡上进行本地部署(INT4量化级别下最低只需6GB显存)。 ChatGLM-6B使用了和ChatGPT相似的技术，针对中文问答和对话进行了优化。经过约1T标识符的中英双语训练，辅以监督微调、反馈自助、人类反馈强化学习等技术的加持，62亿参数的ChatGLM-6B已经能生成相当符合人类偏好的回答Hugging Face: Hugging FaceHugging Face是一个知名的开源社区和公司，专注于自然语言处理(NLP)和机器学习(ML)领域。他们开发了许多流行的开源工具和库，使得构建和应用NLP模型更加便捷localGPTgithub PromtEngineer/localGPT等待</content>
	</entry>
	<entry>
		<title>huggingface基本使用教程</title>
		<url>/article/57912.html</url>
		<content>huggingface概述Hugging Face官网任务分类和示例LLM高效调参_PEFT库简介及使用Hugging Face是一个知名的开源社区和公司，专注于自然语言处理(NLP)和机器学习(ML)领域。他们开发了许多流行的开源工具和库，使得构建和应用NLP模型更加便捷Hugging face起初是一家总部位于纽约的聊天机器人初创服务商，他们本来打算创业做聊天机器人，然后在github上开源了一个Transformers库，虽然聊天机器人业务没搞起来，但是他们的这个库在机器学习社区迅速大火起来。目前已经共享了超100,000个预训练模型，10,000个数据集，变成了机器学习界的github在这里主要有以下大家需要的资源Datasets：数据集，以及数据集的下载地址Models：包括各种处理CV和NLP等任务的模型，上面模型都是可以免费获得主要包括计算机视觉、自然语言处理、语音处理、多模态、表格处理、强化学习course：免费的nlp课程docs：文档展开细节Computer Vision(计算机视觉任务)：包括lmage Classification(图像分类)，lmage Segmentation(图像分割)、zero-Shot lmage Classification(零样本图像分类)、lmage-to-Image(图像到图像的任务)、Unconditional lmage Generation(无条件图像生成)、Object Detection(目标检测)、Video Classification(视频分类)、Depth Estimation(深度估计，估计拍摄者距离图像各处的距离)Natural Language Processing(自然语言处理)：包括Translation(机器翻译)、Fill-Mask(填充掩码，预测句子中被遮掩的词)、Token Classification(词分类)、Sentence Similarity(句子相似度)、Question Answering(问答系统)，Summarization(总结，缩句)、Zero-Shot Classification (零样本分类)、Text Classification(文本分类)、Text2Text(文本到文本的生成)、Text Generation(文本生成)、Conversational(聊天)、Table Question Answer(表问答，1.预测表格中被遮掩单词2.数字推理，判断句子是否被表格数据支持)Audio(语音)：Automatic Speech Recognition(语音识别)、Audio Classification(语音分类)、Text-to-Speech(文本到语音的生成)、Audio-to-Audio(语音到语音的生成)、Voice Activity Detection(声音检测、检测识别出需要的声音部分)Multimodal(多模态)：Feature Extraction(特征提取)、Text-to-Image(文本到图像)、Visual Question Answering(视觉问答)、Image2Text(图像到文本)、Document Question Answering(文档问答)Tabular(表格)：Tabular Classification(表分类)、Tabular Regression(表回归)Reinforcement Learning(强化学习)：Reinforcement Learning(强化学习)、Robotics(机器人)安装安装transformers库1pip install transformers模型下载加速如何快速下载huggingface模型——全方法总结git clone官方提供了 git clone repo_url 的方式下载，这种方法相当简单，然而却是最不推荐直接用的方法，缺点有二：不支持断点续传，断了重头再来clone 会下载历史版本占用磁盘空间，即使没有历史版本huggingface-clihf的模型下载工具: download-files-from-the-hubhuggingface-cli 隶属于 huggingface_hub 库，不仅可以下载模型、数据，还可以可以登录huggingface、上传模型、数据等huggingface-cli 属于官方工具，其长期支持肯定是最好的。优先推荐！安装依赖1pip install -U huggingface_hub注意：huggingface_hub 依赖于 Python&gt;=3.8，此外需要安装 0.17.0 及以上的版本，推荐0.19.0+基本用法1huggingface-cli download --resume-download bigscience/bloom-560m --local-dir bloom-560m下载数据集1huggingface-cli download --resume-download --repo-type dataset lavita/medical-qa-shared-task-v1-toy值得注意的是，有个--local-dir-use-symlinks False 参数可选，因为huggingface的工具链默认会使用符号链接来存储下载的文件，导致--local-dir指定的目录中都是一些链接文件，真实模型则存储在~/.cache/huggingface下，如果不喜欢这个可以用 --local-dir-use-symlinks False取消这个逻辑多线程下载器多线程加速是一种有效、显著提高下载速度的方法经典多线程工具推荐两个：IDM、Aria2。 IDM 适用于 Windows、aria2 适用于 Linux。本文头图就是 IDM 工具。因此获取URL后，可以利用这些多线程工具来下载。以我的一次实测为例，单线程700KB/s，IDM 8线程 6MB/s。千兆宽带下，利用IDM能跑到80MB/s+手动获取仓库中所有 URL 比较麻烦，作者写了一个命令行脚本 hdf.sh（Gitst链接），结合自动获取 url 以及 aria2 多线程下载，适合于 Linux123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136#!/usr/bin/env bash# Color definitionsRED='\033[0;31m'GREEN='\033[0;32m'YELLOW='\033[1;33m'NC='\033[0m' # No Colortrap 'printf &quot;${YELLOW}\nDownload interrupted. If you re-run the command, you can resume the download from the breakpoint.\n${NC}&quot;; exit 1' INTdisplay_help() {    cat &lt;&lt; EOFUsage:  hfd &lt;model_id&gt; [--include include_pattern] [--exclude exclude_pattern] [--hf_username username] [--hf_token token] [--tool wget|aria2c] [-x threads] [--dataset]Description:  Downloads a model or dataset from Hugging Face using the provided model ID.Parameters:  model_id        The Hugging Face model ID in the format 'repo/model_name'.  --include       (Optional) Flag to specify a string pattern to include files for downloading.  --exclude       (Optional) Flag to specify a string pattern to exclude files from downloading.  exclude_pattern The pattern to match against filenames for exclusion.  --hf_username   (Optional) Hugging Face username for authentication.  --hf_token      (Optional) Hugging Face token for authentication.  --tool          (Optional) Download tool to use. Can be wget (default) or aria2c.  -x              (Optional) Number of download threads for aria2c.  --dataset       (Optional) Flag to indicate downloading a dataset.Example:  hfd bigscience/bloom-560m --exclude safetensors  hfd meta-llama/Llama-2-7b --hf_username myuser --hf_token mytoken --tool aria2c -x 8  hfd lavita/medical-qa-shared-task-v1-toy --datasetEOF    exit 1}MODEL_ID=$1shift# Default valuesTOOL=&quot;wget&quot;THREADS=1HF_ENDPOINT=${HF_ENDPOINT:-&quot;https://huggingface.co&quot;}while [[ $# -gt 0 ]]; do    case $1 in        --include) INCLUDE_PATTERN=&quot;$2&quot;; shift 2 ;;        --exclude) EXCLUDE_PATTERN=&quot;$2&quot;; shift 2 ;;        --hf_username) HF_USERNAME=&quot;$2&quot;; shift 2 ;;        --hf_token) HF_TOKEN=&quot;$2&quot;; shift 2 ;;        --tool) TOOL=&quot;$2&quot;; shift 2 ;;        -x) THREADS=&quot;$2&quot;; shift 2 ;;        --dataset) DATASET=1; shift ;;        *) shift ;;    esacdone# Check if aria2, wget, curl, git, and git-lfs are installedcheck_command() {    if ! command -v $1 &amp;&gt;/dev/null; then        echo -e &quot;${RED}$1 is not installed. Please install it first.${NC}&quot;        exit 1    fi}[[ &quot;$TOOL&quot; == &quot;aria2c&quot; ]] &amp;&amp; check_command aria2c[[ &quot;$TOOL&quot; == &quot;wget&quot; ]] &amp;&amp; check_command wgetcheck_command curl; check_command git; check_command git-lfs[[ -z &quot;$MODEL_ID&quot; || &quot;$MODEL_ID&quot; =~ ^-h ]] &amp;&amp; display_helpMODEL_DIR=&quot;${MODEL_ID#*/}&quot;if [[ &quot;$DATASET&quot; == 1 ]]; then    MODEL_ID=&quot;datasets/$MODEL_ID&quot;fiecho &quot;Downloading to ./$MODEL_DIR&quot;if [ -d &quot;$MODEL_DIR/.git&quot; ]; then    printf &quot;${YELLOW}%s exists, Skip Clone.\n${NC}&quot; &quot;$MODEL_DIR&quot;    cd &quot;$MODEL_DIR&quot; &amp;&amp; GIT_LFS_SKIP_SMUDGE=1 git pull || { printf &quot;Git pull failed.\n&quot;; exit 1; }else    REPO_URL=&quot;$HF_ENDPOINT/$MODEL_ID&quot;    GIT_REFS_URL=&quot;${REPO_URL}/info/refs?service=git-upload-pack&quot;    echo &quot;Test GIT_REFS_URL: $GIT_REFS_URL&quot;    response=$(curl -s -o /dev/null -w &quot;%{http_code}&quot; &quot;$GIT_REFS_URL&quot;)    if [ &quot;$response&quot; == &quot;401&quot; ] || [ &quot;$response&quot; == &quot;403&quot; ]; then        if [[ -z &quot;$HF_USERNAME&quot; || -z &quot;$HF_TOKEN&quot; ]]; then            printf &quot;${RED}HTTP Status Code: $response.\nThe repository requires authentication, but --hf_username and --hf_token is not passed. Please get token from https://huggingface.co/settings/tokens.\nExiting.\n${NC}&quot;            exit 1        fi        REPO_URL=&quot;https://$HF_USERNAME:$HF_TOKEN@${HF_ENDPOINT#https://}/$MODEL_ID&quot;    elif [ &quot;$response&quot; != &quot;200&quot; ]; then        echo -e &quot;${RED}Unexpected HTTP Status Code: $response.\nExiting.\n${NC}&quot;; exit 1    fi    echo &quot;git clone $REPO_URL&quot;    GIT_LFS_SKIP_SMUDGE=1 git clone &quot;$REPO_URL&quot; &amp;&amp; cd &quot;$MODEL_DIR&quot; || { printf &quot;${RED}Git clone failed.\n${NC}&quot;; exit 1; }    for file in $(git lfs ls-files | awk '{print $3}'); do        truncate -s 0 &quot;$file&quot;    donefiprintf &quot;\nStart Downloading lfs files, bash script:\n&quot;files=$(git lfs ls-files | awk '{print $3}')declare -a urlsfor file in $files; do    url=&quot;$HF_ENDPOINT/$MODEL_ID/resolve/main/$file&quot;    file_dir=$(dirname &quot;$file&quot;)    mkdir -p &quot;$file_dir&quot;    if [[ &quot;$TOOL&quot; == &quot;wget&quot; ]]; then        download_cmd=&quot;wget -c \&quot;$url\&quot; -O \&quot;$file\&quot;&quot;        [[ -n &quot;$HF_TOKEN&quot; ]] &amp;&amp; download_cmd=&quot;wget --header=\&quot;Authorization: Bearer ${HF_TOKEN}\&quot; -c \&quot;$url\&quot; -O \&quot;$file\&quot;&quot;    else        download_cmd=&quot;aria2c -x $THREADS -s $THREADS -k 1M -c \&quot;$url\&quot; -d \&quot;$file_dir\&quot; -o \&quot;$(basename &quot;$file&quot;)\&quot;&quot;        [[ -n &quot;$HF_TOKEN&quot; ]] &amp;&amp; download_cmd=&quot;aria2c --header=\&quot;Authorization: Bearer ${HF_TOKEN}\&quot; -x $THREADS -s $THREADS -k 1M -c \&quot;$url\&quot; -d \&quot;$file_dir\&quot; -o \&quot;$(basename &quot;$file&quot;)\&quot;&quot;    fi    [[ -n &quot;$INCLUDE_PATTERN&quot; &amp;&amp; $file != *&quot;$INCLUDE_PATTERN&quot;* ]] &amp;&amp; printf &quot;# %s\n&quot; &quot;$download_cmd&quot; &amp;&amp; continue    [[ -n &quot;$EXCLUDE_PATTERN&quot; &amp;&amp; $file == *&quot;$EXCLUDE_PATTERN&quot;* ]] &amp;&amp; printf &quot;# %s\n&quot; &quot;$download_cmd&quot; &amp;&amp; continue    printf &quot;%s\n&quot; &quot;$download_cmd&quot;    urls+=(&quot;$url|$file&quot;)donefor url_file in &quot;${urls[@]}&quot;; do    IFS='|' read -r url file &lt;&lt;&lt; &quot;$url_file&quot;    file_dir=$(dirname &quot;$file&quot;)    if [[ &quot;$TOOL&quot; == &quot;wget&quot; ]]; then        [[ -n &quot;$HF_TOKEN&quot; ]] &amp;&amp; wget --header=&quot;Authorization: Bearer ${HF_TOKEN}&quot; -c &quot;$url&quot; -O &quot;$file&quot; || wget -c &quot;$url&quot; -O &quot;$file&quot;    else        [[ -n &quot;$HF_TOKEN&quot; ]] &amp;&amp; aria2c --header=&quot;Authorization: Bearer ${HF_TOKEN}&quot; -x $THREADS -s $THREADS -k 1M -c &quot;$url&quot; -d &quot;$file_dir&quot; -o &quot;$(basename &quot;$file&quot;)&quot; || aria2c -x $THREADS -s $THREADS -k 1M -c &quot;$url&quot; -d &quot;$file_dir&quot; -o &quot;$(basename &quot;$file&quot;)&quot;    fi    [[ $? -eq 0 ]] &amp;&amp; printf &quot;Downloaded %s successfully.\n&quot; &quot;$url&quot; || { printf &quot;${RED}Failed to download %s.\n${NC}&quot; &quot;$url&quot;; exit 1; }doneprintf &quot;${GREEN}Download completed successfully.\n${NC}&quot;该工具同样支持设置镜像端点的环境变量:1export HF_ENDPOINT=&quot;https://hf-mirror.com&quot;基本命令：1./hdf.sh bigscience/bloom-560m --tool aria2c -x 4如果没有安装 aria2，则可以默认用 wget：1./hdf.sh bigscience/bloom-560m   镜像网站Huggingface-镜像网站可下载模型和数据集，解决Huggingface无法访问问题使用以下py脚本可以快速生成下载模型等文件的sh脚本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v1.0@author: huangyc@file: download_hf_models.py@Description: @time: 2024/1/18 11:08&quot;&quot;&quot;import contextlibimport sysfrom typing import Listfrom urllib.parse import unquote, urlparseimport requestsfrom basic_support.logger.logger_config import logger@contextlib.contextmanagerdef print_to_file(file: str, mode: str = 'w', encoding='utf-8', errors=None, newline=None, closefd=True):    &quot;&quot;&quot;    将print重定向输出到文件    :param file: 文件名    :param mode: 读写模式    :param encoding: 文件编码    :param errors:    :param newline:    :param closefd:    :return:    &quot;&quot;&quot;    f = open(file=file, mode=mode, encoding=encoding, errors=errors, newline=newline, closefd=closefd)    # 保存原来的sys.stdout    original_stdout = sys.stdout    # 将sys.stdout重定向到文件流    sys.stdout = f    yield    # 恢复原来的sys.stdout    sys.stdout = original_stdout    f.close()def extract_main_domain(url):    parsed_url = urlparse(url)    main_domain = f&quot;{parsed_url.scheme}://{parsed_url.hostname}&quot;    return main_domaindef gen_download_hf_models_script(model_url: str, file_name: str, filter_types: List[str] = None):    &quot;&quot;&quot;    产生下载hf模型的脚本    :param model_url: 支持 https://hf-mirror.com 和 https://huggingface.co/models                    如: https://hf-mirror.com/baichuan-inc/Baichuan2-13B-Chat/tree/v2.0    :param file_name: 输出文件名字, 如nohup_download_baichuan2.sh    :param filter_types: 需要过滤的文件类型[暂时没实现]    :return:    &quot;&quot;&quot;    from bs4 import BeautifulSoup    # 获取主要域名    main_domain = extract_main_domain(model_url)    # 输出主要域名    logger.info(f&quot;解析到域名为：{main_domain}&quot;)    # 发送HTTP GET请求获取网页内容    logger.info(&quot;开始解析下载&quot;)    response = requests.get(model_url)    logger.info(&quot;网页下载完成, 准备解析下载地址&quot;)    html_content = response.text    # 使用BeautifulSoup对HTML内容进行解析    soup = BeautifulSoup(html_content, 'html.parser')    download_files = soup.findAll('a', {'title': 'Download file'})    with print_to_file(file_name):        print('echo &quot;开始下载模型等文件&quot;\n')        for idx, download_file in enumerate(download_files):            href = unquote(download_file.get('href'))            print('date +&quot;当前时间为: %Y-%m-%d %H:%M:%S&quot;')            url = f&quot;{main_domain}{href}&quot;            file_name =os.path.basename(href).split('?')[0]            print(f'wget -O &quot;{file_name}&quot; &quot;{url}&quot;')            print(f'echo &quot;下载完成&quot;\n')        print('date +&quot;当前时间为: %Y-%m-%d %H:%M:%S&quot;')        print('echo &quot;Download completed successfully.&quot;')    logger.info(&quot;下载地址解析完毕&quot;)    logger.info(f&quot;脚本输出路径为: {file_name}&quot;)if __name__ == '__main__':    test_url = r&quot;https://hf-mirror.com/baichuan-inc/Baichuan2-13B-Chat/tree/v2.0&quot;    gen_download_hf_models_script(model_url=test_url, file_name='noup_download_baichuan2.sh')快速开始hf快速开始教程下图是huggingface模块关系图12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v1.0@author: huangyc@file: noup_huggingface.py@Description:@time: 2024/2/3 9:49&quot;&quot;&quot;from datasets import load_datasetfrom transformers import AutoModelForSequenceClassificationfrom transformers import AutoTokenizerfrom transformers import DataCollatorWithPaddingfrom transformers import Trainerfrom transformers import TrainingArgumentsdef run():    model_name = &quot;distilbert-base-uncased&quot;    output_dir = &quot;path/to/save/folder/&quot;    # 加载预训练模型    model = AutoModelForSequenceClassification.from_pretrained(model_name)    # 加载分词器    tokenizer = AutoTokenizer.from_pretrained(model_name)    # 加载数据集    dataset = load_dataset(&quot;rotten_tomatoes&quot;)    def tokenize_dataset(p_dataset):        &quot;&quot;&quot;            定义数据处理函数            @param p_dataset:            @return:            &quot;&quot;&quot;        return tokenizer(p_dataset[&quot;text&quot;])    # 对数据集调用处理函数(这里主要做分词)    dataset = dataset.map(tokenize_dataset, batched=True)    # 有些还需要做标签对齐    # label2id = {&quot;contradiction&quot;: 0, &quot;neutral&quot;: 1, &quot;entailment&quot;: 2}    # mnli = load_dataset(&quot;glue&quot;, &quot;mnli&quot;, split=&quot;train&quot;)    # mnli_aligned = mnli.align_labels_with_mapping(label2id, &quot;label&quot;)    # 定义一个数据收集器    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)    # 配置训练参数    training_args = TrainingArguments(output_dir, learning_rate=2e-5, per_device_train_batch_size=8,                                      per_device_eval_batch_size=8, num_train_epochs=2, )    # 定义一个trainer    trainer = Trainer(model=model, args=training_args, train_dataset=dataset[&quot;train&quot;], eval_dataset=dataset[&quot;test&quot;],                      tokenizer=tokenizer, data_collator=data_collator, )    # 开始训练    trainer.train()if __name__ == '__main__':    run()对于使用序列到序列模型（Seq2Seq）的任务，如翻译或摘要，应该使用Seq2SeqTrainer和Seq2SeqTrainingArguments类您可以通过继承Trainer内部的方法来自定义训练循环的行为。这使您能够自定义特征，如损失函数、优化器和调度器。查看Trainer参考以了解哪些方法可以被继承自定义训练循环的另一种方式是使用回调（Callbacks）。您可以使用回调与其他库集成以及检查训练循环，以报告进度或提前停止训练回调不会修改训练循环本身的任何内容。若要自定义像损失函数这样的内容，您需要继承TrainerdatasetsHuggingFace datasets库总结安装下面三个命令都用于安装Hugging Face的datasets库的不同配置pip install datasets：这个命令安装的是datasets库的基本配置，它提供了对常见的自然语言处理(NLP)任务和数据集的支持，例如文本分类、命名实体识别、问答系统等。如果您只需要处理文本数据或进行常见的NLP任务，这个基本配置就足够了pip install datasets[audio]：这个命令安装的是datasets库的”audio”配置。它包含了对声音和音频数据集的支持，例如自动语音识别(ASR)和音频分类任务。如果您需要处理声音和音频数据，比如进行语音识别或音频分类，安装这个配置会提供相应的功能和数据集支持pip install datasets[vision]：这个命令安装的是datasets库的”vision”配置。它包含了对图像和计算机视觉任务的支持，例如图像分类、目标检测和分割等。如果您需要处理图像数据或进行计算机视觉任务，安装这个配置会提供相应的功能和数据集支持通过安装不同的配置，您可以选择仅安装您需要的功能和支持的任务类型，以减少库的安装和存储空间。根据您的具体需求，选择适合的配置进行安装即可12345678# 安装基础版pip install datasets# 安装for声音pip install datasets[audio]# 安装for图像pip install datasets[vision]快速开始视觉123456789101112131415161718192021222324252627282930from datasets import load_dataset, Imagefrom torch.utils.data import DataLoaderfrom torchvision.transforms import Compose, ColorJitter, ToTensor# 加载数据集dataset = load_dataset(&quot;beans&quot;, split=&quot;train&quot;)jitter = Compose(    [ColorJitter(brightness=0.5, hue=0.5), ToTensor()])def transforms(examples):    examples[&quot;pixel_values&quot;] = [jitter(image.convert(&quot;RGB&quot;)) for image in examples[&quot;image&quot;]]    return examplesdataset = dataset.with_transform(transforms)def collate_fn(examples):    images = []    labels = []    for example in examples:        images.append((example[&quot;pixel_values&quot;]))        labels.append(example[&quot;labels&quot;])            pixel_values = torch.stack(images)    labels = torch.tensor(labels)    return {&quot;pixel_values&quot;: pixel_values, &quot;labels&quot;: labels}# 定义DataLoaderdataloader = DataLoader(dataset, collate_fn=collate_fn, batch_size=4)nlp使用 Hugging Face 提供的datasets库加载了GLUE(General Language Understanding Evaluation)数据集中的MRPC(Microsoft Research Paraphrase Corpus)部分的训练集。这个数据集用于句子对的相似性判断任务1234567891011121314151617181920212223242526272829from datasets import load_datasetfrom transformers import AutoModelForSequenceClassification, AutoTokenizerimport torchdataset = load_dataset(&quot;glue&quot;, &quot;mrpc&quot;, split=&quot;test&quot;)# load a pretrained BERT model and its corresponding tokenizer from the 🤗 Transformers library.model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-uncased&quot;)tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)def encode(examples):    return tokenizer(examples[&quot;sentence1&quot;], examples[&quot;sentence2&quot;], truncation=True, padding=&quot;max_length&quot;)dataset = dataset.map(encode, batched=True)dataset[0]{'sentence1': 'Amrozi accused his brother , whom he called &quot; the witness &quot; , of deliberately distorting his evidence .','sentence2': 'Referring to him as only &quot; the witness &quot; , Amrozi accused his brother of deliberately distorting his evidence .','label': 1,'idx': 0,'input_ids': array([  101,  7277,  2180,  5303,  4806,  1117,  1711,   117,  2292, 1119,  1270,   107,  1103,  7737,   107,   117,  1104,  9938, 4267, 12223, 21811,  1117,  2554,   119,   102, 11336,  6732, 3384,  1106,  1140,  1112,  1178,   107,  1103,  7737,   107, 117,  7277,  2180,  5303,  4806,  1117,  1711,  1104,  9938, 4267, 12223, 21811,  1117,  2554,   119,   102]),'token_type_ids': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),'attention_mask': array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])}#  Rename the label column to labels, which is the expected input name in BertForSequenceClassificationdataset = dataset.map(lambda examples: {&quot;labels&quot;: examples[&quot;label&quot;]}, batched=True)dataset.set_format(type=&quot;torch&quot;, columns=[&quot;input_ids&quot;, &quot;token_type_ids&quot;, &quot;attention_mask&quot;, &quot;labels&quot;])dataloader = torch.utils.data.DataLoader(dataset, batch_size=32)概述datasets库中的Dataset对象通常用来处理和存储数据。当数据需要载入模型进行训练或评估时，DataLoader被用来创建数据的迭代器，允许批量处理和并行加载DataCollator则用于将这些批次的数据整理成模型需要的格式，进行适当的填充或其他预处理步骤简单来说，你可以这样想象它们的工作流：Dataset负责数据的存储和预处理DataLoader负责从Dataset中抽取数据，组成批次，并可选择并行加载数据DataCollator负责将DataLoader提供的批次数据进行填充和整理，以确保模型可以正确处理DataCollator类huggingface的DataCollator和pytorch的collate_fn的关系在PyTorch中，collate_fn 是 DataLoader 的一个参数，用于指定如何将多个数据样本组合成一个批次这个函数接受一个样本列表作为输入，然后返回一个批次，通常是通过堆叠（stacking）或填充（padding）样本来实现collate_fn 在处理长度不一致的数据时特别有用，例如文本数据或时间序列数据Hugging Face的 DataCollator 基本上是 collate_fn 的一个扩展或包装器。它通常是一个类，实现了一个 __call__ 方法，该方法的功能与 collate_fn 相同在Hugging Face的Transformers库中，有预先定义的 DataCollator 类，它们被设计用来处理特定类型的数据和模型需求，如填充到相同长度，或者为了语言模型训练而进行数据掩蔽的任务下面是一个示例，展示了在PyTorch和Hugging Face的Transformers中如何使用 collate_fn 和 DataCollator：在PyTorch中使用自定义 collate_fn：1234567from torch.utils.data import DataLoaderdef custom_collate_fn(batch):    # 自定义的堆叠、填充逻辑    passdata_loader = DataLoader(dataset, batch_size=32, collate_fn=custom_collate_fn)在Hugging Face的Transformers中使用 DataCollator：1234567from transformers import DataCollatorWithPaddingfrom torch.utils.data import DataLoader# 对于一些特定的任务，Transformers库提供了预定义的DataCollatordata_collator = DataCollatorWithPadding(tokenizer)data_loader = DataLoader(dataset, batch_size=32, collate_fn=data_collator)在这个例子中，DataCollatorWithPadding 是Hugging Face提供的一个类，它使用给定的tokenizer来自动处理批次的填充当创建 DataLoader 实例时，你可以直接将 data_collator 作为 collate_fn 的值传入，这是因为 DataCollatorWithPadding 类的实例是可调用的，这样 DataLoader 在每个批次准备数据时会调用 data_collator总的来说，Hugging Face的 DataCollator 提供了一个更高级别、更方便的接口，尤其是为了与 Transformers 库中的NLP模型和任务配合使用，而PyTorch的 collate_fn 是这个接口的底层机制，提供了自定义数据组合逻辑的基础功能小结可以查看huggingface的Trainer类，很容易发现他们之间的关系：Hugging Face的 DataCollator 基本上是 collate_fn 的一个扩展或包装器。它通常是一个类，实现了一个 __call__ 方法，该方法的功能与 collate_fn 相同123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172from torch.utils.data import DataLoaderclass Trainer:    def __init__(            self,            model: Union[PreTrainedModel, nn.Module] = None,            args: TrainingArguments = None,            data_collator: Optional[DataCollator] = None,            train_dataset: Optional[Dataset] = None,            eval_dataset: Optional[Union[Dataset, Dict[str, Dataset]]] = None,            tokenizer: Optional[PreTrainedTokenizerBase] = None,            model_init: Optional[Callable[[], PreTrainedModel]] = None,            compute_metrics: Optional[Callable[[EvalPrediction], Dict]] = None,            callbacks: Optional[List[TrainerCallback]] = None,            optimizers: Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR] = (None, None),            preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,    ):        ...        default_collator = default_data_collator if tokenizer is None else DataCollatorWithPadding(tokenizer)        self.data_collator = data_collator if data_collator is not None else default_collator        ...    def get_train_dataloader(self) -&gt; DataLoader:        &quot;&quot;&quot;        Returns the training [`~torch.utils.data.DataLoader`].        Will use no sampler if `train_dataset` does not implement `__len__`, a random sampler (adapted to distributed        training if necessary) otherwise.        Subclass and override this method if you want to inject some custom behavior.        &quot;&quot;&quot;        if self.train_dataset is None:            raise ValueError(&quot;Trainer: training requires a train_dataset.&quot;)        train_dataset = self.train_dataset        data_collator = self.data_collator        if is_datasets_available() and isinstance(train_dataset, datasets.Dataset):            train_dataset = self._remove_unused_columns(train_dataset, description=&quot;training&quot;)        else:            data_collator = self._get_collator_with_removed_columns(data_collator, description=&quot;training&quot;)        if isinstance(train_dataset, torch.utils.data.IterableDataset):            if self.args.world_size &gt; 1:                train_dataset = IterableDatasetShard(                    train_dataset,                    batch_size=self._train_batch_size,                    drop_last=self.args.dataloader_drop_last,                    num_processes=self.args.world_size,                    process_index=self.args.process_index,                )            return DataLoader(                train_dataset,                batch_size=self._train_batch_size,                collate_fn=data_collator,                num_workers=self.args.dataloader_num_workers,                pin_memory=self.args.dataloader_pin_memory,            )        train_sampler = self._get_train_sampler()        return DataLoader(            train_dataset,            batch_size=self._train_batch_size,            sampler=train_sampler,            collate_fn=data_collator,            drop_last=self.args.dataloader_drop_last,            num_workers=self.args.dataloader_num_workers,            pin_memory=self.args.dataloader_pin_memory,            worker_init_fn=seed_worker,        )加载数据集查看数据集描述12345678910from datasets import load_dataset_builderds_builder = load_dataset_builder(&quot;rotten_tomatoes&quot;)ds_builder.info.descriptionMovie Review Dataset. This is a dataset of containing 5,331 positive and 5,331 negative processed sentences from Rotten Tomatoes movie reviews. This data was first used in Bo Pang and Lillian Lee, ``Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales.'', Proceedings of the ACL, 2005.ds_builder.info.features{'label': ClassLabel(num_classes=2, names=['neg', 'pos'], id=None), 'text': Value(dtype='string', id=None)}加载数据集123from datasets import load_datasetdataset = load_dataset(&quot;rotten_tomatoes&quot;, split=&quot;train&quot;)当一个数据集由多个文件(我们称之为分片)组成时，可以显著加快数据集的下载和准备步骤您可以使用num_proc参数选择并行准备数据集时要使用的进程数。在这种情况下，每个进程被分配了一部分分片来进行准备12345from datasets import load_datasetoscar_afrikaans = load_dataset(&quot;oscar-corpus/OSCAR-2201&quot;, &quot;af&quot;, num_proc=8)imagenet = load_dataset(&quot;imagenet-1k&quot;, num_proc=8)ml_librispeech_spanish = load_dataset(&quot;facebook/multilingual_librispeech&quot;, &quot;spanish&quot;, num_proc=8)查看数据集的分片名称，并加载指定的分片名称123456789101112131415161718192021222324252627282930313233343536373839404142434445464748from datasets import get_dataset_split_namesfrom datasets import load_datasetget_dataset_split_names(&quot;rotten_tomatoes&quot;)['train', 'validation', 'test']# 加载指定分片dataset = load_dataset(&quot;rotten_tomatoes&quot;, split=&quot;train&quot;)Dataset({    features: ['text', 'label'],    num_rows: 8530})# 还可以这么写：train_test_ds = datasets.load_dataset(&quot;bookcorpus&quot;, split=&quot;train+test&quot;)train_10_20_ds = datasets.load_dataset(&quot;bookcorpus&quot;, split=&quot;train[10:20]&quot;)train_10pct_ds = datasets.load_dataset(&quot;bookcorpus&quot;, split=&quot;train[:10%]&quot;)train_10_80pct_ds = datasets.load_dataset(&quot;bookcorpus&quot;, split=&quot;train[:10%]+train[-80%:]&quot;)val_ds = datasets.load_dataset(&quot;bookcorpus&quot;, split=[f&quot;train[{k}%:{k+10}%]&quot; for k in range(0, 100, 10)])train_ds = datasets.load_dataset(&quot;bookcorpus&quot;, split=[f&quot;train[:{k}%]+train[{k+10}%:]&quot; for k in range(0, 100, 10)])train_50_52_ds = datasets.load_dataset(&quot;bookcorpus&quot;, split=&quot;train[50%:52%]&quot;)train_52_54_ds = datasets.load_dataset(&quot;bookcorpus&quot;, split=&quot;train[52%:54%]&quot;)# 18 records, from 450 (included) to 468 (excluded).train_50_52pct1_ds = datasets.load_dataset(&quot;bookcorpus&quot;, split=datasets.ReadInstruction(&quot;train&quot;, from_=50, to=52, unit=&quot;%&quot;, rounding=&quot;pct1_dropremainder&quot;))# 18 records, from 468 (included) to 486 (excluded).train_52_54pct1_ds = datasets.load_dataset(&quot;bookcorpus&quot;, split=datasets.ReadInstruction(&quot;train&quot;,from_=52, to=54, unit=&quot;%&quot;, rounding=&quot;pct1_dropremainder&quot;))# Or equivalently:train_50_52pct1_ds = datasets.load_dataset(&quot;bookcorpus&quot;, split=&quot;train[50%:52%](pct1_dropremainder)&quot;)train_52_54pct1_ds = datasets.load_dataset(&quot;bookcorpus&quot;, split=&quot;train[52%:54%](pct1_dropremainder)&quot;)# 加载全部数据dataset = load_dataset(&quot;rotten_tomatoes&quot;)DatasetDict({    train: Dataset({        features: ['text', 'label'],        num_rows: 8530    })    validation: Dataset({        features: ['text', 'label'],        num_rows: 1066    })    test: Dataset({        features: ['text', 'label'],        num_rows: 1066    })})查看数据集子集，一个数据下可能还有很多子数据集123456from datasets import get_dataset_config_namesconfigs = get_dataset_config_names(&quot;PolyAI/minds14&quot;)print(configs)['cs-CZ', 'de-DE', 'en-AU', 'en-GB', 'en-US', 'es-ES', 'fr-FR', 'it-IT', 'ko-KR', 'nl-NL', 'pl-PL', 'pt-PT', 'ru-RU', 'zh-CN', 'all']加载指定子数据集123from datasets import load_datasetmindsFR = load_dataset(&quot;PolyAI/minds14&quot;, &quot;fr-FR&quot;, split=&quot;train&quot;) # 指定子数据集是fr-FR指定数据集的文件, 避免load过多的数据12data_files = {&quot;validation&quot;: &quot;en/c4-validation.*.json.gz&quot;}c4_validation = load_dataset(&quot;allenai/c4&quot;, data_files=data_files, split=&quot;validation&quot;)load本地的json、csv文件等，可以load远程文件、sql等1234567#{&quot;version&quot;: &quot;0.1.0&quot;,# &quot;data&quot;: [{&quot;a&quot;: 1, &quot;b&quot;: 2.0, &quot;c&quot;: &quot;foo&quot;, &quot;d&quot;: false},#          {&quot;a&quot;: 4, &quot;b&quot;: -5.5, &quot;c&quot;: null, &quot;d&quot;: true}]#}from datasets import load_datasetdataset = load_dataset(&quot;json&quot;, data_files=&quot;my_file.json&quot;, field=&quot;data&quot;)通过python对象来创建dataset1234567891011121314from datasets import Datasetimport pandas as pd# 字典方式my_dict = {&quot;a&quot;: [1, 2, 3]}dataset = Dataset.from_dict(my_dict)# list方式my_list = [{&quot;a&quot;: 1}, {&quot;a&quot;: 2}, {&quot;a&quot;: 3}]dataset = Dataset.from_list(my_list)# pandas方式df = pd.DataFrame({&quot;a&quot;: [1, 2, 3]})dataset = Dataset.from_pandas(df)load多个文本文件: 文本必须一行就是一条样本12345from datasets import load_datasetdataset = load_dataset(&quot;text&quot;, data_files={&quot;train&quot;: [&quot;my_text_1.txt&quot;, &quot;my_text_2.txt&quot;], &quot;test&quot;: &quot;my_test_file.txt&quot;})# Load from a directorydataset = load_dataset(&quot;text&quot;, data_dir=&quot;path/to/text/dataset&quot;)离线load: 将环境变量HF_DATASETS_OFFLINE设置为1以启用完全离线模式进阶加载数据集从脚本加载数据集您可能在本地计算机上有一个🤗Datasets的加载脚本。在这种情况下，通过将以下路径之一传递给load_dataset()来加载数据集：加载脚本文件的本地路径。 包含加载脚本文件的目录的本地路径(仅当脚本文件与目录具有相同的名称时)1234dataset = load_dataset(&quot;path/to/local/loading_script/loading_script.py&quot;, split=&quot;train&quot;)# equivalent because the file has the same name as the directorydataset = load_dataset(&quot;path/to/local/loading_script&quot;, split=&quot;train&quot;)可以从Hub上下载加载脚本，并对其进行编辑以添加自己的修改。将数据集仓库下载到本地，以便加载脚本中相对路径引用的任何数据文件都可以被加载1git clone https://huggingface.co/datasets/eli5在加载脚本上进行编辑后，通过将其本地路径传递给load_dataset()来加载它123from datasets import load_dataseteli5 = load_dataset(&quot;path/to/local/eli5&quot;)csv+json方式数据集可以从存储在计算机上的本地文件和远程文件中加载。这些数据集很可能以csv、json、txt或parquet文件的形式存储。load_dataset()函数可以加载这些文件类型的数据集1234567891011121314151617181920212223242526from datasets import load_dataset# csv方式dataset = load_dataset(&quot;csv&quot;, data_files=&quot;my_file.csv&quot;)# json方式# {&quot;a&quot;: 1, &quot;b&quot;: 2.0, &quot;c&quot;: &quot;foo&quot;, &quot;d&quot;: false}# {&quot;a&quot;: 4, &quot;b&quot;: -5.5, &quot;c&quot;: null, &quot;d&quot;: true}dataset = load_dataset(&quot;json&quot;, data_files=&quot;my_file.json&quot;)# {&quot;version&quot;: &quot;0.1.0&quot;,#  &quot;data&quot;: [{&quot;a&quot;: 1, &quot;b&quot;: 2.0, &quot;c&quot;: &quot;foo&quot;, &quot;d&quot;: false},#           {&quot;a&quot;: 4, &quot;b&quot;: -5.5, &quot;c&quot;: null, &quot;d&quot;: true}]# }dataset = load_dataset(&quot;json&quot;, data_files=&quot;my_file.json&quot;, field=&quot;data&quot;)# 从http方式加载csvbase_url = &quot;https://rajpurkar.github.io/SQuAD-explorer/dataset/&quot;dataset = load_dataset(&quot;json&quot;, data_files={&quot;train&quot;: base_url + &quot;train-v1.1.json&quot;, &quot;validation&quot;: base_url + &quot;dev-v1.1.json&quot;}, field=&quot;data&quot;)# Parquet方式dataset = load_dataset(&quot;parquet&quot;, data_files={'train': 'train.parquet', 'test': 'test.parquet'})base_url = &quot;https://storage.googleapis.com/huggingface-nlp/cache/datasets/wikipedia/20200501.en/1.0.0/&quot;data_files = {&quot;train&quot;: base_url + &quot;wikipedia-train.parquet&quot;}wiki = load_dataset(&quot;parquet&quot;, data_files=data_files, split=&quot;train&quot;)sql方式使用from_sql()方法可以通过指定连接到数据库的URI来读取数据库内容。您可以读取表名或执行查询操作1234from datasets import Datasetdataset = Dataset.from_sql(&quot;data_table_name&quot;, con=&quot;sqlite:///sqlite_file.db&quot;)dataset = Dataset.from_sql(&quot;SELECT text FROM table WHERE length(text) &gt; 100 LIMIT 10&quot;, con=&quot;sqlite:///sqlite_file.db&quot;)For more details, check out the how to load tabular datasets from SQL databases guide.探索数据集下标12345678910111213# 第一个样本dataset[0]#{'label': 1,# 'text': 'the rock is destined to be the 21st century\'s new &quot; conan &quot; and that he\'s going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .'}# 最后一个样本dataset[-1]# 只取text列dataset[&quot;text&quot;] # 返回a list of 样本列# 第一个样本text列dataset[0][&quot;text&quot;] # 性能：dataset[0]['text']比dataset['text'][0]快2倍。数据切片12345# Get the first three rowsdataset[:3]# Get rows between three and sixdataset[3:6]迭代方式，streaming=True1234567891011121314151617from datasets import load_datasetiterable_dataset = load_dataset(&quot;food101&quot;, split=&quot;train&quot;, streaming=True)for example in iterable_dataset:    print(example)    break    {'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512 at 0x7F0681F5C520&gt;, 'label': 6}# Get first three exampleslist(iterable_dataset.take(3))[{'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=384x512 at 0x7F7479DEE9D0&gt;,  'label': 6}, {'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x512 at 0x7F7479DE8190&gt;,  'label': 6}, {'image': &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x383 at 0x7F7479DE8310&gt;,  'label': 6}]排序+shuffle+选择+filter+切分数据集+分片12345678910111213141516171819202122232425# sort: 按某一列排序dataset.sort(&quot;label&quot;)# 打乱shuffled_dataset = sorted_dataset.shuffle(seed=42)# 选择small_dataset = dataset.select([0, 10, 20, 30, 40, 50])# 匹配查找start_with_ar = dataset.filter(lambda example: example[&quot;sentence1&quot;].startswith(&quot;Ar&quot;))len(start_with_ar)start_with_ar[&quot;sentence1&quot;]# 匹配查找：根据下标even_dataset = dataset.filter(lambda example, idx: idx % 2 == 0, with_indices=True)# 切分dataset.train_test_split(test_size=0.1)# 分片# 数据集支持分片，将非常大的数据集划分为预定义数量的块。 在 shard() 中指定 num_shards 参数以确定要将数据集拆分成的分片数。 您还需要使用 index 参数提供要返回的分片。from datasets import load_datasetdatasets = load_dataset(&quot;imdb&quot;, split=&quot;train&quot;)print(dataset)dataset.shard(num_shards=4, index=0) # 四分之一列重命名+移除列+转换格式+flatten1234567891011121314151617181920from datasets import ClassLabel, Valuefrom datasets import load_dataset# 列重命名dataset = dataset.rename_column(&quot;sentence1&quot;, &quot;sentenceA&quot;)# 去掉某一列dataset = dataset.remove_columns([&quot;sentence1&quot;, &quot;sentence2&quot;])# 转换格式：一列或者多列new_features = dataset.features.copy()new_features[&quot;label&quot;] = ClassLabel(names=[&quot;negative&quot;, &quot;positive&quot;])new_features[&quot;idx&quot;] = Value(&quot;int64&quot;)dataset = dataset.cast(new_features)# 转换格式：一列dataset = dataset.cast_column(&quot;audio&quot;, Audio(sampling_rate=16000))# 将某一列的key\value拉平dataset = load_dataset(&quot;squad&quot;, split=&quot;train&quot;) # ???map转换1234567891011121314151617181920212223242526272829303132333435363738394041424344454647from multiprocess import set_start_methodfrom datasets import load_datasetimport torchimport osset_start_method(&quot;spawn&quot;)# remove_columns 转换的同时去掉某一列updated_dataset = dataset.map(lambda example: {&quot;new_sentence&quot;: example[&quot;sentence1&quot;]}, remove_columns=[&quot;sentence1&quot;])updated_dataset.column_names# with_indices: 对下标处理updated_dataset = dataset.map(lambda example, idx: {&quot;sentence2&quot;: f&quot;{idx}: &quot; + example[&quot;sentence2&quot;]}, with_indices=True)updated_dataset[&quot;sentence2&quot;][:5]#如果您设置with_rank=True，map()也适用于进程的等级。 这类似于with_indices参数。 映射函数中的with_rank参数位于索引1之后(如果它已经存在)def gpu_computation(example, rank):    os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = str(rank % torch.cuda.device_count())    # Your big GPU call goes here    return examplesupdated_dataset = dataset.map(gpu_computation, with_rank=True)# 多线程updated_dataset = dataset.map(lambda example, idx: {&quot;sentence2&quot;: f&quot;{idx}: &quot; + example[&quot;sentence2&quot;]}, num_proc=4)# batchedchunked_dataset = dataset.map(chunk_examples, batched=True, remove_columns=dataset.column_names)# 数据增强def augment_data(examples):    outputs = []    for sentence in examples[&quot;sentence1&quot;]:        words = sentence.split(' ')        K = randint(1, len(words)-1)        masked_sentence = &quot; &quot;.join(words[:K]  + [mask_token] + words[K+1:])        predictions = fillmask(masked_sentence)        augmented_sequences = [predictions[i][&quot;sequence&quot;] for i in range(3)]        outputs += [sentence] + augmented_sequences    return {&quot;data&quot;: outputs}augmented_dataset = smaller_dataset.map(augment_data, batched=True, remove_columns=dataset.column_names, batch_size=8)augmented_dataset[:9][&quot;data&quot;]# 处理多splitdataset = load_dataset('glue', 'mrpc')encoded_dataset = dataset.map(lambda examples: tokenizer(examples[&quot;sentence1&quot;]), batched=True)encoded_dataset[&quot;train&quot;][0]合并+拼接数据集1234567891011121314from datasets import concatenate_datasets, load_datasetfrom datasets import Dataset# 加载数据集bookcorpus = load_dataset(&quot;bookcorpus&quot;, split=&quot;train&quot;)wiki = load_dataset(&quot;wikipedia&quot;, &quot;20220301.en&quot;, split=&quot;train&quot;)wiki = wiki.remove_columns([col for col in wiki.column_names if col != &quot;text&quot;])  # only keep the 'text' columnassert bookcorpus.features.type == wiki.features.typebert_dataset = concatenate_datasets([bookcorpus, wiki])# 可以换concate的方向bookcorpus_ids = Dataset.from_dict({&quot;ids&quot;: list(range(len(bookcorpus)))})bookcorpus_with_ids = concatenate_datasets([bookcorpus, bookcorpus_ids], axis=1)相互穿插1234567891011121314151617import torch# 按概率穿插seed = 42probabilities = [0.3, 0.5, 0.2]d1 = Dataset.from_dict({&quot;a&quot;: [0, 1, 2]})d2 = Dataset.from_dict({&quot;a&quot;: [10, 11, 12, 13]})d3 = Dataset.from_dict({&quot;a&quot;: [20, 21, 22]})dataset = interleave_datasets([d1, d2, d3], probabilities=probabilities, seed=seed)dataset[&quot;a&quot;]# 按所有的样本都出现过一次后，马上停止d1 = Dataset.from_dict({&quot;a&quot;: [0, 1, 2]})d2 = Dataset.from_dict({&quot;a&quot;: [10, 11, 12, 13]})d3 = Dataset.from_dict({&quot;a&quot;: [20, 21, 22]})dataset = interleave_datasets([d1, d2, d3], stopping_strategy=&quot;all_exhausted&quot;)dataset[&quot;a&quot;]format1234567dataset.set_format(type=&quot;torch&quot;, columns=[&quot;input_ids&quot;, &quot;token_type_ids&quot;, &quot;attention_mask&quot;, &quot;label&quot;])# 返回一个新datasetdataset = dataset.with_format(type=&quot;torch&quot;, columns=[&quot;input_ids&quot;, &quot;token_type_ids&quot;, &quot;attention_mask&quot;, &quot;label&quot;])# 查看dataset.format保存12345678from datasets import load_from_diskencoded_dataset.save_to_disk(&quot;path/of/my/dataset/directory&quot;)# 从本地load上来reloaded_dataset = load_from_disk(&quot;path/of/my/dataset/directory&quot;)encoded_dataset.to_csv(&quot;path/of/my/dataset.csv&quot;)Dataset.to_json()Preprocess处理文本处理：用transformers的tokenizer1234567891011from transformers import AutoTokenizerfrom datasets import load_datasettokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-uncased&quot;)dataset = load_dataset(&quot;rotten_tomatoes&quot;, split=&quot;train&quot;)tokenizer(dataset[0][&quot;text&quot;]){'input_ids': [101, 1103, 2067, 1110, 17348, 1106, 1129, 1103, 6880, 1432, 112, 188, 1207, 107, 14255, 1389, 107, 1105, 1115, 1119, 112, 188, 1280, 1106, 1294, 170, 24194, 1256, 3407, 1190, 170, 11791, 5253, 188, 1732, 7200, 10947, 12606, 2895, 117, 179, 7766, 118, 172, 15554, 1181, 3498, 6961, 3263, 1137, 188, 1566, 7912, 14516, 6997, 119, 102],  'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],  'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}分词器返回一个包含三个项目的字典：input_ids：表示文本中各个标记的数字token_type_ids：如果有多个序列，指示一个标记属于哪个序列attention_mask：指示一个标记是否应该被掩盖(masked)1dataset.set_format(type=&quot;torch&quot;, columns=[&quot;input_ids&quot;, &quot;token_type_ids&quot;, &quot;attention_mask&quot;, &quot;labels&quot;])音频信号：重新采样音频信号123456789101112from transformers import AutoFeatureExtractorfrom datasets import load_dataset, Audiofeature_extractor = AutoFeatureExtractor.from_pretrained(&quot;facebook/wav2vec2-base-960h&quot;)dataset = load_dataset(&quot;PolyAI/minds14&quot;, &quot;en-US&quot;, split=&quot;train&quot;)dataset[0][&quot;audio&quot;]{'array': array([ 0.        ,  0.00024414, -0.00024414, ..., -0.00024414,         0.        ,  0.        ], dtype=float32), 'path': '/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-US~JOINT_ACCOUNT/602ba55abb1e6d0fbce92065.wav', 'sampling_rate': 8000}MInDS-14数据集卡会告诉您采样率为8kHzWav2Vec2模型卡说它是在16kHz语音音频上采样的。 这意味着您需要对MInDS-14数据集进行上采样以匹配模型的采样率使用cast_column()函数并在Audio功能中设置sampling_rate参数以对音频信号进行上采样。 当您现在调用音频列时，它会被解码并重新采样到16kHz：123456789101112dataset = dataset.cast_column(&quot;audio&quot;, Audio(sampling_rate=16_000))dataset[0][&quot;audio&quot;]# 加速：使用 map() 函数将整个数据集重新采样到16kHzdef preprocess_function(examples):    audio_arrays = [x[&quot;array&quot;] for x in examples[&quot;audio&quot;]]    inputs = feature_extractor(        audio_arrays, sampling_rate=feature_extractor.sampling_rate, max_length=16000, truncation=True    )    return inputsdataset = dataset.map(preprocess_function, batched=True)图像增强在图像数据集中，最常见的预处理操作之一是数据增强(data augmentation)，这是一种在不改变数据含义的情况下对图像引入随机变化的过程这可以包括改变图像的颜色属性或随机裁剪图像。您可以自由选择任何数据增强库，并且🤗Datasets将帮助您将数据增强应用到您的数据集中12345678910111213141516from transformers import AutoFeatureExtractorfrom datasets import load_dataset, Imagefrom torchvision.transforms import RandomRotationfeature_extractor = AutoFeatureExtractor.from_pretrained(&quot;google/vit-base-patch16-224-in21k&quot;)dataset = load_dataset(&quot;beans&quot;, split=&quot;train&quot;)rotate = RandomRotation(degrees=(0, 90))def transforms(examples):    examples[&quot;pixel_values&quot;] = [rotate(image.convert(&quot;RGB&quot;)) for image in examples[&quot;image&quot;]]    return examples# 应用图像转换dataset.set_transform(transforms)dataset[0][&quot;pixel_values&quot;]label id对齐在Transformers库中，label id对齐(label ID alignment)通常指的是将标签与模型输出的预测结果对齐。当使用预训练模型进行分类或回归等任务时，通常需要将标签映射为模型期望的标签ID具体来说，对于分类任务，常见的做法是将标签映射为整数标签ID。例如，如果有三个类别[“cat”, “dog”, “bird”]，可以将它们映射为[0, 1, 2]，并将模型的输出标签预测结果与这些标签ID进行对齐对于回归任务，可能需要将连续值的标签进行离散化或归一化，并将其映射为标签ID。例如，将一个连续的目标值范围映射为一组离散的标签ID在使用Transformers库进行训练或评估时，您需要确保标签与模型的输出结果具有相同的标签ID对齐，以便正确计算损失、评估指标和解码预测结果需要注意的是，标签ID对齐的具体实现方式可能因任务和库的使用而有所不同。在具体的代码实现中，您可能需要根据您的数据集和模型设置进行相应的标签ID对齐操作123456from datasets import load_datasetlabel2id = {&quot;contradiction&quot;: 0, &quot;neutral&quot;: 1, &quot;entailment&quot;: 2}mnli = load_dataset(&quot;glue&quot;, &quot;mnli&quot;, split=&quot;train&quot;)mnli_aligned = mnli.align_labels_with_mapping(label2id, &quot;label&quot;)构建数据集如果您使用自己的数据，可能需要创建一个数据集。使用🤗Datasets创建数据集可以享受到该库的所有优势：快速加载和处理数据、流式处理大型数据集、内存映射等等。您可以使用🤗Datasets的低代码方法轻松快速地创建数据集，减少启动训练模型所需的时间。在许多情况下，只需将数据文件拖放到Hub上的数据集仓库中，就可以轻松完成在本教程中，您将学习如何使用🤗Datasets的低代码方法创建各种类型的数据集：基于文件夹的构建器(Folder-based builders)，用于快速创建图像或音频数据集使用from_方法从本地文件创建数据集基于文件夹的构建器有两个基于文件夹的构建器：ImageFolder(图像文件夹构建器)和AudioFolder(音频文件夹构建器)它们是低代码方法，可以快速创建包含数千个示例的图像、语音和音频数据集。它们非常适用于在扩展到更大的数据集之前，快速原型化计算机视觉和语音模型基于文件夹的构建器会使用您的数据，并自动生成数据集的特征、划分和标签。具体来说：ImageFolder使用Image特征来解码图像文件。它支持许多图像扩展格式，例如jpg和png，还支持其他格式。您可以查看支持的图像扩展格式的完整列表AudioFolder使用Audio特征来解码音频文件。它支持音频扩展格式，如wav和mp3，您可以查看支持的音频扩展格式的完整列表例如，如果您的图像数据集(对于音频数据集也是一样)存储如下所示：1234567pokemon/train/grass/bulbasaur.pngpokemon/train/fire/charmander.pngpokemon/train/water/squirtle.pngpokemon/test/grass/ivysaur.pngpokemon/test/fire/charmeleon.pngpokemon/test/water/wartortle.png12345from datasets import ImageFolderfrom datasets import AudioFolderdataset = load_dataset(&quot;imagefolder&quot;, data_dir=&quot;/path/to/pokemon&quot;)dataset = load_dataset(&quot;audiofolder&quot;, data_dir=&quot;/path/to/folder&quot;)数据集中可以包含有关数据集的其他信息，例如文本标题或转录，可以使用包含在数据集文件夹中的metadata.csv文件来进行存储metadata文件需要有一个file_name列，将图像或音频文件与其相应的元数据进行关联1234file_name, textbulbasaur.png, There is a plant seed on its back right from the day this Pokémon is born.charmander.png, It has a preference for hot things.squirtle.png, When it retracts its long neck into its shell, it squirts out water with vigorous force.To learn more about each of these folder-based builders, check out the and ImageFolder or AudioFolder guides.基于文件的构建器使用 from_generator() 方法是从生成器创建数据集的最节省内存的方式，这是由于生成器的迭代行为。这在处理非常大的数据集时特别有用，因为数据集是逐步在磁盘上生成的，然后进行内存映射，这样可以避免将整个数据集加载到内存中1234567from datasets import Datasetdef gen():    yield {&quot;pokemon&quot;: &quot;bulbasaur&quot;, &quot;type&quot;: &quot;grass&quot;}    yield {&quot;pokemon&quot;: &quot;squirtle&quot;, &quot;type&quot;: &quot;water&quot;}ds = Dataset.from_generator(gen)ds[0]{&quot;pokemon&quot;: &quot;bulbasaur&quot;, &quot;type&quot;: &quot;grass&quot;}基于生成器的IterableDataset需要使用for循环进行迭代，例如：1234567from datasets import IterableDatasetds = IterableDataset.from_generator(gen)for example in ds:    print(example)    {&quot;pokemon&quot;: &quot;bulbasaur&quot;, &quot;type&quot;: &quot;grass&quot;}{&quot;pokemon&quot;: &quot;squirtle&quot;, &quot;type&quot;: &quot;water&quot;}使用from_dict()方法是从字典创建数据集的简单直接的方式：12345from datasets import Datasetds = Dataset.from_dict({&quot;pokemon&quot;: [&quot;bulbasaur&quot;, &quot;squirtle&quot;], &quot;type&quot;: [&quot;grass&quot;, &quot;water&quot;]})ds[0]{&quot;pokemon&quot;: &quot;bulbasaur&quot;, &quot;type&quot;: &quot;grass&quot;}分享数据集点击您的个人资料并选择新的数据集以创建一个新的数据集仓库。 为您的数据集选择一个名称，并选择它是一个公共数据集还是私有数据集。公共数据集对任何人可见，而私有数据集只能由您或您组织的成员查看一旦您的数据集存储在Hub上，任何人都可以使用load_dataset()函数加载它：123from datasets import load_datasetdataset = load_dataset(&quot;stevhliu/demo&quot;)使用Python进行上传喜欢以编程方式上传数据集的用户可以使用huggingface_hub库。该库允许用户从Python中与Hub进行交互首先安装该库：1pip install huggingface_hub要在Hub上使用Python上传数据集，您需要登录到您的Hugging Face账户：1huggingface-cli login使用push_to_hub()函数帮助您将文件添加、提交和推送到您的仓库：12345from datasets import load_datasetdataset = load_dataset(&quot;stevhliu/demo&quot;)# dataset = dataset.map(...)  # 在这里进行所有的数据处理dataset.push_to_hub(&quot;stevhliu/processed_demo&quot;)如果要将数据集设置为私有，请将private参数设置为True。该参数仅在首次创建仓库时有效1dataset.push_to_hub(&quot;stevhliu/private_processed_demo&quot;, private=True)评估指标安装一种用于轻松评估机器学习模型和数据集的库只需一行代码，您就可以访问数十种不同领域(自然语言处理、计算机视觉、强化学习等)的评估方法无论是在本地机器上还是在分布式训练环境中，您都可以以一种一致且可重复的方式评估您的模型安装1pip install evaluate测试123python -c &quot;import evaluate; print(evaluate.load('exact_match').compute(references=['hello'], predictions=['hello']))&quot;{'exact_match': 1.0}快速开始指标种类Evaluate Metric卡片实例🤗Evaluate提供了广泛的评估工具。它涵盖了文本、计算机视觉、音频等多种形式，并提供了用于评估模型或数据集的工具。这些工具分为三个类别评估类型 典型的机器学习流程涉及到不同方面的评估，对于每个方面，🤗 Evaluate都提供了相应的工具：指标(Metric)：用于评估模型的性能，通常涉及模型的预测结果和一些真实标签。您可以在evaluate-metric中找到所有集成的指标比较(Comparison)：用于比较两个模型。可以通过将它们的预测结果与真实标签进行比较并计算它们的一致性来进行比较。您可以在evaluate-comparison中找到所有集成的比较方法测量(Measurement)：数据集和训练在其上的模型同样重要。通过测量可以研究数据集的特性。您可以在evaluate-measurement中找到所有集成的测量方法每个评估模块都作为一个Space存储在Hugging Face Hub上。它们提供了一个交互式小部件和一个文档卡片，用于记录其使用方法和限制评估工具之间的关系和区别Evaluate库中的Metric(指标)、Comparison(比较)和Measurement(测量)是三种不同的评估工具，用于评估机器学习模型和数据集。它们之间的关系和区别如下：Metric(指标)：用途：用于评估模型的性能具体含义：指标通过将模型的预测结果与真实标签进行比较来衡量模型的表现示例：准确率、精确率、召回率、F1分数等目的：提供了对模型性能的定量评估，帮助衡量模型在特定任务上的表现Comparison(比较)：用途：用于比较两个模型之间的差异具体含义：比较工具将两个模型的预测结果与真实标签进行对比，计算它们之间的一致性或差异程度示例：一致性指标、相对误差等目的：帮助评估不同模型之间的性能差异，找到更好的模型或进行模型选择Measurement(测量)：用途：用于研究数据集的属性和特性具体含义：测量工具用于对数据集进行分析，探索数据集的结构、分布、偏差等方面的信息示例：数据集大小、样本分布、类别不平衡度等目的：提供对数据集的详细了解，帮助了解数据集的特点和潜在问题这三种评估工具在Evaluate库中各自独立，用于不同的评估目的。Metric用于衡量模型性能，Comparison用于比较不同模型之间的性能差异，Measurement用于研究和了解数据集的特性。通过使用这些工具，可以全面评估和理解机器学习模型和数据集的表现和特点指标加载官方+社区 指标在使用Hugging Face的Evaluate库加载评估工具时，可以通过显式指定评估的类型来确保加载正确的工具。这可以防止名称冲突或混淆，确保您使用的是期望的评估工具123456789import evaluateaccuracy = evaluate.load(&quot;accuracy&quot;)# 显式指定评估的类型word_length = evaluate.load(&quot;word_length&quot;, module_type=&quot;measurement&quot;)# 社区指标element_count = evaluate.load(&quot;lvwerra/element_count&quot;, module_type=&quot;measurement&quot;)查看可用的模块方法1234567evaluate.list_evaluation_modules(  module_type=&quot;comparison&quot;,  include_community=False,  with_details=True)[{'name': 'mcnemar', 'type': 'comparison', 'community': False, 'likes': 1}, {'name': 'exact_match', 'type': 'comparison', 'community': False, 'likes': 0}]指标计算计算指标当涉及到计算实际得分时，有两种主要的方法：一体式计算(All-in-one)：通过一次性将所有必要的输入传递给compute()方法来计算得分123accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1]){'accuracy': 0.5}逐步计算(Incremental)：通过使用EvaluationModule.add()或EvaluationModule.add_batch()将必要的输入逐步添加到模块中，然后在最后使用 EvaluationModule.compute()计算得分1234567891011# add的方式for ref, pred in zip([0,1,0,1], [1,0,0,1]):    accuracy.add(references=ref, predictions=pred)accuracy.compute(){'accuracy': 0.5}# add_batch的方式for refs, preds in zip([[0,1],[0,1]], [[1,0],[0,1]]):    accuracy.add_batch(references=refs, predictions=preds)accuracy.compute(){'accuracy': 0.5}在你需要以批量方式从模型中获取预测结果时特别有用：1234for model_inputs, gold_standards in evaluation_dataset:    predictions = model(model_inputs)    metric.add_batch(references=gold_standards, predictions=predictions)metric.compute()分布式指标在分布式环境中计算指标可能会有些棘手。指标评估是在不同的数据子集上的单独Python进程或节点中执行的通常情况下，当一个指标得分是可加的()时，你可以使用分布式的reduce操作来收集每个数据子集的得分。但是当指标是非可加的()时，情况就不那么简单了。例如，你不能将每个数据子集的F1分数相加作为最终的指标克服这个问题的常见方法是回退到单进程评估，但指标在单个GPU上进行评估，这会导致效率降低🤗Evaluate通过仅在第一个节点上计算最终的指标来解决了这个问题预测结果和参考结果被分别计算并提供给每个节点的指标，这些结果暂时存储在Apache Arrow表中，避免了GPU或CPU内存的混乱当你准备使用compute()计算最终指标时，第一个节点能够访问所有其他节点上存储的预测结果和参考结果。一旦它收集到所有的预测结果和参考结果，compute()将进行最终的指标评估这个解决方案使得🤗Evaluate能够在分布式设置中执行分布式预测，这对于提高评估速度非常重要。同时，你还可以使用复杂的非可加指标，而不浪费宝贵的GPU或CPU内存组合评估通常情况下，我们不仅想评估单个指标，而是想评估一系列不同的指标，以捕捉模型性能的不同方面例如，对于分类问题，除了准确度外，通常还会计算F1分数、召回率和精确度，以便更好地了解模型的性能。当然，你可以加载一系列指标并依次调用它们。然而，一种更方便的方法是使用combine()函数将它们捆绑在一起：123456789clf_metrics = evaluate.combine([&quot;accuracy&quot;, &quot;f1&quot;, &quot;precision&quot;, &quot;recall&quot;])clf_metrics.compute(predictions=[0, 1, 0], references=[0, 1, 1]){  'accuracy': 0.667,  'f1': 0.667,  'precision': 1.0,  'recall': 0.5}自动化评估使用evaluate.evaluator()提供了自动化的评估功能，只需要一个模型、数据集和度量指标，与EvaluationModules中的度量指标相比，它不需要模型的预测结果。因此，使用给定的度量指标在数据集上评估模型更容易，因为推理过程是在内部处理的为了实现这一点，它使用了transformers库中的pipeline抽象。然而，只要符合pipeline接口，你也可以使用自己的框架1234from transformers import pipelinefrom datasets import load_datasetfrom evaluate import evaluatorimport evaluate为了使用evaluator进行评估，让我们加载一个基于IMDb训练的transformers pipeline（但你也可以传递自己的自定义推理类来适应任何遵循pipeline调用API的框架），并使用IMDb的测试集和准确度度量指标进行评估123456789pipe = pipeline(&quot;text-classification&quot;, model=&quot;lvwerra/distilbert-imdb&quot;, device=0)data = load_dataset(&quot;imdb&quot;, split=&quot;test&quot;).shuffle().select(range(1000))metric = evaluate.load(&quot;accuracy&quot;)task_evaluator = evaluator(&quot;text-classification&quot;)results = task_evaluator.compute(model_or_pipeline=pipe, data=data, metric=metric,                       label_mapping={&quot;NEGATIVE&quot;: 0, &quot;POSITIVE&quot;: 1},){'accuracy': 0.934}仅仅计算度量指标的值通常还不足以知道一个模型是否显著优于另一个模型。通过使用自助法(bootstrapping)，evaluate计算置信区间和标准误差，这有助于估计分数的稳定性1234567891011results = eval.compute(model_or_pipeline=pipe, data=data, metric=metric,                       label_mapping={&quot;NEGATIVE&quot;: 0, &quot;POSITIVE&quot;: 1},                       strategy=&quot;bootstrap&quot;, n_resamples=200){'accuracy':    {      'confidence_interval': (0.906, 0.9406749892841922),      'standard_error': 0.00865213251082787,      'score': 0.923    }}评估器期望数据输入具有”text”和”label”列。如果您的数据集不同，可以使用关键字参数input_column=”text”和label_column=”label”来提供列名目前只支持”text-classification”任务，将来可能会添加更多的任务类型结果存储评估结果save和push保存和分享评估结果是一个重要的步骤。我们提供evaluate.save()函数来方便地保存指标结果。你可以传递一个特定的文件名或目录。在后一种情况下，结果将保存在一个带有自动创建的文件名的文件中除了目录或文件名，该函数还接受任意的键值对作为输入，并将它们存储在一个JSON文件中123456789101112131415161718result = accuracy.compute(references=[0,1,0,1], predictions=[1,0,0,1])hyperparams = {&quot;model&quot;: &quot;bert-base-uncased&quot;}evaluate.save(&quot;./results/&quot;, experiment=&quot;run 42&quot;, **result, **hyperparams)PosixPath('results/result-2022_05_30-22_09_11.json')# result-2022_05_30-22_09_11.json{    &quot;experiment&quot;: &quot;run 42&quot;,    &quot;accuracy&quot;: 0.5,    &quot;model&quot;: &quot;bert-base-uncased&quot;,    &quot;_timestamp&quot;: &quot;2022-05-30T22:09:11.959469&quot;,    &quot;_git_commit_hash&quot;: &quot;123456789abcdefghijkl&quot;,    &quot;_evaluate_version&quot;: &quot;0.1.0&quot;,    &quot;_python_version&quot;: &quot;3.9.12 (main, Mar 26 2022, 15:51:15) \n[Clang 13.1.6 (clang-1316.0.21.2)]&quot;,    &quot;_interpreter_path&quot;: &quot;/Users/leandro/git/evaluate/env/bin/python&quot;}除了指定的字段，它还包含有用的系统信息，用于重现结果，你还应该将它们报告到模型在Hub上的存储库中1234567891011evaluate.push_to_hub(  model_id=&quot;huggingface/gpt2-wikitext2&quot;,  # model repository on hub  metric_value=0.5,                       # metric value  metric_type=&quot;bleu&quot;,                     # metric name, e.g. accuracy.name  metric_name=&quot;BLEU&quot;,                     # pretty name which is displayed  dataset_type=&quot;wikitext&quot;,                # dataset name on the hub  dataset_name=&quot;WikiText&quot;,                # pretty name  dataset_split=&quot;test&quot;,                   # dataset split used  task_type=&quot;text-generation&quot;,            # task id, see https://github.com/huggingface/datasets/blob/master/src/datasets/utils/resources/tasks.json  task_name=&quot;Text Generation&quot;             # pretty name for task)上传自己的指标Creating and sharing a new evaluation可视化当比较多个模型时，仅通过查看它们的得分往往很难发现它们之间的差异。而且通常情况下，并没有一个单一的最佳模型，而是在准确性和延迟等方面存在着权衡，因为较大的模型可能具有更好的性能但也更慢。我们正在逐步添加不同的可视化方法，例如绘图，以便更轻松地选择适合特定用例的最佳模型。例如，如果您有多个模型的结果列表（以字典形式），您可以将它们传递给radar_plot()函数进行可视化：123456789101112import evaluatefrom evaluate.visualization import radar_plotdata = [   {&quot;accuracy&quot;: 0.99, &quot;precision&quot;: 0.8, &quot;f1&quot;: 0.95, &quot;latency_in_seconds&quot;: 33.6},   {&quot;accuracy&quot;: 0.98, &quot;precision&quot;: 0.87, &quot;f1&quot;: 0.91, &quot;latency_in_seconds&quot;: 11.2},   {&quot;accuracy&quot;: 0.98, &quot;precision&quot;: 0.78, &quot;f1&quot;: 0.88, &quot;latency_in_seconds&quot;: 87.6},    {&quot;accuracy&quot;: 0.88, &quot;precision&quot;: 0.78, &quot;f1&quot;: 0.81, &quot;latency_in_seconds&quot;: 101.6}   ]model_names = [&quot;Model 1&quot;, &quot;Model 2&quot;, &quot;Model 3&quot;, &quot;Model 4&quot;]plot = radar_plot(data=data, model_names=model_names)plot.show()选择合适指标评估指标可以分为三个高级类别：通用指标：适用于各种情况和数据集的指标，例如精确度和准确度12345precision_metric = evaluate.load(&quot;precision&quot;)results = precision_metric.compute(references=[0, 1], predictions=[0, 1])print(results){'precision': 1.0}任务特定指标：仅适用于特定任务的指标，例如机器翻译(通常使用BLEU或ROUGE指标进行评估)或命名实体识别(通常使用seqeval进行评估)数据集特定指标：旨在衡量模型在特定基准数据集上的性能，例如GLUE基准测试具有专门的评估指标transformers概述What 🤗 Transformers can do🤗 Transformers提供了API和工具，可轻松下载和训练最先进的预训练模型。使用预训练模型可以减少计算成本、碳足迹，并节省从头开始训练模型所需的时间和资源。这些模型支持不同领域的常见任务，包括：📝 自然语言处理：文本分类、命名实体识别、问答系统、语言建模、摘要生成、翻译、多项选择和文本生成🖼️ 计算机视觉：图像分类、目标检测和分割🗣️ 音频：自动语音识别和音频分类🐙 多模态：表格问答、光学字符识别、从扫描文档中提取信息、视频分类和视觉问答🤗 Transformers支持在PyTorch、TensorFlow和JAX之间进行框架互操作。这提供了在模型的不同阶段使用不同框架的灵活性；可以在一个框架中用三行代码训练模型，然后在另一个框架中加载模型进行推理。模型还可以导出为ONNX和TorchScript等格式，以便在生产环境中进行部署安装1pip install transformers datasets快速开始Pipelinepipeline()是使用预训练模型进行推理的最简单和最快捷的方法。您可以直接使用pipeline()进行许多任务的推理，涵盖了不同的模态，下表列出了其中一些任务TaskDescriptionModalityPipeline identifierText classificationassign a label to a given sequence of textNLPpipeline(task=“sentiment-analysis”)Text generationgenerate text given a promptNLPpipeline(task=“text-generation”)Summarizationgenerate a summary of a sequence of text or documentNLPpipeline(task=“summarization”)Image classificationassign a label to an imageCVpipeline(task=“image-classification”)Image segmentationassign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation)CVpipeline(task=“image-segmentation”)Object detectionpredict the bounding boxes and classes of objects in an imageCVpipeline(task=“object-detection”)Audio classificationassign a label to some audio dataAudiopipeline(task=“audio-classification”)Automatic speech recognitiontranscribe speech into textAudiopipeline(task=“automatic-speech-recognition”)Visual question answeringanswer a question about the image, given an image and a questionMultimodalpipeline(task=“vqa”)Document question answeringanswer a question about a document, given an image and a questionMultimodalpipeline(task=“document-question-answering”)Image captioninggenerate a caption for a given imageMultimodalpipeline(task=“image-to-text”)基本使用首先，通过创建pipeline()的实例并指定要使用的任务，开始使用它。在本指南中，我们以情感分析的pipeline()为例：123from transformers import pipelineclassifier = pipeline(&quot;sentiment-analysis&quot;)pipeline()会下载并缓存用于情感分析的默认预训练模型和分词器。现在，您可以在目标文本上使用分类器了：123classifier(&quot;We are very happy to show you the 🤗 Transformers library.&quot;)[{'label': 'POSITIVE', 'score': 0.9998}]如果您有多个输入，请将输入作为列表传递给pipeline()，以返回一个字典列表123456results = classifier([&quot;We are very happy to show you the 🤗 Transformers library.&quot;, &quot;We hope you don't hate it.&quot;])for result in results:    print(f&quot;label: {result['label']}, with score: {round(result['score'], 4)}&quot;)label: POSITIVE, with score: 0.9998label: NEGATIVE, with score: 0.5309pipeline()还可以对任何您喜欢的任务迭代整个数据集。在这个例子中，让我们选择自动语音识别作为我们的任务1234import torchfrom transformers import pipelinespeech_recognizer = pipeline(&quot;automatic-speech-recognition&quot;, model=&quot;facebook/wav2vec2-base-960h&quot;)加载您想要迭代的音频数据集(有关更多详细信息，请参阅🤗 Datasets快速入门)。例如，加载MInDS-14数据集：123from datasets import load_dataset, Audiodataset = load_dataset(&quot;PolyAI/minds14&quot;, name=&quot;en-US&quot;, split=&quot;train&quot;)您需要确保数据集的采样率与facebook/wav2vec2-base-960h 训练时使用的采样率相匹配1dataset = dataset.cast_column(&quot;audio&quot;, Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))调用”audio”列时，音频文件会自动加载和重新采样。从前四个样本中提取原始波形数组，并将其作为列表传递给pipeline：1234result = speech_recognizer(dataset[:4][&quot;audio&quot;])print([d[&quot;text&quot;] for d in result])['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', &quot;FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE&quot;, &quot;I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS&quot;, 'HOW DO I FURN A JOINA COUT']对于输入较大的更大数据集(如语音或视觉数据)，您可以将生成器传递给pipeline，而不是将其作为列表加载到内存中在pipeline中使用其他模型和分词器pipeline()可以适应Hub中的任何模型，这使得对pipeline()进行其他用途的调整变得容易例如，如果您想要一个能够处理法语文本的模型，请使用Hub上的标签来过滤合适的模型。通过对过滤结果进行排序，您可以获得一个针对法语文本进行情感分析的多语言BERT模型在pipeline中使用另一个模型和分词器pipeline()可以适应Hub中的任何模型，这使得将pipeline()适应其他用例变得容易12345678from transformers import AutoTokenizer, AutoModelForSequenceClassificationmodel_name = &quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;model = AutoModelForSequenceClassification.from_pretrained(model_name)tokenizer = AutoTokenizer.from_pretrained(model_name)classifier = pipeline(&quot;sentiment-analysis&quot;, model=model, tokenizer=tokenizer)classifier(&quot;Nous sommes très heureux de vous présenter la bibliothèque 🤗 Transformers.&quot;)AutoClassAutoClass是一种快捷方式，它可以根据模型的名称或路径自动获取预训练模型的架构。您只需要选择与您的任务相匹配的AutoClass和相应的预处理类AutoTokenizerAutoTokenizer分词器负责将文本预处理为模型输入的数字数组。有多个规则来规定分词的过程，包括如何拆分一个单词以及以何种级别拆分单词最重要的是，您需要使用相同的模型名称来实例化一个分词器，以确保您使用了与预训练模型相同的分词规则使用AutoTokenizer加载一个分词器将return_tensors参数设置为pt以返回适用于PyTorch的张量，或者设置为tf以返回适用于TensorFlow的张量123456789101112from transformers import AutoTokenizermodel_name = &quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;tokenizer = AutoTokenizer.from_pretrained(model_name)encoding = tokenizer(&quot;We are very happy to show you the 🤗 Transformers library.&quot;, return_tensors=&quot;pt&quot;){'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}tokenizer.decode(encoding[&quot;input_ids&quot;])&quot;We are very happy to show you the 🤗 Transformers library.&quot;分词器返回一个包含三个项目的字典：input_ids：表示文本中各个标记的数字token_type_ids：如果有多个序列，指示一个标记属于哪个序列attention_mask：指示一个标记是否应该被掩盖(masked)分词器还可以接受一个输入列表，并对文本进行填充和截断，以返回具有统一长度的批处理数据123456pt_batch = tokenizer(    [&quot;We are very happy to show you the 🤗 Transformers library.&quot;, &quot;We hope you don't hate it.&quot;],    padding=True,    truncation=True,    max_length=512,    return_tensors=&quot;pt&quot;)pad + truncation1234567891011121314151617181920212223242526272829303132333435# paddingbatch_sentences = [    &quot;But what about second breakfast?&quot;,    &quot;Don't think he knows about second breakfast, Pip.&quot;,    &quot;What about elevensies?&quot;,]encoded_input = tokenizer(batch_sentences, padding=True){'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],                [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],                [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],  'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],  'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],                     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],                     [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}# truncation  将truncation参数设置为True，可以将序列截断为模型所能接受的最大长度batch_sentences = [    &quot;But what about second breakfast?&quot;,    &quot;Don't think he knows about second breakfast, Pip.&quot;,    &quot;What about elevensies?&quot;,]encoded_input = tokenizer(batch_sentences, padding=True, truncation=True){'input_ids': [[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],                [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],                [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],  'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],  'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],                     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],                     [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]}AutoModel🤗Transformers提供了一种简单而统一的方法来加载预训练模型实例。这意味着您可以像加载AutoTokenizer一样加载AutoModel唯一的区别是选择正确的AutoModel来适应任务。对于文本(或序列)分类，您应该加载AutoModelForSequenceClassification123456from transformers import AutoModelForSequenceClassificationmodel_name = &quot;nlptown/bert-base-multilingual-uncased-sentiment&quot;pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)pt_outputs = pt_model(**pt_batch)模型将最终的激活值存储在logits属性中。应用softmax函数到logits上以获取概率值1234567from torch import nnpt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=&lt;SoftmaxBackward0&gt;)在huggingface库中，AutoModel类可以根据给定的checkpoint自动选择并加载适合的模型。它支持各种不同的模型架构，包括：AutoModel: 用于通用的模型加载，根据checkpoint自动选择适合的模型架构AutoModelForSequenceClassification: 用于序列分类任务的模型，如文本分类AutoModelForQuestionAnswering: 用于问答任务的模型，如阅读理解AutoModelForTokenClassification: 用于标记分类任务的模型，如命名实体识别AutoModelForMaskedLM: 用于遮蔽语言建模任务的模型，如BERTAutoModelForCausalLM: 用于有因果关系的语言建模任务的模型，如GPTAutoModelForImageClassification: 用于图像分类任务的模型，如ResNetAutoModelForImageSegmentation: 用于图像分割任务的模型，如Mask R-CNN这些仅是AutoModel类的一些示例，实际上还有更多可用的模型架构。您可以根据具体的任务需求选择适合的AutoModel类进行加载和使用其他的Auto类AutoImageProcessor对于视觉任务，图像处理器将图像处理为正确的输入格式123from transformers import AutoImageProcessorimage_processor = AutoImageProcessor.from_pretrained(&quot;google/vit-base-patch16-224&quot;)AutoFeatureExtractor对于音频任务，特征提取器将音频信号处理为正确的输入格式12345from transformers import AutoFeatureExtractorfeature_extractor = AutoFeatureExtractor.from_pretrained(    &quot;ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition&quot;)AutoProcessor多模态任务需要一个处理器来结合两种类型的预处理工具。例如，LayoutLMV2模型需要一个图像处理器来处理图像，还需要一个分词器来处理文本；处理器将两者结合起来123from transformers import AutoProcessorprocessor = AutoProcessor.from_pretrained(&quot;microsoft/layoutlmv2-base-uncased&quot;)模型保存一旦您的模型经过微调，您可以使用PreTrainedModel.save_pretrained()将其与其标记器一起保存起来：1234567# 模型+分词器 保存pt_save_directory = &quot;./pt_save_pretrained&quot;tokenizer.save_pretrained(pt_save_directory)pt_model.save_pretrained(pt_save_directory)# 加载pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory)AutoConfig您可以修改模型的配置类来更改模型的构建方式。配置类指定了模型的属性，例如隐藏层的数量或注意力头数当您从自定义配置类初始化模型时，您将从头开始。模型的属性将被随机初始化，您需要在使用模型之前对其进行训练以获得有意义的结果首先导入AutoConfig，然后加载要修改的预训练模型。在AutoConfig.from_pretrained()中，您可以指定要更改的属性，例如注意力头的数量：12345from transformers import AutoConfigfrom transformers import AutoModelmy_config = AutoConfig.from_pretrained(&quot;distilbert-base-uncased&quot;, n_heads=12)my_model = AutoModel.from_config(my_config)tokenizerhuggingface的分词器的摘要【LLM系列之Tokenizer】如何科学地训练一个LLM分词器参见：LLM Tokenizer分词系列Trainer对于PyTorch，所有模型都是标准的torch.nn.Module，因此您可以在任何典型的训练循环中使用它们。虽然您可以编写自己的训练循环，但🤗Transformers提供了Trainer类，其中包含基本的训练循环，并添加了其他功能，如分布式训练、混合精度等根据您的任务，通常会向Trainer传递以下参数：PreTrainedModel或torch.nn.Module对象123from transformers import AutoModelForSequenceClassificationmodel = AutoModelForSequenceClassification.from_pretrained(&quot;distilbert-base-uncased&quot;)TrainingArguments包含了可以修改的模型超参数，比如学习率、批大小和训练的轮数。如果你不指定任何训练参数，将使用默认值123456789from transformers import TrainingArgumentstraining_args = TrainingArguments(    output_dir=&quot;path/to/save/folder/&quot;,    learning_rate=2e-5,    per_device_train_batch_size=8,    per_device_eval_batch_size=8,    num_train_epochs=2,)Preprocessing类，例如tokenizer(标记器)、image processor(图像处理器)、feature extractor(特征提取器)或processor(处理器)123from transformers import AutoTokenizertokenizer = AutoTokenizer.from_pretrained(&quot;distilbert-base-uncased&quot;)加载数据集123from datasets import load_datasetdataset = load_dataset(&quot;rotten_tomatoes&quot;)  # doctest: +IGNORE_RESULT创建一个函数来对数据集进行标记化处理，然后使用map函数将其应用于整个数据集1234def tokenize_dataset(dataset):    return tokenizer(dataset[&quot;text&quot;])dataset = dataset.map(tokenize_dataset, batched=True)使用DataCollatorWithPadding来从数据集中创建一个批次的示例123from transformers import DataCollatorWithPaddingdata_collator = DataCollatorWithPadding(tokenizer=tokenizer)DataCollatorWithPadding是Hugging Face的transformers库中的一个类，用于在训练过程中创建批次数据。它的作用是将不同长度的样本填充到相同长度，以便能够同时进行批处理具体来说，DataCollatorWithPadding会根据给定的数据集，找到其中最长的样本，并将其他样本填充到相同的长度。填充通常使用特定的填充令牌(token)来完成，这样模型在处理时可以识别出填充部分，并进行相应的处理使用DataCollatorWithPadding可以确保批次数据的长度一致，从而提高训练效率，并避免由于不同长度样本导致的错误现在将所有这些类组合在Trainer中123456789101112from transformers import Trainertrainer = Trainer(    model=model,    args=training_args,    train_dataset=dataset[&quot;train&quot;],    eval_dataset=dataset[&quot;test&quot;],    tokenizer=tokenizer,    data_collator=data_collator,)  # doctest: +SKIPtrainer.train()Trainer类提供了自定义训练循环行为的方法，你可以通过继承Trainer类并重写其中的方法来实现自定义行为。这样你就可以定制诸如损失函数、优化器和学习率调度器等功能。你可以参考Trainer类的文档了解可以重写的方法另一种定制训练循环的方式是使用回调函数(Callbacks)。你可以使用回调函数与其他库进行集成，监视训练过程并报告进展，或在必要时提前停止训练。回调函数不会修改训练循环本身的行为。如果你需要定制损失函数等内容，你需要继承Trainer类来实现教程模型训练使用预训练模型有很多好处。它可以减少计算成本和碳足迹，并且可以让您使用最先进的模型，而无需从头开始训练🤗Transformers提供了对各种任务的数千个预训练模型的访问。当您使用预训练模型时，您可以在特定于您任务的数据集上进行微调训练。这被称为微调，是一种非常强大的训练技术数据准备123456789101112131415161718from datasets import load_datasetfrom transformers import AutoTokenizer# 1. 加载数据集dataset = load_dataset(&quot;yelp_review_full&quot;)dataset[&quot;train&quot;][100]{'label': 0, 'text': 'My expectations for McDonalds are t rarely high. But for one to still fail so spectacularly...that takes something special!\\nThe cashier took my friends\'s order, then promptly ignored me. I had to force myself in front of a cashier who opened his register to wait on the person BEHIND me. I waited over five minutes for a gigantic order that included precisely one kid\'s meal. After watching two people who ordered after me be handed their food, I asked where mine was. The manager started yelling at the cashiers for \\&quot;serving off their orders\\&quot; when they didn\'t have their food. But neither cashier was anywhere near those controls, and the manager was the one serving food to customers and clearing the boards.\\nThe manager was rude when giving me my order. She didn\'t make sure that I had everything ON MY RECEIPT, and never even had the decency to apologize that I felt I was getting poor service.\\nI\'ve eaten at various McDonalds restaurants for over 30 years. I\'ve worked at more than one location. I expect bad days, bad moods, and the occasional mistake. But I have yet to have a decent experience at this store. It will remain a place I avoid unless someone in my party needs to avoid illness from low blood sugar. Perhaps I should go back to the racially biased service of Steak n Shake instead!'}# 可以创建一个较小的数据集子集，用于微调，以减少所需的时间small_train_dataset = tokenized_datasets[&quot;train&quot;].shuffle(seed=42).select(range(1000))small_eval_dataset = tokenized_datasets[&quot;test&quot;].shuffle(seed=42).select(range(1000))# 2. 分词器tokenizer = AutoTokenizer.from_pretrained(&quot;bert-base-cased&quot;)def tokenize_function(examples):    return tokenizer(examples[&quot;text&quot;], padding=&quot;max_length&quot;, truncation=True)tokenized_datasets = dataset.map(tokenize_function, batched=True)Train with PyTorch Trainer123456789101112131415161718192021222324252627282930from transformers import AutoModelForSequenceClassificationfrom transformers import TrainingArgumentsimport numpy as npimport evaluate# 1. 加载模型model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=5)# 2. 定义训练参数  在训练参数中指定evaluation_strategy参数，以在每个epoch结束时报告评估指标training_args = TrainingArguments(output_dir=&quot;test_trainer&quot;, evaluation_strategy=&quot;epoch&quot;)# 3. 加载评估器metric = evaluate.load(&quot;accuracy&quot;)# 在计算度量标准的时候调用compute，以计算您的预测的准确率。在将预测结果传递给compute之前，您需要将预测结果转换为logitsdef compute_metrics(eval_pred):    logits, labels = eval_pred    predictions = np.argmax(logits, axis=-1)    return metric.compute(predictions=predictions, references=labels)# 4. 定义Trainertrainer = Trainer(    model=model,    args=training_args,    train_dataset=small_train_dataset,    eval_dataset=small_eval_dataset,    compute_metrics=compute_metrics,)# 5. 开始训练trainer.train()Train in native PyTorchTrainer负责训练循环，并允许您通过一行代码对模型进行微调。对于喜欢编写自己的训练循环的用户，您也可以在原生PyTorch中对🤗Transformers模型进行微调123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263from torch.utils.data import DataLoaderfrom transformers import AutoModelForSequenceClassificationfrom torch.optim import AdamWfrom transformers import get_schedulerimport torchfrom tqdm.auto import tqdmimport evaluate# 1. 数据集预处理tokenized_datasets = tokenized_datasets.remove_columns([&quot;text&quot;])tokenized_datasets = tokenized_datasets.rename_column(&quot;label&quot;, &quot;labels&quot;)tokenized_datasets.set_format(&quot;torch&quot;)# 2. 定义DataLoadertrain_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)# 3. 加载模型model = AutoModelForSequenceClassification.from_pretrained(&quot;bert-base-cased&quot;, num_labels=5)# 4. 定义优化器optimizer = AdamW(model.parameters(), lr=5e-5)# 5. 定义schedulernum_epochs = 3num_training_steps = num_epochs * len(train_dataloader)lr_scheduler = get_scheduler(    name=&quot;linear&quot;, optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)# 6. 移动模型到指定设备 device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)model.to(device)# 7. 开始训练progress_bar = tqdm(range(num_training_steps))model.train()for epoch in range(num_epochs):    for batch in train_dataloader:        batch = {k: v.to(device) for k, v in batch.items()}        outputs = model(**batch)        loss = outputs.loss        loss.backward()        optimizer.step()        lr_scheduler.step()        optimizer.zero_grad()        progress_bar.update(1)# 8. 验证集评估metric = evaluate.load(&quot;accuracy&quot;)model.eval()for batch in eval_dataloader:    batch = {k: v.to(device) for k, v in batch.items()}    with torch.no_grad():        outputs = model(**batch)    logits = outputs.logits    predictions = torch.argmax(logits, dim=-1)    metric.add_batch(predictions=predictions, references=batch[&quot;labels&quot;])metric.compute()分布式加速huggingface的accelerate模块🤗Accelerate是Hugging Face提供的用于简化分布式训练的库。它旨在使分布式训练更加容易和高效，支持多种深度学习框架，包括PyTorch和TensorFlowAccelerate提供了以下功能：数据并行：Accelerate使用accelerator.DataParallel类来实现数据并行，可以在多个GPU上同时训练模型混合精度训练：Accelerate支持自动混合精度训练，通过将模型参数和梯度转换为半精度浮点数来减少内存占用和计算量分布式训练：Accelerate使用accelerator.DistributedDataParallel类来实现分布式训练，可以在多个机器上并行训练模型训练循环的自动管理：Accelerate提供了一个accelerator.Trainer类，它封装了训练循环，自动处理数据加载、前向传播、反向传播、优化器更新等过程使用Accelerate可以简化分布式训练的配置和管理，使用户能够更轻松地利用多个GPU或多台机器进行训练，并获得更高的训练效率安装1pip install accelerate示例代码，以下代码只列出改变的部分代码只需要在训练循环中添加四行额外的代码即可启用分布式训练123456789101112131415161718192021from accelerate import Accelerator# 1. 定义加速器accelerator = Accelerator()# 2. dataloader包装train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(    train_dataloader, eval_dataloader, model, optimizer)# 3. 反向传播for epoch in range(num_epochs):    for batch in train_dataloader:        outputs = model(**batch)        loss = outputs.loss        accelerator.backward(loss)        optimizer.step()        lr_scheduler.step()        optimizer.zero_grad()        progress_bar.update(1)完整代码如下123456789101112131415161718192021222324252627282930313233343536373839+ from accelerate import Accelerator  from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler+ accelerator = Accelerator()  model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)  optimizer = AdamW(model.parameters(), lr=3e-5)- device = torch.device(&quot;cuda&quot;) if torch.cuda.is_available() else torch.device(&quot;cpu&quot;)- model.to(device)+ train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(+     train_dataloader, eval_dataloader, model, optimizer+ )  num_epochs = 3  num_training_steps = num_epochs * len(train_dataloader)  lr_scheduler = get_scheduler(      &quot;linear&quot;,      optimizer=optimizer,      num_warmup_steps=0,      num_training_steps=num_training_steps  )  progress_bar = tqdm(range(num_training_steps))  model.train()  for epoch in range(num_epochs):      for batch in train_dataloader:-         batch = {k: v.to(device) for k, v in batch.items()}          outputs = model(**batch)          loss = outputs.loss-         loss.backward()+         accelerator.backward(loss)          optimizer.step()          lr_scheduler.step()          optimizer.zero_grad()          progress_bar.update(1)示例代码包括自然语言处理、语音、计算机视觉和多模态PEFT模块huggingface PEFT模块详见兼一书虫-LLM模型微调系列🤗PEFT，即Parameter-Efficient Fine-Tuning(参数高效微调)，是一个用于高效地将预训练语言模型(PLM)适应于各种下游应用的库，而无需对所有模型参数进行微调PEFT方法只微调少量的(额外的)模型参数，显著降低了计算和存储成本，因为对大规模PLM进行完整微调代价过高。最近的最先进的PEFT技术达到了与完整微调相当的性能PEFT与🤗Accelerate库无缝集成，用于利用DeepSpeed和Big Model Inference进行大规模模型微调Supported methods (截至23-06-15)LoRA: LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELSPrefix Tuning: Prefix-Tuning: Optimizing Continuous Prompts for Generation, P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and TasksP-Tuning: GPT Understands, TooPrompt Tuning: The Power of Scale for Parameter-Efficient Prompt TuningAdaLoRA: Adaptive Budget Allocation for Parameter-Efficient Fine-TuningLLaMA-Adapter: Efficient Fine-tuning of Language Models with Zero-init Attention基本使用加载PEFT adapter从🤗Transformers加载并使用PEFT adapter模型时，请确保Hub仓库或本地目录包含一个adapter_config.json文件和adapter权重，如上面的示例图片所示然后，你可以使用AutoModelFor类加载PEFT adapter模型。例如，要加载用于因果语言建模的PEFT adapter模型：指定PEFT模型的id将其传递给AutoModelForCausalLM类1234from transformers import AutoModelForCausalLM, AutoTokenizerpeft_model_id = &quot;ybelkada/opt-350m-lora&quot;model = AutoModelForCausalLM.from_pretrained(peft_model_id)你可以使用AutoModelFor类或基础模型类(如OPTForCausalLM或LlamaForCausalLM)加载PEFT adapter你也可以通过调用load_adapter方法加载PEFT adapter：1234567from transformers import AutoModelForCausalLM, AutoTokenizermodel_id = &quot;facebook/opt-350m&quot;peft_model_id = &quot;ybelkada/opt-350m-lora&quot;model = AutoModelForCausalLM.from_pretrained(model_id)model.load_adapter(peft_model_id)以8或4位加载bitsandbytes集成支持8位和4位精度数据类型，这对于加载大型模型很有用，因为它可以节省内存添加load_in_8bit或load_in_4bit参数到from_pretrained()并设置device_map=&quot;auto&quot;以有效地将模型分配到你的硬件：1234from transformers import AutoModelForCausalLM, AutoTokenizerpeft_model_id = &quot;ybelkada/opt-350m-lora&quot;model = AutoModelForCausalLM.from_pretrained(peft_model_id, device_map=&quot;auto&quot;, load_in_8bit=True)添加新adapter你可以使用~peft.PeftModel.add_adapter向具有现有adapter的模型添加一个新的adapter，只要新的adapter是与当前的adapter类型相同的例如，如果你的模型已经附加了一个LoRA adapter：123456789101112from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizerfrom peft import LoraConfigmodel_id = &quot;facebook/opt-350m&quot;model = AutoModelForCausalLM.from_pretrained(model_id)lora_config = LoraConfig(    target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;],    init_lora_weights=False)model.add_adapter(lora_config, adapter_name=&quot;adapter_1&quot;)要添加新的adapter：12# 使用相同配置附加新的 adaptermodel.add_adapter(lora_config, adapter_name=&quot;adapter_2&quot;)现在你可以使用~peft.PeftModel.set_adapter设置使用哪个adapter：123456789# 使用 adapter_1model.set_adapter(&quot;adapter_1&quot;)output = model.generate(**inputs)print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))# 使用 adapter_2model.set_adapter(&quot;adapter_2&quot;)output_enabled = model.generate(**inputs)print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))启用和禁用adapters一旦你向模型中添加了一个adapter，你可以启用或禁用adapter模块。要启用adapter模块：123456789101112131415161718from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizerfrom peft import PeftConfigmodel_id = &quot;facebook/opt-350m&quot;adapter_model_id = &quot;ybelkada/opt-350m-lora&quot;tokenizer = AutoTokenizer.from_pretrained(model_id)text = &quot;Hello&quot;inputs = tokenizer(text, return_tensors=&quot;pt&quot;)model = AutoModelForCausalLM.from_pretrained(model_id)peft_config = PeftConfig.from_pretrained(adapter_model_id)# 初始化为随机权重peft_config.init_lora_weights = Falsemodel.add_adapter(peft_config)model.enable_adapters()output = model.generate(**inputs)要禁用adapter模块：12model.disable_adapters()output = model.generate(**inputs)训练PEFT adapterTrainer类支持PEFT adapters，因此你可以为你的特定用例训练adapter，只需要添加几行代码例如，训练LoRA adapter，如果你不熟悉使用Trainer细调模型，请Trainer查看教程使用任务类型和超参数定义adapter配置(查看~peft.LoraConfig以了解更多关于超参数的信息)123456789from peft import LoraConfigpeft_config = LoraConfig(    lora_alpha=16,    lora_dropout=0.1,    r=64,    bias=&quot;none&quot;,    task_type=&quot;CAUSAL_LM&quot;,)将adapter添加到模型中1model.add_adapter(peft_config)现在你可以将模型传递给Trainer12trainer = Trainer(model=model, ...)trainer.train()要保存你训练的adapter并重新加载它：12model.save_pretrained(save_dir)model = AutoModelForCausalLM.from_pretrained(save_dir)向PEFT adapter添加额外的可训练层你也可以在附有adapter的模型上通过在你的PEFT配置中传递modules_to_save来调整额外的可训练adapter例如，如果你想在带有LoRA adapter的模型上同时微调lm_head：123456789101112from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizerfrom peft import LoraConfigmodel_id = &quot;facebook/opt-350m&quot;model = AutoModelForCausalLM.from_pretrained(model_id)lora_config = LoraConfig(    target_modules=[&quot;q_proj&quot;, &quot;k_proj&quot;],    modules_to_save=[&quot;lm_head&quot;],)model.add_adapter(lora_config)其他模块核心类LLM 入门笔记-transformers库简单介绍](https://mp.weixin.qq.com/s/svJMXj9UwI2sd_SAtFExig))ModelOutputModelOutput(transformers.utils.ModelOutput)是所有模型输出的基类。简单理解它就是一个字典，在模型的 forward函数里把原本的输出做了一下封装而已，方便用户能直观地知道输出是什么例如CausalLMOutput顾名思义就是用于像 GPT 这样自回归模型的输出，ModelOutput是所有模型输出的基类123456789101112131415161718class ModelOutput(OrderedDict):    def __init_subclass__(cls) -&gt; None:        &quot;&quot;&quot;        这个方法允许对 ModelOutput 的子类进行定制，使得子类在被创建时能够执行特定的操作或注册到某个系统中。        &quot;&quot;&quot;        ...    def __init__(self, *args, **kwargs):        &quot;&quot;&quot;        初始化 ModelOutput 类的实例。        &quot;&quot;&quot;        super().__init__(*args, **kwargs)    def __post_init__(self):        &quot;&quot;&quot;        在初始化 ModelOutput 类的实例之后执行的操作，允许进一步对实例进行处理或设置属性。子类需要用 dataclass 装饰器        &quot;&quot;&quot;        ...基于 ModelOutput，hf 预先定义了 40 多种不同的 sub-class，这些类是 Hugging Face Transformers 库中用于表示不同类型模型输出的基础类，每个类都提供了特定类型模型输出的结构和信息，以便于在实际任务中对模型输出进行处理和使用每个 sub-class 都需要用装饰器 @dataclass，我们以CausalLMOutputWithPast为例看一下源码1234567@dataclassclass CausalLMOutputWithPast(ModelOutput):    loss: Optional[torch.FloatTensor] = None    logits: torch.FloatTensor = None    past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None    hidden_states: Optional[Tuple[torch.FloatTensor]] = None    attentions: Optional[Tuple[torch.FloatTensor]] = None为了保持代码规范，我们需要在模型的forward函数中对输出结果进行封装，示例如下：123456789101112131415class MyModel(PretrainedModel):    def __init__(self):        self.model = ...    def forward(self, inputs, labels):        output = self.model(**inputs)        hidden_states = ...        loss = loss_fn(outputs, labels)        return CausalLMOutputWithPast(            loss=loss,            logits=logits,            past_key_values=outputs.past_key_values,            hidden_states=outputs.hidden_states,            attentions=outputs.attentions,        )这里简单介绍以下几种，更多的可以查看官方文档和源码：BaseModelOutput: 该类是许多基本模型输出的基础，包含模型的一般输出，如 logits、hidden_states 等BaseModelOutputWithNoAttention: 在模型输出中不包含注意力（attention）信息BaseModelOutputWithPast: 包含过去隐藏状态的模型输出，适用于能够迭代生成文本的模型，例如语言模型BaseModelOutputWithCrossAttentions: 在模型输出中包含交叉注意力（cross attentions）信息通常用于特定任务中需要跨注意力的情况，比如机器翻译BaseModelOutputWithPastAndCrossAttentions: 同时包含过去隐藏状态和交叉注意力的模型输出MoEModelOutput: 包含混合专家模型（Mixture of Experts）输出的模型MoECausalLMOutputWithPast: 混合专家语言模型的输出，包括过去隐藏状态Seq2SeqModelOutput: 序列到序列模型输出的基类，适用于需要生成序列的模型CausalLMOutput: 用于生成式语言模型输出的基础类，提供生成文本的基本信息CausalLMOutputWithPast: 生成式语言模型输出的类，包含过去隐藏状态，用于连续生成文本的模型PreTrainedModelPreTrainedModel (transformers.modeling_utils.PretrainedModel) 是所有模型的基类所以你如果看到一个模型取名为LlamaForCausalLM，那你就可以知道这个模型的输出格式大概率就是自回归输出，即前面提到的CausalLMOutputPreTrainedModel 是 Hugging Face Transformers 库中定义预训练模型的基类它继承了 nn.Module，同时混合了几个不同的 mixin 类，如 ModuleUtilsMixin、GenerationMixin、PushToHubMixin 和 PeftAdapterMixin这个基类提供了创建和定义预训练模型所需的核心功能和属性12345678910111213class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin, PeftAdapterMixin):    config_class = None    base_model_prefix = &quot;&quot;    main_input_name = &quot;input_ids&quot;    _auto_class = None    _no_split_modules = None    _skip_keys_device_placement = None    _keep_in_fp32_modules = None    ...    def __init__(self, config: PretrainedConfig, *inputs, **kwargs):        super().__init__()        ...在这个基类中，我们可以看到一些重要的属性和方法：config_class：指向特定预训练模型类的配置文件，用于定义模型的配置base_model_prefix：基本模型前缀，在模型的命名中使用，例如在加载预训练模型的权重时使用main_input_name：指定模型的主要输入名称，通常是 input_ids_init_weights 方法：用于初始化模型权重的方法在这个基类中，大多数属性都被定义为 None 或空字符串，这些属性在具体的预训练模型类中会被重写或填充LLama例子接下来我们将看到如何使用 PretrainedModel 类定义 llama 模型123456789101112131415161718class LlamaPreTrainedModel(PreTrainedModel):    config_class = LlamaConfig    base_model_prefix = &quot;model&quot;    supports_gradient_checkpointing = True    _no_split_modules = [&quot;LlamaDecoderLayer&quot;]    _skip_keys_device_placement = &quot;past_key_values&quot;    _supports_flash_attn_2 = True    def _init_weights(self, module):        std = self.config.initializer_range        if isinstance(module, nn.Linear):            module.weight.data.normal_(mean=0.0, std=std)            if module.bias is not None:                module.bias.data.zero_()        elif isinstance(module, nn.Embedding):            module.weight.data.normal_(mean=0.0, std=std)            if module.padding_idx is not None:                module.weight.data[module.padding_idx].zero_()在这个例子中，首先定义了 LlamaPreTrainedModel 类作为 llama 模型的基类，它继承自 PreTrainedModel。在这个基类中，我们指定了一些 llama 模型特有的属性，比如配置类 LlamaConfig、模型前缀 model、支持梯度检查点（gradient checkpointing）、跳过的模块列表 _no_split_modules 等等。然后，我们基于这个基类分别定义了 LlamaModel、LlamaForCausalLM 和 LlamaForSequenceClassification。这些模型的逻辑关系如下图所示LlamaModel是 llama 模型的主体定义类，也就是我们最常见的普pytorch 定义模型的方法、默认的输出格式为BaseModelOutputWithPast123456789101112131415class LlamaModel(LlamaPreTrainedModel):    def __init__(self, config: LlamaConfig):        super().__init__(config)        self.padding_idx = config.pad_token_id        self.vocab_size = config.vocab_size        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)        self.layers = nn.ModuleList([LlamaDecoderLayer(config) for _ in range(config.num_hidden_layers)])        self.norm = LlamaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)        ...    def forward(self, ...):        ...        return BaseModelOutputWithPast(...)LlamaForCausalLM 适用于生成式语言模型的 llama 模型，可以看到 backbone 就是 LlamaModel，增加了lm_head作为分类器，输出长度为词汇表达大小，用来预测下一个单词。输出格式为CausalLMOutputWithPast1234567891011121314class LlamaForCausalLM(LlamaPreTrainedModel):    # 适用于生成式语言模型的 Llama 模型定义    def __init__(self, config):        super().__init__(config)        self.model = LlamaModel(config)        self.vocab_size = config.vocab_size        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)        ...    def forward(self, ...):        outputs = self.model(...)        ... # 后处理 outputs，以满足输出格式要求        return CausalLMOutputWithPast(...)LlamaForSequenceClassification 适用于序列分类任务的 llama 模型，同样把 LlamaModel作为 backbone， 不过增加了score作为分类器，输出长度为 label 的数量，用来预测类别。输出格式为SequenceClassifierOutputWithPast1234567891011121314class LlamaForSequenceClassification(LlamaPreTrainedModel):    # 适用于序列分类任务的 Llama 模型定义    def __init__(self, config):        super().__init__(config)        self.num_labels = config.num_labels        self.model = LlamaModel(config)        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)        ...    def forward(self, ...):        outputs = self.model(...)        ... # 后处理 outputs，以满足输出格式要求        return SequenceClassifierOutputWithPast(...)每个子类根据特定的任务或应用场景进行了定制，以满足不同任务的需求另外可以看到 hf 定义的模型都是由传入的 config参数定义的，所以不同模型对应不同的配置啦，这也是为什么我们经常能看到有像 BertConfig，GPTConfig这些预先定义好的类例如我们可以很方便地通过指定的字符串或者文件获取和修改不同的参数配置1234config = BertConfig.from_pretrained(&quot;bert-base-uncased&quot;)  # Download configuration from huggingface.co and cache.config = BertConfig.from_pretrained(&quot;./test/saved_model/&quot;)  # E.g. config (or model) was saved using *save_pretrained('./test/saved_model/')*config = BertConfig.from_pretrained(&quot;./test/saved_model/my_configuration.json&quot;)config = BertConfig.from_pretrained(&quot;bert-base-uncased&quot;, output_attentions=True, foo=False)hf 为了造福懒人，提供了更加简便的 API，即 Auto 系列 API。至于有多简便，看看下面的 demo 就知道了12345from transformers import AutoConfig, AutoModel# Download configuration from huggingface.co and cache.config = AutoConfig.from_pretrained(&quot;bert-base-cased&quot;)model = AutoModel.from_config(config)AutoTrainAutoTrain是一个用于自动化训练的库，旨在简化模型训练的过程。它提供了一种简单的方法来定义和训练深度学习模型，自动处理数据加载、批处理、优化器、损失函数等训练过程中的细节。通过使用AutoTrain，你可以更快速地搭建和训练模型，减少样板代码的编写，并且能够轻松地进行超参数搜索和模型选择GradioGradio是一个用于构建交互式界面的库，使你能够轻松地为你的深度学习模型创建Web应用程序。Gradio提供了一个简单而强大的API，可以将模型与用户界面组件(如文本框、滑块、图像上传器等)相连接，从而实现模型的实时推理和可视化。通过Gradio，你可以快速构建一个交互式的演示或部署你的模型到Web上，无需编写复杂的前端代码DiffusersDiffusers是一个用于生成图像、音频甚至分子的三维结构的最新预训练扩散模型的库。无论您是寻找一个简单的推理解决方案，还是想要训练自己的扩散模型，🤗Diffusers都是一个支持两者的模块化工具箱。我们的库着重于易用性而非性能，简洁而非复杂，可定制性而非抽象性，该库主要包含以下三个组件：最新的扩散推理流程，只需几行代码即可实现可互换的噪声调度器，用于在生成速度和质量之间平衡权衡可用作构建块的预训练模型，可以与调度器结合使用，创建您自己的端到端扩散系统AccelerateHugging Face的Accelerate是一个旨在简化和加速深度学习模型训练和推理过程的库它提供了一个高级API，抽象了分布式训练、混合精度和梯度累积等复杂性，使用户能够轻松地充分利用硬件资源的潜力Accelerate兼容PyTorch和TensorFlow，并提供了一套工具和实用程序，以实现跨多个GPU或多台机器的高效分布式训练。它包括以下功能：分布式训练：Accelerate提供了简单易用的接口，使用户能够将训练过程分布到多个GPU或多台机器上。它支持常见的分布式训练策略，如数据并行和模型并行，并自动处理数据的分发和梯度的聚合，使用户无需手动编写复杂的分布式训练代码混合精度训练：Accelerate支持混合精度训练，通过同时使用浮点16位和浮点32位精度来加快模型的训练速度。它自动处理数据类型转换和梯度缩放，用户只需简单地指定使用混合精度训练即可梯度累积：Accelerate支持梯度累积，这在GPU显存有限的情况下特别有用。梯度累积允许在多个小批次上累积梯度，然后进行一次大批次的参数更新，从而减少显存占用并提高训练效率自动调节批次大小：Accelerate可以自动调整批次大小以适应可用的GPU内存。它会动态调整批次大小，以达到最佳的GPU利用率和训练性能总之，Hugging Face的Accelerate是一个功能强大的库，旨在简化和加速深度学习模型的训练和推理过程。它提供了高级API和一系列工具，使用户能够轻松地实现分布式训练、混合精度训练和梯度累积等高效训练策略</content>
	</entry>
	<entry>
		<title>深度学习模型压缩技术</title>
		<url>/article/53377.html</url>
		<content>模型压缩概述深度学习模型压缩与加速七大方法总结！随着深度学习的发展与高性能GPU处理能力的增强，神经网络结构变得越来越复杂，模型参数量也越来越庞大，这使得深度学习在移动嵌入式设备上的部署遇到巨大的困难和挑战因此，如何在不影响深度学习模型性能的情况下进行模型压缩与加速成为了研究热点在深度学习中，模型压缩是一种通过减小模型的大小、参数数量或计算量来降低模型复杂度的技术。以下是一些常见的模型压缩技术：参数剪枝(Pruning)：通过剪除模型中不重要的连接或参数，减少模型的参数数量和计算量。剪枝可以基于权重的重要性进行，剪除较小的权重或通过稀疏化技术将权重设置为零权重共享(Weight Sharing)：将模型中的一些权重共享，减少模型中需要存储的参数数量。这可以通过对权重进行聚类或使用哈希函数将相似的权重映射到同一个值来实现低秩近似(Low-Rank Approximation)：通过将模型的权重矩阵分解为较低秩的矩阵乘积形式，减少模型的参数数量。常见的方法包括奇异值分解(SVD)和矩阵分解技术知识蒸馏(Knowledge Distillation)：将一个复杂的模型的知识传递给一个较简单的模型，通过让学生模型学习教师模型的输出概率分布或中间表示来提高学生模型的性能网络量化(Network Quantization)：减少模型中参数的位数表示，例如将浮点数参数转换为较低位数的整数表示，从而减小模型的存储需求和计算复杂度网络剪枝(Network Pruning)：除了剪枝权重和连接之外，还剪枝模型的网络结构。这包括剪枝层、剪枝通道或剪枝模块等网络蒸馏(Network Distillation)：类似于知识蒸馏，但是不仅传递教师模型的输出概率分布或中间表示，还传递教师模型的激活值或其他辅助信息这些技术可以单独应用或结合使用，以减小深度学习模型的大小、复杂度和计算需求，从而实现模型压缩和加速的效果参数剪枝对模型的网络进行修剪，比如减掉多余的头(因为Transformer使用多头注意力机制)，或者直接粗暴的使用更少的Transformer层数权重共享等待…低秩近似等待…知识蒸馏手写数字识别中多元分类原理_广告行业中那些趣事系列21：从理论到实战BERT知识蒸馏…深度学习中的知识蒸馏技术`一分钟带你认识深度学习中的知识蒸馏概述知识蒸馏的概念最早是2015年Geoffrey Hinton在《Distilling the Knowledge in a Neural Network》这篇论文中提出来的在深度学习中，知识蒸馏(Knowledge Distillation)指的是将一个复杂的模型(称为教师模型)的知识传递给一个较简单的模型(称为学生模型)的过程这种方法旨在通过训练学生模型使其能够学习教师模型的推理能力和泛化能力通常情况下，教师模型是一个较大、复杂、准确度较高的模型，而学生模型则是一个较小、简化的模型知识蒸馏通过让学生模型学习教师模型的输出概率分布或中间表示，从而提高学生模型的性能。这种知识传递可以帮助学生模型更好地捕捉数据集的特征，提高泛化能力，并且在具有较少训练数据的情况下取得较好的性能例子: TextCNN知识蒸馏BERT这类大模型精度高但是线上推理速度慢，传统的文本分类模型比如TextCNN等线上推理速度快(因为模型比较小)但是精度有待提升。针对上面的问题，我们的需求是获得媲美BERT等大模型的精度，还能满足线上推理速度的时延要求业务中可以把BERT作为老师模型去教作为学生模型的TextCNN来学习知识，从而使TextCNN不仅达到了媲美BERT的分类效果，而且还能很好的满足线上推理速度的要求暗知识对于老师或者没有使用知识蒸馏的小模型来说，主要是通过训练数据来学习知识。我们的训练数据集是一张一张手写数字的图片，还有对应0到9十个数字的标签。在这种学习中我们可以用的只有十个类别值，比如一张手写数字1的图片样本的标签是1，告诉模型的知识就是这个样本标签是1，不是其他类别而使用知识蒸馏的时候模型可以学到更多的知识，比如手写数字1的图片样本有0.7的可能是数字1，0.2的可能是数字7，还有0.1的可能是数字9这非常有意思，模型不仅学到了标签本身的知识，还学习到了标签之间的关联知识，就是1和7、9可能存在某些关联，这些知识称为暗知识其他的蒸馏对抗蒸馏、多教师蒸馏、跨模态蒸馏、无数据蒸馏蒸馏的目的模型压缩(Model Compression)：通过知识蒸馏，可以将一个较大、复杂的模型压缩成一个更小、轻量级的模型，减少模型的存储空间和计算资源的消耗。这使得模型可以在资源受限的设备上运行，同时仍保持较高的性能模型加速(Model Acceleration)：知识蒸馏可以加速模型的推理过程，使得模型在实时性要求较高的场景中能够更快地进行预测。通过将较复杂的模型的知识转移给简化的模型，可以加快推理速度提升泛化能力(Improving Generalization)：知识蒸馏可以通过将一个复杂模型的知识传递给一个简化模型，帮助简化模型学习到更好的特征表示和泛化能力。较复杂的模型通常具有更强大的表示能力和表达能力，通过知识蒸馏，这些能力可以传递给简化模型，提升其泛化性能迁移学习(Transfer Learning)：知识蒸馏可以通过将一个在大规模数据集上训练过的复杂模型的知识迁移到一个相似任务的简化模型上，从而加快简化模型在新任务上的学习速度和性能总之，知识蒸馏的主要目的是通过将一个复杂模型的知识转移到一个简化模型上，从而实现模型压缩、加速、提升泛化能力和迁移学习等目标蒸馏机制【深度学习】（一）知识蒸馏根据教师网络是否和学生网络一起更新，可以分为离线蒸馏，在线蒸馏和自蒸馏感性上理解三种蒸馏方式：离线蒸馏(Offline Distillation): 可以理解为知识渊博的老师给学生传授知识在线蒸馏(Online Distillation): 可以理解为教师和学生一起学习自蒸馏(Self-Distillation): 意味着学生自己学习知识离线蒸馏早期的知识蒸馏方法都属于离线蒸馏，将一个预训练好的教师模型的知识迁移到学生网络，所以通常包括两个阶段：在蒸馏前，教师网络在训练集上进行训练教师网络通过logits层信息或者中间层信息提取知识，引导学生网络的训练第一个阶段通常不被认为属于知识蒸馏的一部分，因为默认教师网络本身就是已经预训练好的一般离线蒸馏算法关注与提升知识迁移的不同部分，包括：知识的形式，损失函数的设计，分布的匹配优点实现起来比较简单，形式上通常是单向的知识迁移(即从教师网络到学生网络)，同时需要两个阶段的训练(训练教师网络和知识蒸馏)缺点教师网络通常容量大，模型复杂，需要大量训练时间，还需要注意教师网络和学生网络之间的容量差异，当容量差异过大的时候，学生网络可能很难学习好这些知识在线蒸馏教师模型和学生模型都是to be trained的状态，即教师模型并没有预训练在大容量教师网络没有现成模型的时候，可以考虑使用online distillation。使用在线蒸馏的时候，教师网络和学生网络的参数会同时更新，整个知识蒸馏框架是端到端训练的可以参考本站图像分类算法章节下的DeiTModel，DeiTModel模型是对Vit模型的改进和优化，使用了在线蒸馏策略(还是离线?)自蒸馏自蒸馏中，教师和学生模型使用相同的网络。自蒸馏可以看作是在线蒸馏的一种特殊情况，因为教师网络和学生网络使用的是相同的模型温度参数等待…网络量化等待…网络剪枝等待…网络蒸馏等待…</content>
	</entry>
	<entry>
		<title>作文赏析_语文从我身边走过</title>
		<url>/article/58490.html</url>
		<content> 语文从我身边走过   Your browser does not support the audio element.        语文从我身边倘徉而过，留下唐风宋韵的淋漓酣畅！        语文从我身边流淌而过，留下诗林词苑的慷慨激昂！        语文从我身边飘然而过，留下美文小说的婉转悠扬！        朦胧中芳香扑鼻，是满园春色中，那枝出墙红杏的香甜之味？还是夏日荷塘那靖蜓伫立小荷所溢的清幽？是重阳菊花怒放带出怡人的淡雅之香？还是冬日山园小梅凌寒傲雪浮动的暗香？        如果你是“大漠孤烟直，长河落日圆”的边塞大漠，我愿飞奔在漫天黄沙里；如采你是“惊涛拍岸，卷起千堆雪”的浩瀚长江，我愿遨游于猛浪湍漩中；如果你是“山重水复疑无路，柳暗花明又一村”的深山丛林，我愿跋涉在荆棘小道上。        子夜，雨水夹杂着风的啸鸣，降临人间，撞击着窗棂。此时的雨，声声敲击着心扉，似“巴山夜雨涨秋池”，似“雨打芭蕉湿红杏”……倾听奋斗目标雨声，仿佛听到了黛玉葬花题帕的啼哭；仿佛听到了陆游“铁马冰河”的喟叹；仿佛听到了李商隐怀念友人的声声叹惋。        雨中，全戈铁马正伴随着震天的战鼓从那长卷间奔马而来……雨是秦槐汉柏，仁人志士落下的悲怆之泪；雨是桃花潭水边知已依依不舍的惆怅之泪；雨是茫茫戈壁间，守边将士悉苦寂寥的乡思之泪。        语文，好比一缕轻风，荡涤着我的心海，使我的天空四季带春。        语文，好比一轮骄阳，照耀着我的心灵，使我的世界光明永存。        语文，好比一阵细雨，滋润着我的心田，使我的人生永不枯竭。想采撷一波浪涛编织你飞动的裙角想收集一捧星光点缀你飘远的发梢想挽留一抹斜阳定格你醉人的微笑无须太多言辞只需深深凝眸拂去了年少轻狂只留下丝丝感动        不须“众里寻他千百度”你已“在灯火阑珊处”，落落大方的展露芳容。就像此刻，我自信地坐在班上，闻着窗外阳光的气息，感受着语文的翰墨流香，轻风将我的发丝拂起，拂起我嘴角的一丝微笑，因为我知道，你正从我身边悄悄走过。        抛不完相思血泪抛红豆，开不完春柳春花满画楼，睡不稳纱窗风雨黄昏后，忘不了新愁与旧愁，展不开的眉头，看不清的遗嘱，恰便似遮不住的青山隐隐，流不尽的绿水幽幽……</content>
	</entry>
	<entry>
		<title>图像分割算法</title>
		<url>/article/54081.html</url>
		<content>图像分割UNetU-Net: Convolutional Networks for Biomedical Image Segmentation 2015图像分割必备知识点 | Unet详解 理论+ 代码U-Net是一种常用的卷积神经网络结构，特别适用于图像分割任务。它由Olaf Ronneberger等人在2015年提出，并因其在生物医学图像分割中的出色表现而受到广泛关注U-Net的整体结构呈U字形，因此得名。它包含两个主要部分：编码器(Encoder)和解码器(Decoder)编码器由一系列卷积层和池化层组成，用于提取图像中的特征并逐渐缩小感受野解码器由一系列反卷积层和跳跃连接(Skip Connection)组成，用于将编码器提取的特征映射重新放大，并与解码器中的特征进行融合跳跃连接的作用是将底层的细节信息传递给解码器，有助于更好地恢复分割结果的细节U-Net的设计思想是在特征提取的同时保留更多的上下文信息，以及保留高分辨率的细节。通过编码器和解码器之间的特征传递和融合，U-Net能够同时获得局部和全局的上下文信息，并生成细致准确的分割结果由于其简单而有效的结构，U-Net在医学图像分割、语义分割、遥感图像分析等领域得到了广泛应用，并成为图像分割任务中的经典模型之一语义分割实例分割</content>
	</entry>
	<entry>
		<title>深度学习核心之优化器</title>
		<url>/article/36065.html</url>
		<content>优化算法最优化是指非线性最优化，解非线性最优化的方法有很多比如梯度下降法、共轭梯度法、变尺度法和步长加速法等参考本站链接机器学习_最优化方法概述pytorch优化器飞浆官方文档，总结到位深度学习优化器方法及学习率衰减方式综述An overview of gradient descent optimization algorithms18471951198320112012GD(BGD)SGDSGDM(Momentum)、NAGAdaGradAdadelta、RMSprop201520162018Adam、AdaMaxNadamAMSGrad相对应的论文A Stochastic Approximation Method SGD 1951Learning representations by back-propagating errors Momentum 1983A method for unconstrained convex minimization problem with the rate of convergence  o(1/k2) NAG 1983Adaptive subgradient methods for online learning and stochastic optimization AdaGrad 2011ADADELTA: an adaptive learning rate method. Adadelta 2012Neural Networks for Machine Learning Lecture 6a Overview of mini-batch gradient descent RMSprop 2012Adam: A method for stochastic optimization Adam &amp; AdaMax 2014Incorporating Nesterov Momentum into Adam NAdam 2016以下是优化器的发展脉络，按照时间顺序列出了一些重要的优化器及其年份：Gradient Descent (GD)：最早的优化器之一，用于求解无约束优化问题。没有特定的年份，但早在20世纪50年代就开始被广泛应用Stochastic Gradient Descent (SGD)：引入随机性来估计梯度的优化器，用于大规模数据集和深度学习模型。没有特定的年份，但在深度学习的早期就被广泛使用Momentum（1983）：提出了动量概念，通过累积梯度的指数加权平均来加速收敛AdaGrad（2011）：自适应梯度算法，通过对梯度进行归一化和调整学习率，适应不同参数的更新需求Adadelta（2012）：改进了AdaGrad的缺点，通过考虑历史梯度的平均值来调整学习率RMSprop（2012）：引入了指数加权移动平均的概念，用于调整学习率以平衡历史梯度信息Adam（2014）：结合了动量和自适应学习率的优点，通过自适应调整学习率和梯度的一阶矩估计和二阶矩估计来进行参数更新AdaMax（2014）：基于Adam算法，通过替换二阶矩估计的范数为无穷范数，提供了更稳定的更新规则Nadam（2016）：结合了Nesterov动量和Adam算法，利用动量来加速收敛AMSGrad（2018）：对Adam算法进行了改进，解决了Adam算法学习率下降不稳定的问题这些是一些比较重要的优化器，并且按照时间顺序列出。然而，需要注意的是，并非所有的优化器都是线性发展的，而是相互借鉴、改进和结合的结果。优化器的发展是一个活跃的研究领域，仍然有许多新的优化算法被提出和改进什么是优化器深度学习的目标是通过不断改变网络参数，使得参数能够对输入做各种非线性变换拟合输出，本质上就是一个函数去寻找最优解，所以如何去更新参数是深度学习研究的重点通常将更新参数的算法称为优化器，字面理解就是通过什么算法去优化网络模型的参数梯度下降核心点方向: 确定优化的方向，一般通过求导便可以求得步长: 步子就是决定当前走多大，如果学习率设的过大，梯度会在最优点来回跳动，设的过小需要很久的训练才能达到最优点优化器的主要作用参数更新：优化器根据损失函数的梯度信息，计算出每个参数的更新量，并将更新量应用于参数，从而更新模型的参数。这样，模型的参数就可以朝着能够更好地拟合训练数据的方向进行调整学习率调整：优化器通常会自动调整学习率，以控制参数更新的步幅。学习率决定了每次参数更新的幅度，过大的学习率可能导致参数更新过快而错过最优解，而过小的学习率可能导致收敛速度缓慢。优化器根据当前训练的进度和参数的变化情况，动态地调整学习率，以获得更好的训练效果优化算法选择：优化器提供了多种不同的优化算法，如梯度下降、动量优化、自适应学习率等。这些算法在参数更新的方式、学习率调整策略等方面有所不同，可以根据具体任务的需求选择合适的优化算法通过合适的优化器选择和参数调整，可以提高神经网络的训练效率和性能，加速收敛过程，使得模型能够更好地拟合训练数据，并在测试数据上取得较好的泛化能力优化器分类梯度下降优化器(Gradient Descent Optimizers)：基于梯度信息来更新参数的优化器，包括批量梯度下降(BGD)、随机梯度下降(SGD)和小批量梯度下降(Mini-Batch Gradient Descent，MBGD)等基于动量的优化器(Momentum-based Optimizers)：在梯度下降的基础上引入动量的概念，旨在加速收敛过程并减少震荡，常见的包括动量优化器(Momentum Optimizer)、牛顿加速度动量优化法Nesterov Accelerated Gradient(NAG)等自适应学习率优化器(Adaptive Learning Rate Optimizers)：根据参数更新的情况动态地调整学习率，以提高收敛速度和效果，常见的包括AdaGrad、RMSprop、Adam、AdaDelta、Adamax等学习率衰减优化器(Learning Rate Decay Optimizers)：在训练过程中逐渐减小学习率的优化器，常见的包括Step Decay、Exponential Decay、Piecewise Decay等正则化优化器(Regularization Optimizers)：结合正则化技术，通过对损失函数添加正则化项来控制模型的复杂度，常见的包括L1正则化、L2正则化等二阶优化器(Second-Order Optimizers)：考虑参数二阶信息的优化器，如牛顿法(Newton’s Method)、共轭梯度法(Conjugate Gradient)等不同类型的优化器在更新参数的方式、学习率调整策略、收敛速度、对噪声和局部最优的鲁棒性等方面有所区别，选择合适的优化器取决于具体的问题和数据集特征基本的梯度下降法优化器综述深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）优化器的存在就是确定优化的方向和面对当前的情况动态的调整步子BGD、SGD和MBGD的区别优化器BGDSGDMBGD样本数N(所有)1batch_sizeBGDBGD(Batch Gradient Descent)采用整个训练集的数据来计算cost function对参数的梯度其中为学习率，而为损失函数的一阶导数BGD在计算梯度时会出现冗余因为BGD在每一次迭代中都使用了整个训练集，而且在梯度计算过程中并没有考虑样本之间的相关性因此，对于样本中的某些部分，其梯度计算可能会与其他样本的梯度计算重复，这种冗余计算可能会导致计算效率的降低，特别是在训练集很大的情况下优点收敛稳定：由于每次迭代使用整个训练集的所有样本进行参数更新，收敛过程相对稳定参数更新准确：使用全局梯度来更新参数，对于凸优化问题，可以达到全局最优解缺点训练速度慢：BGD 是一种批量梯度下降算法，每次更新模型参数时使用整个训练数据集计算开销大：需要计算整个训练集的梯度，对于大规模数据集或复杂模型，计算开销较高内存占用高：需要存储整个训练集的数据和梯度信息SGDSGD(Stochastic Gradient Descent)是一种随机梯度下降算法，每次更新模型参数时使用单个样本或一小批样本(通常称为mini-batch，也称MBGD)其中为学习率，而为损失函数的一阶导数，为batch_size，当就是SGD，否则就是MBGD优点计算开销小：每次迭代只使用一个样本进行参数更新，计算开销较小适用于大规模数据集：由于样本的随机选择，可以处理大规模数据集，且易于并行处理缺点参数更新不稳定：由于单个样本的梯度计算可能存在噪声，参数更新不稳定，可能引起参数在最优点附近震荡收敛速度较慢：由于参数更新的不稳定性，收敛速度相对较慢MBGDMBGD(Mini-Batch Gradient Descent)每一次利用一小批样本，即batch_size个样本进行计算，这样它可以降低参数更新时的方差，收敛更稳定，另一方面可以充分地利用深度学习库中高度优化的矩阵操作来进行更有效的梯度计算其中为学习率，而为损失函数的一阶导数，为batch_size，当就是SGD，否则就是MBGD优点平衡了开销和参数稳定性：使用一小批样本进行参数更新，综合了全局梯度和随机梯度的信息，计算开销和参数更新的稳定性得到一定的平衡收敛速度较快：相对于BGD，使用较小的批量样本更新参数，收敛速度更快缺点批量大小需调优：批量大小的选择可能会影响模型的性能，需要进行调优可能导致局部最优：较小的批量样本可能会引入一定的随机性，可能导致陷入局部最优而无法达到全局最优动量优化法SGDM随机梯度下降法虽然有效，但容易陷入局部最小值点，甚至在驻点附近以及梯度值非常小的点附近时参数更新极为缓慢为了抑制SGD的震荡，SGDM(Stochastic Gradient Descent Momentum)认为梯度下降过程可以加入惯性主要思想是下降过程中，如果发现是陡坡，那就利用惯性跑的快一些。因此，其在SGD基础上引入了一阶动量在坡度比较陡的地方，会有较大的惯性，这是下降的多。坡度平缓的地方，惯性较小，下降的会比较慢其中为学习率，表示当前t时刻梯度，表示当前时刻的加权后的梯度，是动量系数而的经验值为0.9(表示最大速度10倍于SGD)，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向一阶动量是各个时刻梯度方向的指数移动平均值，也就是说，时刻的下降方向，不仅由当前点的梯度方向决定，而且由此前累积的下降方向决定梯度是如何累积的这里将展开，可以看到当前时刻的梯度是对历史梯度进行加权得到的，其中，是一个介于0和1之间的参数，控制了历史梯度对当前动量的贡献程度较大的值会使历史梯度的贡献更大，从而使动量更加平滑较小的值会使当前梯度的贡献更大，从而对变化更为敏感这种权重衰减的方式使得历史梯度的贡献逐渐减小，更加关注近期的梯度变化，有助于适应变化的数据和模型参数优点加速收敛：SGDM引入了动量的概念，通过累积之前的动量信息，有助于加速模型的收敛速度，特别是在存在平坦区域的情况下更为明显减少震荡：动量的累积作用可以减少参数更新时的震荡现象，有助于更稳定地更新模型参数尽可能跳出局部最优：动量的引入可以帮助模型跳出局部最优点，以便更好地搜索全局最优点当局部沟壑比较深，动量加持用完了，依然会困在局部最优里来回振荡缺点需要调整超参数：SGDM中的动量系数需要手动设置，选择合适的动量系数对于模型的性能影响较大，需要进行调试和调优可能导致过拟合：当动量系数较大时，SGDM可能在优化过程中过度依赖之前的动量信息，导致模型过拟合难以处理非平稳数据：对于非平稳数据，SGDM的动量累积可能会导致模型在变化快速的方向上过度追踪，而无法及时适应变化改进方法自适应调整动量系数：可以采用自适应的方式来调整动量系数，例如使用自适应的动量方法(如Adam)来根据梯度的变化自动调整动量系数学习率调度策略：结合学习率调度策略，如学习率衰减或自适应学习率方法，可以更好地控制模型的学习速度和方向正则化技术：使用正则化技术，如L1正则化或L2正则化，可以缓解过拟合问题，使模型更具泛化能力因为加入了动量因素，SGDM缓解了SGD在局部最优点梯度为0，无法持续更新的问题和振荡幅度过大的问题，但是并没有完全解决，当局部沟壑比较深，动量加持用完了，依然会困在局部最优里来回振荡NAG深度学习优化函数详解— Nesterov accelerated gradient (NAG)比Momentum更快：揭开Nesterov Accelerated Gradient的真面目动量法每下降一步都是由前面下降方向的一个累积和当前点的梯度方向组合而成。于是一位大神(Nesterov)就开始思考，既然每一步都要将两个梯度方向(历史梯度、当前梯度)做一个合并再下降，那为什么不先按照历史梯度往前走那么一小步，按照前面一小步位置的超前梯度来做梯度合并呢如此一来，小球就可以先不管三七二十一先往前走一步，在靠前一点的位置看到梯度，然后按照那个位置再来修正这一步的梯度方向，同SGDM比较的差一点如下公式所示既然知道会走，就不需要还用当前位置的梯度，可以直接走到的位置计算梯度，这样子就有了超前眼光有了超前的眼光，小球就会更加聪明, 这种方法被命名为牛顿加速梯度(Nesterov accelerated gradient)简称NAG，下图是SGDM下降法与NAG下降法的可视化比较NAG算法公式表达如下：为什么NAG比SGDM快对NAG原来的更新公式进行变换，得到这样的等效形式(具体推导过程)与Momentum的区别在于，本次更新方向多加了一个直观含义就很明显了：如果这次的梯度比上次的梯度变大了，那么有可能会继续变大，可以把预计增大的部分提前加进来；变小的情况类似这个多加上去的项就是在近似目标函数的二阶导嘛，因此，NAG本质上是多考虑了目标函数的二阶导信息，其实所谓往前看的说法，在牛顿法这样的二阶方法中也是经常提到的，从数学角度上看，则是利用了目标函数的二阶导信息自适应学习率优化器AdaGrad李宏毅深度学习笔记-Adagrad算法AdaGrad 算法优化的变量对于目标函数的依赖是各不相同在基本的梯度下降法优化中，有一个常见的问题是，要优化的变量对于目标函数的依赖是各不相同的对于某些变量，已经优化到了极小值附近，但是有的变量仍然在梯度很大的地方，这时候一个统一的全局学习率是可能出现问题的如果学习率太小，则梯度很大的变量会收敛很慢，如果梯度太大，已经优化差不多的变量就可能会不稳定现实世界的数据集中，一些特征是稀疏的(大部分特征为零，所以它是稀疏的)，而另一些则是密集的(dense，大部分特征是非零的)，因此为所有权值保持相同的学习率不利于优化针对这个问题，当时在伯克利加州大学读博士的Jhon Duchi，2011年提出了AdaGrad(Adaptive Gradient)，也就是自适应学习率基本思想AdaGrad的基本思想是对每个变量用不同的学习率，设置了全局学习率之后，每次通过，全局学习率逐参数的除以历史梯度平方和的平方根，使得每个参数的学习率不同这个学习率在一开始会比较大，用于快速梯度下降。随着优化过程的进行，对于已经下降很多的变量，则减缓学习率，对于还没怎么下降的变量，则保持一个较大的学习率公式其中为学习率，而为损失函数的一阶导数，是一个平滑项，避免了除以零(通常取值在左右)，表示元素逐元素相乘操作G{t+1} =G{t}+g{t+1} \odot g{t+1} = \sum _{i=0}^{t} { {g_i} ^2 }\begin{array}{c}G{t+1} = \beta G_t + (1-\beta) \Delta L (w{t})^2\w{t+1} = w_t -  \frac {\alpha}{\sqrt{ G{t} + \epsilon }} \odot \Delta L (w_t)\end{array}G{t+1} = \beta G_t + (1-\beta) \sum {i=0}^{t} { {g_i} ^2 }\mathrm{E}\left[\mathrm{g}^{2}\right]{\mathrm{t}}=\rho * \mathrm{E}\left[\mathrm{g}^{2}\right]{\mathrm{t}-1}+(1-\rho) * \mathrm{~g}_{\mathrm{t}}^{2}\mathrm{w}{\mathrm{t+1}}=\mathrm{w}{\mathrm{t}}-\frac{\eta}{\sqrt{\mathrm{E}\left[\mathrm{g}^{2}\right]{\mathrm{t}}+\epsilon}} * \mathrm{~g}{\mathrm{t}}\Delta x{t}=-\frac{1}{\left|\operatorname{diag}\left(H{t}\right)\right|+\mu} g_{t}\Delta x{t}=-\frac{1}{\left|\operatorname{diag}\left(H{t}\right)\right|} \frac{E\left[g{t-w: t}\right]^{2}}{E\left[g{t-w: t}^{2}\right]} g_{t}\Delta \mathrm{x} \propto \mathrm{g} \propto \frac{\mathrm{df}}{\mathrm{dx}} \propto \frac{1}{x}\Delta \mathrm{x} \propto H^{-1} \mathrm{~g} \propto \frac{\frac{\mathrm{df}}{\mathrm{dx}}}{\frac{\partial^{2} f}{\partial x^{2}}}\frac{\frac{d f}{d x}}{\frac{\partial^{2} f}{\partial x^{2}}}=\frac{1}{\frac{\partial^{2} f}{\partial x^{2}}} g_{t}\frac{\frac{d f}{d x}}{\frac{\partial^{2} f}{\partial x^{2}}}=\frac{1}{\frac{\partial^{2} f}{\partial x^{2}}} g_{t}\frac{\Delta x}{\frac{\partial f}{\partial x}} \approx-\frac{R M S[\Delta x]{t-1}}{R M S[\Delta g]{t}}\operatorname{RMS}[g]{t}=\sqrt{E\left[g^{2}\right]{t}+\epsilon}\Delta x{t}=-\frac{\operatorname{RMS}[\Delta x]{t-1}}{\operatorname{RMS}[g]{t}} g{t}\mathrm{x}{\mathrm{t+1}}=\mathrm{x}{\mathrm{t}} + \Delta x_{t}思路Adam不仅如RMSProp算法那样基于一阶矩均值计算适应性参数学习率，它同时还充分利用了梯度的二阶矩均值(即有偏方差)，适合解决含大规模的数据和参数的优化目标，也适合解决包含高噪声或稀疏梯度的问题，让参数更新时保持稳定其中控制一阶动量，控制二阶动量最终的参数更新公式为默认值设置，，， 优点自适应学习率：Adam通过自适应地调整每个参数的学习率，可以有效地应对不同参数的梯度变化情况。这使得它在训练过程中更容易收敛，并且对于大多数任务具有较好的性能，但是需要注意的是它的效果有时候不如SGDM速度快：Adam结合了动量方法，能够在训练过程中积累梯度的动量，从而加速参数更新的速度，尤其在具有平坦或稀疏梯度的情况下更加明显结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点适用性广泛：也适用于大多非凸优化，适用于大数据集和高维空间缺点内存消耗较大：Adam需要存储每个参数的动量和平方梯度估计，这会占用较大的内存空间，特别是在具有大量参数的深度神经网络中AdaMaxAdaMax是一种自适应学习率优化算法，是Adam优化器的一种变体AdaMax使用了梯度的无穷范数来估计梯度的大小，而Adam使用了梯度的二范数(核心区别)，变化如下所示论文中AdaMax的为什么是选择了无穷范数AdaMax选择了无穷范数(范数)是因为在大多数情况下，范数具有稳定的行为对于一些问题，特别是在深度学习中，范数可以提供更好的数值稳定性和收敛性相比于其他范数，范数能够更好地控制梯度的最大值，从而减少参数更新的不稳定性因此，AdaMax选择了范数作为其更新规则的一部分，以提高优化算法的稳定性和效果Nadam深度学习优化策略NadamNadam(Nesterov-accelerated Adaptive Moment Estimation)是将Adam与Nesterov加速梯度结合在一起，它对学习率的约束将更强，具备二者的优势，使得此算法在某些问题上的效果更好Nadam的更新规则与Adam类似，但在计算梯度更新时引入了Nesterov动量项。具体而言，Nadam在计算梯度的移动平均和梯度更新时，使用了Nesterov动量的修正梯度来更新模型参数。这使得Nadam在处理凸优化问题时能够更好地逼近最优解，并且在处理非凸问题时能够更快地收敛其他优化器AdamW（Adam with Weight Decay）： AdamW是对Adam优化器的改进，通过添加权重衰减（Weight Decay）的正则化项来解决权重衰减对Adam优化器的影响。传统的Adam优化器在计算梯度更新时，会将权重衰减项也纳入梯度计算中，导致权重衰减效果不准确。而AdamW在计算梯度更新时将权重衰减项单独处理，使得权重衰减的效果更加准确和稳定ASGD（Average Stochastic Gradient Descent）： ASGD是一种随机梯度下降法的变体，它通过计算一定数量的随机梯度的平均值来更新模型参数。ASGD使用一个平均模型参数的历史记录，以减小训练过程中参数更新的方差。这样可以使模型的收敛速度更稳定，并且能够在训练过程中逐渐减小学习率，使得模型在训练后期更加趋于收敛LBFGS（Limited-memory Broyden-Fletcher-Goldfarb-Shanno）： LBFGS是一种基于拟牛顿法的优化算法，用于解决无约束非线性优化问题。它利用函数的一阶导数和二阶导数信息来逼近目标函数的局部二次模型，并通过迭代更新参数来寻找最优解。LBFGS使用有限的内存来存储历史信息，以减少内存消耗。由于它不需要显式计算二阶导数矩阵，LBFGS适用于参数较多的问题，并且通常具有较好的收敛性能总结：AdamW是对Adam优化器的改进，解决了权重衰减对Adam优化器的影响ASGD是一种随机梯度下降法的变体，通过平均随机梯度来减小参数更新的方差，提高收敛速度和稳定性LBFGS是一种基于拟牛顿法的优化算法，通过逼近目标函数的局部二次模型来寻找最优解，具有较好的收敛性能和适用性学习率衰减在模型优化中，常用到的几种学习率衰减方法有：分段常数衰减、多项式衰减、指数衰减、自然指数衰减、余弦衰减、线性余弦衰减、噪声线性余弦衰减深度学习优化器方法及学习率衰减方式综述</content>
	</entry>
	<entry>
		<title>深度学习核心之损失函数</title>
		<url>/article/46832.html</url>
		<content>概述一文看尽深度学习中的各种损失函数常用的损失函数合集深度学习常用损失函数的基本形式、原理及特点Loss Functions损失函数(Loss Function)是用来衡量模型预测值与真实值之间差异的函数，它是深度学习中的一个重要组成部分，用于评估模型的性能并指导模型的优化过程损失函数、代价函数、目标函数的关系损失函数(Loss Function)：损失函数是用来衡量模型在单个样本上的预测结果与真实标签之间的差异。它是一个标量值，表示模型预测的误差或损失程度。损失函数通常是针对单个样本计算的，例如均方误差(MSE)、交叉熵损失等。在训练过程中，通过最小化损失函数来优化模型参数，使模型的预测结果与真实标签更接近代价函数(Cost Function)：代价函数是指整个训练集上的平均损失或误差函数。代价函数是损失函数的求和或平均，用于衡量模型在整个训练集上的预测结果与真实标签之间的总体差异。代价函数通常是在训练过程中使用的，用于计算梯度并更新模型参数目标函数(Objective Function)：目标函数是在训练过程中要最小化或最大化的函数，可以是损失函数或代价函数。目标函数是模型训练的目标，通过优化目标函数来调整模型参数，使得模型在训练集上的性能达到最优定义损失函数(Loss Function)代价函数(Cost Function)目标函数(Objective Function)数据集单个样本整个训练集训练要优化函数在实际应用中，损失函数、代价函数和目标函数这些术语有时会被混用，但它们都涉及到衡量模型的预测结果与真实标签之间的差异，并在训练过程中用于优化模型损失函数大致可分为两种：回归损失(针对连续型变量)和分类损失(针对离散型变量)具体使用哪个术语取决于上下文和个人偏好，但它们都指向类似的概念回归损失函数深度学习常用损失函数总览：基本形式、原理、特点L1 LossL1 Loss也称为Mean Absolute Error，即平均绝对误差(MAE)，公式定义为优点: 对离群点(Outliers)或者异常值更具有鲁棒性缺点: 由图可知其在0点处的导数不连续，使得求解效率低下，导致收敛速度慢；对于较小的损失值，其梯度也同其他区间损失值的梯度一样大，所以不利于网络的学习模型预测与真实值之间的误差服从拉普拉斯分布Laplace distribution可以在一定的假设下通过最大化似然可以得到MAE损失的形式，假设模型预测与真实值之间的误差服从拉普拉斯分布Laplace distribution ，则给定一个输入，模型输出真实值的概率为对其求对数可以得到的负对数似然实际上就是MAE损失的形式L2 LossL2 Loss也称为Mean Squred Error，即均方差(MSE)，它衡量的是预测值与真实值之间距离的平方和优点: 收敛速度快，能够对梯度给予合适的惩罚权重，而不是一视同仁，使梯度更新的方向可以更加精确缺点: 对异常值十分敏感，梯度更新的方向很容易受离群点所主导，不具备鲁棒性，假如我们训练数据中存在较大的异常值，此时我们将会有一个巨大的权重更新，这有可能会使模型失去平衡在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的MSE假设了误差服从高斯分布，在高斯分布假设下，可以使用最大化似然得到均方差损失的形式，假设模型预测与真实值之间的误差服从标准高斯分布，则给定一个输入，模型输出真实值的概率为进一步我们假设数据集中个样本点之间相互独立，则给定所有输出所有真实值的概率，即似然Likelihood为所有的累乘通常为了计算方便，我们通常最大化对数似然Log-Likelihood去掉与无关的第一项，然后转化为最小化负对数似然Negative Log-Likelihood可以看到这个实际上就是均方差损失的形式，也就是说在模型输出与真实值的误差服从高斯分布的假设下，最小化均方差损失函数与极大似然估计本质上是一致的因此在这个假设能被满足的场景中(比如回归)，均方差损失是一个很好的损失函数选择当这个假设没能被满足的场景中(比如分类)，均方差损失不是一个好的选择MAE和MSE作为损失函数的主要区别是MSE损失相比MAE通常可以更快地收敛当使用梯度下降算法时，MSE的梯度为，而MAE损失的梯度为，即MSE的梯度的scale会随误差大小变化，而MAE的梯度的scale则一直保持为1，即便在绝对误差很小的时候，MAE的梯度scale也同样为1，这实际上是非常不利于模型的训练的当然你可以通过在训练过程中动态调整学习率缓解这个问题，但是总的来说，损失函数梯度之间的差异导致了MSE在大部分时候比MAE收敛地更快，这也是MSE更为流行的原因MAE损失对于outlier更加健壮，即更加不易受到outlier影响，当误差非常大的时候，MSE损失会远远大于MAE损失MSE假设了误差服从高斯分布，MAE假设了误差服从拉普拉斯分布，拉普拉斯分布本身对于outlier更加robust适用场景Smooth L1 Losspytorch SMOOTHL LOSSSmooth L1 Loss即平滑的L1损失(SLL)，出自Fast RCNN，也称为SLL，Smooth L1 loss也具备了L1 loss和L2 loss各自的优点，本质就是L1和L2的组合Huber Losspytorch Huber LOSSHuber Loss是一种类似于Smooth L1 Loss的损失函数，它也能够平衡L2范数和L1范数之间的权衡Huber loss和Smooth L1 loss具有相同的曲线走势，当Huber loss中的δ等于1时，Huber loss等价于Smooth L1 loss对于Huber损失来说，的选择十分重要，它决定了模型处理局外点的行为。当残差大于时使用L1损失，很小时则使用更为合适的L2损失来进行优化优点零点导数连续: Huber损失函数克服了MAE和MSE的缺点，不仅可以保持损失函数具有连续的导数解决离群点梯度爆炸的问题: 利用MSE梯度随误差减小的特性来得到更精确的最小值，也对局外点具有更好的鲁棒性但Huber损失函数的良好表现得益于精心训练的超参数，当趋向于0时它就退化成了MAE，而当趋向于无穷时则退化为了MSEQuantile Loss分位数回归Quantile Regression是一类在实际应用中非常有用的回归算法，通常的回归算法是拟合目标值的期望或者中位数，而分位数回归可以通过给定不同的分位点，拟合目标值的不同分位数IoU LossUnitBox: An Advanced Object Detection Network 2016分类损失函数Binary Cross Entropy简单的交叉熵损失函数，你真的懂了吗对于分类问题，最常用的损失函数是交叉熵损失函数Cross Entropy Loss考虑二分类，在二分类中我们通常使用Sigmoid函数将模型的输出压缩到区间内，用来代表给定输入，模型判断为正类的概率由于只有正负两类， 因此同时也得到了负类的概率将两条式子合并成一条假设数据点之间独立同分布，则似然可以表示为对似然取对数，然后加负号变成最小化负对数似然，即为交叉熵损失函数的形式可视化下图是对二分类的交叉熵损失函数的可视化，蓝线是目标值为0时输出不同输出的损失，黄线是目标值为1时的损失可以看到约接近目标值损失越小，随着误差变差，损失呈指数增长图中蓝线是的图线，此时损失函数变为图中黄线是的图线，此时损失函数变为从图形中我们可以发现：预测输出与差得越多，的值越大，也就是说对当前模型的惩罚越大，而且是非线性增大，是一种类似指数增长的级别这是由log函数本身的特性所决定的，这样的好处是模型会倾向于让预测输出更接近真实样本标签Cross Entropy Loss交叉熵损失函数（CrossEntropy Loss）在多分类的任务中，交叉樀损失函数的推导思路和二分类是一样的，变化的地方主要有两个维度变化: 真实值现在是一个one-hot向量激活函数: 模型输出的最后的激活函数由原来的Sigmoid函数换成Softmax函数为什么分类用交叉熵损失，而不是均方差损失均方差损失实际上均方差损失假设了误差服从高斯分布，在分类任务下这个假设没办法被满足，因此效果会很差为什么是交叉熵损失呢? 有两个角度可以解释这个事情，一个角度从最大似然的角度，另一个角度是可以用信息论来解释交叉熵损失最大似然角度Softmax函数将每个维度的输出范围都限定在之间，同时所有维度的输出和为1，用于表示一个概率分布其中表示个类别中的一类，同样的假设数据点之间独立同分布，可得到负对数似然为由于是一个one-hot向量，除了目标类为1之外其他类别上的输出都为0，因此上式也可以写为其中是样本的目标类。通常这个应用于多分类的交叉樀损失函数也被称为Softmax Loss或者Categorical Cross Entropy Loss信息论角度假设对于样本存在一个最优分布真实地表明了这个样本属于各个类别的概率，那么我们希望模型的输出尽可能地逼近这个最优分布在信息论中，我们可以使用KL散度(Kullback-Leibler Divergence)来衡量两个分布的相似性给定分布和分布，两者的KL散度公式如下其中第一项为分布的信息熵，第二项为分布和的交叉熵。将最优分布和输出分布带入和得到由于我们希望两个分布尽量相近，因此我们最小化KL散度。同时由于上式第一项信息熵仅与最优分布本身相关，因此我们在最小化的过程中可以忽略掉，变成最小化我们并不知道最优分布，但训练数据里面的目标值可以看做是的一个近似分布这个是针对单个训练样本的损失函数，如果考虑整个数据集，则可以看到通过最小化交叉嫡的角度推导出来的结果和使用最大化似然得到的结果是一致的Hinge Loss合页损失Hinge Loss是另外一种二分类损失函数，适用于maximum-margin的分类，支持向量机Support Vector Machine (SVM)模型的损失函数本质上就是Hinge Loss + L2正则化下图是为正类，即时，不同输出的合页损失示意图可以看到当为正类时，模型输出负值会有较大的惩罚，当模型输出为正值且在区间时还会有一个较小的惩罚即合页损失不仅惩罚预测错的，并且对于预测对了但是置信度不高的也会给一个惩罚，只有置信度高的才会有零损失使用合页损失直觉上理解是要找到一个决策边界，使得所有数据点被这个边界正确地、高置信地被分类Focal LossFocal Loss for Dense Object Detection 2018ppt: Focal Loss for Dense Object DetectionFocal Loss损失函数(超级详细的解读)Focal loss损失函数是为了解决one-stage目标检测中正负样本极度不平衡的问题，由何恺明(Kaiming He)团队提出Focal loss是基于BCE(二分类交叉熵)的。它是一个动态缩放的交叉熵损失，通过一个动态缩放因子，可以动态降低训练过程中易区分样本的权重，从而将重心快速聚焦在那些难区分的样本(有可能是正样本，也有可能是负样本，但都是对训练网络有帮助的样本)正负样本不平衡(Class Imbalance)在一张图像中能够匹配到目标的候选框(正样本)个数一般只有十几个或几十个，而没有匹配到的候选框(负样本)则有10000~100000个这么多的负样本不仅对训练网络起不到什么作用，反而会淹没掉少量但有助于训练的样本Focal loss是为了解决一阶段目标检测模型，那为什么二阶段不用解决在two-stage中分了两步，第一步时同样也会生成许多的负样本以及很少的正样本，但到第二步时，它会在第一步的基础上选取特定数量的正负样本去检测，所以正负样本并不会特别不平衡，二阶段模型还可以采用更复杂的采样策略和hard negative mining (难例挖掘)等方法来处理样本不平衡和难易样本的问题，因此对于二阶段目标检测模型来说，Focal Loss的优势可能相对较小引出Focal loss为了方便接下来的描述，这里先定义为此时cross entropy可以定义为解决类别不平衡的常见方法是为类别1引入一个权重因子，而对于类别非1引入权重因子，这里引出Balanced Cross Entropy(平衡交叉熵)在论文实验中显示，密集检测器训练过程中遇到的类别不平衡问题使得交叉熵损失失去了效果，易于分类的负样本占据了大部分损失并主导了梯度虽然平衡了正样本和负样本的重要性，但它无法区分易于和困难的样本因此，论文提出了新的损失函数以减小易于样本的权重，从而将训练的重点放在困难的负样本上，更具体地说，论文提出在交叉熵损失中添加一个调制因子，其中是可调的Focal参数。我们将这个损失函数称为Focal loss，定义Focal Loss公式如下其中作用是调节难易，较小的值会使得易样本的损失权重下降更慢，而较大的值则会加速易样本的损失权重下降而作用是平衡正负样(正负样本数量不均衡)，当接近0时，负样本的损失贡献被放大，从而平衡了正负样本之间的重要性通过调整和的值，可以根据具体情况调节模型对不同样本的关注程度，提高模型对难样本的学习和训练效果可视化下图可视化了的值，可以观察到增加了分类不准确样本在损失函数中的权重增加了难分样本在损失函数的权重，使得损失函数倾向于难分的样本，有助于提高难分样本的准确度J{KL} = -\sum{i=0}^{C} y{i} \log \left(\hat{y}{i}\right)-y{i} \log \left(y{i}\right)=\sum{i=0}^{C} y{i}\left(\frac{y{i}}{\hat{y}{i}}\right)$$优点：适用于近似复杂的目标分布，如图像如上所示，KL散度损失是从我们网络预测的交叉熵分布与目标分布的熵之间的差异。它告诉我们模型离期望的分布有多远那么我们什么情况下使用它如果我们的任务是生成图像，那么目标分布要复杂得多，在这种情况下，使用KL散度损失的效果最好正则化技术L1正则化在损失函数中添加模型参数的 L1 范数作为正则项。它促使模型参数稀疏化，即将一些参数压缩为零，从而实现特征选择和模型简化L2正则化在损失函数中添加模型参数的 L2 范数作为正则项。它对模型参数进行平滑约束，使模型参数值趋向于较小的值，有助于防止过拟合弹性网正则化弹性网正则化是 L1 正则化和 L2 正则化的一种结合。它同时对模型参数使用 L1 和 L2 正则化，从而综合考虑了稀疏性和平滑性的影响其他正则Dropout：Dropout 是一种在训练过程中随机丢弃部分神经元的技术。它可以防止神经网络过拟合，并提高模型的泛化能力。通过在训练过程中以一定概率将部分神经元置零，Dropout 可以强制模型在没有完整神经网络的情况下进行学习数据增强(Data Augmentation)：数据增强是一种通过对训练数据进行随机变换来扩充数据集的技术。常见的数据增强操作包括随机裁剪、旋转、翻转、平移、缩放等。数据增强可以增加训练数据的多样性，提高模型的泛化能力和鲁棒性图像增强技术：了解常见的图像增强方法，如对比度调整、亮度调整、色彩平衡、直方图均衡化等，以及它们在图像处理和计算机视觉中的应用</content>
	</entry>
	<entry>
		<title>深度学习核心之激活函数</title>
		<url>/article/22410.html</url>
		<content>概述激活函数（Activation Function）激活函数可视化激活函数(Activation Function)是一种添加到人工神经网络中的函数，旨在帮助网络学习数据中的复杂模式引入激活函数的目的不使用激活函数的话，神经网络的每层都只是做线性变换，多层输入叠加后也还是线性变换。因为线性模型的表达能力通常不够所以这时候就体现了激活函数的作用了，激活函数可以引入非线性因素，设有三个线性变换将这三个线性变换加到一起，可以得到所以，如果神经网络只有线性，那么不论有多少隐层，有多少神经元，最终还是线性的此时就需要通过添加激活函数来对每一层的输出做处理，引入非线性因素，使得神经网络可以逼近任意的非线性函数激活函数在神经网络中的应用，除了引入非线性表达能力，其在提高模型鲁棒性、缓解梯度消失问题、将特征输入映射到新的特征空间以及加速模型收敛等方面都有不同程度的改善作用小结:引入非线性表达能力提高模型鲁棒性：非线性的激活函数能够引入非线性变换，从而使神经网络具有更强的表达能力，能够更好地拟合复杂的数据分布和模式缓解梯度消失问题：在深层神经网络中，由于链式求导的关系，梯度在反向传播过程中会逐渐减小，导致梯度消失的问题使用非线性激活函数可以帮助缓解梯度消失，因为这些函数在输入的不同范围内具有不同的斜率，从而允许梯度能够在反向传播中更好地传递常见激活函数Activation Functions with Derivative and Python code: Sigmoid vs Tanh Vs Relu【激活函数合集】盘点当前最流行的激活函数及选择经验激活函数发展历程早期1970198020102013阶跃函数(Step Function)Sigmoid双曲正切(Tanh)ReLU(Rectified Linear Unit)Leaky ReLUMaxout2015201620172020PReLU(Parametric ReLU)ELU(Exponential Linear Unit)GELUS(Gaussian Error Linear Units)SwishMish相对应的论文链接如下Maxout Networks 2013Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification 2015 PReLUFast and Accurate Deep Network Learning by Exponential Linear Units (ELUs) 2016Gaussian Error Linear Units (GELUS) 2016年提出Searching for Activation Functions (Swish) 2017Swish: a Self-Gated Activation Function 2017Mish: A Self Regularized Non-Monotonic Neural Activation Function 2019SigmoidSigmoid函数(Logistic函数)函数的图像看起来像一个S形曲线，函数表达式如下对应的导数为函数图像和对应的导数图像如下图所示作用将输入映射到0到1之间的连续值，常用于二分类问题或输出层的概率预测Sigmoid激活函数的优缺点优点平滑易于求导可以作为概率，辅助模型解缺点梯度消失: Sigmoid函数的导数形式为，其中是Sigmoid函数当输入值非常大时，Sigmoid函数接近于1，导数接近于0，而当输入值非常小时，Sigmoid函数接近于0，导数同样接近于0这意味着在反向传播时，梯度逐渐变小并趋近于零，传递到较早层时，梯度几乎消失了梯度更新慢: 如果神经元的输出大部分位于饱和区域(接近0或1)，那么梯度将非常小，因为sigmoid函数在饱和区域的导数接近于0(如图所示)这样，权重的更新将变得非常缓慢，可能导致训练过程变得困难，并且网络的收敛速度减慢计算效率低: Sigmoid函数执行指数运算，计算机运行得较慢TanhTanh函数(双曲正切函数)函数的图像看起来也像一个S形曲线，函数表达式如下对应的导数为Tanh图像和sigmoid函数比较如下图所示与Sigmoid函数对比Tanh是一个双曲正切函数，Tanh函数和sigmoid函数的曲线相对相似，但是它比sigmoid函数更有一些优势，这两种激活函数均为饱和激活函数优点：范围更广：tanh函数的输出范围是(-1, 1)，而sigmoid函数的输出范围是(0, 1)，tanh函数在均值为0附近更集中，对于某些任务可能更适用收敛速度快：tanh函数在输入的绝对值较大时，输出的梯度也较大，这可以帮助网络更快地收敛具有零中心性：tanh函数的均值为0，相对于sigmoid函数来说更容易处理正负值的输入，训练相对容易缺点：梯度消失问题: 与Sigmoid函数类似，梯度消失问题仍然存在计算代价较高：相对于sigmoid函数来说，tanh函数的计算代价较高，因为它需要进行指数运算ReluReLU函数的主要优点是计算简单、非线性表达能力强、使网络更快速地收敛，并且在深度神经网络中表现出良好的性能，函数表达式如下对应的导数为函数图像和对应的导数图像如下图所示主要优缺点优点：计算速度快：ReLU函数的计算非常简单，只有线性关系，不需要指数计算，不管在前向传播还是反向传播，计算速度都比sigmoid和tanh快缓解梯度消失问题：相比于sigmoid和tanh等函数，在正区间上ReLU函数的导数为常数1，这有助于缓解梯度消失问题，使得深层网络的梯度能够更好地传播，更容易进行反向传播算法的优化，至少x在正区间内，神经元不会饱和激活稀疏性：ReLU函数对于负输入值直接输出为0，这导致神经元的激活变得稀疏，即只有部分神经元会被激活。这种稀疏性有助于减少参数之间的冗余，并且在一定程度上具有正则化的效果缺点：神经元死亡问题：ReLU函数在负区间上输出恒为0，当神经元在训练过程中出现负输入时，会导致该神经元永远不会被激活，称为神经元死亡问题。这会导致一部分神经元失去了学习能力输出不是零中心：ReLU函数的输出范围是，并不以0为中心，这可能会对某些优化算法的收敛速度产生影响训练神经网络的时候，一旦学习率没有设置好，第一次更新权重的时候，输入的是负值，那么这个含有ReLU的神经节点就会死亡，再也不会被激活在实际训练中，如果学习率设置的太高，可能会发现网络中40%的神经元都会死掉，且在整个训练集中这些神经元都不会被激活所以，设置一个合适的较小的学习率会降低这种情况的发生为了解决神经元节点死亡的情况，有人提出了Leaky ReLU、P-ReLU、R-ReLU、ELU等激活函数Leaky ReLULeaky ReLU函数在负数范围内引入了一个小的斜率，解决了ReLU函数中负数部分的死亡问题，函数表达式如下对应的导数为函数图像和对应的导数图像如下图所示优缺点优点:Leaky ReLU中引入了超参数，一般设置为0.01。在反向传播过程中，对于Leaky ReLU的输入小于零的情况，也可以计算得到一个梯度(而不是像ReLU一样值为0)，这样就避免了神经元死亡的问题缺点:稀疏性差: 相较于ReLU，神经网络的稀疏性要差一些额外的超参数: 引入了额外的超参数神经网络不学习值为什么Leaky ReLU比ReLU更好调整负值的零梯度: Leaky ReLU通过把x的非常小的线性分量给予负输入(0.01x)来调整负值的零梯度(zero gradients)问题扩大函数的范围: Leaky ReLU有助于扩大ReLU函数的范围，通常的值为0.01左右取值范围: Leaky ReLU的函数范围是(负无穷到正无穷)从理论上讲，Leaky ReLU具有ReLU的所有优点，而且Dead ReLU不会有任何问题，但在实际操作中，尚未完全证明Leaky ReLU总是比ReLU更好PReLUP-Relu激活函数的解析式P-Relu函数及其导数的图像如下图所示其中是超参数。这里引入了一个随机的超参数，它可以被学习，因为你可以对它进行反向传播这使神经元能够选择负区域最好的梯度，有了这种能力，它们可以变成ReLU或Leaky ReLU与ReLU和Leaky ReLU的关系看一下PReLU的公式：参数α通常为0到1之间的数字，并且通常相对较小如果，则变为ReLU如果，则变为Leaky ReLU如果是可学习的参数，则变为PReLUELUELU as an Activation Function in Neural NetworksCUDA编程入门之激活函数ELUELU激活函数解决了ReLU的一些问题，同时也保留了一些好的方面。这种激活函数要选取一个值；常见的取值是在0.1到0.3之间对应的导数为函数及其导数的图像如下图所示如果我们输入的值大于0，则结果与ReLU一样，即值等于值；但如果输入的值小于0，则我们会得到一个稍微小于0的值，所得到的值取决于输入的值，但还要兼顾参数你可以根据需要来调整这个参数。更进一步，我们引入了指数运算，因此ELU的计算成本比ReLU高优缺点优点与ReLU不同，它没有神经元死亡的问题。 这是因为 ELU 的梯度对于所有负值都是非零的与其他线性非饱和激活函数（如 ReLU 及其变体）相比，它有着更快的训练时间作为非饱和激活函数，它不会遇到梯度爆炸或消失的问题所有点上都是连续的和可微缺点包含指数运算，计算时间长无法避免梯度爆炸问题神经网络无法学习值与Leaky-ReLU和PReLU类似，与ReLU不同的是，ELU没有神经元死亡的问题(ReLU Dying 问题是指当出现异常输入时，在反向传播中会产生大的梯度，这种大的梯度会导致神经元死亡和梯度消失)。它已被证明优于ReLU及其变体，如Leaky-ReLU(LReLU)和Parameterized-ReLU(PReLU)。与ReLU及其变体相比，使用ELU可在神经网络中缩短训练时间并提高准确度GELUGaussian Error Linear Units (GELUS) 2016年提出 论文地址GELU可以看做是RELU的一个平滑版本；GELU激活函数的最大特点是：将非线性与依赖输入数据分布的随机正则化器相结合在一个激活函数的表达中在预训练语言模型中，GELU可以说是主流的激活函数；Bert，RoBERTa，ALBERT，GPT-2等顶尖的NLP预训练模型都是使用的GELUbert中使用的激活函数，作者经过实验证明比relu等要好。原点可导，不会有Dead ReLU问题与Relu和ELU比较如下SoftplusSoftplus函数类似于ReLU函数，但是相对较平滑，像ReLU一样是单侧抑制，它的接受范围很广其对应的导数如下优点：作为relu的一个不错的替代选择，softplus能够返回任何大于0的值与relu不同，softplus的导数是连续的、非零的，无处不在，从而防止出现死神经元缺点：导数常常小于1，也可能出现梯度消失的问题softplus另一个不同于 relu的地方在于其不对称性，不以零为中心，可能会妨碍学习Maxout等待…Swish将Sigmoid函数与线性函数的乘积作为激活函数，平衡了非线性和线性表达能力MishMish: A Self Regularized Non-Monotonic Neural Activation Function 2020SoftSign优点：softsign是 tanh激活函数的另一个替代选择softsign是反对称、去中心、可微分，并返回−1和1之间的值softsign更平坦的曲线与更慢的下降导数表明它可以更高效地学习缺点：导数的计算比tanh更麻烦激活函数的选择选择一个适合的激活函数并不容易，需要考虑很多因素，通常的做法是，如果不确定哪一个激活函数效果更好，可以把它们都试试，然后看看在验证集或者测试集上的效果然后看哪一种表现的更好，就去使用它以下是常见的选择情况：如果输出是0、1值(二分类问题)，则输出层选择Sigmoid函数，然后其它的所有单元都选择ReLU函数如果在隐藏层上不确定使用哪个激活函数，那么通常会使用ReLU激活函数。有时，也会使用Tanh激活函数Sigmoid激活函数：除了输出层是一个二分类问题基本不会用它Tanh激活函数：Tanh是非常优秀的，几乎适合所有场合ReLU激活函数：最常用的默认函数，如果不确定用哪个激活函数，就使用ReLU或者Leaky ReLU，再去尝试其他的激活函数如果遇到了一些死的神经元，我们可以使 Leaky ReLU函数</content>
	</entry>
	<entry>
		<title>深度学习核心基础知识点</title>
		<url>/article/6384.html</url>
		<content>基础知识简单说明下机器学习和深度学习的主要区别：模型结构：一般深度学习基于深度神经网络架构较为复杂化自动提取特征：传统机器学习需要人工构造特征但深度学习基于神经网络自动提取特征数据量：深度学习需要大量的数据集来训练多层神经网络反向传播算法反向传播(Backpropagation)算法是一种用于计算神经网络中各个参数梯度的方法，它基于链式法则和梯度下降算法，下面是反向传播算法的基本步骤：前向传播(Forward Propagation)：输入一个样本，通过神经网络的前向传播计算得到预测输出逐层计算每个神经元的输出值，直至输出层在前向传播过程中，将每一层的输入、权重和激活函数的结果保存下来，用于后续的反向传播计算计算损失函数(Loss Calculation)：根据预测输出和真实标签计算损失函数的值常见的损失函数包括均方误差(Mean Squared Error)、交叉熵损失(Cross-Entropy Loss)等反向传播计算梯度(Backward Propagation)：从输出层开始，根据链式法则计算每个参数的梯度逐层向后传播，通过链式法则将上一层的梯度乘以当前层的局部梯度，得到当前层的梯度根据梯度下降算法更新网络参数参数更新(Parameter Update)：根据计算得到的梯度值，使用梯度下降算法或其变种方法来更新网络的参数常见的梯度下降算法包括随机梯度下降(Stochastic Gradient Descent，SGD)、动量法(Momentum)、Adam等重复以上步骤：对于每个训练样本，重复执行前向传播、损失计算、反向传播和参数更新的步骤迭代训练过程直到达到预设的停止条件，如达到最大迭代次数或损失函数收敛注意，反向传播算法会重复利用前向传播中存储的中间值，以避免重复计算，因此，需要保留前向传播的中间结果，这也会导致模型训练比单纯的预测需要更多的内存(显存)同时这些中间结果占用内存(显存)大小与网络层的数量和批量(batch_size)大小成正比因此使用大batch_size训练更深层次的网络更容易导致内存不足(out of memory)的错误反向传播算法通过有效地计算每个参数的梯度，使得神经网络可以通过梯度下降等优化方法来不断更新参数以最小化损失函数这样，神经网络可以逐步优化自身的权重和偏置，从而提高模型的性能和准确度深度学习优化存在许多挑战，其中一些最令人烦恼的是局部最小值、鞍点和梯度消失局部最小值(local minimum): 对于任何目标函数，如果在处对应的值小于在附近任何其他点的值，那么可能是局部最小值如果在处的值是整个域上目标函数的最小值，那么是全局最小值鞍点(saddle point): 指函数的所有梯度都消失但既不是全局最小值也不是局部最小值的任何位置梯度消失(vanishing gradient): 因为某些原因导致目标函数的梯度接近零(即梯度消失问题)，是在引入ReLU激活函数和ResNet之前训练深度学习模型相当棘手的原因之一在深度学习中，大多数目标函数都很复杂，没有解析解，因此，我们需使用数值优化算法梯度消失和爆炸梯度不稳定的原因：核心在于链式法则，前面层上的梯度是来自后面层上梯度的乘积。当存在过多的层时，就会出现梯度不稳定场景，比如梯度消失和梯度爆炸梯度爆炸在反向传播过程中需要对激活函数进行求导，如果导数大于1，那么随着网络层数的增加梯度更新将会朝着指数爆炸的方式增加这就是梯度爆炸梯度消失同样如果导数小于1，那么随着网络层数的增加梯度更新信息会朝着指数衰减的方式减少这就是梯度消失根本原因梯度消失和梯度爆炸，其根本原因在于反向传播训练法则，属于先天不足计算权值更新信息的时候需要计算前层偏导信息，因此如果激活函数选择不合适比如使用sigmoid，梯度消失就会很明显了，如果使用sigmoid作为损失函数，其梯度是不可能超过0.25，这样经过链式求导之后，很容易发生梯度消失解决方案梯度消失和梯度爆炸是在神经网络训练过程中常见的问题，可能导致模型收敛困难或无法有效更新权重。下面是梯度消失和梯度爆炸的产生原因和解决方法：梯度消失(Gradient Vanishing)原因：在深层神经网络中，梯度通过多个层传递时可能会逐渐衰减。特别是使用具有小梯度的激活函数(如Sigmoid、Tanh)时，梯度消失问题更为严重解决方法：大梯度的激活函数：选择具有较大梯度的激活函数，如ReLU、Leaky ReLU等参数初始化：合理初始化权重，避免初始梯度过小或过大批归一化(Batch Normalization)：通过对每一层的输入进行归一化，可以使得激活函数输入值的分布更稳定，有助于缓解梯度消失问题残差连接(Residual Connections)：引入跨层的直接连接，使得梯度可以更容易地通过网络传播，例如ResNet中的残差连接结构梯度爆炸(Gradient Explosion)原因：在某些情况下，梯度可能会指数级增长，导致数值溢出解决方法：梯度裁剪(Gradient Clipping)：限制梯度的范围，通过设置阈值或缩放梯度来防止梯度爆炸参数初始化：合理初始化权重，避免初始梯度过大使用稀疏连接：减少网络中的连接数量，可以降低梯度爆炸的风险更小的学习率：降低学习率，使得梯度更新更加稳定正则化方法可以在一定程度上缓解梯度爆炸和梯度消失的问题总的来说，解决梯度消失和梯度爆炸问题的方法包括选择合适的激活函数、参数初始化策略、批归一化、残差连接、梯度裁剪等需要根据具体情况综合考虑并适当调整这些方法以提高训练的稳定性和效果泛化问题过拟合(Over fitting)和欠拟合(Under fitting)是指机器学习模型在训练过程中对训练数据的拟合程度不合适的问题过拟合现象过拟合指的是模型过于复杂，过度适应了训练数据的特征，导致在新的未见过的数据上表现不佳过拟合的表现是在训练数据上表现出较好的拟合效果，但在测试数据上的表现较差过拟合的原因可能是模型过于复杂，参数过多，导致模型具有很强的记忆能力而忽略了数据的真实规律产生原因模型太复杂: 训练集的数量级和模型的复杂度不匹配，训练集的数量级要小于模型的复杂度特征分布不一致: 训练集和测试集特征分布不一致学习到噪音: 样本里的噪音数据干扰过大，大到模型过分的记住了噪音特征，而忽略了真实的输入输出间的关系迭代过多: 权值学习迭代次数足够多(Over training)，拟合了训练数据中的噪声和训练样例中没有代表性的特征解决方案简化模型: 使其适合自己训练集的数量级(缩小宽度和减小深度)，使模型更好地学习数据的真实分布数据增强: 训练集越多，过拟合的概率越小。在计算机视觉领域中，增广的方式是对图像旋转，缩放，剪切，添加噪声等正则化: 指通过引入额外新信息来解决过拟合问题的一种，这种额外信息通常的形式是模型复杂性带来的惩罚度。正则化可以保持模型简单dropout: 在训练的时候让神经元以一定概率不工作Early stopping: 迭代次数截断的方法来防止过拟合的，即在模型对训练数据集迭代收敛之前停止迭代来防止过拟合欠拟合现象欠拟合指的是模型过于简单，不能很好地拟合训练数据的特征，导致在训练数据和测试数据上的表现都不佳欠拟合的表现是模型无法捕捉到数据中的复杂关系或规律，无法很好地拟合训练数据欠拟合的原因可能是模型过于简单，参数过少，无法充分学习数据的特征产生原因模型复杂度不足：模型的复杂度不足以捕捉数据中的复杂关系。例如，线性模型无法很好地拟合非线性数据，导致欠拟合特征提取不足：特征提取不足意味着模型无法从原始数据中提取出有用的特征。如果特征不具备较强的区分性，模型就无法很好地进行拟合数据量不足：当训练数据较少时，模型难以学习数据的真实分布和规律。有限的数据样本可能无法提供足够的信息来训练一个复杂的模型过度正则化：过度正则化可以导致模型过于简单，无法很好地拟合训练数据。例如，强制施加较强的正则化项或使用较高的正则化系数会限制模型的灵活性数据噪声：当训练数据中存在噪声或错误标签时，模型可能会过于关注这些异常样本，从而导致欠拟合解决方案特征工程: 欠拟合是由于学习不足，可以考虑添加特征，从数据中挖掘更多的特征，有时候还需要对特征进行变换，使用组合特征和高次特征更强的模型: 模型简单也会导致欠拟合，如线性模型只能拟合一次函数的数据，尝试使用更高级的模型有助于解决欠拟合调整超参数: 调整模型的超参数，如学习率、正则化系数等，以获得更好的模型拟合效果核心网络层全连接层等待…Conv卷积操作一文详解各种卷积操作pytorch中卷积网络的几种卷积和池化pytorch官网关于cnn的说明卷积操作是神经网络中常用的操作之一，特别是在图像处理和计算机视觉任务中卷积操作通过将输入数据与卷积核(也称为滤波器)进行逐元素相乘，并将结果进行求和，从而产生输出特征图在卷积神经网络(Convolutional Neural Network，CNN)中，卷积操作用于提取输入数据中的局部特征，它利用了特征的局部相关性和平移不变性局部相关性: 局部相关性指的是每个神经元只与输入数据的一小块区域连接，可以捕捉到输入数据的局部特征，减少了参数数量，降低了计算复杂度平移不变性: 平移不变性指的是它对输入数据的平移具有不变性，卷积层可以通过共享权重和局部连接的方式来提取特征，从而使得对于不同位置的物体能够产生相似的特征表示卷积输出大小计算公式其中表示输入的特征大小，是填充，是卷积核大小，是步长一维卷积在线可视化二维卷积在线可视化普通卷积深度学习中卷积的概念与信号处理领域的卷积相似(只是深度学习的卷积在运算过程中的滤波器不经过翻转)，卷积核以一定的步长在输入图片上滑动，每一步都将对应元素相乘后求和的过程，如下图所示卷积核大小(kernel_size)卷积核大小定义了卷积的视图，根据卷积核的大小，常见的卷积尺寸有1×1卷积、3×3 卷积、5×5卷积、7×7卷积等卷积：通常用于输出维度的升维或降维。若特征图是尺寸是，卷积核尺寸是，输出通道尺寸是。当我们将次卷积结果连接在一起时，可以得到的输出，从而实现升维或降维的功能卷积：由于大尺寸的卷积核的参数量较大，研究人员发现两个卷积的堆叠，感受野等同于一个卷积，但是参数量却减少了，所以从VGG的时代开始，基本后面的网络都离不开它的身影卷积：卷积核越大，感受野越大，看到的图片信息越多，所获得的全局特征越好。但是这样参数很多，会导致计算量暴增，层次少不利于模型深度的增加，表达能力弱。所以我们会看到在早期的网络会出现大卷积核的堆叠，或者当下研究人员一般将大尺寸卷积放在对输入图片的初始操作处(同理)步长(stride)核的步长定义了卷积核在图像中移动的每一步的大小，代表提取的精度，通常为1，也可以用大于等于2的步长，对图像进行下采样，替代池化操作填充(padding)卷积核与输入图像的尺寸不匹配，这时就需要填充图像，例如输入图片尺寸为，卷积核的大小为如果不进行填充，步长为1的话，当卷积核沿着图片滑动后只能滑动出一个的图片出来，卷积后的图片越变越小，且输入图片边缘像素只被计算过一次而中间像素会被卷积计算多次，意味着丢失图像角落信息，所以为了避免这种情况，需要先对原始图片做边界填充处理3D卷积3D卷积的卷积核可以在输入图像的3个方向，即图像的高度，宽度，深度上移动。并与二维卷积类似，在每个位置各元素先相乘再相加，最后输出一个3D数据卷积特性：相比于普通的二维卷积，多了一个维度(深度)，可以把这个深度当作视频中的连续帧，或者是立体图像中的不同的切片。但是其参数量较大，会延缓网络的推理速度应用场景：3D卷积常应用在视频分类、医学影像、点云处理等领域。比如经典的VoxelNet网络就采用3D CNN提取点云的体素特征做目标检测任务扩张卷积(膨胀、空洞)扩张卷积(Dilated Convolution)，也称为空洞卷积(Atrous Convolution)，最早是由DeepLab团队提出的DeepLab是由Google Brain开发的一种用于图像语义分割的深度学习架构，随后，扩张卷积被广泛应用于其他计算机视觉任务，如图像超分辨率、目标检测、图像生成和图像修复等扩张卷积的主要目的是增加卷积操作的感受野，从而捕捉更大范围的上下文信息传统的卷积操作具有固定的卷积核尺寸和步幅，限制了感受野的大小。而扩张卷积通过在卷积核的元素之间引入固定的间隔(扩张率或空洞率)，使得卷积核在输入特征图上的采样间隔扩大，从而实现感受野的扩大扩张卷积最初用于图像语义分割任务，旨在提高分割结果的精度。语义分割需要将图像中的每个像素分类到不同的语义类别中，因此需要充分考虑像素周围的上下文信息通过使用扩张卷积，DeepLab团队可以更好地捕捉像素之间的长距离依赖关系，提高分割模型对细粒度边界和小尺寸物体的感知能力它在这些任务中的应用主要是为了增加感受野、提高分辨率和实现多尺度特征融合，以增强模型的感知能力和表达能力扩张的好处：使得在相同的计算成本下，避免因池化损失信息而增大了感受野膨胀卷积使用的方法连续使用多个膨胀卷积时应该如何设计它的膨胀系数将膨胀系数设置为锯齿形状，例如[1,2,3,1,2,3]公约数不能大于1，比如可以是[1,2,3]，而不是[2,4,8]分组卷积分组卷积（Group Convolution）Depthwise卷积与Pointwise卷积分组卷积最开始被使用在经典入门卷积神经网络AlexNet上，用于解决显存不足的问题。在现在被广泛用于各种轻量化模型中，用于减少运算量和参数量，其中应用最广的就是深度可分离卷积(Depthwise Separable Convolution)Depthwise卷积(深度卷积)和Pointwise卷积(逐点卷积)Depthwise卷积(深度卷积)和Pointwise卷积(逐点卷积)是MobileNet等轻量级神经网络中常用的卷积操作卷积，合起来被称作Depthwise Separable Convolution(参见Google的Xception)，该结构和常规卷积操作类似，可用来提取特征，但相比于常规卷积操作，其参数量和运算成本较低。所以在一些轻量级网络中会碰到这种结构如MobileNet可以将深度卷积和逐点卷积结合使用，作为分组卷积的一种实现方式，以提高模型的计算效率和参数效率深度卷积用于捕捉空间上的相关信息，逐点卷积用于整合和转换通道间的特征关系。这样的组合可以在一定程度上减少计算量，并保持模型的表达能力传统和分组卷积的比较在传统的卷积操作中，输入特征图的每个通道都与卷积核的每个通道进行卷积运算，输出的特征图是所有通道的叠加在分组卷积中，将输入特征图和卷积核分成多个组，每个组中的通道进行独立的卷积运算，最后将各组的输出特征图进行连接，得到最终的输出结果左图是普通的卷积，右图是分组卷积对于尺寸为的输入矩阵，当标准卷积核的尺寸为，共有个标准卷积核时，标准卷积会对完整的输入数据进行运算，最终得到的输出矩阵尺寸为 。这里我们假设卷积运算前后的特征图尺寸保持不变，则上述过程可以展示为左图分组卷积中，通过指定组数来确定分组数量，将输入数据分成组。需要注意的是，这里的分组指的是在深度上进行分组，输入的宽和高保持不变，即将每个通道的数据分为一组。因为输入数据发生了改变，相应的卷积核也需要进行对应的变化，即每个卷积核的输入通道数也就变为了，而卷积核的大小是不需要改变的同时，每组的卷积核个数也由原来的变为。对于每个组内的卷积运算，同样采用标准卷积运算的计算方式，这样就可以得到组尺寸为的输出矩阵，最终将这组输出矩阵进行拼接就可以得到最终的结果。这样拼接完成后，最终的输出尺寸就可以保持不变，仍然是。分组卷积的运算过程如右图所示分组卷积的主要优点是减少了计算量和参数数量，因为每个组内的卷积操作是独立进行的，相当于将整个卷积操作拆分成了多个较小的卷积操作这在一些计算资源有限的情况下，如移动设备或嵌入式系统中，可以显著减少计算成本，提高模型的速度和效率需要注意的是，分组卷积的使用需要在考虑计算效率的同时权衡模型性能。分组卷积可能会损失一定的表示能力，特别是对于那些具有跨通道相关性的特征因此，在设计网络结构时，需要根据任务的需求和资源的限制来选择合适的分组数和组内通道数，以在效率和准确性之间找到平衡点反卷积(转置)UNet Architecture BreakdownPytorch 转置卷积转置卷积(Transposed Convolution)，也称为反卷积(Deconvolution)，是卷积神经网络中常用的一种操作它与标准卷积操作相反，用于将低维特征映射扩展为高维特征映射在传统的卷积操作中，输入特征图通过卷积核进行卷积运算，得到输出特征图。而转置卷积则是通过对输出特征图应用反向卷积操作，以重建高维特征映射转置卷积常用于以下几个任务和应用中：图像生成：转置卷积被广泛应用于图像生成任务，如图像生成、图像修复等。通过将低维特征映射转换为高维特征映射，可以生成具有更高分辨率和更多细节的图像图像分割：在图像分割任务中，转置卷积常用于将低分辨率的语义特征映射恢复到与输入图像相同的尺寸，以获得像素级别的分割结果目标检测：转置卷积在目标检测任务中被用于生成高分辨率的特征映射，以便更准确地定位和识别目标需要注意的是，转置卷积的名称可能会引起一些误解，因为它实际上并不是真正的卷积操作的逆运算。转置卷积的名称起源于其与卷积操作的相似性，但其计算过程与卷积并不完全相同在神经网络中，我们经常需要上采样来提高低分辨率图片的分辨率。而转置卷积就可以作为一种通过卷积学习参数，从而获得最优上采样的方法可视化解释下图是一个的输入矩阵，用的卷积核进行没有填充，步长为1的卷积操作，结果是一个的矩阵我们将的卷积核重排为的形式(下图第一项)，同时将的输入矩阵展开为的列向量的形式(下图第二项)。通过矩阵乘法得到一个的列向量，可以看出这个列向量正是由上图的卷积输出矩阵展开得到的，也就是说，我们可以将卷积操作写成矩阵乘法运算通过这样的操作，可以把16(的矩阵)个值映射为4(的矩阵)个值，那么将这个操作反过来，我们就可以把4(的矩阵)个值映射为16(的矩阵)最后将输出矩阵可以reshape为的形式卷积特征：虽然转置卷积能够通过学习参数进行最优的上采样，但是实际应用中，研究人员往往更加倾向于使用线性插值的方式做上采样，效率更高可分离卷积空间可分离卷积空间可分离卷积主要处理图像和卷积核的空间维度：宽度和高度将一个卷积核划分为两个较小的卷积核，如将的卷积核分为和的卷积核，再依次进行卷积如此，则可以将一次卷积的9次乘法减少为两次卷积共6次乘法，减少计算量，从而加快网络运行速度深度可分离卷积深度可分离卷积不仅涉及空间维度，还对深度维度进行处理。它主要分为两个过程，分别为逐通道卷积和逐点卷积逐通道卷积: 逐通道卷积的卷积核与通道是一一对应的，所以输出的特征图片的深度和输入的深度完全一样逐点卷积: 逐点卷积的运算与常规卷积运算相似，它的卷积核的尺寸为，为上一层的深度。它是将上一步的特征图在深度方向上进行加权相加，生成新的特征图,有几个卷积核就有几层新的特征图输出卷积特性：深度可分离卷积的优势在于需要提取的属性越多，就能够节省越多的参数，减少计算量。最早出现在mobilenet中，也是用于轻量化网络的特征提取部分，使嵌入式部署的神经网络推理更快速相较于传统的标准卷积，它只需要对每个通道进行卷积操作，而不是对每个通道和每个位置都进行卷积操作。因此，它能够在保持模型性能的同时，显著减少计算开销，特别适用于轻量级模型和计算资源受限的场景可分离卷积常用于移动设备、嵌入式设备和计算资源有限的环境中，如移动端的图像分类、目标检测、语义分割等任务通过采用可分离卷积，可以实现轻量级的模型结构，提高模型的计算效率和速度，同时降低模型的存储空间和内存消耗可变形卷积Deformable Convolutional Networks 2017可变形卷积(Deformable Convolution)是一种具有自适应感受野的卷积操作，能够更好地捕捉图像中非刚性形变的特征。与传统的卷积操作相比，可变形卷积引入了可学习的偏移参数，用于调整卷积核在输入特征图上的采样位置在传统的卷积操作中，卷积核的采样位置是固定的，对输入特征图的不同区域应用相同的采样模式。而可变形卷积通过在每个位置引入偏移参数，使得卷积核可以自适应地调整采样位置，从而适应不同区域的非刚性形变思路可变形卷积是一种卷积核内部点按不规则的方式组合，如下图所示(a)普通卷积采样的9个点(绿点)；(b)(c)(d)都为可变形卷积，在普通卷积采样坐标上加上一个位移量(蓝色箭头)，得到变形的采样位置(深蓝点)对于输入的一张特征图，把普通卷积的过程分成两路，先通过上面一路学习得到offset；下面的可变形卷积是基于上面生成的offset，我们的卷积窗口将由规整的绿色窗口变成蓝色部分，然后再执行普通的卷积，这种实现方式相当于于比正常的卷积操作多学习了卷积核的偏移offset如下图所示，左边普通卷积方法没有提取到完整绵羊的特征，而右边的可变形卷积方法提取到了完整的绵羊的特征实现DCN中的两个问题QAQA1: 如何将可变形卷积变成单独的一个层，而不影响别的层在实际操作时，并不是真正地把卷积核进行扩展，而是对卷积前图片的像素重新整合，变相地实现卷积核的扩张。也就是说，实际上变的是每次进行卷积后得到的带偏移值的坐标值，根据这些坐标取像素点，然后双线性差值，得到新feature map，然后作为输出并成为下一层的新输入QA2: 在前向传播实现可变性卷积中，如何能有效地进行反向传播在图片像素整合时，需要对像素进行偏移操作，偏移量的生成会产生浮点数类型，而偏移量又必须转换为整形，直接对偏移量取整的话无法进行反向传播，这时采用双线性差值的方式来得到对应的像素可变形卷积的使用程度取决于具体的应用场景和任务要求目标检测：可变形卷积在目标检测任务中被广泛使用。由于目标在图像中可能存在尺度变化、形变等非刚性变化，传统的固定感受野的卷积核难以准确捕捉目标的细节和形状信息。可变形卷积通过引入可学习的偏移参数，能够自适应地调整感受野，从而提高目标检测的精度和鲁棒性语义分割：对于语义分割任务，可变形卷积也被广泛使用。语义分割需要对图像中的每个像素进行分类，因此准确地捕捉像素周围的上下文信息至关重要。可变形卷积能够通过自适应调整采样位置来更好地捕捉非刚性形变的目标边界和细节，提高语义分割的精度和细节保留能力人体姿态估计：在人体姿态估计任务中，可变形卷积也被广泛应用。人体姿态具有复杂的非刚性形变，传统的卷积操作往往无法准确地捕捉到人体关节的位置和姿态信息。通过引入可变形卷积，可以更好地建模人体的非刚性形变，提高姿态估计的准确性和鲁棒性需要注意的是，可变形卷积的计算量较大，可能会增加模型的复杂性和训练的难度因此，它通常在需要对非刚性形变建模的任务中使用，并且在设计模型时需要根据具体情况进行权衡和选择池化操作卷积神经网络池化方法综述那些鬼斧神工的池化操作，看完我炸裂！池化的作用抑制噪声，降低信息冗余提升模型的尺度不变性、旋转不变性降低模型计算量防止过拟合池化回传梯度的原则是保证传递的loss(或者说梯度)总和不变最大池化: 取每个块的最大值作为下一层的一个元素值，因此下一个元素的Loss只来源于这个最大值，因此梯度更新也只更新这个最大值，其他值梯度为0平均池化: 将输入区域内的梯度均匀地分配给该区域中的每个位置。反向传播过程中，将梯度均匀分配给输入区域内的所有位置池化输出大小计算公式最大最小池化定义最大池化(Max Pooling): 选择输入区域中的最大值作为输出，忽略其他值最小池化(Min Pooling): 选择输入区域中的最小值作为输出，忽略其他值下图是最大池化的示例图重叠池化和非重叠池化池化又分为重叠池化和非重叠池化非重叠池化: stride=kernel size的情况重叠池化: stride&lt;kernel size的情况重叠池化相比于非重叠池化不仅可以提升预测精度，同时在一定程度上可以缓解过拟合平均池化平均池化(Average Pooling): 计算输入区域内的平均值作为输出，将输入区域的值平均分配最大池化和均值池化的弊端如下所示随机池化Stochastic Pooling for Regularization of Deep Convolutional Neural Networks 2013Stochastic pooling是论文《Stochastic Pooling for Regularization of Deep Convolutional Neural Networks》中提到的一种池化策略，大意是只需对特征区域元素按照其概率值大小随机选择，元素值大的被选中的概率也大下表是随机池化在CIFAR-10上的表现，可以看出，使用随机池化效果和采用dropout的结果接近，证明了其有一定防止过拟合的作用Lp池化Lp池化(Lp Pooling)是通过调整参数p的值来实现不同的池化方式，当p=1时，为平均池化，当p趋近于无穷大时，为最大池化Lp池化的好处是可以在一定程度上保留更多的信息。平均池化会平均化局部区域的特征，可能丢失一些细节信息；而最大池化只保留局部区域的最大值，可能丢失其他重要的信息Lp池化通过调整参数p的值，可以在特征汇聚时平衡信息的丰富性和抗噪性能池化具体操作如下公式所描述当p=1时，使用局部求和，而p为无穷大时，对应max-pooling在Lp池化中，p的选择需要根据具体任务和数据集进行调整较小的p值可以更加注重细节信息，适用于需要保留细粒度特征的任务较大的p值可以更加注重全局信息，适用于更加整体化的任务通常情况下，p取2时能够获得较好的性能组合池化组合池化则是同时利用最大值池化与均值池化两种的优势而引申的一种池化策略，常见组合策略有Cat和Add这两种常常被当做分类任务的一个trick，其作用就是丰富特征层，maxpool更关注重要的局部特征，而average pooling更关注全局特征123456789def add_avgmax_pool2d(x, output_size=1):    x_avg = F.adaptive_avg_pool2d(x, output_size)    x_max = F.adaptive_max_pool2d(x, output_size)    return 0.5 * (x_avg + x_max)def cat_avgmax_pool2d(x, output_size=1):    x_avg = F.adaptive_avg_pool2d(x, output_size)    x_max = F.adaptive_max_pool2d(x, output_size)    return torch.cat([x_avg, x_max], 1)SPP池化Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition 2015SPP(空间金字塔池化)是在SPPNet中提出的，SPPNet提出比较早，在RCNN之后提出的，用于解决重复卷积计算和固定输出的两个问题，具体方法如下图所示空间金字塔池化的基本思想是将输入特征图分割成多个不同大小的子区域，并对每个子区域进行池化操作。这些子区域的大小和数量构成了一个金字塔形状的层次结构然后，对每个子区域进行池化操作（通常是最大池化），生成固定大小的特征向量。最后，将所有尺度上得到的特征向量拼接在一起，形成最终的特征表示通过空间金字塔池化，模型能够在不同尺度上捕捉到更加丰富的局部特征，从而增强了模型对尺度变化和物体大小变化的鲁棒性这对于处理不同尺度的物体检测、图像分类和语义分割等任务非常有用ROI池化ROI池化(Region of Interest Pooling)是在目标检测任务中广泛使用的操作。它对于来自输入列表的每个感兴趣区域，它采用与其对应的输入特征图的一部分并将其缩放到某个预定义的大小。这可以显着加快训练和测试时间，它允许重新使用卷积网络中的特征映射，同时也允许以端到端的方式训练物体检测系统ROI Pooling与SPP池化的区别通过上面的介绍，可以看到两者起到的作用是相同的，把不同尺寸的特征输入转化为相同尺寸的特征输出SPP针对同一个输入使用了多个不同尺寸的池化操作，把不同尺度的结果拼接作为输出；而ROI Pooling可看作单尺度的SPP，对于一个输入只进行一次池化操作反池化[CNN] 卷积、反卷积、池化、反池化反池化(Unpooling)与池化操作相反，用于恢复特征图的尺寸，常见的方法包括最大反池化和平均反池化在传统的池化操作中，通过将特征图的大小减小，以实现特征的降维和减少计算量。然而，这也导致了信息的损失和空间分辨率的降低为了补偿这种信息损失，反池化和Unpooling操作用于还原特征图的空间分辨率，使其与输入特征图具有相同的尺寸反池化通常是通过转置卷积(Transpose Convolution)来实现的。转置卷积通过在特征图之间插入空白像素或零填充，并使用适当的卷积核对这些空白像素进行卷积运算，以恢复特征图的尺寸Unpooling则是在反池化操作中与转置卷积结合使用的一种方式。在池化操作中，通常会记录池化操作时的最大值或平均值的位置。在Unpooling中，根据这些位置信息，将反池化操作的输出值放置回原始特征图的对应位置。这样可以在恢复空间分辨率的同时保留一定的位置信息反池化和Unpooling在卷积神经网络中起着重要的作用，特别是在语义分割和图像重建等任务中。它们帮助网络恢复输入图像的详细结构和空间信息，从而提高模型的精度和质量其他池化LIP池化: LIP: Local Importance-based Pooling自适应池化(Adaptive Pooling): 根据输入的尺寸自动调整池化窗口的大小，适应不同大小的输入归一化BatchNormalization、LayerNormalization、InstanceNorm、GroupNorm简介神经网络中有各种归一化算法，从公式看它们都差不多：无非是减去均值，除以标准差，再施以线性映射归一化算法的主要区别在于操作的feature map维度不同下图来自何凯明的Group Normalization 2018可以看出BN: 针对整个batch不同通道，具体来说，就是把第1个样本的第1个通道，加上第2个样本第1个通道…加上第N个样本第1个通道，求平均，得到通道1的均值LN：针对每个样本，对每个样本的C、H、W维度上的数据求均值和标准差，保留N维度IN：针对每个样本下的每个通道，对每个样本的H、W维度的数据求均值和标准差，保留N 、C维度，也就是说，它只在channel内部求均值和标准差GN：针对每个样本下的多个通道，计算均值和标准差时，把每一个样本feature map的channel分成G组，每组将有C/G个channel，然后将这些channel中的元素求均值和标准差。各组channel用其对应的归一化参数独立地归一化批归一化背景机器学习领域有个很重要的假设：IID 独立同分布假设，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障在把数据喂给机器学习模型之前，白化(whitening)是一个重要的数据预处理步骤独立: 去除特征之间的相关性同分布: 使得所有特征具有相同的均值和方差每批训练数据的分布各不相同，那么网络需要在每次迭代中去学习适应不同的分布，这样将会大大降低网络的训练速度。对于深度网络的训练是一个非常复杂的过程，只要网络的前面几层发生微小的改变，那么这些微小的改变在后面的层就会被累积放大下去一旦网络某一层的输入数据的分布发生改变，那么这一层网络就需要去适应学习这个新的数据分布，所以如果训练过程中，训练数据的分布一直在发生变化，那么将会影响网络的训练速度一文搞懂BN的原理及其实现过程（Batch Normalization）深入理解NLP中LayerNorm的原理以及LN的代码详解引入批归一化Batch Normalization的原因：在图像预处理过程中通常会对图像进行标准化处理，也就是image normalization，使得每张输入图片的数据分布能够统均值为，方差为的分布。这样能够加速网络的收敛。但是当一张图片输入到神经网络经过卷积计算之后，这个分布就不会满足刚才经过image normalization操作之后的分布了，可能适应了新的数据分布规律，这个时候将数据接入激活函数中，很可能一些新的数据会落入激活函数的饱和区，导致神经网络训练的梯度消失这个时候我们引入Batch Normalization的目的就是使我们卷积以后的feature map满足均值为0，方差为1的分布规律，再接入激活函数就不会发生这样的情况数据归一化都是在数据输入时做的，但是实际上在任何位置都是可以进行数据归一化，在神经网络里上一层网络的输出正好就是下一层网络的输入优点加速收敛：归一化操作可以减少网络中不稳定的因素，有助于梯度的传播和收敛。它使得网络在训练过程中更容易优化，加速了训练的速度减少梯度弥散问题：神经网络在深层时容易出现梯度消失或梯度爆炸的问题。Batch Normalization通过将数据归一化到一个合适的范围，使得激活函数的输入更加稳定，减少了梯度消失的风险提高泛化能力：Batch Normalization作为一种正则化方法，有助于减少过拟合的风险。通过在训练过程中引入一些噪声，它可以提高模型的泛化能力，并使得网络对输入数据的变化更加鲁棒Batch Normalization在每个小批量训练样本中对输入进行标准化，使其具有零均值和单位方差图像下的BN示例下图展示了一个batch size为2(两张图片，每张图片有3个通道，其中颜色红，绿，蓝分别代表r,g,b通道)的Batch Normalization的原理，首先会统计每个通道数目所有点的像素值，求得均值和方差，然后在每个通道上分别用该点的像素值减均值除方差得到该点的像素值，此过程就是BN。最后将其接入到激活函数中如下图所示，其中红，绿，蓝分别代表图像不同的通道，假设假设feature map1、feature map2分别是由image1、image2经过一系列卷积池化后得到的特征矩阵其中每个网格的值代表该点的像素值，分别统计feature map1和feature map2每个通道的像素值，得到一个矩阵，在使用BN的计算公式计算经过BN以后每个通道每个像素点的像素值定义为以下矩阵可以计算得到和即得到均值和方差为和这有助于解决训练过程中的内部协变量偏移(Internal Covariate Shift)问题，即网络在训练过程中的每一层的输入分布发生变化，导致网络的训练变得困难代码示例123456789101112131415161718192021222324252627import torch.nn as nnimport torchdata = [[[1, 2, 5], [2, 5, 8.5], [3, 3, 3]],        [[2, 8, 4], [1, 3, 9], [2, 6, 4]],        [[1, 1, 1], [1, 3, 5], [0.5, 6, 0.2]]]data = torch.tensor(data)data_bn = nn.BatchNorm1d(3)(data)data_ln = nn.LayerNorm(3)(data)mean = torch.sum(data_bn)mu = torch.sum(th.pow(data_bn - mean, 2) / 27)print(data_bn)print(mean)print(mu)&gt;&gt;&gt;tensor([[[-0.7734, -0.3384,  0.9667],         [-0.7714,  0.2967,  1.5428],         [-0.0400, -0.0400, -0.0400]],        [[-0.3384,  2.2718,  0.5317],         [-1.1274, -0.4154,  1.7208],         [-0.5542,  1.5027,  0.4742]],        [[-0.7734, -0.7734, -0.7734],         [-1.1274, -0.4154,  0.2967],         [-1.3256,  1.5027, -1.4798]]], grad_fn=&lt;NativeBatchNormBackward0&gt;)tensor(-5.9605e-07, grad_fn=&lt;SumBackward0&gt;)  # 约等于0tensor(1.0000, grad_fn=&lt;SumBackward0&gt;)层归一化深度学习基础之BatchNorm和LayerNorm为什么Transformer使用LayerNorm ，而不使用BatchNormBN的特点是强行拉平数据之间的分布，使得模型收敛速度更快，并且起到了正则化的作用，使模型效果更佳。但是，BatchNorm对Batch Size大小很敏感，并且在LSTM网络上效果极差。LayerNorm是横向归一化，不受Batch Size大小的影响，并且可以很好地应用在时序数据中，而且不需要额外的储存空间。《Rethinking Batch Normalization in Transformers》一文对比了LayerNorm和BatchNorm对于Transformer的作用，并且提出了一种新的归一化方式批归一化和层归一化比较适用场景batch norm: 适用于CV，因为计算机视觉喂入的数据都是像素点，可以说数据点与点之间是可以比较的，所以使用batch norm可以有比较好的效果，layer norm: NLP里，每个词的词向量是一组向量表示一个词，一个词向量割裂开来看是没有意义的，因此不同词向量里的数据点是不能混为一谈的所以batch norm之后可能会使得词损失语义，效果就可能不好了，但是使用layer norm只是让各个词向量进行标准化，就能够有比较理想的效果了归一化维度batch norm: 以CV为例，归一化的是相同通道，均值和方差计算是每个通道的值layer norm: 以NLP为例，归一化的每个词向量，一个词向量自己做归一化实例归一化等待…组归一化12345678910def GroupNorm(x, gamma, beta, G, eps=1e-5):    # x: input features with shape [N,C,H,W]    # gamma, beta: scale and offset, with shape [1,C,1,1]    # G: number of groups for GN    N, C, H, W = x.shape    x = tf.reshape(x, [N, G, C // G, H, W])    mean, var = tf.nn.moments(x, [2, 3, 4], keepdims=True)    x = (x - mean) / tf.sqrt(var + eps)    x = tf.reshape(x, [N, C, H, W])    return x * gamma + betaEmbedding层等待…Dropout层等待…指标等待…</content>
	</entry>
	<entry>
		<title>图像分类算法</title>
		<url>/article/451.html</url>
		<content>图像分类计算机视觉中图像分类任务脉络梳理An Analysis of Deep Neural Network Models for Practical Applications经典模型综述模型综述LeNet-5: 早期卷积神经网络中最有代表性的架构，是Yann LeCun在1998年设计的，用于手写数字识别的卷积神经网络AlexNet: 2012年ILSVRC冠军，6千万参数。由于准确率远超传统方法的第二名（top5错误率为15.3%，第二名为26.2%），引起了很大的轰动。自此之后，CNN成为在图像识别分类的核心算法模型，带来了深度学习的大爆发ZF-Net: 2013年ILSVRC冠军，结构和AlexNet区别不大，分类效果也差不多。这篇文章的贡献在于，提出了一种CNN特征可视化方法：反池化、反激活、反卷积，从而成为CNN特征可视化的开山之作GoogLeNet: 2014年ILSVRC冠军网络。同样也是5+3的模式（以池化层为界），参数量约为5百万，核心模块是Inception Module。Inception历经了V1、V2、V3、V4等多个版本的发展，不断趋于完善Inception V1：主要提出了多分支(多分辨率的filter组合)的网络Inception V2： 主要提出了BN层，提高网络性能(减少梯度消失和爆炸、防止过拟合、代替dropout层、使初始化学习参数更大)Inception V3：主要提出了分解卷积，把大卷积因式分解成小卷积和非对称卷积VGG: 2014年ILSVRC亚军网络，1.38亿参数。由于网络结构十分简单，很适合迁移学习ResNet: 2015年ILSVRC冠军网络。核心是带短连接的残差模块，其中主路径有两层卷积核（Res34），短连接把模块的输入信息直接和经过两次卷积之后的信息融合，相当于加了一个恒等变换。短连接是深度学习又一重要思想，除计算机视觉外，短连接思想也被用到了机器翻译、语音识别/合成领域ResNeXt: ResNet的另一改进。主要是采用了VGG堆叠思想和Inception的split-transform-merge思想，在不增加参数复杂度的前提下提高准确率。ResNeXt发现，增加分支数是比加深或加宽更有效地提升网络性能的方式DenseNet: CVPR2017的oral。主要思想是将每一层都与后面的层连接起来，如果一个网络中有L层，那么会有L(L+1)/2个连接。通过这样的密集连接，每一层在正向时候都能直接接受原始输入信号，在反向时候也都能直接接受损失函数的梯度，即这种连接方式使得特征和梯度的传递更加有效，网络也就更加容易训练当然，如果全部采用这种密集连接的方式，特征图的厚度就会很大。于是采用两种方式降低参数量：一是将密集连接的层做成一个模块，整个网络采用模块堆叠的方式，而不是所有层全部密集连接；二是在dense block中引入bottleneck layer，即卷积3x3前增加1x1卷积，以此来减少feature map数量缺点是太吃显存。通常占用显存的主要是推断过程中产生的feature map和参数量。有些框架会有优化，自动把比较靠前的层的feature map释放掉，所以显存就会减少，或者inplace操作通过重新计算的方法减少一部分显存，但是densenet因为需要重复利用比较靠前的feature map，所以无法释放，导致显存占用过大SENet: 2017年ILSVRC冠军网络。是一个模块，可以和其他的网络架构结合，比如GoogLeNet、ResNet等历史脉络19982012201320142014LeNet-5AlexNetZF-NetGoogLeNetV1、V2、V3、V4VGG2015201620172017ResNetResNeXtDenseNetSENet20202021202220232024VitDeiT、ClipTOnICS分类数据集ImageNetImageNet是计算机视觉领域常用的数据集之一。在 图像分类、目标分割和 目标检测中有着无法撼动的地位ImageNet最初是由李飞飞等人在CVPR 2009年发表的论文——「ImageNet: A Large-Scale Hierarchical Image Database」中发布的多年来，ImageNet 的相关论文对业内有极大的影响ImageNet本身则是一个海量的带标注图像数据集。通过众包等方式进行标注，从2007年开始直到2009年完成。ImageNet有超过1500万张图片，仅汽车图像的数量达到了70万张，类别数量为2567个。如此巨量、 标注错误极低且免费的数据集，已经成为图像处理领域研究者首先接触的数据集之一毫不夸张的说，ImageNet是图像处理算法的试金石。从2010年起，每年ImageNet官方会举办挑战赛。2017年后的比赛由Kaggle社区主持。自2012年Hinton等的团队提出AlexNet开始，每年都有层出不穷的模型希望在ImageNet排行榜上取得一席之地torchvisionModels and pre-trained weights — Torchvision main documentation (pytorch.org)Datasetstorchvision是PyTorch库中的一个子模块，专门用于处理计算机视觉任务。它提供了许多有用的函数、工具和预训练模型，使得处理图像和视频数据变得更加简单和高效torchvision的功能主要分为以下几个方面：数据集和数据加载：torchvision提供了常见的计算机视觉数据集，如MNIST、CIFAR10、ImageNet等。它还提供了方便的数据加载函数和数据转换工具，使得加载和预处理数据变得简单。可以使用这些功能来准备训练数据集、验证数据集和测试数据集数据转换：torchvision包含了各种常用的数据转换操作，例如图像缩放、裁剪、旋转、翻转、标准化等。这些转换操作可以方便地应用于数据集，以增强数据的多样性和适应模型的需求模型和预训练模型：torchvision提供了一些经典的计算机视觉模型，如AlexNet、VGG、ResNet、Inception等。这些模型都在大规模图像数据集上进行了预训练，可以用于图像分类、目标检测、语义分割等任务。此外，torchvision还提供了加载和使用这些预训练模型的便捷接口图像工具：torchvision还包含了一些图像处理工具，如绘制边界框、绘制图像网格、绘制类别标签等。这些工具可以用于可视化和调试模型的输出结果总之，torchvision是一个功能强大的PyTorch模块，提供了许多处理计算机视觉任务所需的工具和功能。它简化了数据加载、数据转换、模型加载和预测等操作，为计算机视觉研究人员和开发者提供了便利LenetGradient-Based Learning Applied to Document Recognition 1998LeNet-5, convolutional neural networks手写字体识别模型LeNet5诞生于1994年，是最早的卷积神经网络之一。LeNet5通过巧妙的设计，利用卷积、参数共享、池化等操作提取特征，避免了大量的计算成本，最后再使用全连接神经网络进行分类识别，这个网络也是最近大量神经网络架构的起点LeNet是由Yann Lecun(2018年图灵奖得主，CNN的缔造者)创造的CNN经典网络，是卷积神经网络史上的开篇之作代码1234567891011121314151617181920212223242526272829303132333435363738394041424344import torchimport torch.nn as nnimport torch.nn.functional as Fclass LeNet5(nn.Module):    def __init__(self, num_classes, grayscale=False):         &quot;&quot;&quot;        num_classes: 分类的数量        grayscale：是否为灰度图        &quot;&quot;&quot;        super(LeNet5, self).__init__()        self.grayscale = grayscale        self.num_classes = num_classes        if self.grayscale: # 可以适用单通道和三通道的图像            in_channels = 1        else:            in_channels = 3        # 卷积神经网络        self.features = nn.Sequential(            nn.Conv2d(in_channels, 6, kernel_size=5),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2),            nn.Conv2d(6, 16, kernel_size=5),            nn.ReLU(),            nn.MaxPool2d(kernel_size=2)   # 原始的模型使用的是 平均池化        )        # 分类器        self.classifier = nn.Sequential(            nn.Linear(16*5*5, 120),  # 这里把第三个卷积当作是全连接层了            nn.ReLU(),            nn.Linear(120, 84),             nn.ReLU(),            nn.Linear(84, num_classes)        )    def forward(self, x):        x = self.features(x) # 输出 16*5*5 特征图        x = torch.flatten(x, 1) # 展平 （1， 16*5*5）        logits = self.classifier(x) # 输出 10        probas = F.softmax(logits, dim=1)        return logits, probasAlexNetAlexNet与LeNet区别：层数更多: 相对较小的LeNet相比，AlexNet包含8层变换，其中有5层卷积和2层全连接隐藏层，以及1个全连接输出层激活函数: AlexNet将sigmoid激活函数改成了更加简单的ReLU激活函数dropout: AlexNet通过dropout来控制全连接层的模型复杂度。而LeNet并没有使用丢弃法数据增强: AlexNet引入了大量的图像增广，如翻转、裁剪和颜色变化，从而进一步扩大数据集来缓解过拟合最大池化: 用MaxPooling而不是AvgPooling模型结构比较VggVery Deep Convolutional Networks For Large-Scale Image RecognitionVGG — Torchvision main documentation (pytorch.org)概述快速理解VGG网络VGG(Visual Geometry Group)是一种经典的卷积神经网络架构，是牛津大学计算机视觉组(Visual Geometry Group)和谷歌DeepMind一起研究出来的深度卷积神经网络。其在在2014年的ImageNet大规模视觉识别挑战(ILSVRC-2014)中获得了亚军，其主要贡献是通过增加网络的深度来提高准确率，当年获得冠军的是GoogLeNet虽然其屈居亚军，但是由于其规律的设计、简洁可堆叠的卷积块，且在其他数据集上都有着很好的表现，从而被人们广泛使用，从这点上还是超过了GoogLenetVGG16相比AlexNet的一个改进是采用连续的几个的卷积核代替AlexNet中的较大卷积核(，，)VGG网络的核心思想使用多个连续的卷积层来替代较大感受野的卷积层，这种设计的优势可以增加网络的深度，使网络能够更好地捕捉图像的细节和复杂特征对于给定的感受野(与输出有关的输入图片的局部大小)，采用堆积的小卷积核是优于采用大的卷积核参数更少: 比如，3个步长为1的卷积核的一层层叠加作用可看成一个大小为7的感受野(其实就表示3个连续卷积相当于一个卷积)其参数总量为，如果直接使用卷积核，其参数总量为，这里指的是输入和输出的通道数很明显减少了参数，而且3x3卷积核有利于更好地保持图像性质上图就是用两个卷积级联(叠加)起来代替一个卷积，同理可以用三个卷积级联(叠加)起来代替一个卷积简洁一致VGG网络的一个重要特点是其简洁而一致的结构。它使用了小尺寸的卷积核()，并且在每个卷积层块中都使用了相同数量的卷积层和池化层，这种设计使得网络的结构非常规整，方便理解和实现VGG网络的架构可以根据深度的不同进行分类，最常见的是VGG16和VGG19。VGG16包含16个卷积层和3个全连接层，而VGG19则包含19个卷积层和3个全连接层。这些网络在ImageNet图像分类任务上取得了很好的性能，成为了后续卷积神经网络设计的重要参考尽管VGG网络已经被更先进的网络架构所取代，但其简洁而一致的结构以及良好的性能使其仍然被广泛应用于图像分类、特征提取和迁移学习等任务。同时，VGG网络也为后续深度学习研究提供了重要的启示，尤其是关于网络深度和卷积核尺寸对性能的影响模型整体结构VGGNet以下6种不同结构，我们以通常所说的VGG-16(即下图D列)为例，展示其结构示意图官方给出的VGG系列神经网络的参数量如下：NetworkA, A-LRNBCDE参数量(in millions)133133134138144对于VGG16来讲，它的网络结结构图就如下所示vgg-block块由n个相同结构的卷积层+1个的池化层构成，意味着输入和输出的尺寸一样，且卷积层可以堆叠复用对于Vgg-16，整个网络有5个vgg-block块和5个maxpool层逐个相连，然后进入FC层，直到最后1000路softmax输出来计算一下VGG16的参数量layershapefilter参数数量(带bias)2-block4-block6-block8-block10-block12-Dense13-Dense14-Dense总数VGG的特点vgg-block内的卷积层都是同结构的池化层都得上一层的卷积层特征缩减一半深度较深，参数量够大较小的filter size/kernel size数据增强方面VGG网络中，数据增强使用的是Multi-Scale这里的Multi-Scale主要是将图像放大到随机的大小，然后再裁剪到224*224的图像核心代码-经典卷积神经网络——VGG1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283import torchfrom torch import nnfrom torchvision import transformsimport torchvisionfrom torch.utils import datafrom d2l import torch as d2limport numpy as npimport matplotlib.pyplot as plt# 设计VGG块，多个卷积过后一个最大池化层# 卷积过后的输入输出图片大小不变，通道有变化# 经过最大池化后，宽高缩减一半def vgg_block(num_convs, in_channels, out_channels):    layers = []    for _ in range(num_convs):        layers.append(nn.Conv2d(in_channels,out_channels,kernel_size=3,padding=1))        layers.append(nn.ReLU())        in_channels = out_channels    layers.append(nn.MaxPool2d(kernel_size=2,stride=2))    return nn.Sequential(*layers)# VGG架构，VGG块(卷积层，outtput),经过五层VGG块过后，宽高为（7，7）# 这个架构可以称为VGG-11,1+1+2*3+1+1+1 = 11conv_arch = ((1, 64), (1, 128), (2, 256), (2, 512), (2, 512))def vgg(conv_arch):    conv_blocks = []    in_channels = 1    for (num_convs, out_channels) in conv_arch:        conv_blocks.append(vgg_block(num_convs,in_channels,out_channels))        in_channels = out_channels    return nn.Sequential(        *conv_blocks,        nn.Flatten(),        # 全连接层部分        nn.Linear(out_channels * 7 * 7, 4096), nn.ReLU(), nn.Dropout(0.5),        nn.Linear(4096, 4096), nn.ReLU(), nn.Dropout(0.5),        nn.Linear(4096, 10)        )net = vgg(conv_arch)# 观察每个层的输出情况x = torch.randn(1,1,224,224)for layer in net:    x = layer(x)    print(layer.__class__.__name__,&quot;output shape:&quot;,x.shape)small_conv_arch = ((1, 16), (1, 32), (2, 32), (2, 64), (2, 64))net = vgg(small_conv_arch)# 现在使用mnist数据集测试一下结果def load_data_fashion_mnist(batch_size, resize=None):    &quot;&quot;&quot;下载或者加载Fashion-MNIST数据集&quot;&quot;&quot;    trans = [transforms.ToTensor()]    if resize:        # 需要把图片拉长,正常时不会这么做的        trans.insert(0, transforms.Resize(resize))    trans = transforms.Compose(trans) # 这是一步可以去掉的操作，这个就是把多个图像处理的步骤整合到一起    mnist_train = torchvision.datasets.FashionMNIST(        root=&quot;../data/&quot;,        train=True,        transform=trans,        download=False # 要是没下载过就选择true    )    mnist_test = torchvision.datasets.FashionMNIST(        root=&quot;../data/&quot;,        train=False,        transform=trans,        download=False # 要是没下载过就选择true    )    return (data.DataLoader(mnist_train,batch_size=batch_size,shuffle=True,num_workers=0),            data.DataLoader(mnist_test,batch_size=batch_size,shuffle=True,num_workers=0))    batch_size = 64learning_rate = 0.01epochs = 10train_iter,test_iter = load_data_fashion_mnist(batch_size,resize=(224))d2l.train_ch6(net,train_iter,test_iter,epochs,lr=learning_rate,device=d2l.try_gpu())    缺点尽管VGG在深度学习中具有重要的地位和影响力，但它也存在一些缺点，包括：大量参数：VGG网络具有很深的结构，其中包含多个卷积层和全连接层。这导致了网络中的参数数量很大，需要更多的计算资源和存储空间。在训练和推理过程中，这会增加计算的复杂性和时间成本计算资源要求高：由于VGG网络的深度和参数量较大，需要较高的计算资源来进行训练和推理。这对于一些资源受限的环境来说可能是一个挑战，特别是在移动设备或嵌入式系统上应用VGG网络时过度拟合：由于VGG网络的深度和参数量较大，它对于较小的数据集容易发生过拟合的情况。在应用VGG网络时，如果训练数据不够丰富，模型可能会过度依赖于训练集的特点，导致在新数据上的泛化能力下降缺乏空间信息利用：VGG网络仅使用了池化层来减小特征图的尺寸，但在减小尺寸的同时丢失了一部分空间信息。相比于一些具有跳跃连接或注意力机制的网络，VGG在利用图像中的空间关系方面相对较弱较高的内存需求：由于VGG网络中的卷积层和全连接层较多，其生成的特征图较大，需要较大的内存来存储中间结果。这可能会限制VGG网络在一些内存受限的设备或平台上的应用GoogleNetRethinking the Inception Architecture for Computer Vision 2015Inception-V3论文翻译——中英文对照GoogLeNet网络系列解读GoogLeNet是由Google团队在2014年提出的一种深度卷积神经网络架构，也被称为Inception网络。相比于传统的卷积神经网络，GoogLeNet采用了一种特殊的模块化设计，旨在提高网络的计算效率和表达能力GooLeNet深度只有22层，但大小却比AlexNet和VGG小很多，GooLeNet的参数为500万个，AlexNet参数个数是GooLeNet的12倍，VGGNet参数又是AlexNet的3倍InceptionV1 如何提升网络性能一般提升网络性能最直接的方法是增加网络深度和宽度，深度指网络层数，宽度指神经元数量，但是会存在一些问题：参数太多，如果训练数据集有限，很容易产生过拟合网络越大，参数越多，则计算复杂度越大，难以应用网络越深，容易出现梯度弥散问题(梯度越往后越容易消失)，难以优化模型有一种解决方式是增加网络的深度和宽度的同时减少参数，为了减少参数一种方式是将全连接变成稀疏连接(Dropout)但实际上稀疏连接的计算性能并不会有质的提升。这是因为大部分硬件是针对密集矩阵计算优化的GooLeNet提出了一种Inception网络结构，构造一种“基础神经元”结构，来搭建一个稀疏性，高计算性能的网络结构。既能保持网络结构的稀疏性，又能利用密集矩阵的高计算性能Inception模块GoogLeNet的核心是Inception模块，这是一种多尺度特征提取模块。它通过并行地使用不同大小的卷积核和池化操作来捕捉图像中不同尺度的特征。这样的设计可以在保持计算效率的同时，增加网络对不同尺度信息的感知能力另一个值得注意的特点是GoogLeNet中采用了卷积核的卷积层，称为瓶颈层。这些卷积层主要用于降低输入通道的维度，减少网络的参数量和计算复杂度。同时，它们还能够引入非线性变换，提高网络的表达能力GoogLeNet还采用了全局平均池化层，将最后一个卷积层的特征图进行平均池化，得到全局的特征表示。这样可以显著减少全连接层的参数量，提高模型的泛化能力，并且降低过拟合的风险Inception网络和VGG网络VGG网络注重增加网络的深度来提取更复杂的特征，而Inception网络则通过并行的卷积分支来捕捉多尺度的特征信息。因此，Inception网络相对于VGG网络来说具有更高的计算效率和参数效率Inception-v1Going Deeper with Convolutions 2014 Inception-v1Inception Module是GoogLeNet的核心组成单元，结构如下图Inception Module基本组成结构有四个成分。卷积，卷积，卷积，最大池化，最后对四个成分运算结果进行通道上组合这就是Inception Module的核心思想，通过多个卷积核提取图像不同尺度的信息，最后进行融合，可以得到图像更好的表征辅助分类器(期望缓解梯度消失问题)完整的结构可以看原论文，或者是这个链接为了避免梯度消失，网络额外增加2个辅助的softmax用于向前传导梯度(辅助分类器)，辅助分类器是将中间某一层的输出用作分类，并按一个较小的权重(0.3)加到最终分类结果中，这样就相当于做了模型融合，同时给网络增加了反向传播的梯度信号，也提供了额外的正则化，对于整个网络的训练很有裨益，实际测试时会去掉这两个额外的softmaxInception-v2Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift 2015 Inception-v2Inception v2在原始的Inception v1的基础上引入了Batch Normalization(批量归一化)技术，这是它的主要贡献。Batch Normalization是一种用于加速神经网络训练和提高网络性能的技术通过使用Batch Normalization，Inception v2实现了以下几个重要的优势：加速训练：Batch Normalization可以使网络更快地收敛，因为它减少了训练过程中的梯度消失和梯度爆炸问题，从而加速了梯度传播和参数更新提高网络的稳定性：Batch Normalization 使得网络对输入数据的变化更加鲁棒，减少了对输入数据分布和大小的敏感性，提高了网络的稳定性减少对超参数的敏感性：Batch Normalization 减少了网络对学习率和权重初始化等超参数的敏感性，使得网络更容易调优和训练正则化效果：Batch Normalization 具有一定的正则化效果，可以减少过拟合问题，提高网络的泛化能力因此，Inception v2的主要贡献在于引入了Batch Normalization技术，使得网络的训练更加稳定和高效，进一步推动了深度学习模型的发展和应用相比较于v1卷积层被替换为两个连续的卷积层. 网络的最大深度增加9个权重层. 参数量增加了大约25%，计算量增加了大约30%使用BN层，将每一层的输出都规范化到一个的正态分布，提高网络收敛速度Inception-v3Rethinking the Inception Architecture for Computer Vision 2015 Inception-v3Inception V3一个最重要的改进是分解(Factorization)，将分解成两个一维的卷积(，)，也是一样(，)，这样的好处，既可以加速计算，又可以将1个卷积拆成2个卷积，使得网络深度进一步增加，增加了网络的非线性(每增加一层都要进行ReLU)，另外，网络输入从变为了在Inception v2的基础上引入了一些重要的改进，其主要贡献如下：辅助分类器：Inception v3在网络的中间层添加了辅助分类器，这些分类器有助于在训练过程中引导梯度流动和提供正则化。辅助分类器位于网络的不同层级，并与主分类器共同进行训练。这些辅助分类器有助于减轻梯度消失问题，提高网络的稳定性和收敛速度更深的网络结构：Inception v3相对于之前的版本增加了更多的网络层，使得网络更深。更深的网络结构有助于提高特征表示的能力，使得模型能够更好地学习复杂的图像特征更多的1x1卷积核：Inception v3进一步增加了网络中的1x1卷积核的数量。1x1卷积核具有降低通道数和维度的作用，它能够减少网络的计算量，并引入了更多的非线性变换，提高了网络的表达能力和特征提取能力分支结构：Inception v3中的Inception模块引入了分支结构，即在不同尺度上使用不同大小的卷积核进行特征提取。这种分支结构有助于捕捉不同尺度的图像特征，并提高了网络对图像的感知能力其他优化措施：Inception v3还引入了其他一些优化措施，如使用更小的卷积核、引入批量归一化等，以进一步提升网络的性能和训练效果Inception-v4与ResNetInception-v4, Inception-ResNet and the Impact of Residual Connections on Learning 2016 Inception-v4Inception-v4与Inception-ResNet结构详解(原创)微软亚洲研究院的何恺明在2015年提出了震惊业界的ResNet结构，这种结构和以往的Inception结构走了两条不同的道路：前者主要关注加大网络深度后的收敛问题而Inception更关注特征维度上的利用如果把这两种方法结合起来会有什么效果呢？Szegedy在2016年就试验了一把，把这两种 最顶尖的结构混合到一起提出了Inception-ResNet，它的收敛速度更快但在错误率上和同层次的Inception相同；Szegedy还对自己以前提出的Inception-v3进行了一番改良，提出了Inception-v4Inception-v4网络结构Inception-v4与Inception-ResNet集成的结构在ImageNet竞赛上达到了3.08%的top5错误率，也算当时的state-of-art performance了Inception-v4网络，对于Inception块的每个网格大小进行了统一Inception V4主要利用残差连接（Residual Connection）来改进V3结构，得到Inception-ResNet-v1，Inception-ResNet-v2，Inception-v4网络Inception-ResNet-v1结构Inception-ResNet-v2结构ResNetDeep residual learning for image recognition 2015深度学习——ResNet超详细讲解，详解层数计算、各层维度计算ResNet(Residual Network)是一种深度残差网络，它是由Kaiming He等人于2015年提出的。ResNet的核心思想是引入了残差连接（Residual Connection），通过跨层直接连接来解决深层网络训练中的梯度消失和模型退化问题传统的深度神经网络在层数增加时会面临梯度消失和梯度爆炸的问题，导致模型难以训练。ResNet通过在网络中添加残差块（Residual Block），允许信息在跳过层的路径上直接传递，使得网络可以更容易地学习恒等映射。具体来说，残差块将输入和输出进行相加，然后通过激活函数进行非线性变换。这样的设计允许网络在需要时将残差信号传递到后续层，解决了梯度消失和模型退化的问题ResNet的一个重要变种是ResNet-50，它由50个卷积层组成，其中包括残差块、池化层和全连接层。ResNet-50在ImageNet图像分类任务上取得了很好的性能，成为当时最先进的模型之一事实上，ResNet并不是第一个利用近路连接、Highway Networks引入门控近路连接的。这些参数化门控制允许多少信息流过近路(shortcut)。类似的想法可以在长短期记忆网络(LSTM)单元中找到，其中存在参数化的忘记门，其控制多少信息将流向下一个时间步。因此，ResNet可以被认为是Highway Networks的一种特殊情况层数越多越好吗在ResNet之前的网络层数都不是很高，14年的VGG网络才只有19层，但是ResNet的网络层数达到了惊人的152层。许多人会有一个直观的印象，也就是网络层数越多，训练效果越好，但是这样的话VGG网络为什么不采取152层而是采用19层呢？其实是因为训练模型的准确度不一定和模型层数呈真相关的关系。因为随着网络层数的加深，网络准确需出现饱和，会出现下降的现象56层的网络比20层网络的训练效果要差，许多人第一反应就是过拟合，但事实并不如此，因为过拟合现象的训练集准确度会很高，但是从图中可以看出56层网络的训练集准确度同样很低。很显然可知的是，随着层度加深，会出现梯度消失或梯度爆炸的问题，使得深度模型很难训练，但是已经存在BatchNorm等手段缓解这一问题，因此如何解决深度网络的退化问题是神经网络发展的下一个方向模型官方给了两个ResNet块的结构图，图一为BasicBlock也就是最常规的块，图二被成为BottleBlockBasicBlock(常规)(两层结构)在ResNet34的时候是用的这个BottleBlock(三层结构)在ResNet50/101/152的时候用的是这个是参考GoogleNet的方式对网络内容进行的一定优化在计算前先接用的卷阶层降维，既保持精度又减少计算量，再对64维进行计算后经过的卷积恢复残差对残差网络的理解为什么残差学习相对更容易，从直观上看残差学习需要学习的内容少，因为残差一般会比较小，学习难度小点。不过我们可以从数学的角度来分析这个问题，首先残差单元可以表示为：其中和分别表示的是第个残差单元的输入和输出，注意每个残差单元一般包含多层结构。是残差函数，表示学习到的残差，而表示恒等映射，是ReLU激活函数。基于上式，我们求得从浅层到深层的学习特征为:利用链式规则，可以求得反向过程的梯度：式子的第一个因子表示的损失函数到达的梯度，小括号中的1表明短路机制可以无损地传播梯度，而另外一项残差梯度则需要经过带有weight的层，梯度不是直接传递过来的。残差梯度不会那么巧全为-1，而且就算其比较小，有1的存在也不会导致梯度消失。所以残差学习会更容易。要注意上面的推导并不是严格的证明如果从ResNet的论文来看，确实ResNet出发点不是梯度消失而是网络退化；但是Kaiming隔年的论文确实有提到，残差结构可以使得反向的梯度总不消失，即便中间权重矩阵很小残差映射更容易学习有个原因是反向传播的时候，分走了一部分梯度，所以同样的误差得到的梯度更小到一定深度的时候，梯度会变成0，但是我们还有上一层的梯度，所以说不会比之前的差DenseNetDensely Connected Convolutional Networks 2018概述作为CVPR2017年的Best Paper，DenseNet脱离了加深网络层数(ResNet)和加宽网络结构(Inception)来提升网络性能的定式思维，从特征的角度考虑，通过特征重用和旁路(Bypass)设置，既大幅度减少了网络的参数量，又在一定程度上缓解了gradient vanishing问题的产生。结合信息流和特征复用的假设，DenseNet当之无愧成为2017年计算机视觉顶会的年度最佳论文DenseNet作为另一种拥有较深层数的卷积神经网络，具有如下优点:相比ResNet拥有更少的参数数量旁路加强了特征的重用网络更易于训练，并具有一定的正则效果缓解了gradient vanishing和model degradation的问题何恺明在提出ResNet时做出了这样的假设：若某一较深的网络多出另一较浅网络的若干层有能力学习到恒等映射，那么这一较深网络训练得到的模型性能一定不会弱于该浅层网络通俗的说就是如果对某一网络中增添一些可以学到恒等映射的层组成新的网路，那么最差的结果也是新网络中的这些层在训练后成为恒等映射而不会影响原网络的性能同样DenseNet在提出时也做过假设：与其多次学习冗余的特征，特征复用是一种更好的特征提取方式模型DenseNet是一种深度神经网络架构，其核心思想是密集连接(Dense Connectivity)。相比于传统的神经网络结构，如VGG和ResNet，DenseNet通过引入密集连接的方式，在网络中每一层都与前面所有层直接相连，从而增强了信息流动和特征重用的能力DenseNet的主要特点如下：密集连接：在DenseNet中，每个层都与前面所有层直接相连。具体而言，某一层的输入包括它之前所有层的输出。这种密集连接的方式使得信息可以在网络中自由地流动，促进了特征的传递和共享混合特征重用：由于密集连接的存在，每个层可以直接访问之前所有层的特征图。这样，低层特征可以直接传递给后续层，实现了混合特征重用。这种特征重用机制有效地利用了网络中的信息，增强了特征的多样性和丰富性基本组件：DenseNet的基本组件是”Dense Block”，它由多个具有相同输出通道数的卷积层组成。在每个Dense Block内部，层与层之间通过密集连接相连。为了控制参数数量和计算量，每个卷积层通常采用较小的3x3卷积过渡层：为了控制网络的宽度，DenseNet在相邻的Dense Block之间引入了过渡层(Transition Layer)。过渡层由一个1x1卷积层和一个2x2的平均池化层组成，它可以减小特征图的尺寸并降低通道数，从而减少计算量DenseNet的优点包括模型参数相对较少、特征重用性强、梯度传播更加顺畅等Dense BlockDenseNet中的核心组件是”Dense Block”，它由多个密集连接的卷积层组成。Dense Block的设计旨在促进特征的传递和重用，增强网络的表示能力具体来说，Dense Block由一系列堆叠在一起的卷积层组成，每个卷积层都直接连接到前面所有层的输出。这意味着某一层的输入是其之前所有层的输出的串联。这种密集连接的方式使得信息可以在网络中自由地流动，从而有效地提高了特征传递和共享的能力为了控制参数数量和计算量，每个卷积层通常采用具有相同输出通道数的卷积。这样，每个卷积层都可以利用之前层的丰富特征来生成更加复杂和抽象的特征表示。这种密集连接的方式不仅增加了特征的多样性，还减轻了梯度消失的问题，使得网络更容易训练在每个Dense Block之间，为了控制网络的宽度和深度，通常会引入过渡层(Transition Layer)。过渡层由一个卷积层和一个的平均池化层组成。卷积层用于降低通道数，减少计算量。平均池化层则用于减小特征图的尺寸，进一步减少参数和计算复杂度模型DenseNet是一种基于密集连接的卷积神经网络(CNN)，其主要特点是在网络中引入了密集连接层，从而改善了信息的流动和梯度的传递。下面是DenseNet的网络结构：1.输入层：输入层接收输入数据，并将其送入第一个卷积层中2.卷积层：DenseNet中的卷积层通常采用的卷积核，并采用padding来保持特征图的大小不变。在每个卷积层后面，都会接上BN层和ReLU激活函数3.密集块(Dense Block)：密集块是DenseNet的核心，它由多个密集连接层组成。每个密集块中，所有前面层的输出都会与当前层的输入进行连接，并通过一个非线性变换进行处理4.过渡层(Transition Block)：为了避免网络过深导致梯度消失和计算资源过度消耗，DenseNet中采用了过渡层来控制网络的大小。在每个密集块之间，都会接上一个过渡层，它包含一个的卷积层、BN层和平均池化层，其中平均池化的步幅为2，用于减少特征图的大小5.全局池化层和全连接层：最后，DenseNet使用全局平均池化层将特征图降维为一个向量，然后通过一个全连接层进行分类最后一个池化用的是全局池化层具体来说，DenseNet的密集连接机制使得前面的层可以直接连接到后面的层，从而保留了更多的特征信息。然而，这种密集连接也导致了特征图的尺寸逐渐增大。为了控制模型的复杂性和计算量，并且能够更好地适应不同尺度的输入，DenseNet引入了全局池化层全局池化层可以将整个特征图转化为固定长度的特征向量，这样可以有效地降低特征的维度，并且保留了全局感受野的特征信息。通过将特征图的每个通道进行平均池化或最大池化操作，全局池化层可以捕捉到整个特征图的统计特征，从而对全局信息进行汇聚使用全局池化层的好处是减少了模型的参数数量和计算量，同时仍然能够保留重要的全局特征。这有助于提高模型的效率和泛化能力，并且在训练和推断阶段都能够更好地适应不同尺度的输入图像ResnextResnet性能最好的变体是ResnextVitVit An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale 2020Vision Transformer详解Vision Transformer (base-sized model)概述ViT(Vision Transformer)是一种基于Transformer的视觉模型，它将自然语言处理中的Transformer模型成功应用于计算机视觉任务传统的计算机视觉模型主要基于卷积神经网络(CNN)，而ViT尝试使用Transformer的自注意力机制来处理图像数据ViT的关键思想将输入图像切分为固定大小的图像块(patches)，并将这些图像块展平为序列形式的输入。每个图像块通过一个线性映射层进行特征嵌入，然后通过添加位置嵌入来引入位置信息之前学习的Transformer结构中，输入需要是一个二维的矩阵，矩阵的形状可以表示为，其中是sequence的长度，而是sequence中每个向量的维度因此，在ViT算法中，首先需要设法将的三维图像转化为的二维输入ViT中的具体实现方式为: 将的图像，变为一个的序列。这个序列可以看作是一系列展平的图像块，也就是将图像切分成小块后，再将其展平。该序列中一共包含了个图像块，每个图像块的维度则是。其中是图像块的大小，是通道数量。经过如上变换，就可以将视为sequence的长度了但是，此时每个图像块的维度是，而我们实际需要的向量维度是，因此我们还需要对图像块进行Embedding。这里Embedding的方式非常简单，只需要对每个的图像块做一个线性变换，将维度压缩为即可模型结构ViT模型通常包含多个Transformer编码器层，每个层由多头自注意力机制和前馈神经网络组成在ViT中，序列中的每个位置都可以进行自注意力计算，使模型能够在全局上对图像进行编码和理解最后，ViT模型将序列的表示通过一个池化操作得到整个图像的表示，然后可以通过一个线性分类器进行分类或进行其他任务优缺点ViT的优点之一是它能够捕捉全局信息，并且在一些计算机视觉任务上取得了很好的表现，例如图像分类、目标检测和图像分割然而，ViT对于大尺寸高分辨率图像的处理相对较慢，且对于空间信息的建模相对较弱，因此在处理具有细粒度结构的图像时可能存在一定的限制ViT通过将图像划分为序列，并利用Transformer的自注意力机制，将自然语言处理中的Transformer模型引入了计算机视觉领域，为图像理解任务提供了一种新的思路和方法Transformer模型是如何在CV领域里用起来的在计算机视觉领域，Transformer模型通常用于处理序列数据和实现一些特定任务，而不是直接应用于图像输入。以下是一些使用Transformer模型的常见方式：图像分类：可以将图像划分为网格单元，并将每个单元的特征表示为序列。然后，将序列输入Transformer模型进行分类任务。这样做的一个例子是Vision Transformer(ViT)模型，它将图像划分为图像块，然后通过Transformer编码器对这些块进行处理目标检测：一种使用Transformer的目标检测方法是将图像划分为一组固定大小的区域，然后对每个区域提取特征，并将这些特征序列输入Transformer模型中进行对象分类和边界框回归。这种方法的一个例子是DETR(Detection Transformer)模型图像生成：Transformer模型也可以用于生成视觉内容，如图像生成、图像描述生成等任务。通过将Transformer模型作为生成器，可以学习生成高质量的图像或图像描述需要注意的是，由于图像数据的高维性和空间结构，直接将Transformer模型应用于整个图像通常不是常见的做法。相比之下，卷积神经网络(CNN)在计算机视觉领域中更为常见，因为它们更适合处理图像数据的局部特征和空间结构。但是，通过将Transformer模型与CNN结合使用，可以利用Transformer模型的序列建模能力和注意力机制来处理图像中的序列或局部特征，从而提高计算机视觉任务的性能Tranformer和CNN比较Tranformer相较于CNN结构，缺少一定的平移不变性和局部感知性，因此在数据量不充分时，很难达到同等的效果，表现为使用中等规模的ImageNet训练的Tranformer会比ResNet在精度上低几个百分点当有大量的训练样本时，结果则会发生改变。使用大规模数据集进行预训练后，再使用迁移学习的方式应用到其他数据集上，可以达到或超越当前的SOTA水平DeiTModelTraining data-efficient image transformers &amp; distillation through attention 2021视觉Transformer经典论文——ViT、DeiT的与原理解读与实现huggingface DeiTModelDeiT：注意力Attention也能蒸馏DeiTModel(Data-efficient Image Transformers Model)和ViT(Vision Transformer)之间存在关系，DeiTModel可以看作是对ViT模型的改进和优化ViT在大数据集 mageNet-21k(14million)或者JFT-300M(300million)上进行训练，Batch Size 128下NVIDIA A100 32G GPU的计算资源加持下预训练ViT-Base/32需要3天时间ViT是一种基于Transformer的图像分类模型，通过将图像拆分成固定大小的图像块，并使用线性嵌入将每个图像块转换为向量序列，然后将序列输入到Transformer编码器中进行处理。ViT模型在图像分类任务中取得了出色的性能，但需要大量的训练数据和计算资源Facebook与索邦大学Matthieu Cord教授合作DeiTModel，DeiT模型(8600万参数)仅用一台GPU服务器在53 hours train，20 hours finetune，仅使用ImageNet就达到了 84.2 top-1准确性，而无需使用任何外部数据进行训练。性能与最先进的卷积神经网络(CNN)可以抗衡较于Vit的改进点DeiTModel则是在ViT的基础上进行了改进，旨在提高数据效率训练策略: 高低精度+数据增强更少的数据和计算资源: DeiT模型使用更少的数据和计算资源，仅使用ImageNet数据集进行训练，并在较短的时间内完成训练蒸馏机制: DeiT模型还引入了一种教师-学生策略，通过蒸馏机制使学生模型从教师模型中学习，这种策略有助于提高模型的泛化能力和性能知识蒸馏DeiT：注意力Attention也能蒸馏知识蒸馏使用的是Teacher—Student模型，其中Teacher是知识的输出者，Student是知识的接受者。知识蒸馏的过程分为2个阶段:原始模型训练: 训练Teacher模型, 简称为Net-T，它的特点是模型相对复杂，也可以由多个分别训练的模型集成而成。我们对Teacher模型不作任何关于模型架构、参数量、是否集成方面的限制，唯一的要求就是，对于输入X, 其都能输出Y，其中Y经过softmax的映射，输出值对应相应类别的概率值精简模型训练: 训练Student模型, 简称为Net-S，它是参数量较小、模型结构相对简单的单模型。同样的，对于输入X，其都能输出Y，Y经过softmax映射后同样能输出对应相应类别的概率值将问题限定在分类问题下，或者其他本质上属于分类问题的问题，该类问题的共同点是模型最后会有一个softmax层，其输出值对应了相应类别的概率值。知识蒸馏时，由于已经有了一个泛化能力较强的Net-T，我们在利用Net-T来蒸馏训练Net-S时，可以直接让Net-S去学习Net-T的泛化能力上图是教师学生模型的一般形式，就是有两部分的loss，一个关注真实标签，另一个关注Net-T的输出，训练的模型可以兼顾自身和教师模型的约束，即下面左图比较了改变训练策略和添加蒸馏学习的结果比较，右图是DeiT模型结构左图图中的指标均为在ImageNet数据集上进行训练，且在ImageNet数据集上评估的结果其中Ours(Deit)为使用与ViT完全一致的网络结构，但是改进了训练策略而Ours⚗(DeiT⚗)则是在DeiT的基础上继续使用了蒸馏学习的方式进行改进可以看到，ViT算法在这种中等规模的数据集上，指标远不如CNN网络EfficientNet而通过改变训练策略，使用蒸馏学习，网络结构与ViT基本一致的DeiT性能有了很大的提升，超过了EfficientNet右图DeiT与ViT的主要差异在于引入了一个distillation token，其主要用于网络训练中的蒸馏学习这个distillation token与class token很像，其在self-attention layers中会跟class token以及图像patch不断交互而distillation token与class token唯一区别在于，class token的目标是跟真实的label一致，而distillation token是要跟蒸馏学习中教师网络预测的label一致在最终预测时，网络既会输出class token的结果，也会输出distillation token的结果，论文答案是将两者的softmax结果进行相加，即可简单地得到算法的最终预测结果这里在计算和Net-T的loss时，还可以细分为两种，分别是软蒸馏(soft distillation)和硬蒸馏(hard distillation)软蒸馏: 将学生网络的输出结果与教师网络的softmax输出结果取KL Loss硬蒸馏: 将学生网络的输出结果与教师网络的标签取交叉熵损失Hard Label也可以通过标签平滑技术(Label smoothing)转换成Soft Labe，其中真值对应的标签被认为具有1-esilon的概率，剩余的esilon由剩余的类别共享ViT、Deit这类视觉transformer是如何处理变长序列输入的当增加输入图像的分辨率时，例如DeiT从224到384，一般来说会保持patch size(例如9)，因此patch的数量N会发生了变化由于Transformer结构的原因，内置了position embedding位置编码的差值，一般将位置编码双线性插值到图片分辨率，当N发生变化时，模型的权重不需要做出任何变化也可以以同样的方式计算出Q、K、V的值，所以Visual transformer的模型结构适用于任何长度的sequence。最终输出预测的时候，看样子序列长了好多，但其实还是只取cls token输出作为输出预测ClipLearning Transferable Visual Models From Natural Language Supervision Clip 2021CLIP openai官方源码Openai连接文本和图像CLIP模型(Huggingface版)zero-shot分类代码案例2021年见证了vision transformer的大爆发，随着谷歌提出ViT之后，一大批的vision transformer的工作席卷计算机视觉任务。除了vision transformer，另外一个对计算机视觉影响比较大的工作就是Open AI在2021年1月份发布的DALL-E和CLIP，这两个都属于结合图像和文本的多模态模型DALL-E是基于文本来生成模型的模型CLIP是用文本作为监督信号来训练可迁移的视觉模型这两个工作也像ViT一样带动了一波新的研究高潮概述Openai连接文本和图像CLIP模型(Huggingface版)zero-shot分类代码案例CLIP的英文全称是Contrastive Language-Image Pre-training，即一种基于对比文本-图像对的预训练方法或者模型CLIP是一种基于对比学习的多模态模型，与CV中的一些对比学习方法如moco和simclr不同的是，CLIP的训练数据是文本-图像对：一张图像和它对应的文本描述，这里希望通过对比学习，模型能够学习到文本-图像对的匹配关系如上图(1)所示，CLIP包括两个模型：Text Encoder和Image Encoder，其中Text Encoder用来提取文本的特征，可以采用NLP中常用的text transformer模型；而Image Encoder用来提取图像的特征，可以采用常用CNN模型或者vision transformer对提取的文本特征和图像特征进行对比学习对于一个包含N个文本-图像对的训练batch，将N个文本特征和N个图像特征两两组合，CLIP模型会预测出N方个可能的文本-图像对的相似度，这里的相似度直接计算文本特征和图像特征的余弦相似性(cosine similarity)，即上图所示的矩阵这里共有个正样本，即真正属于一对的文本和图像(矩阵中的对角线元素)，而剩余的个文本-图像对为负样本那么CLIP的训练目标就是最大N个正样本的相似度，同时最小化N方-N个负样本的相似度为了训练CLIP，OpenAI从互联网收集了共4个亿的文本-图像对zero-shot分类与CV中常用的先预训练然后微调不同，CLIP可以直接实现zero-shot的图像分类，即不需要任何训练数据，就能在某个具体下游任务上实现分类，这也是CLIP亮点和强大之处根据任务的分类标签构建每个类别的描述文本：A photo of {label}，然后将这些文本送入Text Encoder得到对应的文本特征将要预测的图像送入Image Encoder得到图像特征，然后与N个文本特征计算缩放的余弦相似度(和训练过程一致)然后选择相似度最大的文本对应的类别作为图像分类预测结果，进一步地，可以将这些相似度看成logits，送入softmax后可以到每个类别的预测概率12345678910111213141516from PIL import Imagefrom transformers import CLIPProcessor,CLIPModelmodel = CLIPModel.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)processor = CLIPProcessor.from_pretrained(&quot;openai/clip-vit-base-patch32&quot;)#这里加入自己图片的地址就行image = Image.open('xxx.jpg')#这里加入类别的标签类别text = ['plane','car','dog','bird']inputs = processor(text=text,images = image,return_tensors=&quot;pt&quot;,padding=True)outputs = model(**inputs)logits_per_image = outputs.logits_per_imageprobs = logits_per_image.softmax(dim=1)for i in range(len(text)):    print(text[i],&quot;:&quot;,probs[0][i])使用CLIP进行zero-shot分类，另外一个比较重要的地方是文本描述的生成，上面的例子我们采用分类标签，但其实也有其它选择比如我们直接用类别标签，这其实属于最近NLP领域比较火的一个研究：prompt learning或者prompt engineering扩展CLIP是基于文本-图像对来做的，但是它可以扩展到文本-视频，比如VideoCLIP就是将CLIP应用在视频领域来实现一些zero-shot视频理解任务VQGAN+CLIP实现各种图像生成模型TOnICSCurriculum Learning for Data-Efficient Vision-Language Alignment TOnICS 2022超越CLIP的多模态模型，只需不到1%的训练数据！南加大最新研究来了 CLIP(Contrastive Language–Image Pre-training)，是一种基于对比的图片-文本学习的跨模态预训练模型，由OpenAI于2021年1月发布它存在一个缺点就是数据需求太大：4亿个图像文本对、256个GPU，这对许多公司和个人都很不友好对此，南加州大学的最新研究发现了一种基于本体的课程学习(Curriculum Learning)算法，只需不到1%的训练数据就能达到CLIP同款效果，甚至在图像检索方面表现更好</content>
	</entry>
	<entry>
		<title>目标检测与跟踪算法</title>
		<url>/article/34192.html</url>
		<content>目标检测经典目标检测Object Detection模型整理深入浅出Yolo系列之Yolov5核心基础知识完整讲解目标检测 YOLO系列算法Object Detection in 20 Years: A Survey论文详解R-CNN史上最全讲解YOLOv1论文翻译解读YOLO v4：物体检测的最佳速度和精度Yolo系列代码概述目标检测——RCNN与YOLO系列核心问题分类问题：即图片(或某个区域)中的图像属于哪个类别定位问题：目标可能出现在图像的任何位置大小问题：目标有各种不同的大小形状问题：目标可能有各种不同的形状算法分类Anchor Based模型Anchor Free模型One-stage模型YoloV2-5系列、SSD、RetinaNetYoloV1、FCOS、CornerNetTwo-stage模型Faster RCNN、Cascade RCNN、MaskRCNNtwo stage：先进行区域生成，该区域称为region proposal（RP，一个有可能包含物体的预选框）；再通过卷积神经网络进行样本分类任务流程：特征提取—生成RP—分类/定位回归常见two stage：R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN、R-FCNone stage：不用RP，直接在网络中提取特征来预测物体的分类和位置任务流程：特征提取—分类/定位回归常见one stage：OverFeat、YOLOv1、YOLOv2、YOLOv3、SSD、RetinaNet目标检测分为两大系列——RCNN系列和YOLO系列：RCNN系列是基于区域检测的代表性算法YOLO是基于区域提取的代表性算法还有著名的SSD是基于前两个系列的改进发展脉络20142015201620172018R-CNNFast R-CNNFaster R-CNNYOLOYOLOSSDYOLOv2RetinaNetMask R-CNNFPNYOLOv3Cascade R-CNN202020222023YOLOv4YOLOv5EfficientDetYOLOv6(美团)YOLOv7YOLOv8指标和数据map指标mAP—目标检测模型的评估指标YOLO 模型的评估指标——IOU、Precision、Recall、F1-score、mAPmAP@0.5在YOLO模型中，你会见到mAP@0.5这样的表现形式，这种形式表示在IOU阈值为0.5的情况下，mAP的值为多少。当预测框与标注框的IOU大于0.5时，就认为这个对象预测正确，在这个前提下再去计算mAP。一般来说，mAP@0.5即为评价YOLO模型的指标之一mAP@[0.5:0.95]YOLO模型中还存在mAP@[0.5:0.95]这样一种表现形式，这形式是多个IOU阈值下的mAP，会在q区间[0.5,0.95]内，以0.05为步长，取10个IOU阈值，分别计算这10个IOU阈值下的mAP，再取平均值。mAP@[0.5:0.95]越大，表示预测框越精准，因为它去取到了更多IOU阈值大的情况数据集准备How to Train YOLOv8 Object Detection on a Custom Dataset深度学习系列之Anchor based 和 Anchor free 目标检测方法RCNN系列候选区域的产生选择性搜索算法 （Selective Search)选择性搜索算法(Selective Search)超详解（通俗易懂版）很多目标检测技术都会涉及候选框(bounding boxes)的生成，物体候选框获取当前主要使用图像分割与区域生长技术。区域生长(合并)主要由于检测图像中存在的物体具有局部区域相似性(颜色、纹理等)滑动窗口法滑动：首先对输入图像进行不同窗口大小的滑窗进行从左往右、从上到下的滑动检测：每次滑动时候对当前窗口执行分类器(分类器是事先训练好的)。如果当前窗口得到较高的分类概率，则认为检测到了物体不同尺度：对每个不同窗口大小的滑窗都进行检测后，会得到不同窗口检测到的物体标记，这些窗口大小会存在重复较高的部分，NMS：采用非极大值抑制(Non-Maximum Suppression, NMS)的方法进行筛选，最终，经过NMS筛选后获得检测到的物体选择性搜索：selective search(简称SS)方法是当下最为熟知的图像bounding boxes提取算法，由Koen E.A于2011年提出只对图像中最有可能包含物体的区域进行搜索以此来提高计算效率，图像中物体可能存在的区域应该是有某些相似性或者连续性区域的分割：对输入图像进行分割算法产生许多小的子区域合并：根据这些子区域之间相似性(相似性标准主要有颜色、纹理、大小等等)进行区域合并，不断的进行区域迭代合并候选框：每次迭代过程中对这些合并的子区域做bounding boxes(外切矩形)，这些子区域外切矩形就是通常所说的候选框滑窗法简单易于理解，但是不同窗口大小进行图像全局搜索导致效率低下，而且设计窗口大小时候还需要考虑物体的长宽比。所以，对于实时性要求较高的分类器，不推荐使用滑窗法选择搜索计算效率优于滑窗法，由于采用子区域合并策略，所以可以包含各种大小的疑似物体框，合并区域相似的指标多样性，提高了检测物体的概率数据表示预测输出可以表示为：其中， 为预测结果的置信概率， 为边框坐标， 为属于某个类别的概率，通过预测结果，实际结果，构建损失函数，损失函数包含了分类、回归两部分组成效果评估使用IoU(Intersection over Union，交并比)来判断模型的好坏。所谓交并比，是指预测边框、实际边框交集和并集的比率，一般约定0.5为一个可以接受的值非极大值抑制预测结果中，可能多个预测结果间存在重叠部分，需要保留交并比最大的、去掉非最大的预测结果，这就是非极大值抑制(Non-Maximum Suppression，简写作NMS)，非极大值抑制的流程如下：根据置信度得分进行排序选择置信度最高的比边界框添加到最终输出列表中，将其从边界框列表中删除计算所有边界框的面积计算置信度最高的边界框与其它候选框的IoU。删除IoU大于阈值的边界框重复上述过程，直至边界框列表为空bbox回归训练：其实就是训练  矩阵向  矩阵靠齐的过程【目标检测】基础知识：IoU、NMS、Bounding box regression即: 给定 ，寻找一种映射，使得主要操作就是平移+缩放rcnnRCNN分为三个module：独立类别的候选区域（category-independent region proposals），生成一组对检测器可用的检测坐标常见的候选区生成的方法有很多（objectness、selective search、category-independent object proposals、constrained parametric min-cuts (CPMC) 、multi-scale combinatorial grouping），本文用的是选择搜索。产生了2000个候选区域（region proposal）使用卷积神经网络从每个区域从提取固定的特征向量本文每个区域提取到的固定长度的特征向量是4096，使用的网络是AlexNet需要注意的是 Alextnet 的输入图像大小是，而通过 Selective Search 产生的候选区域大小不一，为了与 Alexnet 兼容，R-CNN 采用了非常暴力的手段，那就是无视候选区域的大小和形状，统一变换到的尺寸(就是只有候选框里保留，剩余部分填充其它像素，或者先在候选框周围加上16的padding，再进行各向异性缩放，这种形变使得mAp提高了3到5个百分点)。有一个细节，在对 Region 进行变换的时候，首先对这些区域进行膨胀处理，在其 box 周围附加了 p 个像素，也就是人为添加了边框，在这里 p=16在 ImageNet 上先进行预训练，然后利用成熟的权重参数在 PASCAL VOC 数据集上进行 fine-tune，如果不针对特定任务进行fine-tuning，而是把CNN当做特征提取器，卷积层所学到的特征其实就是基础的共享特征提取层，就类似于SIFT算法一样，可以用于提取各种图片的特征，而f6、f7所学习到的特征是用于针对特定任务的特征。训练过程：首先对 PASCAL VOC数据集 进行Selective Search，搜索到2000个Region Proposal对Pre-trained模型进行fine-tuning。将原来预训练模型最后的1000-way的全连接层（分类层）换成21-way的分类层（20类物体+背景），然后计算每个region proposal和ground truth 的IoU，对于IoU&gt;0.5的region proposal被视为正样本，否则为负样本（即背景）。另外，由于对于一张图片的多有候选区域来说，负样本是远远大于正样本数，所以需要将正样本进行上采样来保证样本分布均衡。在每次迭代的过程中，选择层次采样，每个mini-batch中采样两张图像，从中随机选取32个正样本和96个负样本组成一个mini-batch（128，正负比：1：3）。我们使用0.001的学习率和SGD来进行训练，提取特征的CNN网络经过了预训练和微调后不再训练，就固定不变了，只单纯的作为一个提特征的工具了SVM线性分类器，对特征进行分类：在训练CNN提取特征时，设置的IOU是0.5以上为正样本，小于0.5的是负样本。但在SVM分类中，只有bbox完全包围了物体（也可以理解为IOU＞0.7时）才是正样本，IOU小于0.3的是负样本。前者是大样本训练，后者是小样本训练，svm适用于少样本训练，如果用CNN反而不合适用SVM对每个特征向量进行评分，然后用非极大值抑制简单说就是：给定一张输入图片，从图片中提取 2000 个类别独立的候选区域对于每个区域利用 CNN 抽取一个固定长度的特征向量再对每个特征向量利用 SVM 进行目标分类测试步骤：Region proposal的确定：VOC测试图像输入后，利用SS搜索方法，根据相似度从大到小排序，筛选出2000个region proposalsRP的Features提取：将RP通过resize成，然后分别输入进CNN特征提取网络，得到了2000个4096维featuresSVM分类：将(2000,4096)维矩阵输入进SVM分类器中，最终得到(2000，21)矩阵。每一行的21个列值，分别代表了这个RP属于每一个类的可能性。通过提前设置好的background阈值和所属于类的阈值，筛选出满足条件的m个RP区域BoundingBox-Regression：将(m,4096)维矩阵输入进 (4096,4)的回归矩阵 d dd 中，最后输出(m,4)偏移矩阵。代表RP中心点的位置偏移 和 bbox的尺寸变换将SVM筛选出的m个RP区域对应的特征向量，组成(m,4096)矩阵 代入 (4096,4)的回归矩阵d中，最后输出(m,4)偏移矩阵Non-maximum suppression处理：只画出SVM筛选出的m个RP区域的修正后的检测框，进行非极大值抑制(NMS)，得到最终检测结果缺点：重复计算，每个region proposal，都需要经过一个AlexNet特征提取，为所有的RoI（region of interest）提取特征大约花费47秒，占用空间selective search方法生成region proposal，对一帧图像，需要花费2秒三个模块（提取、分类、回归）是分别训练的，并且在训练时候，对于存储空间消耗较大spp-netSPP-Net: 出自2015年发表在IEEE上的论文-《Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition》所有的神经网络都是需要输入固定尺寸的图片，比如(ImageNet)、(LenNet)、等。这样对于我们希望检测各种大小的图片的时候，需要经过crop，或者warp等一系列操作，这都在一定程度上导致图片信息的丢失和变形，限制了识别精确度为什么要固定输入图片的大小？：卷积层的参数和输入大小无关，它仅仅是一个卷积核在图像上滑动，不管输入图像多大都没关系，只是对不同大小的图片卷积出不同大小的特征图，但是全连接层的参数就和输入图像大小有关，因为它要把输入的所有像素点连接起来,需要指定输入层神经元个数和输出层神经元个数，所以需要规定输入的feature的大小，因此，固定长度的约束仅限于全连接层SPP-Net在最后一个卷积层后，接入了金字塔池化层，使用这种方式，可以让网络输入任意的图片，而且还会生成固定大小的输出金字塔池化，使得任意大小的特征图都能够转换成固定大小的特征向量，这就是空间金字塔池化的意义(多尺度特征提取出固定大小的特征向量)SPP-Net，整个过程是：首先通过选择性搜索，对待检测的图片进行搜索出2000个候选窗口。这一步和R-CNN一样。特征提取阶段。这一步就是和R-CNN最大的区别了，这一步骤的具体操作如下：把整张待检测的图片，输入CNN中，进行一次性特征提取，得到feature maps，然后在feature maps中找到各个候选框的区域，再对各个候选框采用金字塔空间池化，提取出固定长度的特征向量。而R-CNN输入的是每个候选框，然后在进入CNN，因为SPP-Net只需要一次对整张图片进行特征提取，速度会大大提升最后一步也是和R-CNN一样，采用SVM算法进行特征向量分类识别难点候选区域（原图与特征图）的映射假设表示特征图上的坐标点，坐标点表示原输入图片上的点，那么它们之间有如下转换关系，这种映射关心与网络结构有关：其中就是CNN中所有的strides的乘积，包含了池化、卷积的stridefast-rcnnFast-Rcnn：提出了ROI poolingR-CNN存在一些问题：训练分多步：R-CNN的训练先要fine tuning一个预训练的网络，然后针对每个类别都训练一个SVM分类器，最后还要用regressors对bounding-box进行回归，另外region proposal也要单独用selective search的方式获得，步骤比较繁琐时间和内存消耗大：在训练SVM和回归的时候需要用网络训练的特征作为输入，特征保存在磁盘上再读入的时间消耗还是比较大的测试慢：每张图片的每个region proposal都要做卷积，重复操作太多虽然在Fast RCNN之前有提出过SPPnet算法来解决RCNN中重复卷积的问题，但是SPPnet依然存在和RCNN一样的一些缺点比如：训练步骤过多，需要训练SVM分类器，需要额外的回归器，特征也是保存在磁盘上因此Fast RCNN相当于全面改进了原有的这两个算法，不仅训练步骤减少了，也不需要额外将特征保存在磁盘上基于VGG16的Fast RCNN算法的速度：在训练速度上比RCNN快了将近9倍，比SPPnet快大概3倍测试速度比RCNN快了213倍，比SPPnet快了10倍在VOC2012上的mAP在66%左右网络有两个输入：图像和对应的region proposal。其中region proposal由selective search方法得到，没有表示在流程图中对每个类别都训练一个回归器，且只有非背景的region proposal才需要进行回归ROI pooling：ROI Pooling的作用是对不同大小的region proposal，从最后卷积层输出的feature map提取大小固定的feature map(ROI Pooling使用自适应(根据输入feature的大小自调整)池化区域，不再固定池化区域大小，而固定池化区域个数，这样就确保了输入什么大小的feature，输出的feature大小完全相等，等于池化区域个数)。简单讲可以看做是SPPNet的简化版本，因为全连接层的输入需要尺寸大小一样，所以不能直接将不同大小的region proposal映射到feature map作为输出，需要做尺寸变换。在文章中，VGG16网络使用的参数，即将一个的region proposal分割成大小的网格，然后将这个region proposal映射到最后一个卷积层输出的feature map，最后计算每个网格里的最大值作为该网格的输出，所以不管ROI pooling之前的feature map大小是多少，ROI pooling后得到的feature map大小都是简单说ROI pooling就是：把图片上selective search选出的候选框映射到特征图上对应的位置，这个映射是根据输入图片缩小的尺寸来的；将映射后的区域划分为相同大小的sections（sections数量与输出的维度相同）对每个sections进行max pooling操作； 这样我们就可以从不同大小的方框得到固定大小的相应 的feature maps两个loss：第一个优化目标是分类，使用softmax（就不用像前面的R-CNN和SPP再用SVM了），第二个优化目标是bbox regression，使用了一个平滑的L1-lossROI Pooling 与 SPP 的区别：通过上面的介绍，可以看到两者起到的作用是相同的，把不同尺寸的特征输入转化为相同尺寸的特征输出。SPP针对同一个输入使用了多个不同尺寸的池化操作，把不同尺度的结果拼接作为输出；而ROI Pooling可看作单尺度的SPP，对于一个输入只进行一次池化操作可以看出Fast RCNN主要有3个改进：卷积不再是对每个region proposal进行，而是直接对整张图像，这样减少了很多重复计算。原来RCNN是对每个region proposal分别做卷积，因为一张图像中有2000左右的region proposal，肯定相互之间的重叠率很高，因此产生重复计算用ROI pooling进行特征的尺寸变换，因为全连接层的输入要求尺寸大小一样，因此不能直接把region proposal作为输入将regressor放进网络一起训练，每个类别对应一个regressor，同时用softmax代替原来的SVM分类器在实际训练中，每个mini-batch包含2张图像和128个region proposal（或者叫ROI），也就是每张图像有64个ROI。然后从这些ROI中挑选约25%的ROI，这些ROI和ground truth的IOU值都大于0.5。另外只采用随机水平翻转的方式增加数据集测试的时候则每张图像大约2000个ROI总结：Fast RCNN将RCNN众多步骤整合在一起，不仅大大提高了检测速度，也提高了检测准确率。其中，对整张图像卷积而不是对每个region proposal卷积，ROI Pooling，分类和回归都放在网络一起训练的multi-task loss是算法的三个核心。另外还有SVD分解等是加速的小贡献，数据集的增加时mAP提高的小贡献当然Fast RCNN的主要缺点在于region proposal的提取使用selective search，目标检测时间大多消耗在这上面（提region proposal 2~3s，而提特征分类只需0.32s），这也是后续Faster RCNN的改进方向之一缺点：依旧采用selective search提取region proposal（耗时2~3秒，特征提取耗时0.32秒）无法满足实时应用，没有真正实现端到端训练测试利用了GPU，但是region proposal方法是在CPU上实现的总结RCNN:给定一张输入图片，通过 Selective Search从图片中提取 2000 个类别独立的候选区域对于每个区域利用 CNN 抽取一个固定长度的特征向量对每个特征向量利用 SVM 进行目标分类对于SVM分好类的Region Proposal做边框回归，用Bounding box回归值校正原来的建议窗口，生成预测窗口坐标缺点：重复计算，每个region proposal，都需要经过一个AlexNet特征提取selective search方法生成region proposal，对一帧图像，需要花费2秒三个模块(提取、分类、回归)是分别训练的，并且在训练时候，对于存储空间消耗较大训练分为多个阶段，步骤繁琐：微调网络+训练SVM+训练边框回归器SVM和回归是事后操作，在SVM和回归过程中CNN特征没有被学习更新SPPNet: 金字塔池化层当网络输入的是一张任意大小的图片，这个时候我们可以一直进行卷积、池化，直到网络的倒数几层的时候，也就是我们即将与全连接层连接的时候，就要使用金字塔池化，使得任意大小的特征图都能够转换成固定大小的特征向量，这就是空间金字塔池化的意义（多尺度特征提取出固定大小的特征向量）首先通过选择性搜索，对待检测的图片进行搜索出2000个候选窗口。这一步和R-CNN一样特征提取阶段,把整张待检测的图片，输入CNN中，进行一次性特征提取，得到feature maps，然后在feature maps中找到各个候选框的区域，再对各个候选框采用金字塔空间池化，提取出固定长度的特征向量，只需要一次对整张图片进行特征提取，速度会大大提升最后一步也是和R-CNN一样，采用SVM算法进行特征向量分类识别Fast-RCNN:给定一张输入图片，通过 Selective Search从图片中提取 2000 个类别独立的候选区域对每个类别都训练一个回归器，且只有非背景的region proposal才需要进行回归。ROI pooling：ROI Pooling的作用是对不同大小的region proposal，从最后卷积层输出的feature map提取大小固定的feature map。简单讲可以看做是SPPNet的简化版本(把图片上selective search选出的候选框映射到特征图上对应的位置，这个映射是根据输入图片缩小的尺寸来的)，因为全连接层的输入需要尺寸大小一样，所以不能直接将不同大小的region proposal映射到feature map作为输出，需要做尺寸变换两个loss：第一个优化目标是分类，使用softmax（就不用像前面的R-CNN和SPP再用SVM了），第二个优化目标是bbox regression，使用了一个平滑的L1-loss优点卷积不再是对每个region proposal进行，而是直接对整张图像，这样减少了很多重复计算用ROI pooling进行特征的尺寸变换，因为全连接层的输入要求尺寸大小一样将regressor放进网络一起训练，每个类别对应一个regressor，同时用softmax代替原来的SVM分类器 三个核心：对整张图像卷积、ROI Pooling、分类和回归一起训练的multi-task loss 主要缺点在于region proposal的提取使用selective search，目标检测时间大多消耗在这上面相比R-CNN，主要两处不同：最后一层卷积层后加了一个ROI pooling layer；损失函数使用了多任务损失函数(multi-task loss)，将边框回归直接加入到CNN网络中训练faster-rcnn推荐这篇文章-Object Detection and Classification using R-CNNsFaster R-CNN：提出了RPN(region proposal network)主要就是多了一个RPN(region proposal network)，就是在卷积提取特征之后，多出一条路来进行候选框的提取推荐有关RPN层的文章：RPN层解析RPN只是将框内认为是目标，框外认为是背景，做了个二分类，至于框内目标具体是啥，最终是交给分类网络去做下图来自Faster-RCNN（二）之RPN层Faster-RCNN: RPN(region proposal network)，就是在卷积提取特征之后，多出一条路来进行候选框的提取将整张图片输入CNN，进行特征提取用RPN生成建议窗口(proposals)，每张图片生成300个建议窗口通过RoI pooling层使每个RoI生成固定尺寸的feature map利用Softmax Loss(探测分类概率) 和Smooth L1 Loss(探测边框回归)对分类概率和边框回归(Bounding box regression)联合训练相比Fast R-CNN，主要两处不同：使用RPN(Region Proposal Network)代替原来的Selective Search方法产生建议窗口；产生建议窗口的CNN和目标检测的CNN共享如何高效快速产生建议框？Faster R-CNN创造性地采用卷积网络自行产生建议框，并且和目标检测网络共享卷积网络，使得建议框数目从原有的约2000个减少为300个，且建议框的质量也有本质的提高Yolo系列Yolov1网络结构YOLO_v1讲解浅谈不同版本YOLO的区别（V1-V3）输出层的含义\begin{array}{l} loss = \lambda{\text {coord }} \sum{i=0}^{S^{2}} \sum{j=0}^{B} \mathbb{1}{i j}^{\mathrm{obj}}\left(x{i}-\hat{x}{i}\right)^{2}+\left(y{i}-\hat{y}{i}\right)^{2} \ \qquad + \lambda{\text {coord }} \sum{i=0}^{S^{2}} \sum{j=0}^{B} \mathbb{1}{i j}^{\mathrm{obj}}\left(\sqrt{w{i}}-\sqrt{\hat{w}{i}}\right)^{2}+\left(\sqrt{h{i}}-\sqrt{\hat{h}{i}}\right)^{2} \ \qquad +\sum{i=0}^{S^{2}} \sum{j=0}^{B} \mathbb{1}{i j}^{\mathrm{obj}}\left(C{i}-\hat{C}_{i}\right)^{2} \ \qquad + \lambda{\text {noobj }} \sum{i=0}^{S^{2}} \sum{j=0}^{B} \mathbb{1}{i j}^{\mathrm{noobj}}\left(C{i}-\hat{C}{i}\right)^{2} \ \qquad +\sum{i=0}^{S^{2}} \mathbb{1}{i}^{\mathrm{obj}} \sum{c \in \text { classes }} \left(p{i}(c)-\hat{p}_{i}(c)\right)^{2}\end{array}Score{i j} = P ( C{i} |  Object ) * Confidence _{j} 3表示三个候选框，4表示的是xywh的位置信息，1表示是否为背景，20为物体的类别数(这里用的是voc数据集)因为有三个输出，因此该网络有个预测框对于这些预测框，如果其与任何真实框的IoU大于一定的阈值（如0.5），则将其分配给对应的输出层作为正样本。否则，将其视为负样本YOLOv3 使用的损失函数是组合了多个部分的综合损失函数，其中包括定位损失（Localization Loss）、分类损失（Classification Loss）和置信度损失（Confidence Loss）只有正样本才有这三类损失，而负样本只有置信度损失以下是 YOLOv3 的损失函数的伪代码实现，包含详细注释：12345678910111213141516171819202122232425262728293031def yolo_loss(pred_boxes, pred_cls, target_boxes, target_cls):    # pred_boxes: 预测的边界框坐标，shape 为 [batch_size, num_anchors, grid_size, grid_size, 4]    # pred_cls: 预测的类别概率，shape 为 [batch_size, num_anchors, grid_size, grid_size, num_classes]    # target_boxes: 真实的边界框坐标，shape 为 [batch_size, num_anchors, grid_size, grid_size, 4]    # target_cls: 真实的类别标签，shape 为 [batch_size, num_anchors, grid_size, grid_size, num_classes]        # 计算预测框和真实框的IoU    iou_scores = calculate_iou(pred_boxes, target_boxes)  # shape: [batch_size, num_anchors, grid_size, grid_size]        # 计算类别损失，使用交叉熵损失函数    cls_loss = cross_entropy_loss(pred_cls, target_cls)  # shape: [batch_size, num_anchors, grid_size, grid_size]        # 计算边界框损失，使用平滑L1损失函数    box_loss = smooth_l1_loss(pred_boxes, target_boxes)  # shape: [batch_size, num_anchors, grid_size, grid_size]        # 计算正样本的掩码，即与真实框IoU大于阈值的位置为1，其余为0    pos_mask = (iou_scores &gt; positive_iou_threshold).float()  # shape: [batch_size, num_anchors, grid_size, grid_size]        # 计算负样本的掩码，即与真实框IoU小于阈值的位置为1，其余为0    neg_mask = (iou_scores &lt; negative_iou_threshold).float()  # shape: [batch_size, num_anchors, grid_size, grid_size]        # 将正样本和负样本的掩码相乘得到最终的样本掩码    sample_mask = pos_mask + neg_mask  # shape: [batch_size, num_anchors, grid_size, grid_size]        # 对类别损失和边界框损失按照样本掩码进行加权求和    loss = lambda_cls * cls_loss * sample_mask + lambda_box * box_loss * sample_mask        # 对损失进行归约操作，取平均或求和    loss = reduction_func(loss)  # 可根据实际情况选择平均或求和        return loss正负样本数在 YOLOv3 中，通常会为每个目标选择一个正样本，即与真实目标框具有最高 IoU 的预测框。这确保了每个目标都有至少一个正样本来参与损失函数的计算和网络的训练。至于负样本的选择，一般会设置一个阈值来确定预测框与真实框之间的 IoU 阈值。如果预测框的 IoU 低于该阈值，则被视为负样本。对于每个负样本，可以选择保留一定数量的负样本，以确保正负样本的平衡性。具体来说，关于正负样本的选择数量并没有一个固定的标准，它可以根据具体的数据集和应用场景来确定。在实践中，可以根据数据集的统计信息和训练效果进行调整，以找到一个适合的正负样本比例，从而平衡目标检测的准确性和效率。yolov5Yolov8Yolov8 github地址和文档目标检测Modelsize (pixels)mAPval 50-95Speed CPU ONNX (ms)Speed A100 TensorRT (ms)params (M)FLOPs (B)YOLOv8n64037.380.40.993.28.7YOLOv8s64044.9128.41.2011.228.6YOLOv8m64050.2234.71.8325.978.9YOLOv8l64052.9375.22.3943.7165.2YOLOv8x64053.9479.13.5368.2257.8tensorRt加速可以用tensorRt加速，环境教程可以参考这个文档进行tensort加速，cmake编译失败，缺少zlibwapi.dll文件，解决办法，去cudnn官网下载zlib123dllx64lib文件放到C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\libdll文件放到C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.1\binYoloV8提供了导出工具，详见文档，python代码导出可以这样子写123def run():    model = YOLO(&quot;../../weights/yolov8s.pt&quot;)  # load an official detection model    model.export(format='engine', device='0', workspace=8, batch=4, dynamic=True)或者直接命令行执行1yolo mode=export model=yolov8s.pt format=engine device=0 workspace=8 batch=4 dynamic=True关键参数如下：device: 其中device必须是GPU，可以是多张显卡workspace: TensorRT: workspace size (GB)batch: 最大批次大小，当设置参数时必须有dynamic=True，可以运行最大不超过batch的数量dynamic: 动态参数，ONNX/TF/TensorRT: dynamic axesimgsz: image size as scalar or (h, w) list, i.e. (640, 480)注意事项：TensorRT发布的模型(engine)不能跨平台使用例如linux发布的模型不能在windows下用TensorRT发布的模型需要在相同GPU算力(compute capability)的情况下使用否则会导致compute capability不匹配问题，例如算力6.1发布的模型不能在7.5上用动态batch和动态宽高的处理方式动态batch：源自tensorRT编译时对batch的处理，若静态batch则意味着无论你多少图，都按照固定大小batch推理，耗时是固定的导出模型时，注意view操作不能固定batch维度数值，通常写-1导出模型时，通常可以指定dynamic_axes，实际上不指定也没关系动态宽高：源自onnx导出时指定的宽高是固定的，trt编译时也得到固定大小引擎，此时若你想得到一个不同大小的trt引擎时，就需要动态宽高的存在。而使用trt的动态宽高会带来太多不必要的复杂度，这里使用中间方案，编译时修改onnx输入实现相对动态，避免重回pytorch再做导出不建议使用dynamic_axes指定0以外的维度为动态，复杂度太高，并且存在有的layer不支持，这种需求也不常用，性能也很差真正需要的，是onnx文件已经导出，但是输入shape固定了，此时希望修改这个onnx的输入shape步骤一: 使用TRT::compile函数的inputsDimsSetup参数重定义输入的shape步骤二: 使用TRT:set_layer_hook_reshape钩子动态修改reshape的参数实现适配其他检测系列SSDFPNFeature Pyramid Networks for Object Detection概述FPN(feature pyramid networks) 是何凯明等作者提出的适用于多尺度目标检测算法原来多数的object detection算法(比如 faster rcnn)都是只采用顶层特征做预测，但我们知道低层的特征语义信息比较少，但是目标位置准确；高层的特征语义信息比较丰富，但是目标位置比较粗略另外虽然也有些算法采用多尺度特征融合的方式，但是一般是采用融合后的特征做预测，而本文不一样的地方在于预测是在不同特征层独立进行的，这里主要借鉴了ResNet的残差: 结合了浅层特征和深层特征SSD的检测策略: 在不同分辨率的特征图上分别做预测特征金字塔图示，越粗表示特征语义更强薰风读论文：Feature Pyramid Network 详解特征金字塔网络FPN的来龙去脉图中将特征金字塔和其他的金字塔做了比较(a)是传统中的图片金字塔，图片缩放到不同的大小，分别预测，每个特征提取/预测都是独立进行的，同一张图片的不同分辨率，也很难共享它们中间提取的特征，让模型预测的过程费时费力(b)是原生的CNN提取的特征，由于后续存在池化和降采样，浅层网络的特征图可以保留更多的分辨率，但是特征语义较为低级(c)是SSD中的特征，把不同分辨率特征，在不同分辨率的特征上直接预测，那么大物体小物体都能预测到，但仍存在底层特征语义不够和最高分辨率不高的问题(d)是FPN中用的特征金字塔，结合深浅特征，兼顾分辨率与特征语义RetinaNetFocal Loss for Dense Object Detection 2018ppt: Focal Loss for Dense Object DetectionRetinaNet是使用FPN和Focal Loss(详情看本站深度学习核心之损失函数部分)的目标检测模型，能够有效解决类别不平衡问题它通过特征金字塔网络生成多尺度的特征图，并使用Focal Loss重点关注难以分类的样本，从而提高了检测性能DETREnd-to-End Object Detection with Transformers DETR 2020深度学习之目标检测（十一）—DETR详解继Transformer应用于图像分类后，Transformer应用于图像目标检测的开山之作–DEtection TRansformer，其大大简化了目标检测的框架，更直观DETR是Facebook团队于2020年提出的基于Transformer的端到端目标检测，没有非极大值抑制NMS后处理步骤、没有anchor等先验知识和约束，整个由网络实现端到端的目标检测实现，大大简化了目标检测的pipeline。结果在COCO数据集上效果与Faster RCNN相当，在大目标上效果比Faster RCNN好，且可以很容易地将DETR迁移到其他任务例如全景分割目标跟踪万字长文 | 多目标跟踪最新综述(基于Transformer/图模型/检测和关联/孪生网络)【yolov4目标检测】(2) 多目标跟踪，案例：车辆行人的跟踪和计数，附python完整代码和数据集yolov4-deepsort tensorflow代码转载：一线算法工程师整理！超实用的3大多目标跟踪算法Deep-Sort 多目标跟踪算法原理和代码解析Darklabel多目标跟踪标注工具万字综述：目标检测模型YOLOv1-v7深度解析目标跟踪 = 目标检测+目标跟踪算法目标追踪算法分为单目标追踪SOT(Single-Object Track)和多目标追踪MOT(Multi-Object Track)[1][2]在单目标跟踪中，使用给定的初始目标位置，在后续视频帧中对给定的物体进行位置预测多目标跟踪算法，大部分都是不考虑初始目标位置的，目标可自行消失与产生目标跟踪分类目标跟踪通常可分为单目标跟踪和多目标跟踪两类单目标跟踪多目标跟踪SDE(separate detecting and embeding)每部分独立优化能够取得比较高的精度，缺点就是计算量会增加JDE(joint detecting and embeding)JDE将目标检测与REID特征提取放在一个网络，这样能有效减少计算量，但是多任务学习的精度会低些解决的任务和视频目标检测相同的点在于都需要对每帧图像中的目标精准定位，不同点在于目标跟踪不考虑目标的识别问题SDE将REID特征提取和目标检测分为两个独立网络来实现，这样做的优点是每部分独立优化能够取得比较高的精度，缺点就是计算量会增加；JDE将目标检测与REID特征提取放在一个网络，这样能有效减少计算量，但是多任务学习的精度目前来说还没有SDE高。在工程应用上我更偏向于JDE，毕竟跟踪要保证实时性，在能够提取一个不太差的REID特征基础上，加强检测器性能和优化数据关联部分也能一定程度上弥补REID特征不够好带来的性能损失SortDeepSort目标跟踪基础——DeepSORT【MOT】详解DeepSORT多目标追踪模型StrongSortStrongSort相比于DeepSort的区别：使用BoT替代CNN做外表特征的提取使用EMA(exponential moving average：指数移动平均)策略更新新帧中的目标外观特征EMA更新策略不仅提高了匹配质量，而且减少了时间消耗在做Kalman filter之前，使用ECC（enhanced correlation coefficient：增强相关系数）进行相机运动补偿；并使用NSA Kalman代替Kalman进行运动特征获取将运动信息和外观信息结合来进行匹配使用Vanilla全局线性赋值代替了匹配级联BotSortBoT-SORT: Robust Associations Multi-Pedestrian TrackingBoT-SORT ｜超越 DeepSORT、StrongSORT++ 和 ByteTrackByteTrackByteTrack: Multi-Object Tracking by Associating Every Detection Box依赖的算法卡尔曼滤波卡尔曼滤波被广泛应用于无人机、自动驾驶、卫星导航等领域简单来说，其作用就是基于传感器的测量值来更新预测值，以达到更精确的估计假设我们要跟踪小车的位置变化，如下图所示，蓝色的分布是卡尔曼滤波预测值，棕色的分布是传感器的测量值，灰色的分布就是预测值基于测量值更新后的最优估计在目标跟踪中，需要估计track的以下两个状态：均值(Mean)：表示目标的位置信息，由bbox的中心坐标 (cx, cy)，宽高比r，高h，以及各自的速度变化值组成由8维向量表示为 x = [cx, cy, r, h, vx, vy, vr, vh]，各个速度值初始化为0协方差(Covariance )：表示目标位置信息的不确定性，由8x8的对角矩阵表示，矩阵中数字越大则表明不确定性越大，可以以任意值初始化卡尔曼滤波分为两个阶段：(1) 预测track在下一时刻的位置，(2) 基于detection来更新预测的位置。匈牙利匹配算法目标跟踪初探(DeepSORT)先介绍一下什么是分配问题（Assignment Problem）：假设有N个人和N个任务，每个任务可以任意分配给不同的人，已知每个人完成每个任务要花费的代价不尽相同，那么如何分配可以使得总的代价最小举个例子，假设现在有3个任务，要分别分配给3个人，每个人完成各个任务所需代价矩阵(cost matrix)如下所示(这个代价可以是金钱、时间等等)：Task_1Task_2Task_3Person_1154045Person_2206035Person_3204025怎样才能找到一个最优分配，使得完成所有任务花费的代价最小呢？匈牙利算法(又叫KM算法)就是用来解决分配问题的一种方法，它基于定理：如果代价矩阵的某一行或某一列同时加上或减去某个数，则这个新的代价矩阵的最优分配仍然是原代价矩阵的最优分配算法步骤(假设矩阵为N阶方阵)：对于矩阵的每一行，减去其中最小的元素对于矩阵的每一列，减去其中最小的元素用最少的水平线或垂直线覆盖矩阵中所有的0如果线的数量等于N，则找到了最优分配，算法结束，否则进入步骤5找到没有被任何线覆盖的最小元素，每个没被线覆盖的行减去这个元素，每个被线覆盖的列加上这个元素，返回步骤3继续拿上面的例子做演示：step1 每一行最小的元素分别为15、20、20，减去得到：Task_1Task_2Task_3Person_102530Person_204015Person_30205step2 每一列最小的元素分别为0、20、5，减去得到：Task_1Task_2Task_3Person_10525Person_202010Person_3000step3 用最少的水平线或垂直线覆盖所有的0，得到：Task_1Task_2Task_3Person_10525Person_202010Person_3000step4 线的数量为2，小于3，进入下一步；step5 现在没被覆盖的最小元素是5，没被覆盖的行(第一和第二行)减去5，得到：Task_1Task_2Task_3Person_1-5020Person_2-5155Person_3000被覆盖的列(第一列)加上5，得到：Task_1Task_2Task_3Person_10020Person_20155Person_3500跳转到step3，用最少的水平线或垂直线覆盖所有的0，得到：Task_1Task_2Task_3Person_10020Person_20155Person_3500step4：线的数量为3，满足条件，算法结束显然，将任务2分配给第1个人、任务1分配给第2个人、任务3分配给第3个人时，总的代价最小(0+0+0=0)：所以原矩阵的最小总代价为40+20+25=85Task_1Task_2Task_3Person_1154045Person_2206035Person_3204025sklearn里的linear_assignment()函数以及scipy里的linear_sum_assignment()函数都实现了匈牙利算法123456789101112131415import numpy as np from scipy.optimize import linear_sum_assignment cost_matrix = np.array([    [15,40,45],    [20,60,35],    [20,40,25]]) matches = linear_sum_assignment(cost_matrix)print('scipy API result:\n', matches) &gt;&gt;&gt;scipy API result: (array([0, 1, 2], dtype=int64), array([1, 0, 2], dtype=int64))追踪指标多目标跟踪评价指标总结——MOTA、IDF1、HOTA等MOTA(Multiple Object Tracking Accuracy): 指标体现多目标跟踪的准确度MOTA指标是衡量多目标跟踪算法精确性方面最重要的指标，以1为最佳情况，数值越高代表跟踪精确度越好IDF1: 指标代表被检测和跟踪的目标中获取正确的ID的检测目标的比例，综合考虑ID准确率和ID召回率，代表两者的调和均值其中，IDP代表ID跟踪的准确率，IDR代表ID跟踪的召回率，IDF1指标更聚焦于跟踪算法跟踪某个目标的时间长短，考察跟踪的连续性和重识别的准确性，IDF1以1为最佳情况，数值越高代表跟踪特定目标的精度越好HOTA道路监控管理Roboflow数据集标注目标检测算法_模型训练目标跟踪算法_ByteTrack(实时性)物体检测分类主动学习和强化学习主动学习：是一种通过主动选择最有价值的样本进行标注的机器学习或人工智能方法。其目的是使用尽可能少的、高质量的样本标注使模型达到尽可能好的性能。也就是说，主动学习方法能够提高样本及标注的增益，在有限标注预算的前提下，最大化模型的性能，是一种从样本的角度，提高数据效率的方案，因而被应用在标注成本高、标注难度大等任务中，例如医疗图像、无人驾驶、异常检测、基于互联网大数据的相关问题强化学习介绍及应用强化学习：强化学习是一个非常吸引人的人工智能领域，2016年 Alpha Go在围棋领域挑战李世石，以几乎碾压的结果夺冠，引起了人们对于人工智能的广泛讨论。2019年Alpha Star横空出世，在复杂的星际争霸2游戏中达到能和人类顶级玩家PK的水平，登上Nature。这两次与人类顶级玩家的抗衡之战，背后的技术都是强化学习。强化学习是机器学习领域的一个分支，强调基于环境而行动，以取得最大化的长期利益。与监督学习、非监督学习不同，监督学习解决如分类、回归等感知和认知类的任务，而强化学习处理决策问题，着重于环境的交互、序列决策、和长期收益。强化学习与环境的交互模式可以抽象为：智能体Agent在环境Environment中学习，根绝环境的状态State，执行动作Action，并根据环境反馈的奖励Reward来指导输出更好的动作</content>
	</entry>
	<entry>
		<title>数据标注工具</title>
		<url>/article/42221.html</url>
		<content>label-studiolabel-studio社区版和企业版比较 基于UIE的命名实体识别下的label-studio-ml教程Specific-examples-for-pre-annotationsLabel Studio是一个开源的，可配置的数据注释工具，其目的是使您能够使用标准化输出格式的最方便的界面标记不同类型的数据特色：支持多人协作，支持主动学习、支持多种的标注的任务缺点：在目标检测标注时，好像没有十字辅助线支持的标注任务，包括了如下所示，具体的见官方git介绍label studio 结合 MMDetection 实现数据集自动标记、模型迭代训练的闭环Label Studio使用技巧关键的两个git仓库，其中label-studio的官方文档在这里label-studio， 进行普通的图片标记工作，如果要使用其提供的辅助预标记功能，则需要进行后续配置label-studio-ml-backend，主要提供深度学习服务，包括预标记和模型训练，结合前端形成模型迭代训练的主动学习效果版本1234567891011-- 前端label-studio==1.10.0.post0-- 后端label-studio-ml==1.0.9gunicorn==20.1.0rq==1.10.1-- 下面这两个自己会安装label-studio-converter==0.0.57label-studio-tools==0.0.3前后端结合可以达到模型自动标记数据集、数据集更新迭代训练模型的闭环前端安装123pip install label-studio# 如果安装psycopg2==2.9.5 失败, 可以先装下下面的库sudo yum install python-devel postgresql-devel postgresql-libs gcc环境安装完成后在任意位置打开命令行，使用以下命令启动 label studio12# 不指定路径时, 默认放在了 /root/.local/share/label-studiolabel-studio --data-dir Label-Studio -p 80其中 --data-dir 用于指定工作目录， -p 用来指定运行端口，运行成功后会当前目录会生成 Label-Studio 目录浏览器打开label studio工作界面，创建用户后即可登录使用(这里的创建很简单，符合规范就可以)文件夹中主要包括了export和media文件夹，以及一个label_studio.sqlite3数据库文件数据库配置数据库默认使用的是sqlite3，还可以配置其他的关系库，如PostgreSQL等，通常如果你想标注数百万个任务，或者预计会有很多并发用户，或者你计划在现实生活中的项目中工作，那么使用PostgreSQL数据库官方提供的设置pg的环境变量好像不起效果，不知道把数据库建哪里去了，这里直接在命令行带上pg库的参数1DJANGO_DB=default POSTGRE_NAME=label_studio_db POSTGRE_USER=postgres POSTGRE_PASSWORD=xxx POSTGRE_PORT=5432 POSTGRE_HOST=192.168.123.xx LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT=/home/huangyc/label_ws label-studio start --data-dir /home/huangyc/label_ws -p 8080LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT用于指定本地文件的位置，项目设置里面配置本地路径为/home/huangyc/label_ws/media(这只是个例子好像配成/home/huangyc/label_ws/media/upload也是一样的)，此时可以访问12# 注意?d后面的路径是LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT配置的路径后面继续开始http://192.168.123.xx:8080/data/local-files?d=media/upload/3/demo.jpg当前目录结构如下12345678910111213(label38) [root@uslave02 label_ws]# tree -d.├── export└── media    ├── avatars    ├── export    └── upload        ├── 1        ├── 3            ├── demo.jpg        └── 48 directories注意这里需要提前创建好数据库label_studio_db，启动完成控制台输出如下的日志1234567891011121314151617label-studio start --data-dir /home/huangyc/label_ws -p 8080=&gt; Database and media directory: /root/.local/share/label-studio=&gt; Static URL is set to: /static/=&gt; Database and media directory: /home/huangyc/label_ws=&gt; Static URL is set to: /static/Starting new HTTPS connection (1): pypi.org:443https://pypi.org:443 &quot;GET /pypi/label-studio/json HTTP/1.1&quot; 200 56156Initializing database..Performing system checks...[2023-03-29 05:01:46,560] [django::register_actions_from_dir::97] [INFO] No module named 'data_manager.actions.__pycache_'[2023-03-29 05:01:46,560] [django::register_actions_from_dir::97] [INFO] No module named 'data_manager.actions.__pycache_'System check identified no issues (1 silenced).March 29, 2023 - 05:01:46Django version 3.2.16, using settings 'label_studio.core.settings.label_studio'Starting development server at http://0.0.0.0:8080/Quit the server with CONTROL-C.此时数据库label_studio_db里面会建好所有的表，比如htx_user存的就是注册的用户信息如果要后台执行，可以新建一个start.sh脚本1DJANGO_DB=default POSTGRE_NAME=label_studio_db POSTGRE_USER=postgres POSTGRE_PASSWORD=xxx POSTGRE_PORT=5432 POSTGRE_HOST=192.168.123.xx label-studio start --data-dir /home/huangyc/label_ws -p 8080然后执行nohup sh start.sh &gt; log_start.log &amp;导入现有标注将现有标注转为json格式可以用label-studio-converter工具，安装命令为pip install label-studio-converter转换的例子，目录结构为:12345678910111213Q:.├─images   ├─ 1.jpg   ├─ 2.jpg└─labels   ├─ 1.txt   ├─ 2.txt├─ classes.txt   # 注意这里的classes.txt里面的类别顺序一定要正确，如果是从label-studio前端导出的，好像顺序会有问题，可以用notes.json去获取正确的顺序# 代码为annotation_labels = json_load_op('notes.json')['categories']annotation_labels = sorted(annotation_labels, key=lambda k: int(k['id']))annotation_labels = [anno['name'] for anno in annotation_labels]命令行为1label-studio-converter import yolo -i project-1 -o ls-tasks.json --image-root-url /data/local-files?d=tmp/images输出ls-tasks.json和ls-tasks.label_config.xml两个文件，第一个是数据的索引和标签信息，第二个是项目的标签配置文件此时要把文件上传到LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT/tmp/images下才可以看到如果这里转换出来的路径不对，可以修改label_studio_converter\imports\yolo.py.py下的12345678910111213task = {            &quot;data&quot;: {                # eg. '../../foo+you.py' -&gt; '../../foo%2Byou.py'                &quot;image&quot;: f&quot;{image_root_url}{sp}{image_file_base}&quot; # 改完后的            },            # 'annotations' or 'predictions'            out_type: [                {                    &quot;result&quot;: [],                    &quot;ground_truth&quot;: False,                }            ]        }不然会访问不到数据如果数据没有标注信息，可以将数据传到LABEL_STUDIO_LOCAL_FILES_DOCUMENT_ROOT/tmp/images下，并生成如下json文件，并在项目界面导入该文件即可1234[{&quot;image&quot;:&quot;/data/local-files?d=tmp/images/175.jpg&quot;},{&quot;image&quot;:&quot;/data/local-files?d=tmp/images/696.jpg&quot;}]Cloudreve数据导入cloudreve私有云盘配置后台运行安装配置CloudreveCloudreve可助你即刻构建出兼备自用或公用的网盘服务，通过多种存储策略的支持、虚拟文件系统等特性实现灵活的文件管理体验Linux 下，直接解压并执行主程序即可：12345678#解压获取到的主程序tar -zxvf cloudreve_VERSION_OS_ARCH.tar.gz# 赋予执行权限chmod +x ./cloudreve# 启动 Cloudrevenohup ./cloudreve &gt; start.log &amp;Windows下，直接解压获取到的 zip 压缩包，启动 cloudreve.exe 即可Cloudreve在首次启动时，会创建初始管理员账号，请注意保管管理员密码，此密码只会在首次启动时出现。如果您忘记初始管理员密码，需要删除同级目录下的cloudreve.db，重新启动主程序以初始化新的管理员账户。或者使用./cloudreve --database-script ResetAdminPassword重置密码Cloudreve默认会监听5212端口。你可以在浏览器中访问http://服务器IP:5212进入 Cloudreve。以上步骤操作完后，最简单的部署就完成了。你可能需要一些更为具体的配置，才能让 Cloudreve 更好的工作登录管理员，编辑存储策略下的Default storage policy，将存储文件名改为{originname}，不然会默认会带上一些前缀Cloudreve的文件实际上是存储在/home/huangyc/cloudreve/uploads/1下，在此目录下新建软链接label_ws到label-studio的工作目录下，至此123[root@uslave02 1]# ln -s /home/huangyc/label_ws/uploads label_ws[root@uslave02 1]# pwd/home/huangyc/cloudreve/uploads/1此时，界面网页上可以看到label_ws文件夹，后续的项目文件可以传到这里，至此Cloudreve配置完成label-studio项目设置这里可以将label-studio项目的云存储位置设置为本地/home/huangyc/label_ws/uploads，然后导入一下json文件到项目123[{&quot;image&quot;:&quot;/data/local-files?d=uploads/tricycle_samples/灭鬼之刃.png&quot;}]最后通过Cloudreve，把文件灭鬼之刃.png上传到label_ws/tricycle_samples下就可以看到图片了，注意：这里的label_ws相当于/home/huangyc/label_ws/uploads，这就是软链接的魅力新建标注项目在 label studio 前端主页中选择创建项目，主要流程填写项目基本信息导入数据选择标记模板: label studio内置了很多常见的深度学习标记模板以下是目标检测的模板12345678910&lt;View&gt;  &lt;Image name=&quot;image&quot; value=&quot;$image&quot; zoom=&quot;true&quot; showMousePos=&quot;true&quot; zoomControl=&quot;true&quot; rotateControl=&quot;true&quot;/&gt;  &lt;View&gt;    &lt;Filter toName=&quot;label&quot; minlength=&quot;0&quot; name=&quot;filter&quot;/&gt;    &lt;RectangleLabels name=&quot;label&quot; toName=&quot;image&quot;&gt;      &lt;Label value=&quot;tricycle&quot; background=&quot;#ca9eff&quot; category=&quot;0&quot;/&gt;      &lt;Label value=&quot;person&quot; background=&quot;#50b01c&quot; category=&quot;1&quot;/&gt;    &lt;/RectangleLabels&gt;  &lt;/View&gt;&lt;/View&gt;category很重要，能保证标签的顺序，这里要从0开始此时我们已经可以通过 label studio 进行普通的图片标记工作，如果要使用其提供的辅助预标记功能，则需要进行后续配置后端介绍label studio ml是label studio的后端配置，其主要提供了一种能够快速将AI模型封装为label studio可使用的预标记服务(提供模型预测服务)安装: 依赖了rq库和redis库，注意有些库的版本不能过高1pip install label-studio-ml模型创建创建后端模型导入相关库12345678910111213import osfrom PIL import Imagefrom label_studio_ml import modelfrom label_studio_ml.model import LabelStudioMLBasefrom label_studio_ml.utils import get_envfrom label_studio_tools.core.utils.io import get_local_pathmodel.LABEL_STUDIO_ML_BACKEND_V2 = Trueos.environ['HOSTNAME'] = 'http://192.168.123.xx:8080'os.environ['API_KEY'] = 'TOKEN,在项目的settings里面复制'os.environ['LABEL_STUDIO_ML_BACKEND_V2'] = 'True'模型类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980class MyModel(LabelStudioMLBase):    def __init__(self, **kwargs):        super(MyModel, self).__init__(**kwargs)        # 按 mmdetection 的方式加载模型及权重        print(kwargs)        print(&quot;初始化完成&quot;)    def predict(self, tasks, **kwargs):        # 获取待标记图片        print(&quot;开始预测&quot;)        images = [get_local_path(task['data']['image'], hostname=HOSTNAME, access_token=API_KEY) for task in tasks]        results = []        all_scores = []        # 这里只是示例, 返回结果都一样        for img_path in images:            for _ in range(1):                w, h = Image.open(img_path).size                pixel_x, pixel_y, pixel_width, pixel_height = convert_to_ls(0, 0, 200, 250, w, h)                result = {                    'id': '0',  # 必须为 str，否则前端不显示                    'from_name': 'label',                    'to_name': 'image',                    'type': 'rectanglelabels',                    'value': {                        'rectanglelabels': ['tricycle'],                        'x': pixel_x,  # xy 为左上角坐标点                        'y': pixel_y,                        'width': pixel_width,  # width,height 为宽高                        'height': pixel_height                    },                    'score': 0.95                }                results.append(result)                all_scores.append(0.95)        print(tasks)        print(kwargs)        avg_score = sum(all_scores) / max(len(all_scores), 1)        results = [{            'result': results,            'score': avg_score        }]        return results    def fit(self, completions, num_epochs=5, **kwargs):        &quot;&quot;&quot; 模型训练 &quot;&quot;&quot;        print(&quot;开始训练&quot;)        if self.gen_train_data(project_id):            # 训练模型            return {'model_path': r'\runs\detect\TriCycle\weights\best.pt'}        else:            raise &quot;gen_train_data error&quot;    def gen_train_data(self, project_id):        &quot;&quot;&quot; 获取数据训练数据 &quot;&quot;&quot;        print(&quot;获取数据 project_id&quot;, project_id)        import zipfile        import glob        download_url = f'{HOSTNAME.rstrip(&quot;/&quot;)}/api/projects/{project_id}/export?export_type=COCO&amp;download_all_tasks=false&amp;download_resources=true'        response = requests.get(download_url, headers={'Authorization': f'Token {API_KEY}'})        zip_path = os.path.join(conf['workdir'], &quot;train.zip&quot;)        train_path = os.path.join(conf['workdir'], &quot;train&quot;)        with open(zip_path, 'wb') as file:            file.write(response.content)  # 通过二进制写文件的方式保存获取的内容            file.flush()        f = zipfile.ZipFile(zip_path)  # 创建压缩包对象        f.extractall(train_path)  # 压缩包解压缩        f.close()        os.remove(zip_path)        if not os.path.exists(os.path.join(train_path, &quot;images&quot;, str(project_id))):            os.makedirs(os.path.join(train_path, &quot;images&quot;, str(project_id)))        for img in glob.glob(os.path.join(train_path, &quot;images&quot;, &quot;*.jpg&quot;)):            basename = os.path.basename(img)            shutil.move(img, os.path.join(train_path, &quot;images&quot;, str(project_id), basename))        return True使用步骤启动后端服务: 分为三步，生成服务代码+启动服务+连接服务+训练模型第一步：生成服务代码1label-studio-ml init backend/model --script label_studio_backend/yolo_detection.py --forcelabel-studio-ml init 命令提供了一种根据后端模型自动生成后端服务代码的功能， model 为输出目录， --script 指定后端模型路径， --force 表示覆盖生成。该命令执行成功后会在 backend 目录下生成 model 目录主要包括了_wsgi.py、docker-compose.yml、Dockerfile、yolo_detection.py等文件第二步：启动服务如果有依赖的文件，需要自己复制到 model 目录下，接着启动后端服务123label-studio-ml start backend/model --host 0.0.0.0 -p 8888# orpython backend/model/_wsgi.py --host 0.0.0.0 -p 8888  # 方便debug启动成功的话，控制台会输出如下的日志12345678910=&gt; ROOT =  Q:\pyCharmWS\object_detection\smart_city_management\label_studio_backend\backend\yolo-detector=&gt; LABEL STUDIO HOSTNAME =  http://192.168.123.xx:8080 * Serving Flask app &quot;label_studio_ml.api&quot; (lazy loading) * Environment: production   WARNING: This is a development server. Do not use it in a production deployment.   Use a production WSGI server instead. * Debug mode: off[2023-03-29 14:21:59,286] [WARNING] [werkzeug::_log::225]  * Running on all addresses.   WARNING: This is a development server. Do not use it in a production deployment.[2023-03-29 14:21:59,286] [INFO] [werkzeug::_log::225]  * Running on http://10.10.0.xx:8888/ (Press CTRL+C to quit)PS: 测试的时候发现，直接执行_wsgi.py文件一样可以启动后端，跟踪label_studio_ml的server.py文件可以看到执行核心代码12345678910def main():    args, subargs = get_args()    if args.command == 'init':        create_dir(args)    elif args.command == 'start':        start_server(args, subargs)    elif args.command == 'deploy':        if args.provider == 'gcp':            deploy_to_gcp(args)第三步：连接服务在我们创建的前端项目中依次选择 Settings -&gt; Machine Learning -&gt; Add model ，然后输入后端地址 http://10.100.143.xxx:8888/(这里是后端的地址和端口)，点击保存此时我们从前端项目中打开待标记图片，前端会自动请求后端对其进行标记(调用后端的 predict 方法)，等待片刻后即可看见预标记结果，我们只需要大致核对无误后点击 submit 即可如果觉得每次打开图片都需要等待片刻才会收到后端预测结果比较费时，可以在 Settings -&gt; Machine Learning 设置中选择打开 Retrieve predictions when loading a task automatically ，此后前端会在我们每次打开项目时自动对所有任务进行自动预测，基本能够做到无等待第四步：训练模型在 Settings -&gt; Machine Learning 中点击后端服务的 Start Training 按钮，即可调用后端模型使用已标记信息进行训练也可以 Settings -&gt; Machine Learning 中允许模型自动训练，但训练频率过高会影响程序效率目标检测任务注意事项predict返回的坐标为convert_to_ls转换后的坐标值123456789101112131415161718192021222324252627282930# convert from LS percent units to pixels def convert_from_ls(result):    if 'original_width' not in result or 'original_height' not in result:        return None    value = result['value']    w, h = result['original_width'], result['original_height']    if all([key in value for key in ['x', 'y', 'width', 'height']]):        return w * value['x'] / 100.0, \               h * value['y'] / 100.0, \               w * value['width'] / 100.0, \               h * value['height'] / 100.0# convert from pixels to LS percent units def convert_to_ls(x, y, width, height, original_width, original_height):    return x / original_width * 100.0, y / original_height * 100.0, \           width / original_width * 100.0, height / original_height * 100# convert from LSoutput = convert_from_ls(task['annotations'][0]['result'][0])if output is None:    raise Exception('Wrong convert') pixel_x, pixel_y, pixel_width, pixel_height = outputprint(pixel_x, pixel_y, pixel_width, pixel_height)# convert back to LS x, y, width, height = convert_to_ls(pixel_x, pixel_y, pixel_width, pixel_height, 600, 403)print(x, y, width, height)例子Labelstudio的UIE半监督智能标注方案本地版，赶快用起来啦基于ner的模型实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v1.0@author: huangyc@file: uie_model.py@Description:            label-studio-ml init backend/model --script uie_model.py --force@time: 2023/12/14 17:04&quot;&quot;&quot;import jsonimport osfrom typing import List, Dictimport requestsfrom basic_support.logger.logger_config import loggerfrom label_studio_ml import modelfrom label_studio_ml.model import LabelStudioMLBasefrom label_studio_ml.utils import get_envmodel.LABEL_STUDIO_ML_BACKEND_V2 = Trueos.environ['HOSTNAME'] = 'http://192.168.xx.xx:8080'os.environ['API_KEY'] = 'd818922361ade7e2d570346d2bd6fe1668fd4e81'os.environ['LABEL_STUDIO_ML_BACKEND_V2'] = 'True'HOSTNAME = get_env('HOSTNAME')API_KEY = get_env('API_KEY')ROOT = os.path.join(os.path.dirname(__file__))model_url = 'http://region-46.seetacloud.com:12851/xxx'def anno(sentences: List[str]) -&gt; List[Dict]:    &quot;&quot;&quot;    content = [&quot;2018年10月12日，当事人办理个体工商户营业执照，经营场所位于金湖县园林南路5号自北向南第10间，经营范围为食品经营，新鲜水果零售等。&quot;]    resps = [{'日期时间': [{'text': '2018年10月12日', 'start': 0, 'end': 11, 'probability': 0.9213111485894672}],          '地名': [{'text': '金湖县', 'start': 33, 'end': 36, 'probability': 0.9889604263625813},                   {'text': '园林南路', 'start': 36, 'end': 40, 'probability': 0.8983399204114662}]}]    :param sentences:    :return:    &quot;&quot;&quot;    x = requests.post(model_url, json=sentences)    if x.status_code == 200:        resps = x.json()['response']    else:        raise Exception(&quot;访问异常&quot;)    predicts = []    for resp in resps:        result = []        scores = []        for label, v in resp.items():            for iv in v:                score = iv['probability']                scores.append(score)                res = {                    'from_name': 'label',                    'to_name': 'text',                    'type': 'labels',                    'value': {                        'start': iv['start'],                        'end': iv['end'],                        'score': score,                        'text': iv['text'],                        'labels': [label]                    }                }                result.append(res)        avg_score = round(sum(scores) / len(scores), 4) if scores else 0.0        predicts.append({&quot;result&quot;: result, 'score': avg_score, 'model_version': 'uie-ner-large'})    return predictsclass UieModel(LabelStudioMLBase):    def __init__(self, **kwargs):        super(UieModel, self).__init__(**kwargs)        # 按 mmdetection 的方式加载模型及权重        print(kwargs)        logger.info(f&quot;初始化完成&quot;)    def predict(self, tasks, **kwargs):        # 获取待标记图片        sentences = []        for data in tasks:            sentence = data['data']['text']            sentences.append(sentence)        return anno(sentences=sentences)    def fit(self, completions: List[Dict], workdir=None, **kwargs):        &quot;&quot;&quot;        模型训练        :param completions: 标注的样本        :param workdir:        :param kwargs:        :return:        &quot;&quot;&quot;        logger.info(f&quot;开始构建训练样本&quot;)        sample_path = self.gen_train_data_path(completions=completions)        return {'path': workdir, 'model_path': None, 'sample_path': sample_path}    def gen_train_data_path(self, completions: List[Dict]):        &quot;&quot;&quot;        获取数据训练数据        :param completions:        :return:        &quot;&quot;&quot;        samples = []        for completion in completions:            sentence = completion['data']['text']            m_id = completion['id']            created_at = completion[&quot;created_at&quot;]            updated_at = completion[&quot;updated_at&quot;]            labels = []            for annotation in completion['annotations']:                for label_info in annotation['result']:                    labels.append(label_info['value'])            sample = {&quot;text&quot;: sentence, &quot;id&quot;: m_id, &quot;label&quot;: labels, &quot;created_at&quot;: created_at, &quot;updated_at&quot;: updated_at}            samples.append(sample)        logger.info(f&quot;构建训练样本完毕，开始输出到文件&quot;)        sample_path = &quot;./doccano_ext.jsonl&quot;        with open(sample_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as outfile:            outfile.write(json.dumps(samples, ensure_ascii=False))        logger.info(f&quot;样本路径为：{sample_path}&quot;)        return sample_pathdoccanodoccano官网Roboflow如何使用 Roboflow 标注关键点</content>
	</entry>
	<entry>
		<title>pytorch学习_基础知识</title>
		<url>/article/53039.html</url>
		<content>PyTorch 是一个开源的机器学习库，广泛应用于计算机视觉和自然语言处理等人工智能领域。由Facebook的人工智能研究团队开发，它基于之前的Torch库。PyTorch以其高度灵活和动态的计算图特性，在科研领域尤其受到青睐。下面是对PyTorch基础知识的一些介绍：核心特性动态计算图：PyTorch 使用动态计算图（也称为Define-by-Run方法），这意味着计算图的构建是即时的，并且可以根据运行中的数据进行改变。这为复杂的动态输入和不同长度的输出提供了便利简洁的接口：PyTorch 提供了简洁直观的API，便于快速实现和调试模型，使得研究人员可以将更多时间投入到实验设计而非代码调试上Python优先：PyTorch 设计为符合Python语言习惯，并且可以无缝集成到Python生态中，与NumPy等库协同工作基础组件张量（Tensors）：张量是PyTorch中的基础数据结构，它类似于NumPy的ndarrays，但它也可以在GPU上运行以加速计算自动微分（Autograd）：PyTorch 的 autograd 模块提供了自动计算梯度的功能，对于实现神经网络中的反向传播算法至关重要神经网络（torch.nn）：torch.nn 模块包含了构建神经网络所需的所有元素。这些可重用的层（例如卷积层、线性层等）和损失函数可以帮助用户轻松构建复杂的网络结构优化（torch.optim）：PyTorch 提供了常用的优化算法，如SGD、Adam等，用于网络参数的迭代优化数据加载（torch.utils.data）：PyTorch 提供了数据加载和处理工具，方便用户创建数据加载管道，加速数据预处理和模型训练过程序列化工具（Serialization）：PyTorch 模型和张量可以通过 torch.save 轻松地序列化到磁盘，并通过 torch.load 进行反序列化CUDA集成PyTorch 提供了与NVIDIA CUDA的深度集成，允许张量和模型被无缝地在GPU上运行，大幅提升了计算速度社区和生态PyTorch 拥有活跃的社区，提供了大量预训练模型和开箱即用的工具。同时，它也是一些高级API（如FastAI）和框架（如Hugging Face的Transformers）的基础PyTorch 不仅适合于研究原型的开发，还能用于生产环境的部署它提供了一系列工具来支持模型的量化、蒸馏和优化，使其在不牺牲性能的情况下运行更快、占用资源更少。随着其持续发展和完善，PyTorch 已经成为了机器学习研究者和开发者的首选工具之一基本操作torch — PyTorch 2.2 documentationpytorch中文文档以下是一些常用的方法torch.is_tensor: 如果obj是一个pytorch张量，则返回True1234x=torch.tensor([1,2,3])torch.is_tensor(x)Out[0]: Truetorch.is_storage: 如何obj是一个pytorch storage对象，则返回True1234x=torch.tensor([1,2,3])torch.is_storage(x)Out[0]: Falsetorch.numel: 返回input 张量中的元素个数1234567a = torch.randn(1,2,3,4,5)torch.numel(a)Out[0]: 120a = torch.zeros(4,4)torch.numel(a)Out[1]: 16torch.set_printoptions: 设置打印选项参数:precision – 浮点数输出的精度位数 (默认为8 )threshold – 阈值，触发汇总显示而不是完全显示(repr)的数组元素的总数 （默认为1000）edgeitems – 汇总显示中，每维（轴）两端显示的项数（默认值为3）linewidth – 用于插入行间隔的每行字符数（默认为80）。Thresholded matricies will ignore this parameter.profile – pretty打印的完全默认值。 可以覆盖上述所有选项 (默认为short, full)创建操作张量创建函数torch.tensor(): 通过复制数据创建一个具有自动求导历史的张量(如果数据是一个张量)123456789101112131415161718&gt;&gt;&gt; torch.tensor([[0.1, 1.2], [2.2, 3.1], [4.9, 5.2]])tensor([[ 0.1000,  1.2000],        [ 2.2000,  3.1000],        [ 4.9000,  5.2000]])&gt;&gt;&gt; torch.tensor([0, 1])  # Type inference on datatensor([ 0,  1])&gt;&gt;&gt; torch.tensor([[0.11111, 0.222222, 0.3333333]],...              dtype=torch.float64,...              device=torch.device('cuda:0'))  # creates a double tensor on a CUDA devicetensor([[ 0.1111,  0.2222,  0.3333]], dtype=torch.float64, device='cuda:0')&gt;&gt;&gt; torch.tensor(3.14159)  # Create a zero-dimensional (scalar) tensortensor(3.1416)&gt;&gt;&gt; torch.tensor([])  # Create an empty tensor (of size (0,))tensor([])torch.sparse_coo_tensor(): 通过坐标格式的索引和值构建稀疏张量使用稀疏矩阵的一个主要优点是，在存储和计算上更加高效，特别是对于非常大的数据集。例如，在矩阵乘法或其他线性代数运算中，利用稀疏性可以显著减少不必要的乘法和加法计算，因为零元素与任何数相乘都是零，并且不会影响加法运算的结果1234567891011# 假设我们有2个非零元素分别在(0, 2)和(1, 0)的位置indices = torch.tensor([[0, 1], [2, 0]])  # 表示非零元素的坐标values = torch.tensor([3, 4])             # 这些非零元素的值# 创建COO格式的稀疏张量sparse_coo = torch.sparse_coo_tensor(indices, values, (2, 3))print(sparse_coo)tensor(indices=tensor([[0, 1],                       [2, 0]]),       values=tensor([3, 4]),       size=(2, 3), nnz=2, layout=torch.sparse_coo)torch.sparse_csr_tensor(): 通过压缩稀疏行格式的索引和值构建稀疏张量torch.sparse_csc_tensor(): 通过压缩稀疏列格式的索引和值构建稀疏张量1234567891011# 定义CSR格式的三个组件：行索引、列索引和值crow_indices = torch.tensor([0, 1, 2])col_indices = torch.tensor([0, 1])values = torch.tensor([1, 2])# 创建CSR格式的稀疏张量sparse_csr = torch.sparse_csr_tensor(crow_indices, col_indices, values)print(sparse_csr)tensor(crow_indices=tensor([0, 1, 2]),       col_indices=tensor([0, 1]),       values=tensor([1, 2]), size=(2, 2), nnz=2, layout=torch.sparse_csr)torch.sparse_bsr_tensor(): 通过块压缩稀疏行格式的索引和2维块构建稀疏张量torch.sparse_bsc_tensor(): 通过块压缩稀疏列格式的索引和2维块构建稀疏张量12345678910111213&gt;&gt;&gt; crow_indices = [0, 1, 2]&gt;&gt;&gt; col_indices = [0, 1]&gt;&gt;&gt; values = [[[1, 2], [3, 4]], [[5, 6], [7, 8]]]&gt;&gt;&gt; torch.sparse_bsr_tensor(torch.tensor(crow_indices, dtype=torch.int64),...                         torch.tensor(col_indices, dtype=torch.int64),...                         torch.tensor(values), dtype=torch.double)tensor(crow_indices=tensor([0, 1, 2]),       col_indices=tensor([0, 1]),       values=tensor([[[1., 2.],                       [3., 4.]],                      [[5., 6.],                       [7., 8.]]]), size=(2, 2), nnz=2, dtype=torch.float64,       layout=torch.sparse_bsr)数据类型转换函数torch.asarray(): 将对象转换为张量torch.as_tensor(): 将数据转换为张量，共享数据并尽可能保留自动求导历史torch.as_strided(): 创建一个具有指定大小、步长和存储偏移的现有张量的视图(不好理解)12345678910111213141516171819202122232425262728293031323334353637383940# 将对象转换为张量&gt;&gt;&gt; a = torch.tensor([1, 2, 3])&gt;&gt;&gt; # Shares memory with tensor 'a'&gt;&gt;&gt; b = torch.asarray(a)&gt;&gt;&gt; a.data_ptr() == b.data_ptr()True&gt;&gt;&gt; # Forces memory copy&gt;&gt;&gt; c = torch.asarray(a, copy=True)&gt;&gt;&gt; a.data_ptr() == c.data_ptr()False# 将数据转换为张量，共享数据并尽可能保留自动求导历史&gt;&gt;&gt; a = numpy.array([1, 2, 3])&gt;&gt;&gt; t = torch.as_tensor(a)&gt;&gt;&gt; ttensor([ 1,  2,  3])&gt;&gt;&gt; t[0] = -1&gt;&gt;&gt; aarray([-1,  2,  3])&gt;&gt;&gt; a = numpy.array([1, 2, 3])&gt;&gt;&gt; t = torch.as_tensor(a, device=torch.device('cuda'))&gt;&gt;&gt; ttensor([ 1,  2,  3])&gt;&gt;&gt; t[0] = -1&gt;&gt;&gt; aarray([1,  2,  3])# 创建一个具有指定大小、步长和存储偏移的现有张量的视图&gt;&gt;&gt; x = torch.randn(3, 3)&gt;&gt;&gt; xtensor([[ 0.9039,  0.6291,  1.0795],        [ 0.1586,  2.1939, -0.4900],        [-0.1909, -0.7503,  1.9355]])&gt;&gt;&gt; t = torch.as_strided(x, (2, 2), (1, 2))&gt;&gt;&gt; ttensor([[0.9039, 1.0795],        [0.6291, 0.1586]])&gt;&gt;&gt; t = torch.as_strided(input=x, size=(2, 2), stride=(1, 2), storage_offset=1)tensor([[0.6291, 0.1586],        [1.0795, 2.1939]])torch.from_file(): 从内存映射文件创建CPU张量torch.from_numpy(): 将numpy数组转换为张量torch.from_dlpack(): 将来自外部库的张量转换为PyTorch张量12345678910111213# 从内存映射文件创建CPU张量&gt;&gt;&gt; t = torch.randn(2, 5, dtype=torch.float64)&gt;&gt;&gt; t.numpy().tofile('storage.pt')&gt;&gt;&gt; t_mapped = torch.from_file('storage.pt', shared=False, size=10, dtype=torch.float64)# 将numpy数组转换为张量&gt;&gt;&gt; a = numpy.array([1, 2, 3])&gt;&gt;&gt; t = torch.from_numpy(a)&gt;&gt;&gt; ttensor([ 1,  2,  3])&gt;&gt;&gt; t[0] = -1&gt;&gt;&gt; aarray([-1,  2,  3])初始化填充函数torch.zeros(): 返回一个指定形状且用0填充的张量torch.zeros_like(): 返回一个与给定张量形状相同且用0填充的张量12345678910&gt;&gt;&gt; torch.zeros(2, 3)tensor([[ 0.,  0.,  0.],        [ 0.,  0.,  0.]])&gt;&gt;&gt; torch.zeros(5)tensor([ 0.,  0.,  0.,  0.,  0.])&gt;&gt;&gt; input = torch.empty(2, 3)&gt;&gt;&gt; torch.zeros_like(input)tensor([[ 0.,  0.,  0.],        [ 0.,  0.,  0.]])torch.ones(): 返回一个指定形状且用1填充的张量torch.ones_like(): 返回一个与给定张量形状相同且用1填充的张量12345678910&gt;&gt;&gt; torch.ones(2, 3)tensor([[ 1.,  1.,  1.],        [ 1.,  1.,  1.]])&gt;&gt;&gt; torch.ones(5)tensor([ 1.,  1.,  1.,  1.,  1.])&gt;&gt;&gt; input = torch.empty(2, 3)&gt;&gt;&gt; torch.ones_like(input)tensor([[ 1.,  1.,  1.],        [ 1.,  1.,  1.]])torch.arange(): 返回一个从start到end（不包含end）且步长为step的1维张量torch.range(): (未来版本弃用)返回一个从start到end（包含end）且步长为step的1维张量123456&gt;&gt;&gt; torch.arange(5)tensor([ 0,  1,  2,  3,  4])&gt;&gt;&gt; torch.arange(1, 4)tensor([ 1,  2,  3])&gt;&gt;&gt; torch.arange(1, 2.5, 0.5)tensor([ 1.0000,  1.5000,  2.0000])特定区间填充函数torch.linspace(): 返回一个从start到end（包括end）且在其中均匀分布的指定大小的1维张量torch.logspace(): 返回一个在对数刻度上从到（包括end）且均匀分布的指定大小的1维张量这两个函数生成的张量常常用于数据预处理、数学模拟、绘图等需要生成规则数列的场景1234567891011121314151617&gt;&gt;&gt; torch.linspace(start=3, end=10, steps=5)tensor([  3.0000,   4.7500,   6.5000,   8.2500,  10.0000])&gt;&gt;&gt; torch.linspace(-10, 10, steps=5)tensor([-10.,  -5.,   0.,   5.,  10.])&gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=5)tensor([-10.,  -5.,   0.,   5.,  10.])&gt;&gt;&gt; torch.linspace(start=-10, end=10, steps=1)tensor([-10.])&gt;&gt;&gt; torch.logspace(start=-10, end=10, steps=5)tensor([ 1.0000e-10,  1.0000e-05,  1.0000e+00,  1.0000e+05,  1.0000e+10])&gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=5)tensor([  1.2589,   2.1135,   3.5481,   5.9566,  10.0000])&gt;&gt;&gt; torch.logspace(start=0.1, end=1.0, steps=1)tensor([1.2589])&gt;&gt;&gt; torch.logspace(start=2, end=2, steps=1, base=2)tensor([4.0])其他辅助函数torch.eye(): 返回一个二维张量，对角线上为1，其他地方为0123456789&gt;&gt;&gt; torch.eye(3)tensor([[ 1.,  0.,  0.],        [ 0.,  1.,  0.],        [ 0.,  0.,  1.]])&gt;&gt;&gt; torch.eye(n=3, m=2)tensor([[1., 0.],        [0., 1.],        [0., 0.]])torch.empty(): 返回一个指定形状且未初始化的张量torch.empty_like(): 返回一个与给定张量形状相同且未初始化的张量torch.empty_strided(): 创建一个具有指定大小和跨度且未初始化的张量1234567891011121314151617&gt;&gt;&gt; torch.empty((2,3), dtype=torch.int64)tensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])&gt;&gt;&gt; a=torch.empty((2,3), dtype=torch.int32, device = 'cuda')&gt;&gt;&gt; torch.empty_like(a)tensor([[0, 0, 0],        [0, 0, 0]], device='cuda:0', dtype=torch.int32)&gt;&gt;&gt; a = torch.empty_strided((2, 3), (1, 2))&gt;&gt;&gt; atensor([[8.9683e-44, 4.4842e-44, 5.1239e+07],        [0.0000e+00, 0.0000e+00, 3.0705e-41]])&gt;&gt;&gt; a.stride()(1, 2)&gt;&gt;&gt; a.size()torch.Size([2, 3])torch.full(): 返回一个指定形状且用给定值填充的张量torch.full_like(): 返回一个与给定张量形状相同且用给定值填充的张量1234567&gt;&gt;&gt; torch.full((2, 3), fill_value=3.141592)tensor([[ 3.1416,  3.1416,  3.1416],        [ 3.1416,  3.1416,  3.1416]])&gt;&gt;&gt; torch.full_like(torch.empty((2,3)), fill_value=9)tensor([[9., 9., 9.],        [9., 9., 9.]])量化函数torch.quantize_per_tensor(): 将浮点张量转换为给定比例和零点的量化张量torch.quantize_per_channel(): 将浮点张量转换为按通道给定比例和零点的量化张量torch.dequantize(): 通过去量化量化张量来返回一个fp32张量复数和其他特殊类型函数torch.complex(): 构造一个其实部等于real、虚部等于imag的复数张量1234567&gt;&gt;&gt; real = torch.tensor([1, 2], dtype=torch.float32)&gt;&gt;&gt; imag = torch.tensor([3, 4], dtype=torch.float32)&gt;&gt;&gt; z = torch.complex(real, imag)&gt;&gt;&gt; ztensor([(1.+3.j), (2.+4.j)])&gt;&gt;&gt; z.dtypetorch.complex64torch.polar(): 根据极坐标的绝对值abs和角度angle构造复数张量的笛卡尔坐标123456&gt;&gt;&gt; import numpy as np&gt;&gt;&gt; abs = torch.tensor([1, 2], dtype=torch.float64)&gt;&gt;&gt; angle = torch.tensor([np.pi / 2, 5 * np.pi / 4], dtype=torch.float64)&gt;&gt;&gt; z = torch.polar(abs, angle)&gt;&gt;&gt; ztensor([(0.0000+1.0000j), (-1.4142-1.4142j)], dtype=torch.complex128)torch.heaviside(): 计算输入张量每个元素的Heaviside阶跃函数12345678&gt;&gt;&gt; input = torch.tensor([-1.5, 0, 2.0])&gt;&gt;&gt; values = torch.tensor([0.5])&gt;&gt;&gt; torch.heaviside(input, values)tensor([0.0000, 0.5000, 1.0000])&gt;&gt;&gt; values = torch.tensor([1.2, -2.0, 3.5])&gt;&gt;&gt; torch.heaviside(input, values)tensor([0., -2., 1.])这些函数在数据预处理、模型初始化和其他计算任务中非常有用。通过这些函数，你可以创建大小、形状、种类各异的张量来满足不同的需求索引|切片|连接|换位这部分主要分为索引和切片、合并和拼接、变换和重塑、元素添加与替换、搜索和条件操作、扩展与重复操作索引和切片argwhere: 返回非零元素的索引nonzero:  返回非零元素的索引argwhere 和 nonzero 函数都用于查找非零元素的索引，但它们返回的格式略有不同。在某些编程库中，argwhere 通常返回一个二维数组，其中每一行都是输入中非零元素的索引坐标；而 nonzero 返回的是一个元组，每个元素是一个一维数组，表示非零元素在各个维度上的位置1234567891011121314151617181920212223242526272829303132333435# argwhere&gt;&gt;&gt; t = torch.tensor([1, 0, 1])&gt;&gt;&gt; torch.argwhere(t)tensor([[0],        [2]])&gt;&gt;&gt; t = torch.tensor([[1, 0, 1], [0, 1, 1]])&gt;&gt;&gt; torch.argwhere(t)tensor([[0, 0],        [0, 2],        [1, 1],        [1, 2]])# nonzero&gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]))tensor([[ 0],        [ 1],        [ 2],        [ 4]])&gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],...                             [0.0, 0.4, 0.0, 0.0],...                             [0.0, 0.0, 1.2, 0.0],...                             [0.0, 0.0, 0.0,-0.4]]))tensor([[ 0,  0],        [ 1,  1],        [ 2,  2],        [ 3,  3]])&gt;&gt;&gt; torch.nonzero(torch.tensor([1, 1, 1, 0, 1]), as_tuple=True)(tensor([0, 1, 2, 4]),)&gt;&gt;&gt; torch.nonzero(torch.tensor([[0.6, 0.0, 0.0, 0.0],...                             [0.0, 0.4, 0.0, 0.0],...                             [0.0, 0.0, 1.2, 0.0],...                             [0.0, 0.0, 0.0,-0.4]]), as_tuple=True)(tensor([0, 1, 2, 3]), tensor([0, 1, 2, 3]))&gt;&gt;&gt; torch.nonzero(torch.tensor(5), as_tuple=True)(tensor([0]),)select: 在特定维度进行索引index_select: 根据索引选择数据input (Tensor) – the input tensor.dim (int) – the dimension in which we indexindex (IntTensor or LongTensor) – the 1-D tensor containing the indices to indexmasked_select: 根据布尔掩码选择数据123456789101112131415161718192021222324252627282930313233# 在特定维度进行索引tensor = [[1, 2], [3, 4], [5, 6]]selected_row = select(tensor, dim=0, index=1)print(selected_row) # 输出: [3, 4]# 根据索引选择数据&gt;&gt;&gt; x = torch.randn(3, 4)&gt;&gt;&gt; xtensor([[ 0.1427,  0.0231, -0.5414, -1.0009],        [-0.4664,  0.2647, -0.1228, -1.1068],        [-1.1734, -0.6571,  0.7230, -0.6004]])&gt;&gt;&gt; indices = torch.tensor([0, 2])&gt;&gt;&gt; torch.index_select(x, 0, indices)tensor([[ 0.1427,  0.0231, -0.5414, -1.0009],        [-1.1734, -0.6571,  0.7230, -0.6004]])&gt;&gt;&gt; torch.index_select(x, 1, indices)tensor([[ 0.1427, -0.5414],        [-0.4664, -0.1228],        [-1.1734,  0.7230]])# 根据布尔掩码选择数据&gt;&gt;&gt; x = torch.randn(3, 4)&gt;&gt;&gt; xtensor([[ 0.3552, -2.3825, -0.8297,  0.3477],        [-1.2035,  1.2252,  0.5002,  0.6248],        [ 0.1307, -2.0608,  0.1244,  2.0139]])&gt;&gt;&gt; mask = x.ge(0.5)&gt;&gt;&gt; masktensor([[False, False, False, False],        [False, True, True, True],        [False, False, False, True]])&gt;&gt;&gt; torch.masked_select(x, mask)tensor([ 1.2252,  0.5002,  0.6248,  2.0139])narrow: 缩小张量的一个维度input (Tensor) – the tensor to narrowdim (int) – the dimension along which to narrowstart (int or Tensor) – index of the element to start the narrowed dimension from. Can be negative, which means indexing from the end of dim. If Tensor, it must be an 0-dim integral Tensor (bools not allowed)length (int) – length of the narrowed dimension, must be weakly positive123456789101112&gt;&gt;&gt; x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])&gt;&gt;&gt; torch.narrow(x, 0, 0, 2)tensor([[ 1,  2,  3],        [ 4,  5,  6]])&gt;&gt;&gt; torch.narrow(x, 1, 1, 2)tensor([[ 2,  3],        [ 5,  6],        [ 8,  9]])&gt;&gt;&gt; torch.narrow(x, -1, torch.tensor(-1), 1)tensor([[3],        [6],        [9]])narrow_copy: narrow操作的复制版本take: 根据索引从输入张量中取元素take_along_dim: 沿指定维度根据索引取元素123456789101112131415# 根据索引从输入张量中取元素&gt;&gt;&gt; src = torch.tensor([[4, 3, 5],...                     [6, 7, 8]])&gt;&gt;&gt; torch.take(src, torch.tensor([0, 2, 5]))tensor([ 4,  5,  8])# 沿指定维度根据索引取元素&gt;&gt;&gt; t = torch.tensor([[10, 30, 20], [60, 40, 50]])&gt;&gt;&gt; max_idx = torch.argmax(t)&gt;&gt;&gt; torch.take_along_dim(t, max_idx)tensor([60])&gt;&gt;&gt; sorted_idx = torch.argsort(t, dim=1)&gt;&gt;&gt; torch.take_along_dim(t, sorted_idx, dim=1)tensor([[10, 20, 30],        [40, 50, 60]])unbind: 按维度解绑张量1234&gt;&gt;&gt; torch.unbind(torch.tensor([[1, 2, 3],&gt;&gt;&gt;                            [4, 5, 6],&gt;&gt;&gt;                            [7, 8, 9]]))(tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))unravel_index: 将平面索引转换为坐标索引12345678910111213141516171819202122232425262728&gt;&gt;&gt; import torch&gt;&gt;&gt; torch.unravel_index(torch.tensor(4), (3, 2))(tensor(2), tensor(0))&gt;&gt;&gt; torch.unravel_index(torch.tensor([4, 1]), (3, 2))(tensor([2, 0]), tensor([0, 1]))&gt;&gt;&gt; torch.unravel_index(torch.tensor([0, 1, 2, 3, 4, 5]), (3, 2))(tensor([0, 0, 1, 1, 2, 2]), tensor([0, 1, 0, 1, 0, 1]))&gt;&gt;&gt; torch.unravel_index(torch.tensor([1234, 5678]), (10, 10, 10, 10))(tensor([1, 5]), tensor([2, 6]), tensor([3, 7]), tensor([4, 8]))&gt;&gt;&gt; torch.unravel_index(torch.tensor([[1234], [5678]]), (10, 10, 10, 10))(tensor([[1], [5]]), tensor([[2], [6]]), tensor([[3], [7]]), tensor([[4], [8]]))&gt;&gt;&gt; torch.unravel_index(torch.tensor([[1234], [5678]]), (100, 100))(tensor([[12], [56]]), tensor([[34], [78]]))squeeze: 去除大小为1的维度unsqueeze: 在指定位置添加大小为1的维度12345678910111213141516171819202122232425# 压缩维度&gt;&gt;&gt; x = torch.zeros(2, 1, 2, 1, 2)&gt;&gt;&gt; x.size()torch.Size([2, 1, 2, 1, 2])&gt;&gt;&gt; y = torch.squeeze(x)&gt;&gt;&gt; y.size()torch.Size([2, 2, 2])&gt;&gt;&gt; y = torch.squeeze(x, 0)&gt;&gt;&gt; y.size()torch.Size([2, 1, 2, 1, 2])&gt;&gt;&gt; y = torch.squeeze(x, 1)&gt;&gt;&gt; y.size()torch.Size([2, 2, 1, 2])&gt;&gt;&gt; y = torch.squeeze(x, (1, 2, 3))torch.Size([2, 2, 2])# 增加维度&gt;&gt;&gt; x = torch.tensor([1, 2, 3, 4])&gt;&gt;&gt; torch.unsqueeze(x, 0)tensor([[ 1,  2,  3,  4]])&gt;&gt;&gt; torch.unsqueeze(x, 1)tensor([[ 1],        [ 2],        [ 3],        [ 4]])合并和拼接numpy中的hstack()、vstack()、stack()、concatenate()函数详解cat, concat, concatenate: 将序列的张量在指定维度连接(concat和concatenate是cat的别名)12345678910111213141516&gt;&gt;&gt; x = torch.randn(2, 3)&gt;&gt;&gt; xtensor([[ 0.6580, -1.0969, -0.4614],        [-0.1034, -0.5790,  0.1497]])&gt;&gt;&gt; torch.cat((x, x, x), 0)tensor([[ 0.6580, -1.0969, -0.4614],        [-0.1034, -0.5790,  0.1497],        [ 0.6580, -1.0969, -0.4614],        [-0.1034, -0.5790,  0.1497],        [ 0.6580, -1.0969, -0.4614],        [-0.1034, -0.5790,  0.1497]])&gt;&gt;&gt; torch.cat((x, x, x), 1)tensor([[ 0.6580, -1.0969, -0.4614,  0.6580, -1.0969, -0.4614,  0.6580,         -1.0969, -0.4614],        [-0.1034, -0.5790,  0.1497, -0.1034, -0.5790,  0.1497, -0.1034,         -0.5790,  0.1497]])chunk: 把张量分成指定数量的块12345678910111213141516171819&gt;&gt;&gt; torch.arange(11).chunk(6)(tensor([0, 1]), tensor([2, 3]), tensor([4, 5]), tensor([6, 7]), tensor([8, 9]), tensor([10]))# 创建一个张量x = torch.tensor([1, 2, 3, 4, 5])# 将这个张量分割成3个块chunks = torch.chunk(x, chunks=3, dim=0)# 输出分割后的块for i, chunk in enumerate(chunks):    print(f&quot;Chunk {i}: {chunk}&quot;)Chunk 0: tensor([1, 2])Chunk 1: tensor([3, 4])Chunk 2: tensor([5])    column_stack: 按列堆叠张量创建新张量123456789101112131415&gt;&gt;&gt; a = torch.tensor([1, 2, 3])&gt;&gt;&gt; b = torch.tensor([4, 5, 6])&gt;&gt;&gt; torch.column_stack((a, b))tensor([[1, 4],    [2, 5],    [3, 6]])&gt;&gt;&gt; a = torch.arange(5)&gt;&gt;&gt; b = torch.arange(10).reshape(5, 2)&gt;&gt;&gt; torch.column_stack((a, b, b))tensor([[0, 0, 1, 0, 1],        [1, 2, 3, 2, 3],        [2, 4, 5, 4, 5],        [3, 6, 7, 6, 7],        [4, 8, 9, 8, 9]])hstack: 水平方向堆叠张量vstack(别名row_stack): 垂直方向堆叠张量dstack: 深度方向堆叠张量1234567891011121314151617181920212223242526272829303132333435363738394041# 水平方向堆叠张量&gt;&gt;&gt; a = torch.tensor([1, 2, 3])&gt;&gt;&gt; b = torch.tensor([4, 5, 6])&gt;&gt;&gt; torch.hstack((a,b))tensor([1, 2, 3, 4, 5, 6])&gt;&gt;&gt; a = torch.tensor([[1],[2],[3]])&gt;&gt;&gt; b = torch.tensor([[4],[5],[6]])&gt;&gt;&gt; torch.hstack((a,b))tensor([[1, 4],        [2, 5],        [3, 6]])# 垂直方向堆叠张量&gt;&gt;&gt; a = torch.tensor([1, 2, 3])&gt;&gt;&gt; b = torch.tensor([4, 5, 6])&gt;&gt;&gt; torch.vstack((a,b))tensor([[1, 2, 3],        [4, 5, 6]])&gt;&gt;&gt; a = torch.tensor([[1],[2],[3]])&gt;&gt;&gt; b = torch.tensor([[4],[5],[6]])&gt;&gt;&gt; torch.vstack((a,b))tensor([[1],        [2],        [3],        [4],        [5],        [6]])# 深度方向堆叠张量&gt;&gt;&gt; a = torch.tensor([1, 2, 3])&gt;&gt;&gt; b = torch.tensor([4, 5, 6])&gt;&gt;&gt; torch.dstack((a,b))tensor([[[1, 4],         [2, 5],         [3, 6]]])&gt;&gt;&gt; a = torch.tensor([[1],[2],[3]])&gt;&gt;&gt; b = torch.tensor([[4],[5],[6]])&gt;&gt;&gt; torch.dstack((a,b))tensor([[[1, 4]],        [[2, 5]],        [[3, 6]]])torch.dstack 和 torch.column_stack 函数都是用于堆叠张量的函数，但它们在堆叠的细节上有所不同12345678910111213141516171819202122232425262728A = torch.tensor([[1, 2, 3],                  [4, 5, 6]])B = torch.tensor([[7, 8, 9],                  [10, 11, 12]])A.shapeOut[27]: torch.Size([2, 3])# dstack是一整个对象堆叠dstack_result = torch.dstack((A, B)) dstack_resultOut[30]: tensor([[[ 1,  7],         [ 2,  8],         [ 3,  9]],        [[ 4, 10],         [ 5, 11],         [ 6, 12]]])dstack_result.shapeOut[31]: torch.Size([2, 3, 2])# column_stack专门用于二维张量（矩阵），它会将这些矩阵堆叠成一个更宽的矩阵（即增加列）column_stack_result = torch.column_stack((A, B))column_stack_resultOut[33]: tensor([[ 1,  2,  3,  7,  8,  9],        [ 4,  5,  6, 10, 11, 12]])column_stack_result.shapeOut[34]: torch.Size([2, 6])stack: 在新维度上连接张量序列pytorch的hstack、vstack、dstack、column_stack以及stack函数之间的区别和联系这些堆叠函数之间的联系在于它们的核心目的：将多个张量组合成一个新的、更大的张量不同的函数根据堆叠的方向（尺寸或维度）和具体的操作细节来区分，下面是它们之间联系的一个概览：维度方向的联系：hstack（水平堆叠）通常用于增加列数，适用于1D和2D张量，对于1D张量会先将其视作列向量vstack（垂直堆叠）常用于增加行数，也适用于1D和2D张量，对于1D张量会先将其视作行向量dstack（深度堆叠）是在第三个维度上进行堆叠，适用于创建或扩展为3D张量的情况column_stack与hstack相似，但它是专门设计来处理1D张量，将它们作为列向量来堆叠成2D张量的；对于2D张量，它的行为与hstack相同stack是一个更通用的函数，可以在指定的任何维度上进行堆叠，而不局限于特定的堆叠方向。它总是增加一个新的维度来堆叠张量操作联系：所有这些函数都是用来组合张量的，但是stack函数会创建一个新的维度，而其他函数（hstack, vstack, dstack, column_stack）则在现有的维度上进行操作hstack, vstack, dstack, column_stack可以看作是stack的特例，它们在指定的一个特定的现有维度上进行操作（hstack在最后一个维度，vstack在第一个维度，dstack在第三个维度，column_stack针对1D张量在第二个新建维度，对2D张量在最后一个维度）使用场景联系：当你想要在特定的轴方向上组合数据，而不想增加新的维度时，你会选择使用hstack, vstack, dstack, 或 column_stack当你需要在新的维度上堆叠张量时（例如，在时间序列数据或不同样本之间），你会选择使用stack在实际使用中，选择哪一个函数取决于你的具体需求以及你要操作的张量的维度。这些函数提供了方便的方式来对数据进行重构和整合，这是在准备数据集、构建深度学习模型等场景中非常常见的需求hsplit: 水平方向分割张量vsplit: 垂直方向分割张量dsplit: 深度方向分割张量split: 分割张量成多个块，函数将张量分割成特定大小的块。你可以指定每个块的大小，或者传递一个包含每个块大小的列表。如果张量不能均匀分割，最后一块的大小将小于前面的块tensor_split: 沿特定维度分割张量，基于索引来分割张量的。你可以指定一个分割点的索引列表，函数会在这些索引处分割张量。这些索引指的是分割后每个新张量的第一个元素的索引1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586&gt;&gt;&gt; t = torch.arange(16.0).reshape(4,4)&gt;&gt;&gt; ttensor([[ 0.,  1.,  2.,  3.],        [ 4.,  5.,  6.,  7.],        [ 8.,  9., 10., 11.],        [12., 13., 14., 15.]])# 水平方向分割张量&gt;&gt;&gt; torch.hsplit(t, 2)(tensor([[ 0.,  1.],         [ 4.,  5.],         [ 8.,  9.],         [12., 13.]]), tensor([[ 2.,  3.],         [ 6.,  7.],         [10., 11.],         [14., 15.]]))&gt;&gt;&gt; torch.hsplit(t, [3, 6])(tensor([[ 0.,  1.,  2.],         [ 4.,  5.,  6.],         [ 8.,  9., 10.],         [12., 13., 14.]]), tensor([[ 3.],         [ 7.],         [11.],         [15.]]), tensor([], size=(4, 0)))# 垂直方向分割张量&gt;&gt;&gt; torch.vsplit(t, 2)(tensor([[0., 1., 2., 3.],         [4., 5., 6., 7.]]), tensor([[ 8.,  9., 10., 11.],         [12., 13., 14., 15.]]))&gt;&gt;&gt; torch.vsplit(t, [3, 6])(tensor([[ 0.,  1.,  2.,  3.],         [ 4.,  5.,  6.,  7.],         [ 8.,  9., 10., 11.]]), tensor([[12., 13., 14., 15.]]), tensor([], size=(0, 4)))# 深度方向分割张量&gt;&gt;&gt; t = torch.arange(16.0).reshape(2, 2, 4)&gt;&gt;&gt; ttensor([[[ 0.,  1.,  2.,  3.],         [ 4.,  5.,  6.,  7.]],        [[ 8.,  9., 10., 11.],         [12., 13., 14., 15.]]])&gt;&gt;&gt; torch.dsplit(t, 2)(tensor([[[ 0.,  1.],        [ 4.,  5.]],       [[ 8.,  9.],        [12., 13.]]]), tensor([[[ 2.,  3.],          [ 6.,  7.]],         [[10., 11.],          [14., 15.]]]))&gt;&gt;&gt; torch.dsplit(t, [3, 6])(tensor([[[ 0.,  1.,  2.],          [ 4.,  5.,  6.]],         [[ 8.,  9., 10.],          [12., 13., 14.]]]), tensor([[[ 3.],          [ 7.]],         [[11.],          [15.]]]), tensor([], size=(2, 2, 0)))# 分割张量成多个块&gt;&gt;&gt; torch.split(t, 2, dim=0)(tensor([[0., 1., 2., 3.],         [4., 5., 6., 7.]]), tensor([[ 8.,  9., 10., 11.],         [12., 13., 14., 15.]]))&gt;&gt;&gt; torch.split(t, [1, 3], dim=0)(tensor([[0., 1., 2., 3.]]), tensor([[ 4.,  5.,  6.,  7.],         [ 8.,  9., 10., 11.],         [12., 13., 14., 15.]]))# 沿特定维度分割张量&gt;&gt;&gt; torch.tensor_split(t, [1, 2], dim=0)(tensor([[0., 1., 2., 3.]]), tensor([[4., 5., 6., 7.]]), tensor([[ 8.,  9., 10., 11.],         [12., 13., 14., 15.]]))变换和重塑adjoint: 返回共轭的张量，并交换最后两维conj: 返回共轭位翻转的张量视图gather: 沿指定维度聚集值1torch.gather(input, dim, index, *, sparse_grad=False) -&gt; Tensor其中参数的意义如下：input 是要从中提取数据的张量dim 是要沿着哪个维度进行提取index 是与 input 张量在除了 dim 指定的维度外具有相同大小的张量，包含了要提取的元素的索引sparse_grad 是布尔值，用于指示是否进行稀疏梯度的计算；通常用于高级用途12345678910111213141516# 创建一个 3x3 的矩阵input_tensor = torch.tensor([[1, 2, 3],                             [4, 5, 6],                             [7, 8, 9]])# 创建一个索引，用于选择每一行的第二个元素index = torch.tensor([[1], [1], [1]])# 使用 gather 来提取元素，dim=1 表示沿着列的方向进行操作torch.gather(input_tensor, 1, index)# 输出：[[2], [5], [8]]&gt;&gt;&gt; t = torch.tensor([[1, 2], [3, 4]])&gt;&gt;&gt; torch.gather(t, 1, torch.tensor([[0, 0], [1, 0]]))tensor([[ 1,  1],        [ 4,  3]])movedim(别名moveaxis): 移动张量维度位置的函数，这个操作可以让你指定某个维度（或多个维度）从它的原始位置移动到一个新的位置123456789101112131415161718192021222324252627282930&gt;&gt;&gt; t = torch.randn(3,2,1)&gt;&gt;&gt; ttensor([[[-0.3362],        [-0.8437]],        [[-0.9627],        [ 0.1727]],        [[ 0.5173],        [-0.1398]]])&gt;&gt;&gt; torch.moveaxis(t, 1, 0).shapetorch.Size([2, 3, 1])&gt;&gt;&gt; torch.moveaxis(t, 1, 0)tensor([[[-0.3362],        [-0.9627],        [ 0.5173]],        [[-0.8437],        [ 0.1727],        [-0.1398]]])&gt;&gt;&gt; torch.moveaxis(t, (1, 2), (0, 1)).shapetorch.Size([2, 1, 3])&gt;&gt;&gt; torch.moveaxis(t, (1, 2), (0, 1))tensor([[[-0.3362, -0.9627,  0.5173]],        [[-0.8437,  0.1727, -0.1398]]])tensor = torch.randn(10, 3, 5)torch.movedim(tensor, 1, 0).size()Out[78]: torch.Size([3, 10, 5])permute: 重新排列张量的维度，重组tensor维度，支持高维操作，tensor.permute(dim0, dim1, … dimn)，表示原本的dim0放在第0维度，dim1放在第1维度，…, dimn放在第n维度，必须将所有维度写上reshape: 改变张量的形状，需要指定最终的形状transpose(等价于swapaxes、swapdims): 转置张量的维度捋清pytorch的transpose、permute、view、reshape、contiguouspermute可以完全替代transpose，transpose不能替代permute12345678910111213141516171819202122232425# permute重新排列张量的维度&gt;&gt;&gt; x = torch.randn(2, 3, 5)&gt;&gt;&gt; x.size()torch.Size([2, 3, 5])&gt;&gt;&gt; torch.permute(x, (2, 0, 1)).size()torch.Size([5, 2, 3])# reshape改变张量的形状&gt;&gt;&gt; a = torch.arange(4.)&gt;&gt;&gt; torch.reshape(a, (2, 2))tensor([[ 0.,  1.],        [ 2.,  3.]])&gt;&gt;&gt; b = torch.tensor([[0, 1], [2, 3]])&gt;&gt;&gt; torch.reshape(b, (-1,))tensor([ 0,  1,  2,  3])# transpose转置张量的维度&gt;&gt;&gt; x = torch.randn(2, 3)&gt;&gt;&gt; xtensor([[ 1.0028, -0.9893,  0.5809],        [-0.1669,  0.7299,  0.4942]])&gt;&gt;&gt; torch.transpose(x, dim0=0, dim1=1)tensor([[ 1.0028, -0.1669],        [-0.9893,  0.7299],        [ 0.5809,  0.4942]])t: 转置二维张量的维度期望输入是一个二维或二维以下的张量，并交换维度0和1当输入是一个零维或一维张量时，返回的张量保持不变。当输入是一个二维张量时，这相当于 transpose(input, 0, 1)123456789101112131415161718&gt;&gt;&gt; x = torch.randn(())&gt;&gt;&gt; xtensor(0.1995)&gt;&gt;&gt; torch.t(x)tensor(0.1995)&gt;&gt;&gt; x = torch.randn(3)&gt;&gt;&gt; xtensor([ 2.4320, -0.4608,  0.7702])&gt;&gt;&gt; torch.t(x)tensor([ 2.4320, -0.4608,  0.7702])&gt;&gt;&gt; x = torch.randn(2, 3)&gt;&gt;&gt; xtensor([[ 0.4875,  0.9158, -0.5872],        [ 0.3938, -0.6929,  0.6932]])&gt;&gt;&gt; torch.t(x)tensor([[ 0.4875,  0.3938],        [ 0.9158, -0.6929],        [-0.5872,  0.6932]])元素添加与替换index_add: 根据索引向张量添加元素index_copy: 根据索引复制元素到张量index_reduce: 在指定维度上，根据索引减少元素12345678910111213141516&gt;&gt;&gt; x = torch.ones(5, 3)&gt;&gt;&gt; t = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)&gt;&gt;&gt; index = torch.tensor([0, 4, 2])&gt;&gt;&gt; x.index_add(0, index=index, source=t)tensor([[  2.,   3.,   4.],        [  1.,   1.,   1.],        [  8.,   9.,  10.],        [  1.,   1.,   1.],        [  5.,   6.,   7.]])&gt;&gt;&gt; x.index_add(0, index, t, alpha=-1)tensor([[ 0., -1., -2.],        [ 1.,  1.,  1.],        [-6., -7., -8.],        [ 1.,  1.,  1.],        [-3., -4., -5.]])scatter, scatter_add, scatter_reduce: 根据索引分散和添加元素1scatter(output, dim, index, src) catter函数就是把src数组中的数据重新分配到output数组当中，index数组中表示了要把src数组中的数据分配到output数组中的位置，若未指定，则填充01234567891011&gt;&gt;&gt; input = torch.randn(2, 4)&gt;&gt;&gt; print(input)tensor([[ 1.4782, -1.1345, -1.1457, -0.6050],        [-0.4183, -0.0229,  1.2361, -1.7747]])&gt;&gt;&gt; output = torch.zeros(2, 5)&gt;&gt;&gt; index = torch.tensor([[3, 1, 2, 0], [1, 2, 0, 3]])&gt;&gt;&gt; output = output.scatter(dim=1, index=index, src=input)&gt;&gt;&gt; print(output)tensor([[-0.6050, -1.1345, -1.1457,  1.4782,  0.0000],        [ 1.2361, -0.4183, -0.0229, -1.7747,  0.0000]])一般scatter用于生成onehot向量，如下所示12345678&gt;&gt;&gt; index = torch.tensor([[1], [2], [0], [3]])&gt;&gt;&gt; onehot = torch.zeros(4, 4)&gt;&gt;&gt; onehot.scatter_(1, index, 1)&gt;&gt;&gt; print(onehot)tensor([[0., 1., 0., 0.],        [0., 0., 1., 0.],        [1., 0., 0., 0.],        [0., 0., 0., 1.]])diagonal_scatter: 沿对角线分散元素select_scatter: 在给定索引处分散元素slice_scatter: 在给定维度上分散元素搜索和条件操作where: 根据条件从两个张量中选择元素123456789101112131415161718192021222324&gt;&gt;&gt; x = torch.randn(3, 2)&gt;&gt;&gt; y = torch.ones(3, 2)&gt;&gt;&gt; xtensor([[-0.4620,  0.3139],        [ 0.3898, -0.7197],        [ 0.0478, -0.1657]])&gt;&gt;&gt; torch.where(condition=x &gt; 0, input=1.0, other=0.0)tensor([[0., 1.],        [1., 0.],        [1., 0.]])&gt;&gt;&gt; torch.where(condition=x &gt; 0, input=x, other=y)tensor([[ 1.0000,  0.3139],        [ 0.3898,  1.0000],        [ 0.0478,  1.0000]])&gt;&gt;&gt; x = torch.randn(2, 2, dtype=torch.double)&gt;&gt;&gt; xtensor([[ 1.0779,  0.0383],        [-0.8785, -1.1089]], dtype=torch.float64)&gt;&gt;&gt; torch.where(condition=x &gt; 0, input=x, other=0.)tensor([[1.0779, 0.0383],        [0.0000, 0.0000]], dtype=torch.float64)扩展与重复操作tile: 通过重复张量的元素来构建新张量12345678910&gt;&gt;&gt; x = torch.tensor([1, 2, 3])&gt;&gt;&gt; x.tile((2,))tensor([1, 2, 3, 1, 2, 3])&gt;&gt;&gt; y = torch.tensor([[1, 2], [3, 4]])&gt;&gt;&gt; torch.tile(y, (2, 2))tensor([[1, 2, 1, 2],        [3, 4, 3, 4],        [1, 2, 1, 2],        [3, 4, 3, 4]])随机抽样随机种子torch.seed: 设置torch cpu随机数种子torch.manual_seed: 设置torch cpu随机数种子，torch.manual_seed(seed)torch.cuda.manual_seed: 设置torch cuda随机数种子torch.initial_seed: 查看设置的种子值123456789101112torch.seed()Out[112]: 2362131181677400torch.initial_seed()Out[113]: 2362131181677400torch.manual_seed(101)Out[114]: &lt;torch._C.Generator at 0x137cd248b10&gt;torch.initial_seed()Out[115]: 101torch.cuda.manual_seed(0)torch.initial_seed()Out[116]: 0get_rng_state(): 返回当前随机数生成器的状态。这个状态是一个torch.ByteTensor，它包含了RNG内部的所有状态信息，使得RNG可以在这个状态下继续生成随机数序列。这允许你在某个特定点“保存”RNG的状态，然后在需要的时候恢复到这个状态set_rng_state(state): 设置随机数生成器的状态。state应该是通过get_rng_state()函数获取的状态张量。这个函数用于恢复RNG到一个特定的状态，这样可以从那个状态开始重新生成相同的随机数序列随机采样函数常见的概率分布参考 — 兼一书虫-机器学习概率论(1)torch.rand(): 创建一个具有给定形状的张量，并用区间[0, 1)内的均匀分布的随机数填充torch.rand_like(): 返回一个与给定张量形状相同的张量，并用区间[0, 1)内的均匀分布的随机数填充123456789&gt;&gt;&gt; torch.rand(4)tensor([ 0.5204,  0.2503,  0.3525,  0.5673])&gt;&gt;&gt; torch.rand(2, 3)tensor([[ 0.8237,  0.5781,  0.6879],        [ 0.3816,  0.7249,  0.0998]])&gt;&gt;&gt; torch.rand_like(torch.rand(2, 3))tensor([[0.3885, 0.9888, 0.4838],        [0.8154, 0.6068, 0.6895]])torch.randn(): 返回一个具有给定形状的张量，并用标准正态分布的随机数填充torch.randn_like(): 返回一个与给定张量形状相同的张量，并用标准正态分布的随机数填充123456789&gt;&gt;&gt; torch.randn(4)tensor([-2.1436,  0.9966,  2.3426, -0.6366])&gt;&gt;&gt; torch.randn(2, 3)tensor([[ 1.5954,  2.8929, -1.0923],        [ 1.1719, -0.4709, -0.1996]])&gt;&gt;&gt; torch.randn_like(torch.randn(2, 3))tensor([[ 0.9979,  0.0471, -1.1305],        [ 0.7216, -0.0747,  0.0610]])torch.randint(): 返回一个具有给定形状的张量，并用区间[low, high)内的随机整数填充torch.randint_like(): 返回一个与给定张量形状相同的张量，并用区间[low, high)内的随机整数填充1234567891011&gt;&gt;&gt; torch.randint(low=3, high=5, size=(3,))tensor([4, 3, 4])&gt;&gt;&gt; torch.randint(10, (2, 2))tensor([[0, 2],        [5, 5]])&gt;&gt;&gt; torch.randint(3, 10, (2, 2))tensor([[4, 5],        [6, 7]])&gt;&gt;&gt; torch.randint_like(input=torch.randint(low=3, high=5, size=(3,)), low=6, high=10)tensor([8, 9, 9])torch.randperm(): 返回一个从0到给定参数n - 1的整数的随机排列12&gt;&gt;&gt; torch.randperm(4)tensor([2, 1, 0, 3])torch.bernoulli: 从伯努利分布中提取二进制随机数（0或1），输入张量应为包含用于绘制二进制随机数的概率的张量因此，输入中的所有值都必须在以下范围内(0,1)1234567891011121314151617181920&gt;&gt;&gt; a = torch.empty(3, 3).uniform_(0, 1)  # generate a uniform random matrix with range [0, 1]&gt;&gt;&gt; atensor([[ 0.1737,  0.0950,  0.3609],        [ 0.7148,  0.0289,  0.2676],        [ 0.9456,  0.8937,  0.7202]])&gt;&gt;&gt; torch.bernoulli(a)tensor([[ 1.,  0.,  0.],        [ 0.,  0.,  0.],        [ 1.,  1.,  1.]])&gt;&gt;&gt; a = torch.ones(3, 3) # probability of drawing &quot;1&quot; is 1&gt;&gt;&gt; torch.bernoulli(a)tensor([[ 1.,  1.,  1.],        [ 1.,  1.,  1.],        [ 1.,  1.,  1.]])&gt;&gt;&gt; a = torch.zeros(3, 3) # probability of drawing &quot;1&quot; is 0&gt;&gt;&gt; torch.bernoulli(a)tensor([[ 0.,  0.,  0.],        [ 0.,  0.,  0.],        [ 0.,  0.,  0.]])torch.poisson: 泊松分布用于计算一个事件在平均价值率(时间)的一定时间内发生的可能性。泊松分布是一个离散的概率分布123456&gt;&gt;&gt; rates = torch.rand(4, 4) * 5  # rate parameter between 0 and 5&gt;&gt;&gt; torch.poisson(rates)tensor([[9., 1., 3., 5.],        [8., 6., 6., 0.],        [0., 4., 5., 3.],        [2., 1., 4., 2.]])torch.multinomial: 对input的每一行做n_samples次取值，输出的张量是每一次取值时input张量对应行的下标input (Tensor) – 包含概率值的张量num_samples (int) – 抽取的样本数replacement (bool, optional) – 布尔值，决定是否能重复抽取out (Tensor, optional) – 结果张量12345678weights = torch.Tensor([0, 10, 3, 0])torch.multinomial(weights, 4)Out[0]: tensor([2, 1, 0, 3])# replacement=True时 概率为0的没机会被取到torch.multinomial(weights, 4, replacement=True)Out[1]: tensor([2, 1, 1, 1])torch.normal: 返回一个张量，包含从给定参数means,std的离散正态分布中抽取随机数12345678910torch.normal(mean=torch.arange(1., 11.), std=torch.arange(1, 0, -0.1))Out[0]: tensor([ 0.9732,  2.0833,  2.5282,  4.3588,  5.4837,  5.1150,  7.0366,  7.9774,         9.1679, 10.0248])torch.normal(mean=0.5, std=torch.arange(1., 6.))Out[1]: tensor([ 0.7067,  2.4856, -2.1957, -4.3114, 16.2506])torch.normal(mean=torch.arange(1., 6.))Out[2]: tensor([0.7835, 4.6096, 2.7244, 5.2810, 4.8413])序列化torch.save: 保存一个对象到一个硬盘文件上 参考: Recommended approach for saving a model torch.save(obj, f, pickle_module=pickle, pickle_protocol=DEFAULT_PROTOCOL, _use_new_zipfile_serialization=True)obj – 保存对象f － 类文件对象 (返回文件描述符)或一个保存文件名的字符串pickle_module – 用于pickling元数据和对象的模块pickle_protocol – 指定pickle protocal 可以覆盖默认参数12345678x = torch.tensor([0, 1, 2, 3, 4])# Save to filetorch.save(x, 'tensor.pt')# Save to io.BytesIO bufferbuffer = io.BytesIO()torch.save(x, buffer)torch.load: 从磁盘文件中读取一个通过torch.save()保存的对象torch.load(f, map_location=None, pickle_module=pickle, , weights_only=False, pickle_load_args*)12345678torch.load('tensors.pt', encoding='ascii')torch.load('tensors.pt', map_location=torch.device('cpu'))torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})# Load from io.BytesIO bufferwith open('tensor.pt', 'rb') as f:    buffer = io.BytesIO(f.read())torch.load(buffer)并行化在PyTorch中，CPU操作可以通过多线程来并行化，以此提高性能。这里涉及到两种形式的并行化：intra-op并行和inter-op并行。下面是关于这些函数的解释：get_num_threads(): 这个函数返回用于在CPU上并行执行操作（即intra-op并行）的线程数。Intra-op并行是指单个操作（如矩阵乘法）内部的并行执行。PyTorch会尝试使用所有可用的CPU核心来加速这些操作set_num_threads(int): 这个函数用来设置在CPU上进行intra-op并行操作时使用的线程数。如果你想限制PyTorch使用的CPU核心数量，可以调用这个函数get_num_interop_threads(): 这个函数返回用于CPU上的inter-op并行的线程数。Inter-op并行是指不同操作之间的并行执行。例如，如果你有多个不依赖于彼此的操作，PyTorch可以同时执行它们以提高效率set_num_interop_threads(int): 这个函数用来设置用于inter-op并行的线程数。通过设定线程数，可以控制同时进行的不同操作的数量在多核CPU上，适当地设置这些值可以帮助你更好地利用系统资源，提高程序的运行效率。然而，如果设置的线程数太多，可能会导致线程竞争和上下文切换的开销，反而降低性能通常默认设置是已经针对性能进行了优化，但是在特定的系统和应用场景下，手动调整这些值可以获得更佳的性能表现梯度管理在PyTorch中，梯度计算对于训练神经网络是必要的，因为它们用于优化模型的参数。然而，在某些情况下，比如在模型评估或应用阶段，你可能不需要计算梯度。梯度计算会占用额外的内存和计算资源，禁用它们可以提高效率。为了方便地开启和关闭梯度计算，PyTorch提供了几个上下文管理器：torch.no_grad():123with torch.no_grad():    # 在这个代码块中，所有的操作都不会跟踪梯度    predictions = model(inputs)在这个例子中，model(inputs) 的执行不会计算梯度，这对于模型推断（inference）阶段非常有用，因为它减少了内存消耗并提高了计算速度torch.enable_grad():12345with torch.enable_grad():    # 在这个代码块中，梯度计算是启用的    predictions = model(inputs)    loss = loss_fn(predictions, targets)    loss.backward()这里，即使全局梯度计算被禁用，torch.enable_grad() 仍可以在其作用域内启用梯度计算，以便计算损失函数的梯度torch.set_grad_enabled():123456torch.set_grad_enabled(mode=True) # 启用梯度计算# 后续操作将会跟踪梯度predictions = model(inputs)loss = loss_fn(predictions, targets)loss.backward()torch.set_grad_enabled(mode=False) # 禁用梯度计算在这里，使用torch.set_grad_enabled()函数来全局地控制是否计算梯度。传递True或False可以分别开启或关闭梯度计算torch.is_grad_enabled():1print(torch.is_grad_enabled()) # 打印当前是否启用了梯度计算这个函数用来检查当前是否启用了梯度计算torch.inference_mode():123with torch.inference_mode():    # 在这个代码块中，所有的操作都不会跟踪梯度，且某些优化会被应用以加速推断    predictions = model(inputs)torch.inference_mode() 更适合用在推断阶段，相比torch.no_grad()，它会启用额外的优化，比如禁用自动求导引擎和解除对操作immutable的限制，从而实现更高效的模型推断torch.is_inference_mode_enabled():123with torch.inference_mode():    print(torch.is_inference_mode_enabled()) # 在 inference mode 中，这将输出 Trueprint(torch.is_inference_mode_enabled()) # 在 inference mode 外部，这将输出 False这个函数用来检查当前是否启用了推断模式。在torch.inference_mode()上下文管理器的内部，它会返回True每个上下文管理器和函数都有其用途，根据需要进行梯度计算的控制，可以优化您的PyTorch 程序的性能数学操作torch — PyTorch 2.2 documentation-数学操作基础操作torch.add: 对输入张量input逐元素加上标量值value，并返回结果到一个新的张量torch.addcdiv: 用tensor2对tensor1逐元素相除，然后乘以标量值value 并加到tensor，张量的形状不需要匹配，但元素数量必须一致tensor (Tensor) – 张量，对 tensor1 ./ tensor 进行相加value (Number, optional) – 标量，对 tensor1 ./ tensor2 进行相乘tensor1 (Tensor) – 张量，作为被除数(分子)tensor2 (Tensor) –张量，作为除数(分母)out (Tensor, optional) – 输出张量torch.addcmul: 用tensor2对tensor1逐元素相乘，并对结果乘以标量值value然后加到tensortensor (Tensor) – 张量，对tensor1 ./ tensor 进行相加value (Number, optional) – 标量，对 tensor1 . tensor2 进行相乘tensor1 (Tensor) – 张量，作为乘子1tensor2 (Tensor) –张量，作为乘子2out (Tensor, optional) – 输出张量以上两个可以用于正则化操作1234567891011121314151617181920212223242526272829303132333435363738394041# 对输入张量`input`逐元素加上标量值`value`，并返回结果到一个新的张量a = torch.randn(4)Out[0]: tensor([ 0.3510, -0.2226, -0.7971, -0.2564])torch.add(a, 20)Out[1]: tensor([20.3510, 19.7774, 19.2029, 19.7436])# 用`tensor2`对`tensor1`逐元素相除，然后乘以标量值`value` 并加到`tensor`t = torch.randn(1, 3)t1 = torch.randn(3, 1)t2 = torch.randn(1, 3)t, t1, t2Out[0]: (tensor([[-1.2863,  1.1267, -1.7120]]), tensor([[-0.4294],         [-0.5328],         [-0.5373]]), tensor([[-0.0876,  0.4398,  1.3583]]))torch.addcdiv(t, t1, t2, value=0.1)Out[1]: tensor([[-0.7958,  1.0291, -1.7436],        [-0.6778,  1.0056, -1.7512],        [-0.6727,  1.0046, -1.7515]])# 用`tensor2`对`tensor1`逐元素相乘，并对结果乘以标量值`value`然后加到`tensor`t = torch.randn(1, 3)t1 = torch.randn(3, 1)t2 = torch.randn(1, 3)t, t1, t2Out[0]: (tensor([[-1.2863,  1.1267, -1.7120]]), tensor([[-0.4294],         [-0.5328],         [-0.5373]]), tensor([[-0.0876,  0.4398,  1.3583]]))torch.addcmul(t, t1, t2, value=0.1)Out[1]: tensor([[-1.2825,  1.1078, -1.7703],        [-1.2816,  1.1033, -1.7844],        [-1.2816,  1.1031, -1.7850]])torch.ceil: 对输入input张量每个元素向上取整, 即取不小于每个元素的最小整数torch.clamp(别名torch.clip): 将输入input张量每个元素的夹紧到区间，并返回结果到一个新张量torch.floor: 返回一个新张量，包含输入input张量每个元素的floor，即不小于元素的最大整数1234567891011121314151617# torch.ceila = torch.randn(4)Out[0]: tensor([-0.9105, -0.7277,  0.9516, -0.1081])torch.ceil(a)Out[1]: tensor([-0., -0., 1., -0.])# torch.floora = torch.randn(4)Out[0]: tensor([-0.5661, -0.9135,  1.1018, -0.2633])torch.floor(a)Out[1]: tensor([-1., -1.,  1., -1.])# torch.clampa = torch.randn(4)Out[0]: tensor([-0.9105, -0.7277,  0.9516, -0.1081])torch.clamp(a, min=-0.5, max=0.5)Out[1]: tensor([-0.5000, -0.5000,  0.5000, -0.1081])torch.div(别名torch.divide): 将input逐元素除以标量值value，并返回结果到输出张量out，torch.div(input, value, out=None)两张量input和other逐元素相除，并将结果返回到输出，torch.div(input, other, **, rounding_mode=None, out=None*) → Tensortorch.mul(别米torch.multiply): 用标量值value乘以输入input的每个元素，并返回一个新的结果张量，torch.mul(input, value, out=None)两个张量input,other按元素进行相乘，并返回到输出张量，torch.mul(input, other, out=None)1234567891011121314151617181920212223242526# 元素除a = torch.randn(4)Out[0]: tensor([-0.9105, -0.7277,  0.9516, -0.1081])torch.div(a, 0.5)Out[1]: tensor([-1.8210, -1.4554,  1.9032, -0.2162])a = torch.tensor([[-0.3711, -1.9353, -0.4605, -0.2917],                  [ 0.1815, -1.0111,  0.9805, -1.5923]])b = torch.tensor([ 0.8032,  0.2930, -0.8113, -0.2308])torch.div(a, b, rounding_mode='trunc')Out[2]: tensor([[-0., -6.,  0.,  1.],        [ 0., -3., -1.,  6.]])torch.div(a, b, rounding_mode='floor')Out[3]: tensor([[-1., -7.,  0.,  1.],        [ 0., -4., -2.,  6.]])# 元素乘a = torch.randn(3)Out[0]: tensor([ 0.0603, -0.5258, -0.3810])b = torch.randn(3)Out[1]: tensor([ 1.2408, -1.3506,  0.9296])torch.mul(a, 100)Out[2]: tensor([  6.0299, -52.5785, -38.0989])torch.mul(a, b)Out[3]: tensor([ 0.0748,  0.7101, -0.3542])torch.exp: 返回一个新张量，包含输入input张量每个元素的指数torch.frac: 返回每个元素的分数部分torch.log: 计算input 的自然对数torch.log1p: 计算的自然对数，对值比较小的输入，此函数比torch.log()更准确torch.neg(别名torch.negative): 返回一个新张量，包含输入input 张量按元素取负torch.pow: 对输入input的按元素求exponent次幂值，并返回结果张量，幂值exponent 可以为单一 float 数或者与input相同元素数的张量torch.round: (四舍五入)返回一个新张量，将输入input张量每个元素舍入到最近的整数torch.trunc: (四舍五入(去尾法))返回一个新张量，包含输入input张量每个元素的截断值(标量x的截断值是最接近其的整数)，简而言之，有符号数的小数部分被舍弃torch.rsqrt: 返回一个新张量，包含输入input张量每个元素的平方根倒数torch.fmod: 计算逐元素余数，保留正负号torch.remainder: 计算逐元素余数， 相当于python中的%操作符，不保留正负号torch.reciprocal: 返回一个新张量，包含输入input张量每个元素的倒数，即 1.0/xtorch.sqrt: 返回一个新张量，包含输入input张量每个元素的平方根torch.abs(别名为torch.absolute): 计算输入张量的每个元素绝对值12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879# 返回一个新张量，包含输入`input`张量每个元素的指数torch.exp(torch.Tensor([0, math.log(2)]))Out[0]: tensor([1., 2.])# 返回每个元素的分数部分torch.frac(torch.Tensor([1, 2.5, -3.2]))Out[0]: tensor([ 0.0000,  0.5000, -0.2000])# 计算`input` 的自然对数a = torch.randn(5)Out[0]: tensor([-0.3466,  2.3803, -0.0423, -0.9744,  0.4976])torch.log(a)Out[1]: tensor([    nan,  0.8672,     nan,     nan, -0.6980])# 计算input + 1的自然对数torch.log1p(a)Out[2]: tensor([-0.4256,  1.2180, -0.0432, -3.6633,  0.4039])# 返回一个新张量，包含输入`input` 张量按元素取负a = torch.randn(3)Out[0]: tensor([ 0.0603, -0.5258, -0.3810])torch.neg(a)Out[1]: tensor([-0.0603,  0.5258,  0.3810])# 求指数a = torch.arange(1, 5)Out[0]: tensor([1, 2, 3, 4])exp = torch.arange(1, 5)Out[1]: tensor([1, 2, 3, 4])torch.pow(a, 2)Out[2]: tensor([ 1,  4,  9, 16])torch.pow(a, exp)Out[3]: tensor([  1,   4,  27, 256])torch.pow(2, exp)Out[4]: tensor([ 2,  4,  8, 16])# 四舍五入a = torch.randn(4)Out[0]: tensor([ 0.7995, -2.0975,  0.7273,  0.7539])torch.round(a)Out[1]: tensor([ 1., -2.,  1.,  1.])# 四舍五入(去尾法)a = torch.randn(4)Out[0]: tensor([-2.1647, -0.2294,  0.4943,  1.5146])torch.trunc(a)Out[1]: tensor([-2., -0.,  0.,  1.])# 求平方根倒数a = torch.Tensor([1, 2, 3, 4])Out[0]: tensor([1., 2., 3., 4.])torch.rsqrt(a)Out[1]: tensor([1.0000, 0.7071, 0.5774, 0.5000])# 计算逐元素余数， 保留正负号t = torch.tensor([10, -22, 31, -47])torch.fmod(t, 5)Out[0]: tensor([ 0, -2,  1, -2])# 计算逐元素余数， 相当于python中的%操作符torch.remainder(t, 5)Out[1]: tensor([0, 3, 1, 3])np.mod(np.array([10, -22, 31, -47]), 5)Out[2]: array([0, 3, 1, 3], dtype=int32)# 求1/xa = torch.Tensor([1, 2, 3, 4])Out[0]: tensor([1., 2., 3., 4.])torch.reciprocal(a)Out[1]: tensor([1.0000, 0.5000, 0.3333, 0.2500])# 求平方根a = torch.Tensor([1, 2, 3, 4])Out[0]: tensor([1., 2., 3., 4.])torch.sqrt(a)Out[1]: tensor([1.0000, 1.4142, 1.7321, 2.0000])# 求绝对值torch.abs(torch.FloatTensor([-1, -2, 3]))Out[0]: tensor([1., 2., 3.])三角函数torch.asin(别名torch.arcsin): 返回一个新张量，包含输入input张量每个元素的反正弦函数torch.atan: 返回一个新张量，包含输入input张量每个元素的反正切函数torch.atan2: 返回一个新张量，包含两个输入张量input1和input2的反正切函数torch.cos: 返回一个新张量，包含输入input张量每个元素的余弦torch.acos(别名torch.arccos): 返回一个新张量，包含输入张量每个元素的反余弦torch.cosh: 返回一个新张量，包含输入input张量每个元素的双曲余弦torch.sin: 返回一个新张量，包含输入input张量每个元素的正弦torch.sinh: 返回一个新张量，包含输入input张量每个元素的双曲正弦torch.tan: 返回一个新张量，包含输入input张量每个元素的正切torch.tanh: 返回一个新张量，包含输入input张量每个元素的双曲正切1234567891011121314151617181920212223242526272829303132333435363738394041424344# 反正弦函数a = torch.randn(4)Out[0]: tensor([ 0.2583, -0.5285,  0.8979,  1.0104])torch.asin(a)Out[1]: tensor([ 0.2613, -0.5569,  1.1149,     nan])# 反正切函数a = torch.randn(4)Out[0]: tensor([ 0.2583, -0.5285,  0.8979,  1.0104])b = torch.randn(4)Out[2]: tensor([0.1100, 1.4311, 1.9536, 0.7652])torch.atan(a)Out[1]: tensor([ 0.2528, -0.4862,  0.7316,  0.7906])torch.atan2(a, b)Out[3]: tensor([ 1.1681, -0.3538,  0.4308,  0.9226])# 余弦a = torch.randn(4)Out[0]: tensor([-0.9105, -0.7277,  0.9516, -0.1081])torch.cos(a)Out[1]: tensor([0.6133, 0.7467, 0.5804, 0.9942])# 反余弦torch.acos(torch.FloatTensor([-1, 1, 0]))Out[1]: tensor([3.1416, 0.0000, 1.5708])# 双曲余弦torch.cosh(a)Out[2]: tensor([1.4439, 1.2766, 1.4880, 1.0058])# 正弦a = torch.randn(4)torch.sin(a)Out[0]: tensor([-0.9215,  0.2650,  0.8285,  0.5914])# 双曲正弦torch.sinh(a)Out[1]: tensor([-1.4591,  0.2714,  1.1392,  0.6759])# 正切a = torch.Tensor([1, 2, 3, 4])Out[0]: tensor([1., 2., 3., 4.])torch.tan(a)Out[1]: tensor([ 1.5574, -2.1850, -0.1425,  1.1578])# 双曲正切torch.tanh(a)Out[2]: tensor([0.7616, 0.9640, 0.9951, 0.9993])位操作bitwise_not – 按位非: 计算给定输入张量的按位非，这个操作会将输入张量中的每个位反转，即将所有的1变成0，将所有的0变成1。在整数数据类型中，这通常意味着进行二进制补码的运算bitwise_and – 按位与: 计算两个输入张量的按位与，只有当两个张量在同一位置的位都是1时，结果张量在该位置的位才是1，否则是0bitwise_or – 按位或: 计算两个输入张量的按位或，只要两个张量在同一位置的位中有一个是1，结果张量在该位置的位就是1。如果都是0，结果就是0bitwise_xor – 按位异或: 计算两个输入张量的按位异或，这个操作在两个张量在同一位置的位不同的时候返回1，相同的时候返回0123456789101112131415161718192021# 按位非&gt;&gt;&gt; torch.bitwise_not(torch.tensor([-1, -2, 3], dtype=torch.int8))tensor([ 0,  1, -4], dtype=torch.int8)# 按位与&gt;&gt;&gt; torch.bitwise_and(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))tensor([1, 0,  3], dtype=torch.int8)&gt;&gt;&gt; torch.bitwise_and(torch.tensor([True, True, False]), torch.tensor([False, True, False]))tensor([ False, True, False])# 按位或&gt;&gt;&gt; torch.bitwise_or(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))tensor([-1, -2,  3], dtype=torch.int8)&gt;&gt;&gt; torch.bitwise_or(torch.tensor([True, True, False]), torch.tensor([False, True, False]))tensor([ True, True, False])# 按位异或&gt;&gt;&gt; torch.bitwise_xor(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))tensor([-2, -2,  0], dtype=torch.int8)&gt;&gt;&gt; torch.bitwise_xor(torch.tensor([True, True, False]), torch.tensor([False, True, False]))tensor([ True, False, False])bitwise_left_shift – 按位左移: 计算给定输入张量与另一张量(表示位移数量)的按位左移。这个操作将输入张量的每个位向左移动other指定的位数，左边溢出的位被丢弃，而右边则填充0bitwise_right_shift – 按位右移: 计算给定输入张量与另一张量(表示位移数量)的按位右移。这个操作将输入张量的每个位向右移动other指定的位数，右边溢出的位被丢弃，对于无符号数据类型，左边填充0；对于有符号数据类型，一般会进行算术右移，填充的是最高位的值，即符号位1234567# 按位左移&gt;&gt;&gt; torch.bitwise_left_shift(torch.tensor([-1, -2, 3], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))tensor([-2, -2, 24], dtype=torch.int8)# 按位右移&gt;&gt;&gt; torch.bitwise_right_shift(torch.tensor([-2, -7, 31], dtype=torch.int8), torch.tensor([1, 0, 3], dtype=torch.int8))tensor([-1, -7,  3], dtype=torch.int8)其他操作torch.sigmoid: 返回一个新张量，包含输入input张量每个元素的sigmoid值torch.sign: 符号函数，返回一个新张量，包含输入input张量每个元素的正负1234567891011# 求sigmoid值a = torch.Tensor([1, 2, 3, 4])Out[0]: tensor([1., 2., 3., 4.])torch.sigmoid(a)Out[1]: tensor([0.7311, 0.8808, 0.9526, 0.9820])# 符号函数a = torch.Tensor([1, 2, 3, 4])Out[0]: tensor([1., 2., 3., 4.])torch.sign(a)Out[1]: tensor([1., 1., 1., 1.])torch.lerp: 对两个张量以start，end做线性插值， 将结果返回到输出张量start (Tensor) – 起始点张量end (Tensor) – 终止点张量weight (float) – 插值公式的weightout (Tensor, optional) – 结果张量123456789start = torch.arange(1., 5.)end = torch.empty(4).fill_(10)start, endOut[0]: (tensor([1., 2., 3., 4.]), tensor([10., 10., 10., 10.]))torch.lerp(start, end, 0.5)Out[1]: tensor([5.5000, 6.0000, 6.5000, 7.0000])torch.lerp(start, end, torch.full_like(start, 0.5))Out[2]: tensor([5.5000, 6.0000, 6.5000, 7.0000])归约操作极值操作argmax: 返回张量中最大值的索引argmin: 返回张量中最小值的索引1234567891011121314151617181920212223242526# 返回张量中最大值的索引&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; atensor([[ 1.3398,  0.2663, -0.2686,  0.2450],        [-0.7401, -0.8805, -0.3402, -1.1936],        [ 0.4907, -1.3948, -1.0691, -0.3132],        [-1.6092,  0.5419, -0.2993,  0.3195]])&gt;&gt;&gt; torch.argmax(a, dim=1)tensor([ 0,  2,  0,  1])# 返回张量中最小值的索引&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; atensor([[ 0.1139,  0.2254, -0.1381,  0.3687],        [ 1.0100, -1.1975, -0.0102, -0.4732],        [-0.9240,  0.1207, -0.7506, -1.0213],        [ 1.7809, -1.2960,  0.9384,  0.1438]])&gt;&gt;&gt; torch.argmin(a)tensor(13)&gt;&gt;&gt; torch.argmin(a, dim=1)tensor([ 2,  1,  3,  1])&gt;&gt;&gt; torch.argmin(a, dim=1, keepdim=True)tensor([[2],        [1],        [3],        [1]])amax: 返回指定维度上每个切片的最大值amin: 返回指定维度上每个切片的最小值max: 返回张量中所有元素的最大值min: 返回张量中所有元素的最小值123456789101112131415161718192021222324252627282930313233343536373839# 返回指定维度上每个切片的最大值&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; atensor([[ 0.8177,  1.4878, -0.2491,  0.9130],        [-0.7158,  1.1775,  2.0992,  0.4817],        [-0.0053,  0.0164, -1.3738, -0.0507],        [ 1.9700,  1.1106, -1.0318, -1.0816]])&gt;&gt;&gt; torch.amax(a, dim=1)tensor([1.4878, 2.0992, 0.0164, 1.9700])# 返回指定维度上每个切片的最小值&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; atensor([[ 0.6451, -0.4866,  0.2987, -1.3312],        [-0.5744,  1.2980,  1.8397, -0.2713],        [ 0.9128,  0.9214, -1.7268, -0.2995],        [ 0.9023,  0.4853,  0.9075, -1.6165]])&gt;&gt;&gt; torch.amin(a, dim=1)tensor([-1.3312, -0.5744, -1.7268, -1.6165])# 返回张量中所有元素的最大值&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; atensor([[-1.2360, -0.2942, -0.1222,  0.8475],        [ 1.1949, -1.1127, -2.2379, -0.6702],        [ 1.5717, -0.9207,  0.1297, -1.8768],        [-0.6172,  1.0036, -0.6060, -0.2432]])&gt;&gt;&gt; torch.max(a, 1)torch.return_types.max(values=tensor([0.8475, 1.1949, 1.5717, 1.0036]), indices=tensor([3, 0, 0, 1]))# 返回张量中所有元素的最小值&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; atensor([[-0.6248,  1.1334, -1.1899, -0.2803],        [-1.4644, -0.2635, -0.3651,  0.6134],        [ 0.2457,  0.0384,  1.0128,  0.7015],        [-0.1153,  2.9849,  2.1458,  0.5788]])&gt;&gt;&gt; torch.min(a, 1)torch.return_types.min(values=tensor([-1.1899, -1.4644,  0.0384, -0.1153]), indices=tensor([2, 0, 1, 0]))aminmax: 同时计算最小值和最大值12345678910111213&gt;&gt;&gt; torch.aminmax(torch.tensor([1, -3, 5]))torch.return_types.aminmax(min=tensor(-3), max=tensor(5))&gt;&gt;&gt; # aminmax propagates NaNs&gt;&gt;&gt; torch.aminmax(torch.tensor([1, -3, 5, torch.nan]))torch.return_types.aminmax(min=tensor(nan),max=tensor(nan))&gt;&gt;&gt; t = torch.arange(10).view(2, 5)&gt;&gt;&gt; ttensor([[0, 1, 2, 3, 4],        [5, 6, 7, 8, 9]])&gt;&gt;&gt; t.aminmax(dim=0, keepdim=True)torch.return_types.aminmax(min=tensor([[0, 1, 2, 3, 4]]), max=tensor([[5, 6, 7, 8, 9]]))统计操作mean: 返回张量中所有元素的均值nanmean: 返回张量中所有非NaN元素的均值12345678910111213141516171819202122232425262728# 返回张量中所有元素的均值&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; atensor([[-0.3841,  0.6320,  0.4254, -0.7384],        [-0.9644,  1.0131, -0.6549, -1.4279],        [-0.2951, -1.3350, -0.7694,  0.5600],        [ 1.0842, -0.9580,  0.3623,  0.2343]])&gt;&gt;&gt; torch.mean(a, 1)tensor([-0.0163, -0.5085, -0.4599,  0.1807])&gt;&gt;&gt; torch.mean(a, dim=1, keepdim=True)tensor([[-0.0163],        [-0.5085],        [-0.4599],        [ 0.1807]])# 返回张量中所有非NaN元素的均值&gt;&gt;&gt; x = torch.tensor([[torch.nan, 1, 2], [1, 2, 3]])&gt;&gt;&gt; x.mean()tensor(nan)&gt;&gt;&gt; x.nanmean()tensor(1.8000)&gt;&gt;&gt; x.mean(dim=0)tensor([   nan, 1.5000, 2.5000])&gt;&gt;&gt; x.nanmean(dim=0)tensor([1.0000, 1.5000, 2.5000])# If all elements in the reduced dimensions are NaN then the result is NaN&gt;&gt;&gt; torch.tensor([torch.nan]).nanmean()tensor(nan)median: 返回张量中所有元素的中值nanmedian: 返回张量中所有非NaN元素的中值12345678910111213141516171819# 返回张量中所有元素的中值&gt;&gt;&gt; a = torch.randn(4, 5)&gt;&gt;&gt; atensor([[ 0.2505, -0.3982, -0.9948,  0.3518, -1.3131],        [ 0.3180, -0.6993,  1.0436,  0.0438,  0.2270],        [-0.2751,  0.7303,  0.2192,  0.3321,  0.2488],        [ 1.0778, -1.9510,  0.7048,  0.4742, -0.7125]])&gt;&gt;&gt; torch.median(a, 1)torch.return_types.median(values=tensor([-0.3982,  0.2270,  0.2488,  0.4742]), indices=tensor([1, 4, 4, 3]))# 返回张量中所有非NaN元素的中值&gt;&gt;&gt; a = torch.tensor([[2, 3, 1], [float('nan'), 1, float('nan')]])&gt;&gt;&gt; atensor([[2., 3., 1.],        [nan, 1., nan]])&gt;&gt;&gt; a.median(0)torch.return_types.median(values=tensor([nan, 1., nan]), indices=tensor([1, 1, 1]))&gt;&gt;&gt; a.nanmedian(0)torch.return_types.nanmedian(values=tensor([2., 1., 1.]), indices=tensor([0, 1, 0]))mode: 返回张量在指定维度上每行的众数及其索引123456&gt;&gt;&gt; a = torch.randint(10, (5,))&gt;&gt;&gt; atensor([6, 5, 1, 0, 2])&gt;&gt;&gt; b = a + (torch.randn(50, 1) * 5).long()&gt;&gt;&gt; torch.mode(b, 0)torch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))sum: 返回张量中所有元素的总和nansum: 返回张量中所有元素的和，将NaN视为零1234567891011121314151617181920212223# 返回张量中所有元素的总和&gt;&gt;&gt; a = torch.randn(4, 4)&gt;&gt;&gt; atensor([[ 0.0569, -0.2475,  0.0737, -0.3429],        [-0.2993,  0.9138,  0.9337, -1.6864],        [ 0.1132,  0.7892, -0.1003,  0.5688],        [ 0.3637, -0.9906, -0.4752, -1.5197]])&gt;&gt;&gt; torch.sum(a, 1)tensor([-0.4598, -0.1381,  1.3708, -2.6217])&gt;&gt;&gt; b = torch.arange(4 * 5 * 6).view(4, 5, 6)&gt;&gt;&gt; torch.sum(b, dim=(2, 1))tensor([  435.,  1335.,  2235.,  3135.])# 返回张量中所有元素的和，将NaN视为零&gt;&gt;&gt; torch.nansum(torch.tensor([1., float(&quot;nan&quot;)]))1.0&gt;&gt;&gt; a = torch.tensor([[1, 2], [3., float(&quot;nan&quot;)]])&gt;&gt;&gt; torch.nansum(a)tensor(6.)&gt;&gt;&gt; torch.nansum(a, dim=0)tensor([4., 2.])&gt;&gt;&gt; torch.nansum(a, dim=1)tensor([3., 3.])prod: 返回张量中所有元素的乘积12345678&gt;&gt;&gt; a = torch.randn(4, 2)&gt;&gt;&gt; atensor([[ 0.5261, -0.3837],        [ 1.1857, -0.2498],        [-1.1646,  0.0705],        [ 1.1131, -1.0629]])&gt;&gt;&gt; torch.prod(a, dim=1)tensor([-0.2018, -0.2962, -0.0821, -1.1831])quantile: 计算张量在指定维度上每行的第q个百分位数nanquantile: 类似于quantile，但在计算百分位数时忽略NaN值1234567891011121314151617181920212223242526272829303132333435363738394041424344454647# 计算张量在指定维度上每行的第q个百分位数&gt;&gt;&gt; a = torch.randn(2, 3)&gt;&gt;&gt; atensor([[ 0.0795, -1.2117,  0.9765],        [ 1.1707,  0.6706,  0.4884]])&gt;&gt;&gt; q = torch.tensor([0.25, 0.5, 0.75])&gt;&gt;&gt; torch.quantile(a, q, dim=1, keepdim=True)tensor([[[-0.5661],        [ 0.5795]],        [[ 0.0795],        [ 0.6706]],        [[ 0.5280],        [ 0.9206]]])&gt;&gt;&gt; torch.quantile(a, q, dim=1, keepdim=True).shapetorch.Size([3, 2, 1])&gt;&gt;&gt; a = torch.arange(4.)&gt;&gt;&gt; atensor([0., 1., 2., 3.])&gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='linear')tensor(1.8000)&gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='lower')tensor(1.)&gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='higher')tensor(2.)&gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='midpoint')tensor(1.5000)&gt;&gt;&gt; torch.quantile(a, 0.6, interpolation='nearest')tensor(2.)&gt;&gt;&gt; torch.quantile(a, 0.4, interpolation='nearest')tensor(1.)# 类似于quantile，但在计算百分位数时忽略NaN值&gt;&gt;&gt; t = torch.tensor([float('nan'), 1, 2])&gt;&gt;&gt; t.quantile(0.5)tensor(nan)&gt;&gt;&gt; t.nanquantile(0.5)tensor(1.5000)&gt;&gt;&gt; t = torch.tensor([[float('nan'), float('nan')], [1, 2]])&gt;&gt;&gt; ttensor([[nan, nan],        [1., 2.]])&gt;&gt;&gt; t.nanquantile(0.5, dim=0)tensor([1., 2.])&gt;&gt;&gt; t.nanquantile(0.5, dim=1)tensor([   nan, 1.5000])距离范数norm: 返回给定张量的矩阵范数或向量范数1234567891011121314151617181920212223&gt;&gt;&gt; import torch&gt;&gt;&gt; a = torch.arange(9, dtype= torch.float) - 4&gt;&gt;&gt; b = a.reshape((3, 3))&gt;&gt;&gt; torch.norm(a)tensor(7.7460)&gt;&gt;&gt; torch.norm(b)tensor(7.7460)&gt;&gt;&gt; torch.norm(a, float('inf'))tensor(4.)&gt;&gt;&gt; torch.norm(b, float('inf'))tensor(4.)&gt;&gt;&gt; c = torch.tensor([[ 1, 2, 3], [-1, 1, 4]] , dtype=torch.float)&gt;&gt;&gt; torch.norm(c, dim=0)tensor([1.4142, 2.2361, 5.0000])&gt;&gt;&gt; torch.norm(c, dim=1)tensor([3.7417, 4.2426])&gt;&gt;&gt; torch.norm(c, p=1, dim=1)tensor([6., 6.])&gt;&gt;&gt; d = torch.arange(8, dtype=torch.float).reshape(2, 2, 2)&gt;&gt;&gt; torch.norm(d, dim=(1, 2))tensor([ 3.7417, 11.2250])&gt;&gt;&gt; torch.norm(d[0, :, :]), torch.norm(d[1, :, :])(tensor(3.7417), tensor(11.2250))dist: 返回两个张量差的p-范数p-范数（p-norm）是向量空间中的一种度量，它用于衡量向量的大小或长度1234567891011121314&gt;&gt;&gt; x = torch.randn(4)&gt;&gt;&gt; xtensor([-1.5393, -0.8675,  0.5916,  1.6321])&gt;&gt;&gt; y = torch.randn(4)&gt;&gt;&gt; ytensor([ 0.0967, -1.0511,  0.6295,  0.8360])&gt;&gt;&gt; torch.dist(x, other=y, p=3.5)tensor(1.6727)&gt;&gt;&gt; torch.dist(x, y, 3)tensor(1.6973)&gt;&gt;&gt; torch.dist(x, y, 0)tensor(4.)&gt;&gt;&gt; torch.dist(x, y, 1)tensor(2.6537)均值与方差std: 计算指定维度上的标准差var: 计算指定维度上的方差123456789101112131415161718192021# 计算指定维度上的标准差&gt;&gt;&gt; a = torch.tensor(...     [[ 0.2035,  1.2959,  1.8101, -0.4644],...      [ 1.5027, -0.3270,  0.5905,  0.6538],...      [-1.5745,  1.3330, -0.5596, -0.6548],...      [ 0.1264, -0.5080,  1.6420,  0.1992]])&gt;&gt;&gt; torch.std(a, dim=1, keepdim=True)tensor([[1.0311],        [0.7477],        [1.2204],        [0.9087]])# 计算指定维度上的方差&gt;&gt;&gt; a = torch.tensor(...     [[ 0.2035,  1.2959,  1.8101, -0.4644],...      [ 1.5027, -0.3270,  0.5905,  0.6538],...      [-1.5745,  1.3330, -0.5596, -0.6548],...      [ 0.1264, -0.5080,  1.6420,  0.1992]])&gt;&gt;&gt; torch.std_mean(a, dim=0, keepdim=True)(tensor([[1.2620, 1.0028, 1.0957, 0.6038]]), tensor([[ 0.0645,  0.4485,  0.8707, -0.0665]]))std_mean: 同时计算指定维度上的标准差和均值var_mean: 同时计算指定维度上的方差和均值123456789101112131415161718192021# 同时计算指定维度上的标准差和均值&gt;&gt;&gt; a = torch.tensor(...     [[ 0.2035,  1.2959,  1.8101, -0.4644],...      [ 1.5027, -0.3270,  0.5905,  0.6538],...      [-1.5745,  1.3330, -0.5596, -0.6548],...      [ 0.1264, -0.5080,  1.6420,  0.1992]])&gt;&gt;&gt; torch.var(a, dim=1, keepdim=True)tensor([[1.0631],        [0.5590],        [1.4893],        [0.8258]])# 同时计算指定维度上的方差和均值&gt;&gt;&gt; a = torch.tensor(...     [[ 0.2035,  1.2959,  1.8101, -0.4644],...      [ 1.5027, -0.3270,  0.5905,  0.6538],...      [-1.5745,  1.3330, -0.5596, -0.6548],...      [ 0.1264, -0.5080,  1.6420,  0.1992]])&gt;&gt;&gt; torch.var_mean(a, dim=0, keepdim=True)(tensor([[1.5926, 1.0056, 1.2005, 0.3646]]), tensor([[ 0.0645,  0.4485,  0.8707, -0.0665]]))逻辑操作all: 检查张量中的所有元素是否都满足某个条件(例如是否都为True)12345678910&gt;&gt;&gt; a = torch.rand(4, 2).bool()&gt;&gt;&gt; atensor([[True, True],        [True, False],        [True, True],        [True, True]], dtype=torch.bool)&gt;&gt;&gt; torch.all(a, dim=1)tensor([ True, False,  True,  True], dtype=torch.bool)&gt;&gt;&gt; torch.all(a, dim=0)tensor([ True, False], dtype=torch.bool)any: 检查张量中是否有任何元素满足某个条件(例如是否为True)12345678910&gt;&gt;&gt; a = torch.randn(4, 2) &lt; 0&gt;&gt;&gt; atensor([[ True,  True],        [False,  True],        [ True,  True],        [False, False]])&gt;&gt;&gt; torch.any(a, 1)tensor([ True,  True,  True, False])&gt;&gt;&gt; torch.any(a, 0)tensor([True, True])特殊操作unique: 返回张量中的唯一元素unique_consecutive: 在每组连续相等的元素中只保留第一个元素123456789101112131415161718192021222324252627282930313233# 返回张量中的唯一元素&gt;&gt;&gt; output = torch.unique(torch.tensor([1, 3, 2, 3], dtype=torch.long))&gt;&gt;&gt; outputtensor([1, 2, 3])&gt;&gt;&gt; output, inverse_indices = torch.unique(...     torch.tensor([1, 3, 2, 3], dtype=torch.long), sorted=True, return_inverse=True)&gt;&gt;&gt; outputtensor([1, 2, 3])&gt;&gt;&gt; inverse_indicestensor([0, 2, 1, 2])&gt;&gt;&gt; output, inverse_indices = torch.unique(...     torch.tensor([[1, 3], [2, 3]], dtype=torch.long), sorted=True, return_inverse=True)&gt;&gt;&gt; outputtensor([1, 2, 3])&gt;&gt;&gt; inverse_indicestensor([[0, 2],        [1, 2]])# 在每组连续相等的元素中只保留第一个元素&gt;&gt;&gt; x = torch.tensor([1, 1, 2, 2, 3, 1, 1, 2])&gt;&gt;&gt; output = torch.unique_consecutive(x)&gt;&gt;&gt; outputtensor([1, 2, 3, 1, 2])&gt;&gt;&gt; output, inverse_indices = torch.unique_consecutive(x, return_inverse=True)&gt;&gt;&gt; outputtensor([1, 2, 3, 1, 2])&gt;&gt;&gt; inverse_indicestensor([0, 0, 1, 1, 2, 3, 3, 4])&gt;&gt;&gt; output, counts = torch.unique_consecutive(x, return_counts=True)&gt;&gt;&gt; outputtensor([1, 2, 3, 1, 2])&gt;&gt;&gt; countstensor([2, 2, 1, 2, 1])count_nonzero: 计算给定维度上非零元素的数量12345678910&gt;&gt;&gt; x = torch.zeros(3,3)&gt;&gt;&gt; x[torch.randn(3,3) &gt; 0.5] = 1&gt;&gt;&gt; xtensor([[0., 1., 1.],        [0., 0., 0.],        [0., 0., 1.]])&gt;&gt;&gt; torch.count_nonzero(x)tensor(3)&gt;&gt;&gt; torch.count_nonzero(x, dim=0)tensor([0, 1, 2])logsumexp: 返回在指定维度上，张量所有元素指数的对数和12345&gt;&gt;&gt; a = torch.randn(3, 3)&gt;&gt;&gt; torch.logsumexp(a, 1)tensor([1.4907, 1.0593, 1.5696])&gt;&gt;&gt; torch.dist(torch.logsumexp(a, 1), torch.log(torch.sum(torch.exp(a), 1)))tensor(1.6859e-07)比较操作torch.eq: torch.eq(input, other, out=None) → Tensor比较元素相等性。第二个参数可为一个数或与第一个参数同类型形状的张量torch.ge: torch.ge(input, other, out=None) → Tensor逐元素比较input和other，即是否 input&gt;=othertorch.gt: torch.gt(input, other, out=None) → Tensor逐元素比较input和other ， 即是否input&gt;otherinput&gt;other torch.le: torch.le(input, other, out=None) → Tensor逐元素比较input和other ， 即是否input&lt;=othertorch.lt: torch.lt(input, other, out=None) → Tensor逐元素比较input和other ， 即是否 input&lt;othertorch.ne: torch.ne(input, other, out=None) → Tensor逐元素比较input和other，即是否 input!=other1234567891011121314151617181920212223242526272829torch.eq(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))Out[0]: tensor([[ True, False],        [False,  True]])torch.ge(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))Out[1]: tensor([[ True,  True],        [False,  True]])torch.gt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))Out[2]: tensor([[False,  True],        [False, False]])torch.le(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))Out[3]: tensor([[ True, False],        [ True,  True]])torch.lt(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))Out[4]: tensor([[False, False],        [ True, False]])torch.ne(torch.Tensor([[1, 2], [3, 4]]), torch.Tensor([[1, 1], [4, 4]]))Out[5]: tensor([[False,  True],        [ True, False]])torch.equal: torch.equal(tensor1, tensor2) → bool如果两个张量有相同的形状和元素值，则返回True ，否则 False12torch.equal(torch.Tensor([1, 2]), torch.Tensor([1, 2]))Out[0]: Truetorch.kthvalue: torch.kthvalue(input, k, dim=None, out=None) -&gt; (Tensor, LongTensor)取输入张量input指定维上第k 个最小值。如果不指定dim，则默认为input的最后一维torch.topk: 沿给定dim维度返回输入张量input中 k 个最大值。 如果不指定dim，则默认为input的最后一维。 如果为largest为 False ，则返回最小的 k 个值返回一个元组 (values,indices)，其中indices是原始输入张量input中测元素下标。 如果设定布尔值sorted 为True，将会确保返回的 k 个值被排序torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)参数:input (Tensor) – 输入张量k (int) – “top-k”中的kdim (int, optional) – 排序的维largest (bool, optional) – 布尔值，控制返回最大或最小值sorted (bool, optional) – 布尔值，控制返回值是否排序out (tuple, optional) – 可选输出张量 (Tensor, LongTensor) output buffers123456789101112131415161718192021222324x = torch.arange(1, 6)# torch.kthvaluetorch.kthvalue(x, 4)Out[0]:torch.return_types.kthvalue(values=tensor(4),indices=tensor(3))# torch.topkx = torch.arange(1, 6)Out[1]: tensor([1, 2, 3, 4, 5])torch.topk(x, 3)Out[2]: torch.return_types.topk(values=tensor([5, 4, 3]),indices=tensor([4, 3, 2]))torch.topk(x, 3, 0, largest=False)Out[3]: torch.return_types.topk(values=tensor([1, 2, 3]),indices=tensor([0, 1, 2]))torch.max: 返回输入张量所有元素的最大值torch.max()返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引torch.max(input, dim, max=None, max_indices=None) -&gt; (Tensor, LongTensor)input中逐元素与other相应位置的元素对比，返回最大值到输出张量torch.max(input, other, out=None) → Tensortorch.min: 返回输入张量所有元素的最小值torch.min(input) → float返回输入张量给定维度上每行的最小值，并同时返回每个最小值的位置索引torch.min(input, dim, min=None, min_indices=None) -&gt; (Tensor, LongTensor)input中逐元素与other相应位置的元素对比，返回最小值到输出张量torch.min(input, other, out=None) → Tensor12345678910111213141516171819202122232425262728293031323334a = torch.randn(2, 2)Out[0]: tensor([[-0.1204, -0.5016],        [ 1.2717,  0.7351]])b = torch.randn(2, 2)Out[1]: tensor([[-1.4497,  0.7534],        [ 0.5994, -0.1490]])# 最大值torch.max(torch.arange(1, 5))Out[2]: tensor(4)torch.max(a, 1)Out[3]: torch.return_types.max(values=tensor([-0.1204,  1.2717]),indices=tensor([0, 0]))torch.max(a, b)Out[4]: tensor([[-0.1204,  0.7534],        [ 1.2717,  0.7351]])# 最小值torch.min(torch.arange(1, 5))Out[5]: tensor(1)torch.min(a, 1)Out[6]: torch.return_types.min(values=tensor([-0.5016,  0.7351]),indices=tensor([1, 1]))torch.min(a, b)Out[7]: tensor([[-1.4497, -0.5016],        [ 0.5994, -0.1490]])torch.sort: torch.sort(input, dim=None, descending=False, out=None) -&gt; (Tensor, LongTensor)对输入张量input沿着指定维按升序排序。如果不给定dim，则默认为输入的最后一维。如果指定参数descending为True，则按降序排序返回元组 (sorted_tensor, sorted_indices) ， sorted_indices 为原始输入中的下标123456789101112131415x = torch.randn(3, 4)Out[0]: tensor([[-2.3460,  1.3734,  1.1444, -0.4736],        [-1.1785,  0.8436, -1.4403, -0.1073],        [-0.1198,  0.7067, -0.0734, -1.6181]])sorted, indices = torch.sort(x)sorted, indicesOut[1]: (tensor([[-2.3460, -0.4736,  1.1444,  1.3734],         [-1.4403, -1.1785, -0.1073,  0.8436],         [-1.6181, -0.1198, -0.0734,  0.7067]]), tensor([[0, 3, 2, 1],         [2, 0, 3, 1],         [3, 0, 2, 1]]))其它操作torch.cross: 返回沿着维度dim上，两个张量input和other的向量积（叉积）。 input和other 必须有相同的形状，且指定的dim维上size必须为3如果不指定dim，则默认为第一个尺度为3的维torch.cross(input, other, dim=-1, out=None) → Tensor123456789101112131415161718a = torch.randint(1, 6, (2, 3))Out[0]: tensor([[5, 4, 5],        [4, 2, 3]])b = torch.randint(1, 6, (2, 3))Out[1]: tensor([[1, 1, 2],        [3, 4, 2]])torch.cross(a, a)Out[2]: tensor([[0, 0, 0],        [0, 0, 0]])torch.cross(a, b)Out[3]: tensor([[ 3, -5,  1],        [-8,  1, 10]])torch.diag: torch.diag(input, diagonal=0, out=None) → Tensor如果输入是一个向量(1D 张量)，则返回一个以input为对角线元素的2D方阵如果输入是一个矩阵(2D 张量)，则返回一个包含input对角线元素的1D张量参数diagonal指定对角线:diagonal = 0, 主对角线diagonal &gt; 0, 主对角线之上diagonal &lt; 0, 主对角线之下1234567891011121314151617181920212223242526# 如果输入是一个向量(1D 张量)，则返回一个以`input`为对角线元素的2D方阵a = torch.randn(3)Out[0]: tensor([-0.3509,  0.6176, -1.4976])torch.diag(a)Out[1]: tensor([[-0.3509,  0.0000,  0.0000],        [ 0.0000,  0.6176,  0.0000],        [ 0.0000,  0.0000, -1.4976]])torch.diag(a, 1)Out[2]: tensor([[ 0.0000, -0.3509,  0.0000,  0.0000],        [ 0.0000,  0.0000,  0.6176,  0.0000],        [ 0.0000,  0.0000,  0.0000, -1.4976],        [ 0.0000,  0.0000,  0.0000,  0.0000]])# 如果输入是一个矩阵(2D 张量)，则返回一个包含`input`对角线元素的1D张量# 取得给定矩阵第k个对角线:a = torch.randn(3, 3)Out[3]: tensor([[ 0.8224,  0.7792,  0.2605],        [-0.8646,  0.2568, -0.8189],        [ 1.1693,  0.8108, -1.9662]])torch.diag(a, 0)Out[4]: tensor([ 0.8224,  0.2568, -1.9662])torch.diag(a, 1)Out[5]: tensor([ 0.7792, -0.8189])torch.histc: torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor计算输入张量的直方图。以min和max为range边界，将其均分成bins个直条，然后将排序好的数据划分到各个直条(bins)中参数：input (Tensor) – 输入张量bins (int) – 直方图 bins(直条)的个数(默认100个)min (int) – range的下边界(包含)max (int) – range的上边界(包含)out (Tensor, optional) – 结果张量12345torch.histc(torch.FloatTensor([1, 2, 1]), bins=4, min=0, max=3)Out[0]: tensor([0., 2., 1., 0.])torch.histc(torch.FloatTensor([1, 1, 2, 1]), bins=4, min=0, max=3)Out[1]: tensor([0., 3., 1., 0.])torch.renorm: torch.renorm(input, p, dim, maxnorm, out=None) → Tensor返回一个张量，包含规范化后的各个子张量，使得沿着dim维划分的各子张量的p范数小于maxnorm如果p范数的值小于maxnorm，则当前子张量不需要修改参数：input (Tensor) – 输入张量p (float) – 范数的pdim (int) – 沿着此维切片，得到张量子集maxnorm (float) – 每个子张量的范数的最大值out (Tensor, optional) – 结果张量12345678910111213x = torch.ones(3, 3)x[1].fill_(2)x[2].fill_(3)Out[0]: tensor([[1., 1., 1.],        [2., 2., 2.],        [3., 3., 3.]])torch.renorm(x, p=1, dim=0, maxnorm=5)Out[1]: tensor([[1.0000, 1.0000, 1.0000],        [1.6667, 1.6667, 1.6667],        [1.6667, 1.6667, 1.6667]])torch.trace: 返回输入2维矩阵对角线元素的和(迹)12345678x = torch.arange(1, 10).view(3, 3)Out[0]: tensor([[1, 2, 3],        [4, 5, 6],        [7, 8, 9]])torch.trace(x)Out[1]: tensor(15)torch.tril: torch.tril(input, diagonal=0, out=None) → Tensor返回一个张量out，包含输入矩阵(2D张量)的下三角部分，out其余部分被设为01234567891011121314151617x = torch.arange(1, 10).view(3, 3)Out[0]: tensor([[1, 2, 3],        [4, 5, 6],        [7, 8, 9]])torch.tril(x)Out[1]: tensor([[1, 0, 0],        [4, 5, 0],        [7, 8, 9]])torch.tril(x, diagonal=1)Out[2]: tensor([[1, 2, 0],        [4, 5, 6],        [7, 8, 9]])torch.triu: torch.triu(input, k=0, out=None) → Tensor返回一个张量，包含输入矩阵(2D张量)的上三角部分，其余部分被设为0。这里所说的上三角部分为矩阵指定对角线diagonal之上的元素。参数k控制对角线: - k = 0, 主对角线 - k &gt; 0, 主对角线之上 - k &lt; 0, 主对角线之下1234567891011121314151617x = torch.arange(1, 10).view(3, 3)Out[0]: tensor([[1, 2, 3],        [4, 5, 6],        [7, 8, 9]])torch.triu(x)Out[1]: tensor([[1, 2, 3],        [0, 5, 6],        [0, 0, 9]])torch.triu(x, diagonal=1)Out[2]: tensor([[0, 2, 3],        [0, 0, 6],        [0, 0, 0]])torch.cumprod: torch.cumprod(input, dim, out=None) → Tensor返回输入沿指定维度的累积，例如，如果输入是一个N 元向量，则结果也是一个N 元向量，12345a = torch.Tensor([1, 2, 3, 4])Out[0]: tensor([1., 2., 3., 4.])torch.cumprod(a, dim=0)Out[1]: tensor([ 1.,  2.,  6., 24.])torch.cumsum: torch.cumsum(input, dim, out=None) → Tensor返回输入沿指定维度的累加，例如，如果输入是一个N 元向量，则结果也是一个N 元向量，12345a = torch.Tensor([1, 2, 3, 4])Out[0]: tensor([1., 2., 3., 4.])torch.cumsum(a, dim=0)Out[1]: tensor([ 1.,  3.,  6., 10.])BLAS和LAPACK操作BLAS（Basic Linear Algebra Subprograms）和LAPACK（Linear Algebra Package）是两个广泛使用的数学库，它们提供了一系列的数学运算，这些运算是高性能线性代数计算的基础BLAS 提供了基本的线性代数运算，它主要关注向量与向量之间（Level 1 BLAS）、矩阵与向量之间（Level 2 BLAS）以及矩阵与矩阵之间（Level 3 BLAS）的运算。BLAS 的这些操作是高度优化的，旨在提供高效率的计算，这对于任何需要大量线性代数计算的程序都是非常重要的。例如，BLAS 提供了矩阵乘法、向量加法、标量与向量的乘法等基础操作LAPACK 构建于 BLAS 之上，提供了更复杂的线性代数运算，如求解线性方程组、计算矩阵特征值和特征向量、奇异值分解、LU分解、QR分解等。LAPACK 是为了解决更大规模的线性代数问题而设计的，它能够利用 BLAS 提供的基础操作来实现更高级的数学运算在很多现代的数值计算环境或科学计算库中，例如 NumPy、SciPy、MATLAB 和 R，底层都直接或间接地使用了 BLAS 和 LAPACK 的实现。这些库通常会链接到特定硬件优化版本的 BLAS 和 LAPACK，如 MKL（Intel Math Kernel Library）或 OpenBLAS，以获得更好的性能BLAS 和 LAPACK 是高性能数值计算领域的标准构件，它们为复杂的线性代数运算提供了强大的支持torch.addbmmtorch.addmmtorch.addmvtorch.addrtorch.baddbmmtorch.bmmtorch.btrifacttorch.btrisolvetorch.dot: 计算两个张量的点乘(内乘),两个张量都为1-D 向量torch.dot(tensor1, tensor2) → float12torch.dot(torch.Tensor([2, 3]), torch.Tensor([2, 1]))Out[0]: tensor(7.)torch.linalg.eig: 计算实方阵a 的特征值和特征向量torch.linalg.eig(A, * , out=None)12345678910A = torch.randn(2, 2, dtype=torch.complex128)Out[0]: tensor([[-0.2029-0.0673j, -0.5188-0.6723j],        [-1.1984+0.0585j,  0.5786-0.1849j]], dtype=torch.complex128)L, V = torch.linalg.eig(A)Out[1]: (tensor([-0.7870-0.5003j,  1.1626+0.2481j], dtype=torch.complex128), tensor([[ 0.7596+0.0000j, -0.4008-0.3285j],         [ 0.6258-0.1770j,  0.8552+0.0000j]], dtype=torch.complex128))</content>
	</entry>
	<entry>
		<title>python常用库学习</title>
		<url>/article/31546.html</url>
		<content>图像相关10个Python图像处理工具，非常全了！图像的RGB 色彩模式RGB 三个颜色通道的变化和叠加得到各种颜色，其中R 红色，取值范围，0-255G 绿色，取值范围，0-255B 蓝色，取值范围，0-255一幅图像上的所有像素点的信息就完全可以采用矩阵来表示，通过矩阵的运算实现更加复杂的操作网络图片读取12345678def get_url_img_io(url: str) -&gt; BytesIO:    &quot;&quot;&quot; 获取网络图片的io流 &quot;&quot;&quot;    return BytesIO(requests.get(url).content)def get_url_img(url: str):    &quot;&quot;&quot; 获取网络图片,并转为np.array &quot;&quot;&quot;    return np.asarray(bytearray(get_url_img_io(url).read()), dtype=&quot;uint8&quot;)plt.imshow()函数负责对图像进行处理，并显示其格式plt.show()则是将plt.imshow()处理后的函数显示出来PILPython图像库（PIL(Python Image Library)）是一个第三方Python包，为Python解释器添加了图像处理功能，允许处理照片并执行许多常见的图像文件操作，官方教程打开图像：返回一个PIL.JpegImagePlugin.JpegImageFile对象12345678910111213# 导入 Image 模块import requestsfrom PIL import Image# 图像路径img_path = r'frame.jpg'img_url = r'https://pic.hycbook.com/i/hexo/post_cover/蕾姆10.webp'# 打开本地图片  RGBim = Image.open(img_path) # JpegImageFile image mode=RGB size=1408x1152# 打开网络图片img = Image.open(get_url_img_io(img_url))   # [H,W,C]JpegImageFile对象常用方法123456789101112131415161718import matplotlib.pyplot as plt# Returns a histogram for the imagehistogram = image.histogram()# 可视化颜色分布直方图plt.hist(histogram, bins=len(histogram))plt.xlabel('Histogram')plt.show()# 裁剪图像  用于裁剪的开始和结束x/y坐标# 用于裁剪的坐标将随照片而变化。事实上，可能应该更改此代码，使其接受裁剪坐标作为参数。可以自己反复尝试，找出要使用的裁剪边界框。可以使用像Gimp这样的工具来帮助你，用Gimp绘制一个边界框，并记下它给你的坐标，方便尝试使用Pillowcropped = image.crop((40, 590, 979, 1500))# 重新调整图像大小img = im.resize((240,240))# 图像旋转img.rotate(90).show()使用过滤器：Pillow包含有几个过滤器，可以将其应用于图像。以下是当前支持的筛选器BLURCONTOURDETAILEDGE_ENHANCEEDGE_ENHANCE_MOREEMBOSSFIND_EDGESSHARPENSMOOTHSMOOTH_MORE123from PIL import ImageFilterblurred_image = image.filter(ImageFilter.BLUR)显示图片1234# 打开图片im = Image.open(img_path)# 显示图片im.show()保存图像1234infile = &quot;in.jpg&quot;outfile = &quot;output.jpg&quot;with Image.open(infile) as im:im.save(outfile)类型转换12345# JpegImageFile -&gt; np.array  [H,W,C]np.asanyarray(im)# np.array -&gt; JpegImageFileimg = Image.fromarray(arr)cv2在计算机视觉项目的开发中，OpenCV作为较大众的开源库，拥有了丰富的常用图像处理函数库，采用C/C++语言编写，可以运行在Linux/Windows/Mac等操作系统上，能够快速的实现一些图像处理和识别的任务。此外，OpenCV还提供了Java、python、cuda等的使用接口、机器学习的基础算法调用，从而使得图像处理和图像分析变得更加易于上手，让开发人员更多的精力花在算法的设计上打开图像：cv2.imread(filepath, flags)，返回一个np.array对象flags：读入图片的标志cv2.IMREAD_COLOR：默认参数，读入一副彩色图片，忽略alpha通道cv2.IMREAD_GRAYSCALE：读入灰度图片cv2.IMREAD_UNCHANGED：顾名思义，读入完整图片，包括alpha通道123456789# 图像路径img_path = r'frame.jpg'img_url = r'https://pic.hycbook.com/i/hexo/post_cover/蕾姆10.webp'# 打开网络图片img = get_url_img(img_url)# 打开本地图片image = cv2.imread(img_path)  # [H,W,C]  np.array打开视频常见操作resize:cv2.resize(InputArray src, OutputArray dst, Size, fx, fy, interpolation)fx, fy沿x轴，y轴的缩放系数interpolation插入方式INTER_NEAREST最近邻插值INTER_LINEAR双线性插值（默认设置）INTER_AREA使用像素区域关系进行重采样。INTER_CUBIC4x4像素邻域的双三次插值INTER_LANCZOS48x8像素邻域的Lanczos插值12img_test = cv2.resize(img, (0, 0), fx=0.25, fy=0.25, interpolation=cv2.INTER_NEAREST)img_test = cv2.resize(img_test, (0, 0), fx=4, fy=4, interpolation=cv2.INTER_NEAREST)RGB转灰度：1img2 = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)灰度转RBG：1img3 = cv2.cv2Color(img, cv2.COLOR_GRAY2RGB)显示图片12cv2.imshow(&quot;local pic&quot;, image)cv2.waitKey(0)保存图像1cv2.imwrite(outfile, image)时间相关库协调世界时(Coordinated Universal Time, UTC)是一种标准的时间表述方式，它与时区无关有些计算机，用某时刻与UNIX时间原点之间相差的秒数，来表示那个时刻所对应的时间对于这些计算机来说，UTC是一种非常好的计时方式Python提供了两种时间转换方式旧的方式，是使用内置的time模块，这是一种极易出错的方式新的方式，则是采用内置的datetime模块，该模块的效果非常好小结：time模块需要依赖操作系统而运作。该模块的实际行为，取决于底层的C函数如何与宿主操作系统相交互这种工作方式，使得time模块的功能不够稳定。它无法协调地处理各种本地时区，因此，我们应该尽量少用这个模块如果一定要使用time模块，那就只应该用它在UTC与宿主计算机的当地时区之间进行转换对于其他类型的转换来说，还是使用datetime模块比较好常见的时间模块time: Python内置时间库，通过时间戳或元组表示时间，功能简约但实用datetime: 内置日期库，处理日期时间对象和属性dateutil: 基于datetime库的实用拓展，增强了对时间间隔和时间序列的处理time库的使用 time— Time access and conversionstime库是Python中处理时间的标准库计算机时间的表达提供获取系统时间并格式化输出功能提供系统级精确计时功能，用于程序性能分析时间获取12345678910111213141516# 获取当前时间戳，即计算机内部时间值，浮点数&gt;&gt;&gt;time.time()1568506809.169575# 获取当前时间并以易读方式表示，返回字符串, 将 gmtime()或localtime() 返回的 表示时间的 元组或struct_time 转换成一个字符串，格式：'Sun Jun 20 23:21:051993'。&gt;&gt;&gt;time.ctime()  # 等价于 time.asctime(time.localtime(time.time()))'Sun Sep 15 08:20:16 2019'# 获取当前时间，表示为计算机可处理的时间格式，返回的时间是两个时区对相同时刻的时间表示# 返回的是struct_time格式，是tuple的子类&gt;&gt;&gt;time.gmtime()  # 0时区:time.gmtime() 本地时区:time.localtime(seconds=None)time.struct_time(tm_year=2019, tm_mon=9, tm_mday=15, tm_hour=0, tm_min=21, tm_sec=40, tm_wday=6, tm_yday=258, tm_isdst=0)# time.mktime(t)是localtime()的反函数&gt;&gt;&gt;time.mktime(time.localtime())1568508316.0struct_time元组的属性如下序号属性值0tm_year20081tm_mon1 到 122tm_mday1 到 313tm_hour0 到 234tm_min0 到 595tm_sec0 到 61 (60或61 是闰秒)6tm_wday0到6 (0是周一)7tm_yday1 到 366(儒略历)8tm_isdst-1, 0, 1, -1是决定是否为夏令时的旗帜时间格式化对象转字符串: time.strftime(format, obj)字符串转结构化: time.strptime(time_str, format)1234567891011121314151617181920# strftime(tpl, ts)# tpl是格式化模板字符串，用来定义输出效果# ts是计算机内部时间类型变量&gt;&gt;&gt;t = time.gmtime()&gt;&gt;&gt;time.strftime(&quot;%Y-%m-%d %H:%M:%S&quot;, t)'2019-09-15 00:24:23'# strptime(str, tpl)# str是字符串形式的时间值# tpl是格式化模板字符串，用来定义输入效果&gt;&gt;&gt;timeStr = '2018-01-26 12:55:20'&gt;&gt;&gt;time.strptime(timeStr, &quot;%Y-%m-%d %H:%M:%S&quot;)time.struct_time(tm_year=2019, tm_mon=9, tm_mday=15, tm_hour=0, tm_min=25, tm_sec=50, tm_wday=6, tm_yday=258, tm_isdst=-1)# 结构化对象转时间戳&gt;&gt;&gt;time.mktime(time.localtime())1668562455.0# 时间戳转结构化对象&gt;&gt;&gt;time.localtime(time.time())time.struct_time(tm_year=2022, tm_mon=11, tm_mday=16, tm_hour=9, tm_min=37, tm_sec=9, tm_wday=2, tm_yday=320, tm_isdst=0)常用格式(年月日、时分秒)| 格式化字符串 | 日期/时间说明  | 值范围和实例                   || —————— | ——————— | ——————————————— || %Y           | 年份           | 0000~9999，例如：1900          || %y           | 去掉世纪的年份 | 00~99                          || %m           | 月份           | 01~12，例如：10                || %B           | 月份名称       | January~December，例如：April  || %b           | 月份名称缩写   | Jan~Dec，例如：Apr             || %H           | 小时（24h制）  | 00~23，例如：12                || %h           | 小时（12h制）  | 01~12，例如：7                 || %M           | 分钟           | 00~59，例如：26                || %S           | 秒             | 00~59，例如：26                |其他格式| 格式化字符串 | 日期/时间说明        | 值范围和实例                   || —————— | —————————— | ——————————————— || %w           | 星期中的第几天       | 0~6，0对应星期天               || %d           | 日期，一月中的第几天 | 01~31，例如：25                || %j           | 一年中的第几天       | 001~366                        || %A           | 星期                 | Monday~Sunday，例如：Wednesday || %a           | 星期缩写             | Mon~Sun，例如：Wed             || %p           | 上/下午              | AM, PM，例如：PM               || %Z           | 时区的名字           |                                |将时间以合理的方式展示出来格式化：类似字符串格式化，需要有展示模板展示模板由特定的格式化控制符组成strftime()方法程序计时: 测量起止动作所经历时间的过程测量时间：perf_counter()产生时间：sleep()1234567891011# 返回一个CPU级别的精确时间计数值，单位为秒# 由于这个计数值起点不确定，连续调用差值才有意义&gt;&gt;&gt;start = time.perf_counter()318.66599499718114&gt;&gt;&gt;end = time.perf_counter()341.3905185375658&gt;&gt;&gt;end - start22.724523540384666# s拟休眠的时间，单位是秒，可以是浮点数time.sleep(5)datetime 模块python 处理日期和时间的标准库，常用 datetime 类，及 date 类 和 time 类，可做计算之类的操作time模块比较底层，能完成的功能相对有限，这个时候就需要更高级的datetime模块来参与了可以简单理解为，datetime模块是对time模块进行了更高一层的封装基本定义和属性方法12345678910111213141516171819202122232425262728293031323334import datetimeimport sysfrom typing import Tuplefrom dateutil import relativedeltafrom dateutil.parser import parsedef fn_basic() -&gt; Tuple:    &quot;&quot;&quot; 基本定义和属性方法 &quot;&quot;&quot;    # 获取当前日期时间    d_now = datetime.datetime.now()  # 等价于datetime.datetime.today()    d_utc_now = datetime.datetime.utcnow()  # 获取utc对象, 北京时间+8h=d_utc_now    # 至少要给定年月日    d_free = datetime.datetime(year=2022, month=11, day=11)    print(&quot;d_now &quot;, d_now)    print(f&quot;d_now的详细信息: year:{d_now.year} month:{d_now.month} day:{d_now.day}&quot;)    print(f&quot;d_now的详细信息: hour:{d_now.hour} minute:{d_now.minute} second:{d_now.second} microsecond:{d_now.microsecond}&quot;)    print(&quot;d_free&quot;, d_free)    # 常用属性    d = d_free.date()  # 获取日期对象    t = d_free.time()  # 获取时间对象    dt = datetime.datetime.combine(date=d, time=t)  # 根据date和time, 创建一个datetime对象    ts = d_free.timestamp()  # 获取对象的时间戳    print(&quot;ts    &quot;, ts)    print(sys._getframe().f_code.co_name, &quot;=&quot; * 60, '\n')    return d_now, d_free# 输出d_now  2022-11-15 16:59:45.447093d_now的详细信息: year:2022 month:11 day:15d_now的详细信息: hour:16 minute:59 second:45 microsecond:447093d_free 2022-11-11 00:00:00ts     1668096000.0fn_basic ============================================================ 日期时间差异12345678910111213141516171819def fn_dif(d_now, d_free):    &quot;&quot;&quot; 日期时间差异 &quot;&quot;&quot;    # type(dif) == timedelta    dif = d_now - d_free    print(&quot;d_free+dif==d_now is&quot;, d_free + dif == d_now)    # 下个月 前一天 往后9小时 再往前10分钟    d_dif = d_free + relativedelta.relativedelta(months=1, days=-1, hours=9, minutes=-10)    print(&quot;d_dif &quot;, d_dif)    # 指定时间，只能是整数    d_dif_abs = d_free + relativedelta.relativedelta(month=1, day=5, hour=9, minute=10)    print(&quot;d_dif_abs &quot;, d_dif_abs)    print(sys._getframe().f_code.co_name, &quot;=&quot; * 60, '\n')   # 输出d_free+dif==d_now is Trued_dif  2022-12-10 08:50:00d_dif_abs  2022-01-05 09:10:00fn_dif ============================================================ 日期时间的格式化1234567891011121314151617181920212223242526272829303132def fn_format_transform():    &quot;&quot;&quot; 日期时间的格式化 &quot;&quot;&quot;    date_format_str = &quot;%Y-%m-%d %H:%M:%S&quot;    date_format_str_2 = &quot;{:%Y-%m-%d %H:%M:%S}&quot;    # 获取当前日期时间    d_now = datetime.datetime.now()    # 对象转字符串    date_string = d_now.strftime(date_format_str)    print(&quot;date_string:&quot;, date_string)    date_string_2 = date_format_str_2.format(d_now)    print(&quot;date_string_2:&quot;, date_string_2)    # 字符串转对象    d_from_str = datetime.datetime.strptime(date_string, date_format_str)    print(&quot;d_from_str:&quot;, d_from_str)    # dateutil.parse 可以解析几乎所有人类能够理解的日期表示形式    d_parse = parse(date_string)    print(&quot;parse:&quot;, d_parse)    print(&quot;parse('Jan 31, 2021 10:45 PM'): &quot;, parse('Jan 31, 2022 10:45 PM'))    print(&quot;parse(str(d_now)): &quot;, parse(str(d_now)))    print(sys._getframe().f_code.co_name, &quot;=&quot; * 60, '\n')   # 输出date_string: 2022-11-15 16:59:45date_string_2: 2022-11-15 16:59:45d_from_str: 2022-11-15 16:59:45parse: 2022-11-15 16:59:45parse('Jan 31, 2021 10:45 PM'):  2022-01-31 22:45:00parse(str(d_now)):  2022-11-15 16:59:45.447093fn_format ============================================================ 产生指定范围内的日期时间123456789101112131415161718192021222324252627def gen_date_times():    &quot;&quot;&quot; 产生指定范围内的日期时间 &quot;&quot;&quot;    def date_time(p_start_time, p_end_time, p_format: str = '%Y-%m-%d'):        while p_start_time &lt; p_end_time:            select_time = p_start_time.strftime(p_format)            print(select_time)            p_start_time += datetime.timedelta(days=1)    # 获取当天时间    format_ = '%Y-%m-%d'    end_time = datetime.datetime.now()    start_time = end_time + relativedelta.relativedelta(days=-10)    date_time(p_start_time=start_time, p_end_time=end_time, p_format=format_)# 输出2022-11-052022-11-062022-11-072022-11-082022-11-092022-11-102022-11-112022-11-122022-11-132022-11-142022-11-15calendar库的使用提供与日历相关功能，如：为给定的月份或年份，打印文本日历12345678910111213import calendarc = calendar.month(2019, 9)print(c)   September 2019Mo Tu We Th Fr Sa Su                   1 2  3  4  5  6  7  8 9 10 11 12 13 14 1516 17 18 19 20 21 2223 24 25 26 27 28 2930random库的使用random库是使用随机数的Python标准库伪随机数: 采用梅森旋转算法生成的(伪)随机序列中元素random库主要用于生成随机数使用random库: import random主要方法基本随机数函数： seed(), random()扩展随机数函数： randint(), getrandbits(), uniform(), randrange(), choice(), shuffle()基本随机数函数123456789101112131415161718192021222324# 初始化给定的随机数种子，默认为当前系统时间&gt;&gt;&gt;random.seed(10) #产生种子10对应的序列# 生成一个[0.0, 1.0)之间的随机小数&gt;&gt;&gt;random.random()0.5714025946899135# 返回捕获当前生成器内部状态的对象.该对象可以用于函数setstate()取保存当前的状态.&gt;&gt;&gt; state = random.getstate()&gt;&gt;&gt; random.setstate(state)# 以相同顺序打乱多个数组# 等价于  np.random.get_state()    np.random.shuffle(a)   np.random.set_state(state)a = np.arange(10)b=['A','B','C','D','E','F','G','H','I','J']state=random.getstate()random.shuffle(a)print(a)random.setstate(state)random.shuffle(b)print(b)[9 4 5 0 1 2 6 8 7 3]['J', 'E', 'F', 'A', 'B', 'C', 'G', 'I', 'H', 'D']扩展随机数函数12345678910111213141516171819202122232425# 生成一个[a, b]之间的整数&gt;&gt;&gt;random.randint(10, 100)64# 生成一个[m, n)之间以k为步长的随机整数&gt;&gt;&gt;random.randrange(10, 100, 10)80     # 生成一个k比特长的随机整数&gt;&gt;&gt;random.getrandbits(16)37885# 生成一个[a, b]之间的随机小数&gt;&gt;&gt;random.uniform(10, 100)13.096321648808136# 从序列seq中随机选择一个元素&gt;&gt;&gt;random.choice([1,2,3,4,5,6,7,8,9])8# 将序列seq中元素随机排列，返回打乱后的序列&gt;&gt;&gt; s=[1,2,3,4,5,6,7,8,9];&gt;&gt;&gt; random.shuffle(s);&gt;&gt;&gt; print(s)[3, 5, 8, 9, 6, 1, 2, 7, 4]函数描述randint(a, b)生成一个[a, b]之间的整数randrange(m, n[, k])生成一个[m, n)之间以k为步长的随机整数getrandbits(k)生成一个k比特长的随机整数uniform(a, b)生成一个[a, b]之间的随机小数choice(seq)从序列seq中随机选择一个元素shuffle(seq)将序列seq中元素随机排列，返回打乱后的序列命令行脚本传参命令行运行Python脚本时传入参数的三种方式如果在运行python脚本时需要传入一些参数，例如gpus与batch_size，可以使用如下三种方式。123python script.py 0,1,2 10python script.py --gpus=0,1,2 --batch-size=10python script.py --gpus=0,1,2 --batch_size=10这三种格式对应不同的参数解析方式，分别为sys.argv, argparse, tf.app.run前两者是python自带的功能，后者是tensorflow提供的便捷方式。sys.argvsys模块是很常用的模块， 它封装了与python解释器相关的数据，例如sys.modules里面有已经加载了的所有模块信息，sys.path里面是PYTHONPATH的内容，而sys.argv则封装了传入的参数数据使用sys.argv接收上面第一个命令中包含的参数方式如下：123456import sysgpus = sys.argv[1]# gpus = [int(gpus.split(','))]batch_size = sys.argv[2]print gpusprint batch_sizeargparse官方文档使用argparse从命令行接收bool类型的参数Python argparse库用法总结速览1234567891011121314import argparse# 创建一个解析对象parser = argparse.ArgumentParser(prog='我的解析对象')# 添加你要关注的命令行参数和选项parser.add_argument('--doc_path', '-dp')parser.add_argument('-tip', '--tp')# 解析参数opt = parser.parse_args()# 带--的参数可以直接用名字访问print(opt.tp, opt.doc_path)可以用python arg.py -h获取帮助ArgumentParser1class argparse.ArgumentParser(prog=None, usage=None, description=None, epilog=None, parents=[], formatter_class=argparse.HelpFormatter, prefix_chars='-', fromfile_prefix_chars=None, argument_default=None, conflict_handler='error', add_help=True, allow_abbrev=True, exit_on_error=True)创建一个新的ArgumentParser对象，所有的参数都应当作为关键字参数传入，每个参数在下面都有它更详细的描述prog - 程序的名称 (默认值: os.path.basename(sys.argv[0]))usage - 描述程序用途的字符串（默认值：从添加到解析器的参数生成）description - 在参数帮助文档之后显示的文本 (默认值:无)epilog - Text to display after the argument help (by default, no text)parents - 一个 ArgumentParser 对象的列表，它们的参数也应包含在内formatter_class - 用于自定义帮助文档输出格式的类argparse.RawDescriptionHelpFormatter：表示 description 和 epilog 已经被正确的格式化argparse.RawTextHelpFormatter：保留所有种类文字的空格，包括参数的描述argparse.ArgumentDefaultsHelpFormatter：自动添加默认的值的信息到每一个帮助信息的参数中:argparse.MetavarTypeHelpFormatter：每一个参数中使用 type 的参数名当作它的显示名prefix_chars - 可选参数的前缀字符集合（默认值： ‘-‘），许多命令行会使用 - 当作前缀，比如 -f/--foo。如果解析器需要支持不同的或者额外的字符，比如像 +f 或者 /foo 的选项，可以在参数解析构建器中使用 prefix_chars= 参数fromfile_prefix_chars - 当需要从文件中读取其他参数时，用于标识文件名的前缀字符集合（默认值： None）argument_default - 参数的全局默认值（默认值： None）conflict_handler - 解决冲突选项的策略（通常是不必要的）add_help - 为解析器添加一个 -h/--help 选项（默认值： True），有时候可能会需要关闭额外的帮助信息allow_abbrev - 如果缩写是无歧义的，则允许缩写长选项 （默认值：True）exit_on_error - 正常情况下，当你向 ArgumentParser 的 parse_args() 方法传入一个无效的参数列表时，它将会退出并发出错误信息。如果用户想要手动捕获错误，可通过将 exit_on_error 设为 False 来启用该特性12345678910111213141516171819202122232425262728293031323334353637383940414243import argparse# 实例化parser = argparse.ArgumentParser(prog=&quot;hyc_prog&quot;, usage='%(prog)s [options]',                                 description='[%(prog)s] 参数解析描述，用于演示',                                 epilog=&quot;description 参数后显示额外的对程序的描述&quot;,                                 # 表示 description 和 epilog 已经被正确的格式化了                                 formatter_class=argparse.RawDescriptionHelpFormatter,                                 prefix_chars='-+',  # 解析器需要支持不同的或者额外的字符                                 argument_default=argparse.SUPPRESS, # 全局禁止在 parse_args() 中创建属性                                 )# fromfile_prefix_chars参数with open('args.txt', 'w', encoding=sys.getfilesystemencoding()) as fp:    fp.write('-f\nbar')parser = argparse.ArgumentParser(fromfile_prefix_chars='@')parser.add_argument('-f')parser.parse_args(['-f', 'foo', '@args.txt'])# conflict_handler参数parser = argparse.ArgumentParser(prog='PROG', conflict_handler='resolve')parser.add_argument('-f', '--foo', help='old foo help')parser.add_argument('--foo', help='new foo help')parser.print_help()&gt;&gt;&gt; usage: PROG [-h] [-f FOO] [--foo FOO]options: -h, --help  show this help message and exit -f FOO      old foo help --foo FOO   new foo help# exit_on_error参数parser = argparse.ArgumentParser(exit_on_error=False)parser.add_argument('--integers', type=int)try:    parser.parse_args('--integers a'.split())except argparse.ArgumentError:    print('Catching an argumentError')add_argument1ArgumentParser.add_argument(name or flags...[, action][, nargs][, const][, default][, type][, choices][, required][, help][, metavar][, dest])定义单个的命令行参数应当如何解析，每个形参都在下面有它自己更多的描述，长话短说有：name or flags - 一个命名或者一个选项字符串的列表，例如 foo 或 -f, —fooaction - 当参数在命令行中出现时使用的动作基本类型nargs - 命令行参数应当消耗的数目const - 被一些 action 和 nargs 选择所需求的常数default - 当参数未在命令行中出现并且也不存在于命名空间对象时所产生的值type - 命令行参数应当被转换成的类型choices - A sequence of the allowable values for the argument.required - 此命令行选项是否可省略(仅选项可用)，只能用于可选参数(optional arguments)help - 一个此选项作用的简单描述metavar - 在使用方法消息中使用的参数值示例dest - 被添加到parse_args()所返回对象上的属性名name or flagsname就是指命令行参数中没有’-‘的参数名字例如’myname’，而flags就是指前面有’-‘的参数名，例如’-a’、’—age’其中name对应位置参数，而flags对应可选参数，name在命令行必须输入，并按照顺序喂给程序命令行传入参数时，对于位置参数我们直接给出其值，对于可选参数需要给出其flags。argparse会先将可选参数进行解析，对于剩余未解析的参数，传给位置参数1234567891011import argparseparser = argparse.ArgumentParser(description='a test')parser.add_argument('--age','-a')parser.add_argument('myname')parser.add_argument('sex')# 注意前两个参数必须是 myname和sexargs = parser.parse_args('tom boy --age 22'.split())print(args) # Namespace(age='22', sex='boy', myname='tom')命令行传入参数时，先写位置参数，再写可选参数action: 指定了参数是如何被处理的，支持额操作如下store：这个只是简单的存储这个参数值，这也是默认操作store_const: 存储const指定的值，配合const参数store_false和store_true: 分别对应存储False和True值，它们是store_const的特例append：保存为列表格式，将每个参数的值添加到这个列表123456789101112131415161718192021222324import argparse# action='store_const'  传参时只要传入参数名,会默认附上const, 如果不传该参数名, 默认为Noneparser=argparse.ArgumentParser()parser.add_argument('--foo',action='store_const',const=42)parser.parse_args('--foo'.split())Out[0]: Namespace(foo=42)parser.parse_args(''.split())Out[1]: Namespace(foo=None)# store_false和store_true# 以store_false为例, 当传入参数名时, 值为False, 不传参数名时值为Trueparser.add_argument('--bar', action='store_false')parser.parse_args(''.split())Out[2]: Namespace(bar=True, foo=None)parser.parse_args('--bar'.split())Out[3]: Namespace(bar=False, foo=None)# action='append'parser = argparse.ArgumentParser()parser.add_argument('--foo', action='append')parser.parse_args('--foo 1 --foo 2'.split())Out[5]: Namespace(foo=['1', '2'])nargs: nargs的意思就是输入参数的个数N—某正整数: 指定好后，不能输入多了，也不能输入少了‘?’: 这时代表parser会读取0个或1个参数，具体遵循下面的原则：如果给出了1个参数，照常读取这个参数保存起来如果只给出了flags，比如‘—age’后面未给出具体值，则保存const参数的值（如果const未给出则为None）如果什么都没给，则保存default参数的值（如果default未给出则为None）‘*‘:  不确定具体个数，那么可以用nargs=*‘+’: 要求参数的个数必须大于如果不给flags，parser会用default的值；如果只给flags，不给值，此时会报错而当nargs=’*’时，就算只给flags不给值，也不会报错，会得到一个空列表参数123456789101112131415import argparse# nargs=?parser = argparse.ArgumentParser(description='a test')parser.add_argument('name')parser.add_argument('--age','-a', nargs='?',const=16,default=18)args = parser.parse_args('tom -a 15'.split())print(args) # Namespace(age='15', name='tom')args = parser.parse_args('tom -a'.split())print(args) # Namespace(age=16, name='tom')args = parser.parse_args('tom'.split())print(args) # Namespace(age=18, name='tom')const: 多是配合其它参数出演的配角action参数为 ‘store_const’ 时或是 ‘append_const’ 时nargs参数为 ‘?’ 时default当命令行完全没有提到某个参数时，default参数就会发挥作用，default的默认值为Nonetype这个类型参数可以约束输入参数的类型，当类型转换合法时，会自动帮我们进行类型转换12345678910111213141516171819import argparsedef str2bool(v):    if v.lower() in ('yes', 'true', 't', 'y', '1'):        return True    elif v.lower() in ('no', 'false', 'f', 'n', '0'):        return False    else:        raise argparse.ArgumentTypeError('Unsupported value encountered.')# type=intparser = argparse.ArgumentParser(description='a test')parser.add_argument('name')parser.add_argument('--age','-a',type=int, default='17')args = parser.parse_args('tom'.split())print(args) # Namespace(age=17, name='tom')# 自定义类型parser.add_argument('--is_del_aft', type=str2bool, default=False)choices这个选项参数可以使用列表约束输入参数的取值范围。如果输入参数不在候选参数列表中，程序会报错1234567891011import argparseparser = argparse.ArgumentParser(description='a test')parser.add_argument('name',choices=['tom','Jim','Bob'])parser.add_argument('--age','-a',type=int, default='17')args = parser.parse_args('Jim'.split())print(args) # Namespace(age=17, name='Jim')args = parser.parse_args('Toy'.split())print(args) # test.py: error: argument name: invalid choice: 'Toy' (choose from 'tom', 'Jim', 'Bob')在参数获取阶段，约束好用户输入的参数范围，可以防止意想不到的参数带来的未知后果metavar: 这个参数的功能也是个性化显示帮助信息1234567891011121314151617181920212223242526import argparseparser = argparse.ArgumentParser(description='a test')parser.add_argument('name',metavar='haha')# 加metavarparser.add_argument('--age','-a',type=int, default='17',metavar='hahahaha')parser.parse_args('-h'.split())&gt;&gt;&gt; usage: pydevconsole.py [-h] [--age hahahaha] hahaa testpositional arguments:  hahaoptional arguments:  -h, --help            show this help message and exit  --age hahahaha, -a hahahaha# 不加metavarparser.add_argument('--age','-a',type=int, default='17')&gt;&gt;&gt; usage: pydevconsole.py [-h] [--age AGE] hahaa testpositional arguments:  hahaoptional arguments:  -h, --help         show this help message and exit  --age AGE, -a AGEdest每个参数待parser处理完毕后，都会以‘属性名-属性’的形式保存起来对于位置参数，属性名就是位置参数的name对于可选参数，属性名是可选参数去掉‘—’后的那部分，如果没有‘—’，则为去掉‘-’的那部分为了使得属性名合法，parser还会将单词中间的短横杠变为下划线但如果，你不想用上述方法自动生成的属性名，你想自己指定属性名，就可以设定dest参数来指定。（只能指定可选参数的属性名）123456789101112131415161718import argparse# 创建 ArgumentParser 对象parser = argparse.ArgumentParser(description='An example of using argparse')# 添加位置参数和可选参数parser.add_argument('--input', dest='input_file', help='Input file path')parser.add_argument('--output', dest='output_file', help='Output file path')# 解析命令行参数args = parser.parse_args('--input 输入 --output 输出'.split())# 访问解析后的参数值input_path = args.input_fileoutput_path = args.output_file# 执行相应操作print(f'Input file path: {input_path}')print(f'Output file path: {output_path}')&gt;&gt;&gt;Input file path: 输入Output file path: 输出可以看到，如果不指定dest，年龄的属性名是age。若此时指定了dest=’myage’，那么年龄的属性名就人为设定成了myage实际例子脚本运行命令python script.py -gpus=0,1,2 --batch-size=10中的--batch-size会被自动解析成batch_sizeparser.add_argument 方法的type参数理论上可以是任何合法的类型，但有些参数传入格式比较麻烦例如list，所以一般使用bool, int, str, float这些基本类型就行了，更复杂的需求可以通过str传入，然后手动解析bool类型的解析比较特殊，传入任何值都会被解析成True，传入空值时才为`False123456789101112131415161718192021import argparsedef str2bool(v):    if v.lower() in ('yes', 'true', 't', 'y', '1'):        return True    elif v.lower() in ('no', 'false', 'f', 'n', '0'):        return False    else:        raise argparse.ArgumentTypeError('Unsupported value encountered.')    if __name__ == '__main__':    parser = argparse.ArgumentParser(description='manual to this script')    parser.add_argument('--zook_host', type=str, default='127.0.0.1:2181')    parser.add_argument('--num_of_task', type=int, default=10)    parser.add_argument('--is_del_aft', type=str2bool, default=False)    args = parser.parse_args()    many_tasks_schedule_performance(zook_host=args.zook_host,                                    num_of_task=args.num_of_task,                                    is_del_aft=args.is_del_aft)tf.app.runtensorflow也提供了一种方便的解析方式1234567891011121314151617181920import tensorflow as tftf.app.flags.DEFINE_string('gpus', None, 'gpus to use')tf.app.flags.DEFINE_integer('batch_size', 5, 'batch size')FLAGS = tf.app.flags.FLAGSdef main(_):    print    FLAGS.gpus    print    FLAGS.batch_sizeif __name__ == &quot;__main__&quot;:    tf.app.run()&gt;&gt;&gt; python script.py --gpus=0,1,2 --batch_size=10有几点需要注意tensorflow只提供以下四种方法：tf.app.flags.DEFINE_string, tf.app.flags.DEFINE_integer, tf.app.flags.DEFINE_boolean,  tf.app.flags.DEFINE_float分别对应str, int,bool,float类型的参数这里对bool的解析比较严格，传入1会被解析成True，其余任何值都会被解析成False脚本中需要定义一个接收一个参数的main方法：def main(_):，这个传入的参数是脚本名，一般用不到， 所以用下划线接收。以batch_size参数为例，传入这个参数时使用的名称为--batch_size，也就是说，中划线不会像在argparse 中一样被解析成下划线。tf.app.run()会寻找并执行入口脚本的main方法。也只有在执行了tf.app.run()之后才能从FLAGS中取出参数。从它的签名来看，它也是可以自己指定需要执行的方法的，不一定非得叫main：1run(main=None, argv=None)tf.app.flags只是对argpars的简单封装。代码见https://github.com/tensorflow/tensorflow/blob/r1.2/tensorflow/python/platform/flags.py位运算背景知识🍅二进制在Python中可以通过以”0b”或者”-0b”开头的字符串来表示二进制123# 转二进制print(bin(5))&gt;&gt;&gt; 0b101🥬原码、反码、补码原码: 将一个整数转换成二进制形式，就是其原码。例如6的原码就是0110；-18 的原码就是1000 0000 0001 0010通俗的理解，原码就是一个整数本来的二进制形式反码: 对于正数，它的反码就是其原码(原码和反码相同)；负数的反码是将原码中除符号位以外的所有位(数值位)取反补码: 对于正数，它的补码就是其原码(原码、反码、补码都相同)；负数的补码是其反码加1位运算与: &amp; 按位与或: | 按位或反：~ 按位取反异或: ^ 按位异或按位左移: &lt;&lt; 按位左移，各二进位全部左移n位，高位丢弃，低位补0左移1位相当于 乘以2按位右移: &gt;&gt; 按位右移，所有二进制位向右移动n位，移出的位删掉，进的位补符号位，右移不会改变一个数的符号右移1位相当于 除以2应用场景判断奇数还是偶数使用&amp;运算，与1进行&amp;，如果为1，那么该数为奇数；如果为0，那么该数是偶数交换两个数值第一行，a = a ^ b，很容易理解第二行， b = b ^ a = b ^ a ^ b，由于 b ^ b = 0，所以 b = a ^ 0，即 b = a第三行， a = a ^ b ,由于a在第一步重新赋值，所以，a =  a ^ b ^ a = b，完成了数值交换123a ^= bb ^= aa ^= b寻找数据列表中的独一无二有一个数据列表（2N+1个整数），只有一个数出现了1次，其余N个数都出现了2次。如何找到这个独一无二的数据12345from functools import reducelst = [1,5,6,4,2,6,4,2,1]reduce(lambda a,b : a^b, lst)&gt;&gt;&gt; 输出5计算一个数值的二进制数中有多少个1123456def count_ones(x):    count = 0    while x:        count = count + 1        x = x &amp; (x-1) # 等价于 x = x &amp; (x-1)    return count在一堆数字中找出只出现一次的两个数字1234567891011121314151617# 查找构成res的两个数def split_res(res, lst):    tmp = 1    num_0, num_1 = 0, 0    while not tmp &amp; res:        tmp &lt;&lt;= 1    for i in lst:        if i &amp; tmp:            num_0 ^= i        else:            num_1 ^= i    return num_0, num_1from functools import reducelst = [1,5,6,4,2,6,4,2,1,7]res = reduce(lambda a,b : a^b, lst)num_0, num_1 = split_res(res, lst)路径Python路径操作模块pathlib，看这篇就够了！pathlib和os常用操作对比通过常用路径操作的对比，可以更深刻理解pathlib和os的区别，便于在实际操作中做对照，也便于进行使用替代，详细对比如下：pathlib操作os及os.path操作功能描述Path.resolve()os.path.abspath()获得绝对路径Path.chmod()os.chmod()修改文件权限和时间戳Path.mkdir()os.mkdir()创建目录Path.rename()os.rename()文件或文件夹重命名，如果路径不同，会移动并重新命名Path.replace()os.replace()文件或文件夹重命名，如果路径不同，会移动并重新命名，如果存在，则破坏现有目标。Path.rmdir()os.rmdir()删除目录Path.unlink()os.remove()删除一个文件Path.unlink()os.unlink()删除一个文件Path.cwd()os.getcwd()获得当前工作目录Path.exists()os.path.exists()判断是否存在文件或目录namePath.home()os.path.expanduser()返回电脑的用户目录Path.is_dir()os.path.isdir()检验给出的路径是一个文件Path.is_file()os.path.isfile()检验给出的路径是一个目录Path.is_symlink()os.path.islink()检验给出的路径是一个符号链接Path.stat()os.stat()获得文件属性PurePath.is_absolute()os.path.isabs()判断是否为绝对路径PurePath.joinpath()os.path.join()连接目录与文件名或目录PurePath.nameos.path.basename()返回文件名PurePath.parentos.path.dirname()返回文件路径Path.samefile()os.path.samefile()判断两个路径是否相同PurePath.suffixos.path.splitext()分离文件名和扩展名配置文件解读python配置文件INI/TOML/YAML/ENV的区别iniini文件可能是我们可以使用的最直接的配置文件。ini文件非常适合较小的项目，主要是因为这些文件仅支持1级深的层次结构，ini文件本质上是平面文件，但变量可以属于组123456789101112131415161718[APP]ENVIRONMENT = developmentDEBUG = False[DATABASE]USERNAME: rootPASSWORD: p@ssw0rdHOST: 127.0.0.1PORT: 5432DB: my_database[LOGS]ERRORS: logs/errors.logINFO: data/info.log[FILES]STATIC_FOLDER: staticTEMPLATES_FOLDER: templatespython解析ini文件代码如下所示1234567891011121314151617import configparserconfig = configparser.ConfigParser()path = r'Q:\pyCharmWS\object_detection\test.ini'cfg = config.read(path)# 多种方式获取值config['DATABASE']['HOST']Out[6]: '127.0.0.1'config.get('DATABASE', 'HOST')Out[7]: '127.0.0.1' # 获取指定类型config.getboolean('APP', 'DEBUG')Out[8]: Falseconfig.get('APP', 'DEBUG')Out[9]: 'False'configparser还有许多其他类型检查方法，例如getint()，getfloat()等等tomlTOML文件似乎与ini文件共享某些语法相似之处，但支持更广泛的数据类型以及值本身之间的关系如表中所示，TOML支持嵌套表的概念，该[environments]表后面带有多个子表，通过使用点符号，我们能够创建表的关联，这意味着它们是同一元素的不同实例1234567891011121314151617181920212223242526272829303132333435363738# Keystitle = &quot;My TOML Config&quot;# Tables[project]    name = &quot;Faceback&quot;    description = &quot;Powerful AI which renders the back of somebody's head, based on their face.&quot;    version = &quot;1.0.0&quot;    updated = 1979-05-27T07:32:00Z    author = &quot;Todd Birchard&quot;[database]    host = &quot;127.0.0.1&quot;    password = &quot;p@ssw0rd&quot;    port = 5432    name = &quot;my_database&quot;    connection_max = 5000    enabled = true# Nested `tables`[environments]    [environments.dev]        ip = &quot;10.0.0.1&quot;        dc = &quot;eqdc10&quot;    [environments.staging]        ip = &quot;10.0.0.2&quot;        dc = &quot;eqdc10&quot;    [environments.production]        ip = &quot;10.0.0.3&quot;        dc = &quot;eqdc10&quot;# Array of Tables[[testers]]    id = 1    username = &quot;JohnCena&quot;    password = &quot;YouCantSeeMe69&quot;[[testers]]    id = 3    username = &quot;TheRock&quot;    password = &quot;CantCook123&quot;同样有趣的是概念表列，如下表中的[[testers]]，双括号中的表会自动添加到数组中，其中数组中的每个项目都是具有相同名称的表，等价于下下面JSON所表达的信息12345678910111213{&quot;testers&quot;: [{&quot;id&quot;: 1,&quot;username&quot;: &quot;JohnCena&quot;,&quot;password&quot;: &quot;YouCantSeeMe69&quot;},{&quot;id&quot;: 2,&quot;username&quot;: &quot;TheRock&quot;,&quot;password&quot;: &quot;CantCook123&quot;}]}python解析toml的代码如下12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import tomlpath = r'Q:\pyCharmWS\object_detection\test.toml'cfg = toml.load([path])# Retrieving a dictionarycfg['project']cfg.get('project')# Retrieving a valuecfg['project']['author']cfg.get('project').get('author')print(cfg){'title': 'My TOML Config','project': {'name': 'Faceback','description': &quot;Powerful AI which renders the back of somebody's head, based on their face.&quot;,'version': '1.0.0','updated': datetime.datetime(1979, 5, 27, 7, 32, tzinfo = &lt; toml.tz.TomlTz object at 0x000001AE244933D0 &gt; ),'author': 'Todd Birchard'},'database': {'host': '127.0.0.1','password': 'p@ssw0rd','port': 5432,'name': 'my_database','connection_max': 5000,'enabled': True},'environments': {'dev': {'ip': '10.0.0.1','dc': 'eqdc10'},'staging': {'ip': '10.0.0.2','dc': 'eqdc10'},'production': {'ip': '10.0.0.3','dc': 'eqdc10'}},'testers': [{'id': 1,'username': 'JohnCena','password': 'YouCantSeeMe69'}, {'id': 3,'username': 'TheRock','password': 'CantCook123'}]}yaml系统变量1234from os import environenviron.get('ComSpec')Out[0]: 'C:\\WINDOWS\\system32\\cmd.exe'流</content>
	</entry>
	<entry>
		<title>深度学习在图像领域的应用</title>
		<url>/article/35455.html</url>
		<content>必读！计算机视觉四大基本任务(分类、定位、检测、分割)模型介绍模型可视化netron工具库pytorch模型转onnx模型的方法详解图像分类Stable Diffusion总共包含三个主要的组件人工智能Ai画画——stable diffusion 原理和使用方法详解！1）Clip Text用于文本编码。输入：文本输出：77个token嵌入向量，其中每个向量包含768个维度2）UNet + Scheduler在信息（潜）空间中逐步处理/扩散信息。输入：文本嵌入和一个由噪声组成的初始多维数组（结构化的数字列表，也叫张量tensor）。输出：一个经过处理的信息阵列3）自编码解码器（Autoencoder Decoder），使用处理过的信息矩阵绘制最终图像的解码器。Clip Text 是一种自然语言处理模型，由 OpenAI 开发。它基于 CLIP（Contrastive Language-Image Pretraining）模型，旨在将文本和图像联系起来。Clip Text 模型可以理解和处理文本数据，以便进行各种任务，例如文本分类、情感分析、命名实体识别等。通过训练 Clip Text 模型，可以使其具备对文本的理解能力，从而支持在自然语言处理领域进行各种应用和研究。UNet 是一种用于图像分割的卷积神经网络模型。它最初由 Olaf Ronneberger 等人在 2015 年提出，旨在解决医学图像分割任务中的问题。UNet 的设计灵感来自于生物学中的神经元结构，它具有一个特殊的 U 形结构，因此得名 UNet。UNet 的特点是具有对称的编码器-解码器结构，其中编码器部分由多个卷积和池化层组成，用于逐步提取图像的特征。解码器部分则通过上采样和卷积操作逐步将特征映射恢复到原始图像的尺寸，用于生成分割结果。此外，UNet 还通过跳跃连接（skip connections）在编码器和解码器之间建立了直接连接，以便保留和利用不同层级的特征信息。UNet 在医学图像分割任务中取得了很好的效果，并且在其他领域的图像分割任务中也得到了广泛应用。它的网络结构简单、易于实现和训练，并且能够处理不同尺度和形状的目标物体，因此成为图像分割领域的重要模型之一。目标检测和跟踪图像分割语义分割实例分割图像处理超分辨率除雾图像生成动作识别风格转换人脸识别图像描述OCR深入了解视觉语言模型BEiT: BERT Pre-Training of Image Transformers 2022【深度学习】详解 BEiT《BEIT》-基于图像重建进行预训练！微软提出BEIT，Top-1准确率达86.3%！代码已开源！与其他模型不同，VisionEncoderDecoderModel 是一个标准化的模型，可用于初始化任意图像转文本模型，这类模型可以使用任何预训练的基于Transformer的视觉模型作为编码器(例如ViT(子监督训练)、BEiT 预训练BEiT(自监督训练): 掩码图像建模(MIM)任务，随机mask一定百分比的图像patches，然后预测masked patches对应的视觉tokens、DeiT、Swin)以及任何预训练的语言模型作为解码器(例如RoBERTa、GPT2、BERT、DistilBERT)。事实上，TrOCR是这个标准类的一个实例TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models 2022微软亚洲研究院的研究员们展开了深入研究，提出了首个利用预训练模型的端到端基于Transformer的文本识别OCR模型：TrOCR。该模型简单有效，可以使用大规模合成数据进行预训练，并且能够在人工标注的数据上进行微调。实验证明，TrOCR在打印数据和手写数据上均超过了当前最先进的模型一般的光学字符识别包含两个部分：文本检测和文本识别文本检测: 用于在文本图像中定位文本块，粒度可以是单词级别或是文本行级别目前的解决方案大多是将该任务视为物体检测问题，并采用了如YoLOv5和DBNet的传统物体检测模型文本识别: 致力于理解文本图像并将视觉信号转换为自然语言符号，该任务通常使用编码器-解码器架构现有方法采用了基于CNN网络的编码器进行图像理解，以及基于RNN网络的解码器进行文本生成为了更有效的训练TrOCR模型，研究员们使用了ViT模式的预训练模型和BERT模式的预训练模型，来分别初始化编码器和解码器TrOCRForCausalLMhuggingface TrOCRForCausalLM翻译：The ViTFeatureExtractor class is responsible for preprocessing the input image and RobertaTokenizer decodes the generated target tokens to the target string. The TrOCRProcessor wraps ViTFeatureExtractor and RobertaTokenizer into a single instance to both extract the input features and decode the predicted token ids.ViTFeatureExtractor类负责预处理输入图像，而RobertaTokenizer则将生成的目标标记解码为目标字符串。TrOCRProcessor将ViTFeatureExtractor和RobertaTokenizer封装为单个实例，既可以提取输入特征，又可以解码预测的标记ID翻译：The VisionEncoderDecoderModel can be used to initialize an image-to-text-sequence model with any pretrained vision autoencoding model as the encoder (e.g. ViT, BEiT, DeiT) and any pretrained language model as the decoder (e.g. RoBERTa, GPT2, BERT). The effectiveness of initializing image-to-text-sequence models with pretrained checkpoints has been shown in (for example) TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Minghao Li, Tengchao Lv, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, Furu Wei. An example of how to use a VisionEncoderDecoderModel for inference can be seen in TrOCR.VisionEncoderDecoderModel可以用于初始化一个图像到文本序列模型，其中编码器可以是任何预训练的图像自编码模型（例如ViT、BEiT、DeiT），解码器可以是任何预训练的语言模型（例如RoBERTa、GPT2、BERT）初始化图像到文本序列模型时使用预训练的检查点已经被证明是有效的，例如在《TrOCR：基于预训练模型的基于Transformer的光学字符识别》一文中，作者Minghao Li、Tengchao Lv、Lei Cui、Yijuan Lu、Dinei Florencio、Cha Zhang、Zhoujun Li、Furu Wei展示了这一点关于如何使用VisionEncoderDecoderModel进行推断的示例可以在TrOCR中找到图像特定的归纳偏差图像特定的归纳偏差是指在处理图像数据时，机器学习算法或人类大脑对图像的处理和理解中存在的偏好或倾向性。这种偏差可能导致算法或人类在处理图像时出现系统性的错误或误解。图像特定的归纳偏差可能源自以下几个方面：形状偏差：人类和机器学习算法在处理图像时，可能更容易关注物体的形状和轮廓。这可能导致对于形状特征较强的物体更敏感，而对于纹理、颜色等其他特征的重要性较低。颜色偏差：颜色是图像中的重要特征之一，但人类和机器学习算法可能对不同颜色的感知和辨别能力存在差异。某些颜色的辨别可能更容易，而对于其他颜色的辨别可能相对较差。尺度偏差：图像中的物体尺度可能对于算法或人类的视觉处理产生影响。例如，较大尺度的物体可能更容易被注意到和理解，而较小尺度的物体可能容易被忽略或误解。视角偏差：视角对于理解图像中的物体和场景也具有重要作用。不同的视角可能导致物体的外观发生变化，从而影响算法或人类对物体的认知和解释。这些图像特定的归纳偏差可能会对计算机视觉任务（如物体检测、图像分类等）的性能产生影响，并导致一些常见的误分类或误解。为了减少这些偏差的影响，研究人员和开发者通常会采取一系列的方法，例如使用更丰富的特征表示、进行数据增强、多角度训练等，以提高算法在处理图像数据时的准确性和鲁棒性。基于TrOCR 提一些面试题当涉及到TrOCR（Transformer-based Optical Character Recognition）的面试问题时，可以考虑以下几个方面的问题：什么是TrOCR？它与传统的OCR方法有何不同？TrOCR中使用的是哪种Transformer模型？请描述其结构和主要组成部分。在TrOCR中，图像是如何被处理和输入到Transformer模型中的？TrOCR如何实现对文本的自回归生成？它采用了哪些技术和策略？TrOCR中的预训练和微调是如何进行的？使用了哪些数据集和训练方法？TrOCR在哪些任务和数据集上取得了优秀的性能？可以谈谈其在印刷体和手写体识别任务上的表现。TrOCR的优势和局限性是什么？相对于传统的OCR方法，它有哪些潜在的应用前景和挑战？在TrOCR中，是否存在一些常见的问题或挑战？可以谈谈对应的解决方法或改进思路。TrOCR在实际应用中可能面临的计算资源和效率方面的问题有哪些？如何解决或优化这些问题？除了TrOCR，还有哪些基于Transformer的图像处理任务和应用？可以举例说明。这些问题将涵盖TrOCR的基本概念、模型结构、训练和应用等方面，有助于评估面试者对TrOCR的理解和熟悉程度。根据具体情况，你还可以进一步展开讨论相关的深入问题。回答下你提的十个问题当涉及到TrOCR（Transformer-based Optical Character Recognition）的面试问题时，以下是对你提出的十个问题的简要回答：TrOCR是一种基于Transformer的光学字符识别方法，与传统的OCR方法相比，它利用Transformer模型实现了端到端的文本生成和理解，具有更好的建模能力和上下文理解能力。TrOCR使用的是Transformer模型，它由编码器和解码器组成。编码器采用自注意力机制来处理输入图像的特征表示，解码器采用自回归生成的方式逐步生成输出文本。在TrOCR中，图像经过预处理步骤，如分割为固定大小的图像块，并进行线性嵌入，然后添加位置编码。处理后的图像表示被输入到Transformer编码器中进行处理。TrOCR采用自回归生成的方式对文本进行生成，即逐个生成每个字符或词汇。它使用了类似于语言建模的技术，通过上文的信息来预测下一个字符或词汇。TrOCR的预训练阶段通常使用大规模的合成数据进行训练，以学习图像和文本之间的对应关系。然后，可以通过微调阶段使用真实标注的数据来优化模型的性能。TrOCR在印刷体和手写体识别任务上取得了优秀的性能。它在大规模的OCR数据集上进行了评估，并在准确性和泛化能力方面超过了传统的OCR方法。TrOCR的优势包括能够对长文本进行建模、上下文理解能力强、适应不同语种和字体样式等。然而，TrOCR可能对计算资源要求较高，并且在处理一些特定的复杂场景（如低质量图像或特殊字体）时可能面临挑战。在TrOCR中，一些常见的问题包括文本重复生成、模糊图像的处理、长文本的生成和训练数据的标注质量。解决这些问题的方法可能涉及使用更复杂的解码策略、改进的图像预处理和增强技术以及更好的数据标注和清洗方法。TrOCR在实际应用中可能面临的计算资源和效率方面的问题包括模型大小和推理速度。为了解决这些问题，可以采用模型压缩和加速技术，如剪枝、量化和模型并行化等。除了TrOCR，还有许多基于Transformer的图像处理任务和应用。例如，图像分类、目标检测OCR识别手写文字(中文+英文)参考文献：TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models 2022该论文提出了一种名为TrOCR的基于Transformer的光学字符识别(OCR)模型，通过利用预训练模型进行特征提取和文本生成，实现端到端的文本识别任务TrOCR模型的关键创新点包括以下几个方面：基于Transformer的架构：TrOCR采用了Transformer架构作为其基础模型，其中包括编码器和解码器。编码器用于提取图像特征，解码器用于生成识别的文本序列预训练模型的应用：TrOCR利用预训练的图像和文本模型作为编码器和解码器，如ViT、RoBERTa等。这些预训练模型能够提供丰富的视觉和语言表示能力，有助于提高OCR的准确性大规模合成数据集：为了进行预训练和微调，TrOCR使用了大规模的合成数据集，包括数百万张打印文本图像和手写文本图像。这样可以增加模型在不同领域和样式的文本上的泛化能力实验证明，TrOCR模型在打印文本、手写文本和场景文本识别任务上取得了优异的性能表现，超过了当前的state-of-the-art模型。该论文的贡献在于将Transformer应用于OCR任务，并且通过预训练模型的利用提高了OCR的准确性和泛化能力架构：model(VisionEncoderDecoderModel) = encoder(DeiT)+decoder(TrOCRForCausalLM)1234567891011121314151617181920212223242526272829303132333435363738394041DeiTModel(  (embeddings): DeiTEmbeddings(    (patch_embeddings): DeiTPatchEmbeddings(      (projection): Conv2d(3, 384, kernel_size=(16, 16), stride=(16, 16))    )    (dropout): Dropout(p=0.0, inplace=False)  )  (encoder): DeiTEncoder(    (layer): ModuleList(      (0): DeiTLayer(        (attention): DeiTAttention(          (attention): DeiTSelfAttention(            (query): Linear(in_features=384, out_features=384, bias=True)            (key): Linear(in_features=384, out_features=384, bias=True)            (value): Linear(in_features=384, out_features=384, bias=True)            (dropout): Dropout(p=0.0, inplace=False)          )          (output): DeiTSelfOutput(            (dense): Linear(in_features=384, out_features=384, bias=True)            (dropout): Dropout(p=0.0, inplace=False)          )        )        (intermediate): DeiTIntermediate(          (dense): Linear(in_features=384, out_features=1536, bias=True)          (intermediate_act_fn): GELUActivation()        )        (output): DeiTOutput(          (dense): Linear(in_features=1536, out_features=384, bias=True)          (dropout): Dropout(p=0.0, inplace=False)        )        (layernorm_before): LayerNorm((384,), eps=1e-12, elementwise_affine=True)        (layernorm_after): LayerNorm((384,), eps=1e-12, elementwise_affine=True)      ) * 12    )  )  (layernorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)  (pooler): DeiTPooler(    (dense): Linear(in_features=384, out_features=384, bias=True)    (activation): Tanh()  ))12# 通过线性层将编码器和解码器连接到了一起enc_to_dec_proj = nn.Linear(self.encoder.config.hidden_size, self.decoder.config.hidden_size)1234567891011121314151617181920212223242526272829303132TrOCRForCausalLM(  (model): TrOCRDecoderWrapper(    (decoder): TrOCRDecoder(      (embed_tokens): Embedding(11318, 256, padding_idx=1)      (embed_positions): TrOCRLearnedPositionalEmbedding(514, 256)      (layernorm_embedding): LayerNorm((256,), eps=1e-05, elementwise_affine=True)      (layers): ModuleList(        (0): TrOCRDecoderLayer(          (self_attn): TrOCRAttention(            (k_proj): Linear(in_features=256, out_features=256, bias=True)            (v_proj): Linear(in_features=256, out_features=256, bias=True)            (q_proj): Linear(in_features=256, out_features=256, bias=True)            (out_proj): Linear(in_features=256, out_features=256, bias=True)          )          (activation_fn): ReLU()          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)          (encoder_attn): TrOCRAttention(            (k_proj): Linear(in_features=384, out_features=256, bias=True)            (v_proj): Linear(in_features=384, out_features=256, bias=True)            (q_proj): Linear(in_features=256, out_features=256, bias=True)            (out_proj): Linear(in_features=256, out_features=256, bias=True)          )          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)          (fc1): Linear(in_features=256, out_features=1024, bias=True)          (fc2): Linear(in_features=1024, out_features=256, bias=True)          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)        ) * 6      )    )  )  (output_projection): Linear(in_features=256, out_features=11318, bias=False))实现细节:数据集:</content>
	</entry>
	<entry>
		<title>transformer学习</title>
		<url>/article/44700.html</url>
		<content>LLM总览ChatGPT复现之路LLM物种进化图大模型训练流程图动画科普LLM大模型进阶之路：为何GPT之外一定要关注LLaMA四大步骤预训练监督式微调奖励建模强化学习数据集互联网公开数据集（2万亿token）问题对（1万-10万）人工反馈评价（10万-100万）人工提示词（1万-10万）算法语言模型（预测下一个token）语言模型（预测下一个token）二元分类器（输出奖励）强化学习（最大化奖励）模型基础模型SFT模型奖励模型强化学习模型所需资源千块GPU训练数月几十块GPU训练几天几十块GPU指标（TODO）指令微调TransformerAttention Is All You Need论文解读: Attention is All you needHugging Face的GitHub代码库通常而言，绝大部分NLP问题可以归入上图所示的四类任务中序列标注: 这是最典型的NLP任务，比如中文分词，词性标注，命名实体识别，语义角色标注等都可以归入这一类问题它的特点是句子中每个单词要求模型根据上下文都要给出一个分类类别分类任务: 比如我们常见的文本分类，情感计算等都可以归入这一类。它的特点是不管文章有多长，总体给出一个分类类别即可句子关系判断: 比如Entailment，QA，语义改写，自然语言推理等任务都是这个模式，给定两个句子，模型判断出两个句子是否具备某种语义关系生成式任务: 比如机器翻译，文本摘要，写诗造句，看图说话等都属于这一类，输入文本内容后，需要自主生成另外一段文字预训练语言模型目前有两种预训练语言模型用于下游任务的方法：feature-based(以ELMo为例)和fine-tuning(以BERT、GPT为例)有专门的论文(To Tune or Not to Tune? Adapting Pretrained Representations to Diverse Tasks)讨论了这个话题Feature-based Pre-Training:在Feature-based Pre-Training中，首先使用大规模的未标记数据集对模型进行预训练预训练任务通常是通过自监督学习或其他无监督学习方法来完成，例如预测下一个词语、图像的旋转角度等预训练的目标是学习到具有良好表示能力的特征，能够捕捉数据中的一般性信息预训练模型通常是一个通用的模型，不针对特定的任务。它学习到的特征表示可以应用于各种不同的任务预训练模型可以作为迁移学习的基础，通过将其特征提取部分应用于具体的任务Fine-tuning:Fine-tuning是在预训练模型的基础上，在特定任务的有标签数据集上进行进一步训练和优化Fine-tuning阶段会调整预训练模型的权重和参数，以使其适应目标任务的特定要求Fine-tuning过程中，通常会保持预训练模型的一部分权重固定，只更新部分权重，以保留预训练阶段学习到的通用特征表示Fine-tuning旨在在特定任务的有限标记数据集上优化模型，使其更好地适应该任务的数据和特征总结来说，Feature-based Pre-Training是通过在未标记数据上预训练模型来学习通用的特征表示，而Fine-tuning是在预训练模型的基础上，在特定任务的有标签数据上进行进一步优化和微调Feature-based Pre-Training提供了一种学习通用特征表示的方式，而Fine-tuning则将这些通用特征应用于特定任务，以提升任务性能，一句话概括：Feature-based Pre-Training把输入转特征，特征丢给后面的模型(新模型)，其他就和它无关了Fine-tuning是同一个网络结构，换了数据，可以固定或不固定前几层，继续训练词向量预训练语言模型的前世今生 - 从Word Embedding到BERT年份2013 年2014 年2015 年2016 年2017 年技术word2vecGloVeLSTM/AttentionSelf-AttentionTransformer年份2018 年2019 年2020 年技术GPT/ELMo/BERT/GNNXLNet/BoBERTa/GPT-2/ERNIE/T5GPT-3/ELECTRA/ALBERTOne-hot编码：早期的自然语言处理中，词语通常被表示为离散的one-hot向量。每个词语都被表示为一个维度等于词汇表大小的向量，其中只有一个维度为1，其余维度都为0这种表示方法无法捕捉词语之间的语义关系和相似度Word Embedding：为了克服one-hot编码的局限性，提出了基于分布假设的词向量表示方法，即Word Embedding。Word Embedding使用低维实数向量表示词语，通过训练模型将词语映射到一个连续的向量空间中。其中，每个维度代表一个语义特征。Word Embedding能够捕捉到词语之间的语义关系和相似度，提供更丰富的表示Word2Vec(静态)：Word2Vec是一种经典的词向量模型，由Tomas Mikolov等人于2013年提出。它基于神经网络模型，通过训练预测词语周围的上下文或预测目标词语。Word2Vec模型包括两种算法：CBOW(Continuous Bag-of-Words)和Skip-gram`。这两种算法使用浅层的神经网络来学习词向量，具有高效、快速训练的优势。Word2Vec模型能够生成静态的词向量，但无法捕捉词语的上下文相关特征ELMo(Embeddings from Language Models)(动态)：ELMo是在Word2Vec之后提出的一种上下文相关的词向量表示方法，由Peters等人于2018年提出。ELMo利用双向语言模型，通过训练正向和逆向的LSTM模型来学习词语的上下文表示。ELMo能够根据上下文动态地生成词向量，捕捉到词语在不同上下文中的语义特征。与静态词向量不同，ELMo提供了更丰富、更具语义的词语表示，适用于各种自然语言处理任务ELMo(Embeddings from Language Models)模型在训练过程中使用了双向长短期记忆网络（Bi-LSTM）总的来说，历史发展中，从one-hot编码到Word Embedding，再到Word2Vec和ELMo，词向量表示方法逐渐从离散、静态的表示发展到了连续、上下文相关的表示。这些方法的提出和发展使得自然语言处理模型能够更好地理解和处理文本数据，提高了各种文本相关任务的性能Word Embedding、Word2Vec和ELMo关系如下：Word2Vec是Word Embedding的一种具体实现方式。Word Embedding指的是将词语映射到低维实数向量空间的表示方法，而Word2Vec则是一种用于训练Word Embedding的算法ELMo和Word2Vec是两种不同的词向量表示方法。ELMo是一种上下文相关的词向量表示方法，通过训练双向语言模型来学习词语在不同上下文中的动态表示。而Word2Vec是一种上下文无关的词向量表示方法，通过训练预测词语的上下文或目标词语来学习静态的词向量模型介绍介绍Transformer比较好的文章一个是Jay Alammar可视化地介绍Transformer的博客文章The Illustrated Transformer，非常容易理解整个机制哈佛大学NLP研究组写的The Annotated Transformer，代码原理双管齐下Attention机制Attention机制最早在视觉领域提出，2014年Google Mind发表了《Recurrent Models of Visual Attention》，使Attention机制流行起来，这篇论文采用了RNN模型，并加入了Attention机制来进行图像的分类2005年，Bahdanau等人在论文《Neural Machine Translation by Jointly Learning to Align and Translate》中，将attention机制首次应用在nlp领域，其采用Seq2Seq+Attention模型来进行机器翻译，并且得到了效果的提升，Seq2Seq With Attention中进行了介绍2017 年，Google 机器翻译团队发表的《Attention is All You Need》中，完全抛弃了RNN和CNN等网络结构，而仅仅采用自注意力(self-attention)机制来学习文本表示来进行机器翻译任务，并且取得了很好的效果，注意力机制也成为了大家近期的研究热点本文首先介绍常见的Attention机制，然后对论文《Attention is All You Need》进行介绍，该论文发表在NIPS 2017上Architecture模型结构如下输入层词嵌入编码层位置编码层Encoder多头自注意力残差连接全连接网络Dncoder多头自注意力多头注意力(不是自注意, 因为QK来自Encoder)残差连接全连接网络模型整体结构如下所示Transformer是一种基于自注意力机制的序列到序列模型，广泛应用于自然语言处理任务，如机器翻译、文本摘要和语言生成等Transformer整体结构由以下几个主要组件组成：编码器（Encoder）：编码器负责将输入序列（例如源语言句子）转换为一系列高级特征表示。它由多个相同的层堆叠而成，每个层都包含两个子层：多头自注意力机制和全连接前馈神经网络。自注意力机制允许模型对输入序列中的不同位置进行自适应地关注，从而捕捉序列中的上下文信息解码器（Decoder）：解码器负责从编码器生成的特征表示中生成目标序列（例如目标语言句子）。解码器也由多个相同的层堆叠而成，每个层包含三个子层：多头自注意力机制、编码器-解码器注意力机制和全连接前馈神经网络。编码器-解码器注意力机制用于在生成目标序列时，引入对源语言句子的关注自注意力机制（Self-Attention）：自注意力机制是Transformer的关键组件之一。它允许模型在进行编码或解码时，根据输入序列中不同位置之间的关系，动态地计算注意力权重。通过自适应地关注不同位置的信息，自注意力机制能够捕捉输入序列中的上下文信息，提供更全面的表示注意力机制（Attention）：除了自注意力机制，Transformer还使用编码器-解码器注意力机制。这种注意力机制允许解码器在生成目标序列时，对编码器输出的特征表示进行关注。它能够帮助解码器对源语言句子中与当前生成位置相关的信息进行处理前馈神经网络（Feed-Forward Network）：Transformer中的每个子层都包含一个前馈神经网络。该网络由两个全连接层组成，通过应用非线性激活函数（如ReLU）来对特征表示进行映射和变换。前馈神经网络有助于捕捉特征之间的非线性关系通过编码器和解码器的组合，Transformer模型能够将输入序列转换为输出序列，实现不同的序列到序列任务它的并行计算性质和自注意力机制的能力使得它在处理长序列和捕捉全局依赖关系方面具有优势，成为自然语言处理领域的重要模型，更详细的模型结构如下所示EmbeddingEmbedding层是Transformer模型中的一个重要组成部分，用于将离散的输入序列（如单词、字符等）映射到连续的低维向量表示它负责将输入的符号型数据转换为密集的实数向量，从而能够在模型中进行有效的学习和处理在Transformer中，Embedding层主要有两个作用：词嵌入（Word Embedding）：对于自然语言处理任务，Embedding层将每个词汇或字符映射到一个低维的连续向量表示，称为词嵌入或字符嵌入。这些嵌入向量捕捉了词汇或字符之间的语义和语法关系，能够编码单词的上下文信息，使得模型能够更好地理解和表示输入数据位置编码（Positional Encoding）：Transformer模型中没有使用循环神经网络或卷积神经网络，因此无法直接捕捉输入序列中顺序信息。为了引入位置信息，Embedding层会添加位置编码到词嵌入中。位置编码是一种用于表示输入序列位置的向量，它提供了关于词汇在序列中相对位置的信息，帮助模型理解序列中的顺序关系在实现上，Embedding层可以使用一个矩阵作为参数来进行词嵌入的查找。每个词汇对应矩阵中的一行，通过查找输入序列中的词汇对应的行向量，得到词嵌入表示位置编码通常使用正弦和余弦函数的组合来计算，以获取不同位置的编码向量需要注意的是，Embedding层的参数通常是在模型训练的过程中学习得到的，根据任务和数据来调整嵌入向量的表示能力。通过Embedding层，模型能够在低维连续向量空间中对输入序列进行表征和建模，从而更好地处理自然语言处理等任务导入库123456789101112131415161718#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v1.0@author: huangyc@file: embedding.py@Description: @time: 2023/2/19 15:00&quot;&quot;&quot;import torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.autograd import Variableimport mathimport matplotlib.pyplot as pltimport numpy as npimport copyEmbedding层定义1234567891011121314class Embeddings(nn.Module):    def __init__(self, d_model: int, vocab: int):        &quot;&quot;&quot;        构建Embedding类来实现文本嵌入层        :param d_model: 词嵌入的维度        :param vocab: 词表的大小        &quot;&quot;&quot;        super(Embeddings, self).__init__()        # 定义Embedding层        self.lut = nn.Embedding(vocab, d_model)        self.d_model = d_model    def forward(self, x):        return self.lut(x) * math.sqrt(self.d_model)PositionalEncoding层12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 定义位置编码器类, 我们同样把它看做一个层,因此会继承nn.Moduleclass PositionalEncoding(nn.Module):    def __init__(self, d_model: int, dropout: float, max_len: int = 5000):        &quot;&quot;&quot;        位置编码器类的初始化函数, 共有三个参数        :param d_model: 词嵌入维度        :param dropout: 置0比率        :param max_len: 每个句子的最大长度        :return:        &quot;&quot;&quot;        super(PositionalEncoding, self).__init__()        # 实例化nn中预定义的Dropout层, 并将dropout传入其中,获得对象self.dropout        self.dropout = nn.Dropout(p=dropout)        # 初始化一个位置编码矩阵,它是一个0阵, 矩阵的大小是max_len * d_model        pe = torch.zeros(max_len, d_model)        # 初始化一个绝对位置矩阵, 在我们这里, 词汇的绝对位置就是用它的索引去表示        # 所以我们首先使用arange方法获得一个连续自然数向量, 然后再使用unsqueeze方法拓展向量维度        # #又因为参数传的是1, 代表矩阵拓展的位置, 会使向量变成一个max_len * 1的矩阵        position = torch.arange(0, max_len).unsqueeze(1)        # 绝对位置矩阵初始化之后, 接下来就是考虑如何将这些位置信息加入到位置编码矩阵中        # 最简单思路就是先将max_len * 1的绝对位置矩阵, 变换成max_len * d_model形状, 然后覆盖原来的初始位置编码矩阵即可        # 要做这种矩阵变换, 就需要一个1 * d_model形状的变换矩阵div_term, 我们对这个变换矩阵的要求除了形状外        # 还希望它能够将自然数的绝对位置编码缩放成足够小的数字, 有助于在之后的梯度下降过程中更快的收敛        # 首先使用arange获得一个自然数矩阵, 但是细心的同学们会发现, 我们这里并没有按照预计的一样初始化一个1 * d_model的矩阵        # 而是有了一个跳跃，只初始化了一半即1*d_mode1/2的矩阵. 为什么是一半呢, 其实这里并不是真正意义上的初始化        # 我们可以把它看作是初始化了两次, 而每次初始化的变换矩阵会做不同的处理, 第一次初始化的变换矩阵分布在正弦波上, 第二次初始化的变换矩阵分布在余弦波上        # 并把这两个矩阵分别填充在位置编码矩阵的偶数和奇数位置上, 组成最终的位置编码矩阵        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.) / d_model))        pe[:, 0::2] = torch.sin(position * div_term)        pe[:, 1::2] = torch.cos(position * div_term)        # 这样我们就得到了位置编码矩阵pe, pe现在还是个二维矩阵，要想和embedding的输出(一个三维张量)相加, 需要扩展维度        pe = pe.unsqueeze(0)        # 我们把它认为是对模型效果有帮助的, 但是却不是模型结构中超参数或者参数,不需要随着优化步骤优化        # 注册之后我们就可以在模型保存后重加载时和模型结构与参数一同被加载        self.register_buffer('pe', pe)    def forward(self, x):        &quot;&quot;&quot;        :param x: 文本序列的词嵌入表示        &quot;&quot;&quot;        # 根据句子最大长度切割, pe不需要做梯度求解        x = x + Variable(self.pe[:, :x.size(1)], requires_grad=False)        return self.dropout(x)实际测试 + 位置编码可视化1234567891011121314151617181920212223242526272829303132if __name__ == '__main__':    p_d_model = 512    p_vocab = 1000    p_dropout: float = 0.1    p_max_len = 60    # 词嵌入测试    x = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))    emb = Embeddings(d_model=p_d_model, vocab=p_vocab)    embr = emb(x)    print(&quot;embr&quot;, embr)    print(&quot;embr size&quot;, embr.shape)    # 位置编码测试    pe = PositionalEncoding(d_model=p_d_model, dropout=p_dropout, max_len=p_max_len)    pe_result = pe(embr)    print(&quot;pe_result&quot;, pe_result)    print(&quot;pe_result size&quot;, pe_result.shape)    # 创建一张15x5大小的画布    plt.figure(figsize=(15, 5))    # 实例化PositionalEncoding类得到pe对象, 输入参数是20和0    pe = PositionalEncoding(20, 0)    # 然后向pe传入被Variable封装的tensor, 这样pe会直接执行forward函数,    # 且这个tensor里的数值都是0, 被处理后相当于位置编码张量    y = pe(Variable(torch.zeros(1, 100, 20)))    # 然后定义画布的横纵坐标, 横坐标到100的长度, 纵坐标是某一个词汇中的某维特征在不同长度下对应的值    # 因为总共有20维之多, 我们这里只查看4,5,6,7维的值    plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())    # 在画布上填写维度提示信息    plt.legend([&quot;dim %d&quot; % p for p in [4, 5, 6, 7]])    plt.show()1234567891011121314151617181920212223embr tensor([[[-17.5113,  -6.0699,  11.6839,  ...,  -8.1281,  -7.7986,  35.1275],         [ -6.3789,  -7.7614,  13.2975,  ...,  16.8397, -31.3230, -68.4385],         [ -4.1841,   8.4322,  34.6418,  ...,  38.4747,  -4.9060,  25.4163],         [-23.4562, -28.9742,  18.1234,  ...,  38.6039,  15.0049,  -2.8916]],        [[-21.7485,   0.3263,  54.4449,  ..., -18.3120, -15.5987, -11.4275],         [ -0.6414,   2.9492, -32.3063,  ..., -21.9781, -16.3307, -15.4014],         [-16.1775,  20.8547, -21.0333,  ..., -11.7583,  -7.2429,   5.8607],         [ -4.7708, -51.9955,  14.8529,  ...,  21.0973,  13.4664, -10.8492]]],       grad_fn=&lt;MulBackward0&gt;)embr size torch.Size([2, 4, 512])pe_result tensor([[[-19.4569,  -5.6332,  12.9821,  ...,  -7.9201,  -0.0000,   0.0000],         [ -6.1526,  -8.0235,  15.6882,  ...,  19.8219, -34.8032, -74.9317],         [ -3.6387,   8.9068,  39.5314,  ...,  43.8608,  -5.4509,  29.3514],         [-25.9057, -33.2935,  20.4094,  ...,  44.0043,  16.6725,  -2.1017]],        [[-24.1650,   1.4736,   0.0000,  ..., -19.2356, -17.3319, -11.5861],         [  0.2223,   3.8772, -34.9827,  ..., -23.3090, -18.1452, -16.0015],         [-16.9647,  22.7095, -22.3299,  ..., -11.9537,  -8.0475,   7.6230],         [ -0.0000, -58.8727,   0.0000,  ...,  24.5526,  14.9630,  -0.0000]]],       grad_fn=&lt;MulBackward0&gt;)pe_result size torch.Size([2, 4, 512])Attention超详细图解Self-Attention的那些事儿除了Scaled Dot-Product Attention，Transformer模型中还有几种常见的注意力机制点积注意力(Dot-Product Attention)：它是Scaled Dot-Product Attention的简化版本，直接计算查询（Q）和键（K）之间的点积，然后通过softmax函数将结果转化为注意力权重。点积注意力相比于Scaled Dot-Product Attention没有进行缩放操作加性注意力(Additive Attention)：加性注意力使用了一个额外的前馈神经网络来计算注意力权重。它通过将查询（Q）和键（K）映射到相同的低维空间，然后计算它们的相似度得分，最后将相似度得分通过softmax函数进行归一化。加性注意力在一些场景中能够更好地捕捉输入序列之间的非线性关系缩放点积注意力(Scaled Dot-Product Attention)：它是Transformer中最常用的注意力机制。在计算注意力权重时，对点积注意力进行了缩放操作，通过除以特征维度的平方根，以减小注意力权重的大小变化。这有助于防止梯度消失或梯度爆炸，并使得模型更稳定按位置加权注意力(Relative Positional Attention)：这种注意力机制考虑了位置信息对注意力计算的影响。它引入了位置编码，通过计算相对位置的差异，对注意力权重进行调整。这种注意力机制在处理序列任务时能够更好地建模长距离依赖关系在Transformer中使用的Attention是Scaled Dot-Product Attention，是归一化的点乘Attention，假设输入的query  、key维度、value维度为，那么就计算query和每个key 的点乘操作，并除以，然后应用Softmax函数计算权重在实践中，将query和keys、values分别处理为矩阵，那么计算输出矩阵为:其中, , ，输出矩阵维度为，其中为句子长度，为Embedding后的特征长度其中的维度为，表示句子中每个字之间的关注度(self-attention)，而的维度为，表示attention下的句子特征向量，如下所示attention代码如下: 其中qkv是x经过线性变换之后的结果12345678910def attention(query, key, value, mask=None, dropout=None):    &quot;Compute 'Scaled Dot Product Attention'&quot;    d_k = query.size(-1)    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)    if mask is not None:        scores = scores.masked_fill(mask == 0, -1e9)    p_attn = F.softmax(scores, dim = -1)    if dropout is not None:        p_attn = dropout(p_attn)    return torch.matmul(p_attn, value), p_attn矩阵与其转置的乘积向量数量积的几何意义：一个向量在另一个向量上的投影向量的相似性是用两者的角度余弦来度量，余弦值越大则两者越相似而余弦值等于两者内积与两者模长积的比，当两个向量模长固定的情形下，内积大小则反映了两者相似性的大小1234567import numpy as npmat_a = np.array([1, 2, 2], [4, 5, 8])np.matmul(mat_a,mat_a.T)Out[5]: array([[  9,  30],       [ 30, 105]])那么Scaled Dot-Product Attention的示意图如下所示，Mask是可选的，如果是能够获取到所有时刻的输入，那么就不使用Mask；如果是不能获取到，那么就需要使用Mask使用了Mask的Transformer模型也被称为Transformer Decoder，不使用Mask的Transformer模型也被称为Transformer Encoder如果只对Q、K、V做一次这样的权重操作是不够的，这里提出了Multi-Head Attention操作，包括：首先对做一次线性映射，将输入维度均为的矩阵映射到然后再采用Scaled Dot-Product Attention算出结果多次进行上述两步操作，然后将得到的结果进行合并将合并的结果进行线性变换多头注意力的引入有以下几个目的：平行计算：通过使用多个注意力头，可以并行地计算注意力权重和加权求和，从而加快模型的计算速度。每个注意力头都专注于不同的表示子空间，因此可以独立地计算和处理信息，提高模型的效率。多样性表达：每个注意力头学习到的表示子空间不同，通过多个注意力头的组合，可以获得更丰富、多样性的表示。这有助于模型更好地捕捉输入序列中的不同特征和关系，提高模型的表达能力。组合注意力：多头注意力允许模型在不同的表示子空间上进行多次注意力计算，并将这些计算的结果进行组合。这种组合能够从不同的关注角度和视角来处理输入序列，帮助模型更全面地理解序列中的信息。通过这些方式，多头注意力可以提供更灵活、更强大的建模能力，增强模型对序列中的长距离依赖关系、全局上下文和特征之间复杂关系的建模能力它是Transformer模型在处理自然语言处理任务时取得成功的重要组成部分，总结来说公式如下所示其中第1步的线性变换参数为，第4 步的线性变化参数为，而第三步计算的次数是在论文中取 ，表示每个时刻的输入维度和输出维度， 表示8次Attention操作， 表示经过线性变换之后、进行Attention操作之前的维度进行一次Attention之后输出的矩阵维度是，然后进行次操作合并之后输出的结果是，因此输入和输出的矩阵维度相同这样输出的矩阵，每行的向量都是对向量中每一行的加权，示意图如上所示多头注意力代码实现1234567891011121314151617181920212223242526272829303132class MultiHeadedAttention(nn.Module):    def __init__(self, h, d_model, dropout=0.1):        &quot;Take in model size and number of heads.&quot;        super(MultiHeadedAttention, self).__init__()        assert d_model % h == 0        # We assume d_v always equals d_k        self.d_k = d_model // h        self.h = h        # 多头在这里体现        self.linears = clones(nn.Linear(d_model, d_model), 4)        self.attn = None        self.dropout = nn.Dropout(p=dropout)            def forward(self, query, key, value, mask=None):        if mask is not None:            # Same mask applied to all h heads.            mask = mask.unsqueeze(1)        nbatches = query.size(0)                # 1) Do all the linear projections in batch from d_model =&gt; h x d_k         query, key, value = \            [l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)             for l, x in zip(self.linears, (query, key, value))]                # 2) Apply attention on all the projected vectors in batch.         x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)                # 3) &quot;Concat&quot; using a view and apply a final linear.         x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k)                # 4) 线性变换投影回原始表示维度        return self.linears[-1](x)如果不好理解可以看下这部分代码123456789101112131415161718192021222324def multihead_attention(Q, K, V, num_heads):    # 线性变换得到查询、键和值的表示    Q_transformed = linear_transform(Q)    K_transformed = linear_transform(K)    V_transformed = linear_transform(V)    # 分割头    Q_heads = split_heads(Q_transformed, num_heads)    K_heads = split_heads(K_transformed, num_heads)    V_heads = split_heads(V_transformed, num_heads)    # 每个头的注意力计算    attention_heads = []    for i in range(num_heads):        attention_head = scaled_dot_product_attention(Q_heads[i], K_heads[i], V_heads[i])        attention_heads.append(attention_head)    # 拼接注意力头    concatenated_attention = concatenate_heads(attention_heads)    # 线性变换投影回原始表示维度    output = linear_transform(concatenated_attention)    return outputEncoder编码器和解码器如下所示Decoder在encoder部分中的self-attention是不需要mask的，而decoder部分的self-attention是需要mask的，因为正是有了mask遮挡后面的信息，才能将transformer用来做推理编码器和解码器如下所示编码器把最后一层的KV喂给了编码器，此时Q来源解码器，K=V来源于编码器，是为了让解码器能够在生成输出时使用编码器的信息通过给解码器提供编码器的键和值矩阵，可以实现以下两个目的：上下文信息传递：编码器中的自注意力机制能够捕捉到输入序列中的局部和全局关系，生成对应的键和值。将这些键和值传递给解码器，可以将编码器的上下文信息传递给解码器，以帮助解码器在生成输出时了解输入序列的相关内容。对齐和信息提取：解码器可以通过注意力机制对编码器的键和值进行加权汇总，以获取与当前解码位置相关的信息。通过计算解码器当前位置与编码器中每个位置之间的注意力分数，可以实现对齐和信息提取，使解码器能够专注于与当前位置相关的输入信息。总结来说，通过将编码器的键和值矩阵提供给解码器，可以实现上下文信息传递和对齐机制，帮助解码器在生成输出时利用编码器的信息，从而改善模型的性能和输出质量maskAttention MaskPadding MaskBertThe Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning)ELMo、GPT 和 BERT 三者的区别GPT：GPT 使用Transformer Decoder作为特征提取器，实现了单向编码、具有良好的文本生成能力，然而当前词的语义只能由其前序词决定，并且在语义理解上不足BERT：使用了Transformer Encoder作为特征提取器，为双向编码器，并使用了与其配套的掩码训练方法。虽然使用双向编码让BERT不再具有文本生成能力，但是BERT的语义信息提取能力更强ELMo: 使用自左向右编码和自右向左编码的两个LSTM网络，分别以和为目标函数独立训练，将训练得到的特征向量以拼接的形式实现双向编码，本质上还是单向编码，只不过是两个方向上的单向编码的拼接而成的双向编码bert是自编码模型，而gpt是自回归模型bert概况BERT(Bidirectional Encoder Representations from Transformers)模型的编码器由多个Transformer编码器层组成，通常使用了多个重复的编码器来形成深层的表示每个BERT编码器层包含了以下组件：多头自注意力（Multi-Head Self-Attention）：该层使用多头自注意力机制来对输入序列进行建模。自注意力允许模型在处理序列时关注不同位置之间的相关性，有助于捕捉上下文信息前馈神经网络（Feed-Forward Neural Network）：在自注意力层后面是一个前馈神经网络。该网络通常由两个线性层和激活函数（如ReLU）组成，用于对自注意力输出进行非线性变换和特征提取残差连接（Residual Connections）和层归一化（Layer Normalization）：在每个子层（自注意力和前馈神经网络）之后都应用了残差连接和层归一化操作。这些操作有助于缓解梯度消失问题，并提供更稳定和高效的训练BERT模型中通常会堆叠多个编码器层来形成深层表示。每个编码器层的输出会作为下一层的输入，通过多次重复这个过程，可以逐渐丰富输入序列的表示能力值得注意的是，BERT模型还在编码器输入的开头添加了特殊的标记，如[CLS]（用于分类任务）和[SEP]（用于分隔输入）。这些特殊标记有助于模型在处理不同任务时进行序列级别的操作和分类总结起来，BERT的编码器由多个Transformer编码器层组成，每个编码器层由多头自注意力、前馈神经网络和残差连接/层归一化组成。通过堆叠多个编码器层，BERT模型可以获得深层、高质量的语言表示Architecture输入训练方式由于无法使用标准语言模型的训练模式，BERT借鉴完形填空任务和CBOW的思想，使用语言掩码模型(MLM)方法训练模型训练中的maskMLM方法也就是随机去掉句子中的部分token(单词)，然后模型来预测被去掉的token是什么。这样实际上已经不是传统的神经网络语言模型(类似于生成模型)了，而是单纯作为分类问题，根据这个时刻的hidden state来预测这个时刻的token应该是什么，而不是预测下一个时刻的词的概率分布了随机去掉的token被称作掩码词，在训练中，掩码词将以15%的概率被替换成[MASK]，也就是说随机mask语料中15%的token，这个操作则称为掩码操作在选择15%的词作为掩码词后这些掩码词有三类替换选项：80% 练样本中：将选中的词用 [MASK] 来代替10% 的训练样本中：选中的词不发生变化，该做法是为了缓解训练文本和预测文本的偏差带来的性能损失10% 的训练样本中：将选中的词用任意的词来进行代替，该做法是为了让 BERT 学会根据上下文信息自动纠错预训练模型预训练模型分类自编码类模型、自回归类模型和Encoder-Decoder模型都是在自然语言处理（NLP）领域中常见的模型类型，它们在处理文本任务时有不同的特点和功能自编码类模型：自编码类模型主要用于学习输入数据的表示。它们通常由两个部分组成：编码器和解码器。编码器将输入数据编码为低维度的表示，而解码器则尝试从该表示中重构原始输入。自编码类模型的目标是通过最小化重构误差来学习数据的有用特征。常见的自编码类模型包括BERT、ALBert和RoBERTaBERT：双向Transformer的Encoder，通过预训练方式使用掩码语言建模和下一句预测任务来学习文本的表示ALBert：BERT的改进版本，通过参数共享和参数缩减来提高预训练模型的效率和性能RoBERTa：对BERT进行改进和优化，通过更大的数据和训练步数来提高模型性能自回归类模型：自回归类模型用于生成序列数据，其中模型的输出是基于先前生成的内容的条件概率分布。这意味着模型会根据前面生成的内容来预测下一个标记或词。自回归类模型通常使用循环神经网络（RNN）或Transformer等结构来建模序列数据的依赖关系。常见的自回归类模型包括ELMO、XLnet、GPT1和GPT2ELMO：采用双向LSTM进行预训练，通过将上下文信息融入词向量表示来解决多义词问题XLnet：通过排列组合的方式进行预测，解决了自回归模型的问题，提高了模型的性能GPT1：采用Transformer的解码器进行预训练，使用自回归语言建模方法GPT2：在GPT1的基础上进行改进，具有更强的生成能力和更大的模型参数量Encoder-Decoder模型：Encoder-Decoder模型是一种结构，由两部分组成：编码器和解码器。编码器负责将输入序列编码为固定长度的表示，而解码器利用该表示来生成输出序列。这种模型广泛用于机器翻译、文本摘要和对话生成等任务。T5（Text-to-Text Transfer Transformer）是一种基于Transformer的Encoder-Decoder模型，可以用于处理多种NLP任务T5：采用Transformer的编码器-解码器模型，通过预训练和Fine-tuning来处理NLU和NLG任务，具有统一的框架和多种预训练方法这些模型在NLP任务中取得了显著的成果，并被广泛用于各种文本处理和自然语言处理应用中时间线根据时间线，以下是对目前常见的NLP预训练模型进行的整理：ELMO（2018年3月）：由华盛顿大学提出，采用双向LSTM进行预训练，在不同的层次上提取词向量表示GPT（2018年6月）：由OpenAI提出，使用Transformer的解码器进行预训练，采用自回归语言建模方法BERT（2018年10月）：由Google提出，基于Transformer的编码器，使用掩码语言建模和下一句预测任务进行预训练XLNet（2019年6月）：由CMU和Google Brain提出，采用排列组合的方式进行预测，解决了GPT中自回归模型的问题ERNIE（2019年4月）：由百度提出，结合了知识增强和语义理解目标，通过预训练来提高NLU（自然语言理解）和NLG（自然语言生成）任务的性能BERT-wwm（2019年6月30日）：哈工大和讯飞合作提出，基于BERT的模型，在预训练过程中使用了更大的中文词表RoBERTa（2019年7月26日）：由Facebook提出，对BERT进行了改进和优化，通过更大的数据和训练步数来提高模型性能ERNIE2.0（2019年7月29日）：由百度提出的改进版本，结合了知识增强和预训练目标的多样性，以提高模型的泛化和迁移能力BERT-wwm-ext（2019年7月30日）：哈工大和讯飞合作提出的改进版本，进一步优化了中文词表，提升了预训练模型的性能ALBERT（2019年10月）：由Google提出，通过参数共享和参数缩减的方式来提高预训练模型的效率和性能这些预训练模型在不同的任务和数据集上都取得了很好的效果，并且为各种NLP应用提供了强大的基础模型</content>
	</entry>
	<entry>
		<title>图论算法</title>
		<url>/article/56968.html</url>
		<content>图结构基本概念图论基础和表示图的基本介绍和表示方式 定义图论(Graph Theory)是离散数学的一个分支，是一门研究图(Graph)的学问图的分类有无方向有向图无向图有无权重无权图有权图连通性图论中，连通图基于连通的概念，在一个无向图G中，若从顶点i到顶点j有路径相连(当然从j到i也一定有路径)，则称i和j是连通的如果图中任意两点都是连通的，那么图被称作连通图，如果此图是有向图，则称为强连通图完全图完全图是一个简单的无向图，其中每对不同的顶点之间都恰连有一条边相连图的表示图的表示方式有两种：二维数组表示(邻接矩阵)、链表表示(邻接表)邻接矩阵邻接表十字链表[有向图]图的类型欧拉图欧拉通路、欧拉回路、欧拉图和半欧拉图以及 Hierholzer 算法回路与通路的定义：欧拉通路(欧拉路径): 通过图中所有边恰好一次且行遍所有顶点的通路欧拉回路: 通过图中所有边恰好一次且行遍所有顶点的回路欧拉图与半欧拉图：欧拉图(Eulerian graph): 具有欧拉回路的图称半欧拉图(semi-Eulerian graph): 具有欧拉通路但不具有欧拉回路的图欧拉回路是欧拉路径，欧拉路径不一定是欧拉回路特征在无向图中：无向图G为欧拉图，当且仅当G为连通图且所有顶点的度为偶数无向图G为半欧拉图，当且仅当G为连通图且除了两个顶点的度为奇数之外，其它所有顶点的度为偶数在有向图中有向图G为欧拉图，当且仅当G为连通图且所有顶点的入度等于出度有向图G为半欧拉图，当且仅当G为连通图且存在某一顶点的入度比出度大1，还存在另一个顶点的入度比出度小1，而其它所有顶点的入度等于出度对于有向图的半欧拉图，它的欧拉路径的起点的度是出度=入度+1，终点的度是入度=出度+1Hierholzer算法用于寻找欧拉路径或欧拉回路(该算法假定图有欧拉路径或欧拉回路)，其流程如下：从起点出发，进行深度优先搜索(DFS)每次沿着某条边从某个顶点移动到另外一个顶点的时候，都将这条边标记为已走过如果遇到阻塞(即当前顶点没有后续邻边或该顶点的邻边都走过)，则将当前顶点加入到栈中，并回溯到上一顶点查找其可移动到的顶点哈密顿图图论复习（二）-——哈密顿图及其应用回路与通路的定义：哈密顿路: 给定无向图G中，通过图中每个结点一次而且仅一次的路径哈密顿回路: 给定无向图G中，通过图中每个结点一次而且仅一次的回路哈密顿(Hamilton)图与半哈密顿图：哈密顿图: 具有哈密顿回路的图半哈密顿图: 有哈密顿路径而没有哈密顿回路的图哈密尔顿图和半哈密尔顿图是连通图哈密顿图和欧拉图联系两者都是遍历问题，但是欧拉图考虑的是边，而哈密顿考虑的是结点同时判定欧拉图具有充要条件，但哈密顿图没有简单的充要条件，只有必要条件和充分条件二部图二部图(二分图)总结定义二分图又称作二部图，是图论中的一种特殊模型设G=(V, E)是一个无向图，如果顶点V可分割为两个互不相交的子集(A,B)，并且图中的每条边(i, j)所关联的两个顶点i和j分别属于这两个不同的顶点集($i \in A$，$j \in B$)，则称图G为一个二分图竞赛图(有向完全图)定义竞赛图也叫有向完全图。每对顶点之间都有一条边相连的有向图称为竞赛图简单的性质：竞赛图没有自环，没有二元环，若竞赛图存在环，则一定存在三元环如果存在一个环大于三元，那么一定存在另一个三元的小环任意竞赛图都有哈密顿路径(经过每个点一次的路径，不要求回到出发点)图存在哈密顿回路的充要条件是强联通哈密顿问题中，对于n阶竞赛图，当n大于等于2时一定存在哈密顿通路acm题目题目大意：现在有比赛，所有队伍两两进行比赛，赢的积2分，输的积0分，如果平局的话就各自都积1分现在给出每只队伍的得分情况，判断是否合法思路：给出的 m 个队伍可以构成一个竞赛图，问得分情况是否合法实质是在问竞赛图是否合法根据竞赛图的兰道定理，将得分视为比分序列，将所有得分进行排序，然后依次处理由于每次比赛胜利都会使得总分 +2，那么前i只队伍的得分情况必须大于等于 i*(i-1)当判断到最后一只队伍时，有前 n-1 只队伍的得分必须大于 n*(n-1)遍历算法深度搜索广度搜索最短路径算法FloydDijkstra[最短路径问题]—Dijkstra 算法最详解最小生成树算法PrimKruskal关键路径拓扑排序二分图匹配配对匈牙利算法中心性算法社区发现算法标签传播算法LPA标签传播算法(Label Propagation Algorithm，简称 LPA)是一个在图中快速发现社群的算法Louvain算法</content>
	</entry>
	<entry>
		<title>树算法</title>
		<url>/article/1774.html</url>
		<content>树结构概念</content>
	</entry>
	<entry>
		<title>排序算法</title>
		<url>/article/62104.html</url>
		<content>排序算法十大经典排序算法_动图演示常见的10种排序算法十大经典排序算法动画，看我就够了十大经典排序算法分类十种常见排序算法可以分为两大类：比较类排序：通过比较来决定元素间的相对次序，由于其时间复杂度不能突破$O(n)$，因此也称为非线性时间比较类排序非比较类排序：不通过比较来决定元素间的相对次序，它可以突破基于比较排序的时间下界，以线性时间运行，因此也称为线性时间非比较类排序排序算法比较类交换排序冒泡排序(稳定)快速排序[分治]插入排序简单插入排序(稳定)[打扑克]希尔排序(插入排序的改进)选择排序简单选择排序[选最值]堆排序归并排序(稳定)[递归]二路归并排序多路归并排序非比较类计数排序(稳定)桶排序(稳定)基数排序(稳定)算法复杂度排序方法时间复杂度(均)时间复杂度(最坏)时间复杂度(最好)空间复杂度稳定性本地排序插入排序$O(n^2)$$O(n^2)$$O(n)$$O(1)$稳定都可希尔排序$O(n^{1.3})$$O(n^2)$$O(n)$$O(1)$不稳定都可选择排序$O(n^2)$$O(n^2)$$O(n^2)$$O(1)$不稳定都可堆排序$O(n log_2n)$$O(n log_2n)$$O(n log_2n)$$O(1)$不稳定都可冒泡排序$O(n^2)$$O(n^2)$$O(n)$$O(1)$稳定都可快速排序$O(n log_2n)$$O(n^2)$$O(n log_2n)$$O(n log_2n)$不稳定仅本地归并排序$O(n log_2n)$$O(n log_2n)$$O(n log_2n)$$O(n)$稳定不支持计数排序$O(n+k)$$O(n+k)$$O(n+k)$$O(n+k)$稳定都可桶排序$O(n+k)$$O(n^2)$$O(n)$$O(n+k)$稳定都可基数排序$O(n*k)$$O(n*k)$$O(n*k)$$O(n+k)$稳定都可小结稳定性记忆方法——快希选堆不稳定需要使用额外空间的四种排序——基数、计数、桶排、归并常用时间复杂度大小关系基本概念稳定性假定在待排序的记录序列中，存在多个具有相同的关键字的记录，若经过排序，这些记录的相对次序保持不变即在原序列中，r[i]=r[j]，且r[i]在r[j]之前，而在排序后的序列中，r[i]仍在r[j]之前，则称这种排序算法是稳定的比较类冒泡排序(Bubble Sort)算法描述比较相邻的元素: 如果第一个比第二个大，就交换它们两个对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数针对所有的元素重复以上的步骤，除了最后一个重复步骤1~3，直到排序完成动图演示代码实现123456def bubbleSort(arr):    for i in range(1, len(arr)):        for j in range(0, len(arr)-i):            if arr[j] &gt; arr[j+1]:                arr[j], arr[j + 1] = arr[j + 1], arr[j]    return arr改进与变种鸡尾酒排序(Cocktail Sort)冒泡排序的变形版，也称搅拌排序每一轮排序有两个内循环，每轮结束时同时排好了最大和最小两个元素第一个循环负责把当前最大值冒泡到最右第二个循环把当前最小值冒泡到最左1234567891011121314class Solution:    def MySort(self, arr):        n = len(arr)                for i in range(n-1, -1, -1):            for j in range(1, i+1):                if arr[j] &lt; arr[j-1]:                    arr[j], arr[j-1] = arr[j-1], arr[j]            for j in range(i-1, 0, -1):                if arr[j] &lt; arr[j-1]:                    arr[j], arr[j-1] = arr[j-1], arr[j]        return arr选择排序(Selection Sort)工作原理：首先在未排序序列中找到最小(大)元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小(大)元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕算法描述n个记录的直接选择排序可经过n-1趟直接选择排序得到有序结果，具体算法描述如下：初始状态：无序区为R[1..n]，有序区为空第i趟排序(i=1, 2, 3 … n-1)开始时，当前有序区和无序区分别为R[1..i-1]和R(i..n)该趟排序从当前无序区中选出关键字最小的记录R[k]，将它与无序区的第1个记录R交换使R[1..i]和R[i+1..n)分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区n-1趟结束，数组有序化了动图演示代码实现1234567891011def selectionSort(arr):    for i in range(len(arr) - 1):        # 记录最小数的索引        minIndex = i        for j in range(i + 1, len(arr)):            if arr[j] &lt; arr[minIndex]:                minIndex = j        # i 不是最小数时，将 i 和最小数进行交换        if i != minIndex:            arr[i], arr[minIndex] = arr[minIndex], arr[i]    return arr插入排序(Insertion Sort)插入排序(Insertion-Sort)的算法描述是一种简单直观的排序算法它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入算法描述一般来说，插入排序都采用in-place在数组上实现。具体算法描述如下：从第一个元素开始，该元素可以认为已经被排序取出下一个元素，在已经排序的元素序列中从后向前扫描如果该元素(已排序)大于新元素，将该元素移到下一位置重复步骤3，直到找到已排序的元素小于或者等于新元素的位置将新元素插入到该位置后重复步骤2~5动图演示代码实现123456789def insertionSort(arr):    for i in range(len(arr)):        preIndex = i-1        current = arr[i]        while preIndex &gt;= 0 and arr[preIndex] &gt; current:            arr[preIndex+1] = arr[preIndex]            preIndex-=1        arr[preIndex+1] = current    return arr希尔排序(Shell Sort)1959年Shell发明，第一个突破$O(n^2)$的排序算法，是简单插入排序的改进版，它与插入排序的不同之处在于，它会优先比较距离较远的元素，希尔排序又叫缩小增量排序算法描述先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述：选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1按增量序列个数k，对序列进行k趟排序每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m的子序列，分别对各子表进行直接插入排序。仅增量因子为1时，整个序列作为一个表来处理，表长度即为整个序列的长度。动图演示代码实现12345678910111213141516import mathdef shellSort(arr):    gap=1    while(gap &lt; len(arr)/3):        gap = gap*3+1    while gap &gt; 0:        for i in range(gap,len(arr)):            temp = arr[i]            j = i-gap            while j &gt;=0 and arr[j] &gt; temp:                arr[j+gap]=arr[j]                j-=gap            arr[j+gap] = temp        gap = math.floor(gap/3)    return arr归并排序(Merge Sort)归并排序是建立在归并操作上的一种有效的排序算法。该算法是采用分治法(Divide and Conquer)的一个非常典型的应用将已有序的子序列合并，得到完全有序的序列；即先使每个子序列有序，再使子序列段间有序若将两个有序表合并成一个有序表，称为2-路归并算法描述把长度为n的输入序列分成两个长度为n/2的子序列对这两个子序列分别采用归并排序将两个排序好的子序列合并成一个最终的排序序列动图演示代码实现1234567891011121314151617181920def mergeSort(arr):    import math    if(len(arr)&lt;2):        return arr    middle = math.floor(len(arr)/2)    left, right = arr[0:middle], arr[middle:]    return merge(mergeSort(left), mergeSort(right))def merge(left,right):    result = []    while left and right:        if left[0] &lt;= right[0]:            result.append(left.pop(0))        else:            result.append(right.pop(0));    while left:        result.append(left.pop(0))    while right:        result.append(right.pop(0));    return result快速排序(Quick Sort)基本思想：通过一趟排序将待排记录分隔成独立的两部分，其中一部分记录的关键字均比另一部分的关键字小，则可分别对这两部分记录继续进行排序，以达到整个序列有序算法描述快速排序使用分治法来把一个串(list)分为两个子串(sub-lists)，具体算法描述如下：从数列中挑出一个元素，称为基准(pivot)重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面(相同的数可以到任一边)，在这个分区退出之后，该基准就处于数列的中间位置，这个称为分区(partition)操作递归地(recursive)把小于基准值元素的子数列和大于基准值元素的子数列排序动图演示具体过程，图片来源代码实现1234567891011121314151617181920212223def quickSort(arr, left=None, right=None):    left = 0 if not isinstance(left,(int, float)) else left    right = len(arr)-1 if not isinstance(right,(int, float)) else right    if left &lt; right:        partitionIndex = partition(arr, left, right)        quickSort(arr, left, partitionIndex-1)        quickSort(arr, partitionIndex+1, right)    return arrdef partition(arr, left, right):    pivot = left    index = pivot+1    i = index    while  i &lt;= right:        if arr[i] &lt; arr[pivot]:            swap(arr, i, index)            index+=1        i+=1    swap(arr,pivot,index-1)    return index-1def swap(arr, i, j):    arr[i], arr[j] = arr[j], arr[i]堆排序(Heap Sort)堆排序(Heapsort)是利用堆这种数据结构所设计的一种排序算法堆是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于(或者大于)它的父节点算法描述将初始待排序关键字序列(R1, R2 …. Rn)构建成大顶堆，此堆为初始的无序区将堆顶元素R[1]与最后一个元素R[n]交换，此时得到新的无序区(R1, R2, … Rn-1)和新的有序区(Rn)，且满足R[1, 2 … n-1]&lt;=R[n]由于交换后新的堆顶R[1]可能违反堆的性质，因此需要对当前无序区(R1,R2,……Rn-1)调整为新堆，然后再次将R[1]与无序区最后一个元素交换，得到新的无序区(R1,R2….Rn-2)和新的有序区(Rn-1,Rn)，不断重复此过程直到有序区的元素个数为n-1，则整个排序过程完成动图演示代码实现123456789101112131415161718192021222324252627282930def buildMaxHeap(arr):    import math    for i in range(math.floor(len(arr)/2),-1,-1):        heapify(arr,i)def heapify(arr, i):    left = 2*i+1    right = 2*i+2    largest = i    if left &lt; arrLen and arr[left] &gt; arr[largest]:        largest = left    if right &lt; arrLen and arr[right] &gt; arr[largest]:        largest = right    if largest != i:        swap(arr, i, largest)        heapify(arr, largest)def swap(arr, i, j):    arr[i], arr[j] = arr[j], arr[i]def heapSort(arr):    global arrLen    arrLen = len(arr)    buildMaxHeap(arr)    for i in range(len(arr)-1,0,-1):        swap(arr,0,i)        arrLen -=1        heapify(arr, 0)    return arr非比较类这三种排序算法都利用了桶的概念，但对桶的使用方法上有明显差异：基数排序：根据键值的每位数字来分配桶计数排序：每个桶只存储单一键值桶排序：每个桶存储一定范围的数值计数排序(Counting Sort)计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数算法描述找出待排序的数组中最大和最小的元素统计数组中每个值为i的元素出现的次数，存入数组C的第i项对所有的计数累加(从C中的第一个元素开始，每一项和前一项相加)反向填充目标数组: 将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1动图演示代码实现123456789101112131415def countingSort(arr, maxValue):    bucketLen = maxValue+1    bucket = [0]*bucketLen    sortedIndex =0    arrLen = len(arr)    for i in range(arrLen):        if not bucket[arr[i]]:            bucket[arr[i]]=0        bucket[arr[i]]+=1    for j in range(bucketLen):        while bucket[j]&gt;0:            arr[sortedIndex] = j            sortedIndex+=1            bucket[j]-=1    return arr优缺点当数列最大最小值差距过大时，并不适用计数排序当数列元素不是整数，并不适用计数排序优化版本的计数排序属于稳定排序桶排序(Bucket Sort)桶排序是计数排序的升级版，它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定桶排序(Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序(有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排)算法描述设置一个定量的数组当作空桶遍历输入数据，并且把数据一个一个放到对应的桶里去对每个不是空的桶进行排序从不是空的桶里把排好序的数据拼接起来图片演示代码实现1234567891011def bucket_sort(array):    min_num, max_num = min(array), max(array)    bucket_num = (max_num-min_num)//3 + 1    buckets = [[] for _ in range(int(bucket_num))]    for num in array:        buckets[int((num-min_num)//3)].append(num)    new_array = list()    for i in buckets:        for j in sorted(i):            new_array.append(j)    return new_array基数排序（Radix Sort）基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前算法描述取得数组中的最大数，并取得位数arr为原始数组，从最低位开始取每个位组成radix数组对radix进行计数排序(利用计数排序适用于小范围数的特点)动图演示代码实现123456789101112131415def radix_sort(array):    max_num = max(array)    place = 1    while max_num &gt;= 10**place:        place += 1    for i in range(place):        buckets = [[] for _ in range(10)]        for num in array:            radix = int(num/(10**i) % 10)            buckets[radix].append(num)        j = 0        for k in range(10):            for num in buckets[k]:                array[j] = num                j += 1总结使用场景如果待排序列中数据含有大量重复值——优先使用计数排序如果待排序列中数据近乎有序——优先使用插入排序如果待排序列中数据取值范围有限——优先使用计数排序如果待排序列中数据要求稳定——优先使用归并排序如果待排序列需要使用链表——优先链表归并、链表快排如果待排序列中数据无法全部装到内存——优先使用外部排序优缺点xxx</content>
	</entry>
	<entry>
		<title>机器学习_最优化方法(1)</title>
		<url>/article/58730.html</url>
		<content>写在前面，本系列主要是对下面这本书做的学习笔记常用数学符号的 LaTeX 表示方法Markdown 常用数学符号和公式基本概念前言最优化方法在机器学习领域处于中心地位，绝大多数算法最后都归结于求解最优化问题，从而确定模型参数，或直接获得预测结果有监督学习，通过最小化损失函数或优化其他类型的目标函数确定模型的参数数据降维算法，通过优化某种目标函数，确定降维后的结果，如主成分分析按照优化变量的类型，可以将优化问题分为连续型优化问题与离散型优化问题连续型优化问题的优化变量是连续变量，一般可借助导数求解离散型优化问题的优化变量则为离散值连续型优化问题又可分为凸优化问题与非凸优化问题，凸优化问题可以保证求得全局最优解按照目标函数的数量单目标优化：只有一个目标函数,多目标优化：有多个目标函数按照是否带有约束条件不带约束的优化带约束的优化按照求解方式可分为数值优化：求问题的近似解解析解：求精确的公式解基于概率的优化算法是优化算法家族中一种特殊的存在，典型的是遗传算法与贝叶斯优化通常情况下最优化问题是求函数的极值，其优化变量是整数或实数有一类特殊的优化问题，其目标是寻找某一函数，使得泛函的值最大化或最小化，变分法是求解此类问题的经典方法基本概念最优化问题的目标是求函数或泛函的极值(Extrema)，在基础数学、计算数学、应用数学以及工程、管理、经济学领域均有应用最优化算法是求解最优化问题的方法确定优化目标函数之后，需要根据问题的特点以及现实条件的限制选拝合适的算法在机器学习与深度学习库中，最优化算法通常以优化器(Optimizer)或求解器(Solver)的形式出现问题定义接下来考虑的最优化问题是求解函数极值的问题，包括极大值与极小值要计算极值的函数称为目标函数，其自变量称为优化变量，对于函数其极小值在$x=2$点处取得，此时函数值为$5$，$x=2$ 为该问题的解一般将最优化问题统一表述为极小值问题，对于极大值问题，只需要将目标函数反号，即可转化为极小值问题要求$f(x)$的极大值，等价于求$-f(x)$的极小值，最优化问题可以形式化地定义为其中$x$为优化变量，$f$为目标函数，$X \subseteq \mathbb{R}^{n}$为优化变量允许的取值集合，称为可行域(Feasible Set)，它由目标函数的定义域、等式及不等式约束(Constraint Function)共同确定可行域之内的解称为可行解(Feasible Solution)，下面是一个典型的最优化问题该问题的可行域为区间$[-10,10]$，如不进行特殊说明，这里目标函数均指多元函数，一元函数为其特例，无须单独讨论线性规划：目标函数为一次函数(线性函数)非线性规划：目标函数是非线性函数非线性规划的一种特例是目标函数为二次函数，称为二次规划很多实际应用问题可能带有等式和不等式约束条件，可以写成这里将不等式约束统一写成小于或等于号的形式，满足等式和不等式约束条件的解称为可行解，否则称为不可行解下面是一个带有等式和不等式约束的优化问题等式和不等式约束定义的区域与目标函数定义域的交集为可行域此问题的可行域为约束条件$x+y=1$与$x^{2}+y^{2} \leqslant 4$所定义的交集，是直线位于圆内的部分，如下图所示xxx图可行域示意图在很多实际问题中出现的二次规划可以写成下面的形式其中$x \in \mathbb{R}^{n}$，$Q$是$n \times n$的二次项系数矩阵，$c \in \mathbb{R}^{n}$是一次项系数向量$\boldsymbol{A}$是$m \times n$的不等式约束系数矩阵，$\boldsymbol{b} \in \mathbb{R}^{m}$是不等式约束的常数向量局部最优解与全局最优解假设是一个可行解，如果对可行域内所有点都有，则称为全局极小值类似地可以定义全局极大值全局极小值是最优化问题的解对于可行解，如果存在其邻域，使得该邻域内的所有点即所有满足的点，都有，则称为局部极小值类似地可以定义局部极大值局部极小值可能是最优化问题的解，也可能不是最优化算法的目标是寻找目标函数的全局极值点而非局部极值点。下图为全局最优解与局部最优解的示意图，目标函数为xxx图全局最优解与局部最优解上图中的目标函数有两个局部极大值点、1个局部极小值点，区间$[0,2]$内的局部极大值点也是全局极大值点迭代法的基本思想如果目标函数可导，那么可以利用导数信息确定极值点微积分为求解可导函数极值提供了统一的方法，即寻找函数的驻点根据费马引理，对于一元函数局部极值点必定是导数为0的点；对于多元函数则是梯度为0的点(在数值计算中，也称为静止点(Stationary Point)机器学习中绝大多数目标函数可导，因此这种方法是适用的通过求解驻点来寻找极值虽然在理论上可行，但实现时却存在困难实际问题中目标函数梯度为0的方程组通常难以求解，对于下面的二元目标函数对$x$和$y$分别求偏导数并令它们为0，得到如下方程组显然，这个方程组很难求解，含有指数函数、对数函数、三角函数以及反三角函数的方程一般情况下没有公式解，称为超越方程即使是代数方程(多项式方程)，4次以上的方程没有求根公式方程系数的有限次加减乘除以及开方运算均不可能是方程的根因此，直接解导数为0的方程组不是一种可行的方法对于大多数最优化问题通常只能近似求解，称为数值优化一般采用迭代法，从一个初始可行点$x_{0}$开始，反复使用某种规则迭代直至收敛到最优解具体地，在第次迭代时，从当前点移动到下一个点如果能构造一个数列${x_k}$，保证它收敛到梯度为0的点，即下面的极限成立则能找到函数的极值点算法的核心1️⃣如何定义从上一个点移动到下一个点的规则，这些规则一般利用一阶导数(梯度)或二阶导数(黑塞矩阵)因此，迭代法的核心是得到如下式形式的迭代公式梯度下降法，牛顿法及拟牛顿法均采用了此思路，区别在于构造迭代公式的方法不同，迭代法的原理如图所示xxx图迭代法的原理2️⃣初始点$\boldsymbol{x}_{0}$的选择，通常用常数或随机数进行初始化算法要保证对任意可行的$x_{0}$均收敛到极值点处一阶优化算法一阶优化算法利用目标函数的一阶导数构造迭代公式，典型代表是梯度下降法及其变种本节介绍基本的梯度下降法、最速下降法、梯度下降法的其他改进版本(包括动量项、 AdaGrad、RMSProp、AdaDelta、Adam 算法等)以及随机梯度下降法梯度下降法梯度下降法(Gradient Descent Method)由数学家柯西提出，它沿着当前点处梯度相反的方向进行迭代，得到，直至收致到梯度为0的点其理论依据：在梯度不为0的任意点处，梯度正方向是函数值上升的方向，梯度反方向是函数值下降的方向下面先通过例子说明，然后给出严格的证明1️⃣首先考虑一元函数的情况，如图所示xxx图一元函数的导数值与函数单调性的关系对于一元函数，梯度是一维的，只有两个方向：沿着$x$轴向右和向左如果导数为正，则梯度向右；否则向左当导数为正时，是增函数，$x$变量向右移动时(即沿着梯度方向)函数值增大，否则减小，对于上图所示的函数当$x&lt;x_{0}$时，导数为正，此时向左前进函数值减小，向右则增大当$x&gt;x_{0}$时，导数为负，此时向左前进函数值增大，向右则减小2️⃣接下来考虑二元函数，二元函数的梯度有无穷多个方向对于函数$x^{2}+y^{2}$，其在$(x，y)$点处的梯度值为$(2 x，2 y)$，函数在$(0,0)$点处有极小值，在任意点$(x，y)$处，从$(0,0)$点指向$(x，y)$方向(即梯度方向)的函数值都是单调递增的该函数的形状如下图所示xxx图4.6$x^{2}+y^{2}$的形状下图为该函数的等高线，在同一条等高线上的所有点处函数值相等xxx图4.6$x^{2}+y^{2}$的等高线在任意点处，梯度均为从原点指向该点，是远离原点的方向，函数值单调增下面考虑函数$-x^{2}-y^{2}$，其在$(x，y)$点处的梯度值为$(-2 x,-2 y)$，函数在$(0,0)$点处有极大 值，在任意点$(x，y)$处，从$(x，y)$点指向$(0,0)$方向的函数值都是单调递增的，$(-x,-y)$即梯度 方向，下图为该目标函数的形状xxx图$-x^{2}-y^{2}$的形状下面给出严格的证明，将函数在$\boldsymbol{x}$点处作一阶泰勒展开对上式变形，函数的增量与自变量增量、函数梯度的关系为如果令$\Delta \boldsymbol{x}=\nabla f(\boldsymbol{x})$，则有如果$\Delta x$足够小，则可以忽略高阶无穷小项，有如果在$\boldsymbol{x}$点处梯度不为$\boldsymbol{0}$，则能保证移动到$\boldsymbol{x}+\Delta \boldsymbol{x}$时函数值增大，相反地，如果令$\Delta \boldsymbol{x}=-\nabla f(\boldsymbol{x})$，则有即函数值减小，事实上，只要确保则有因此，选择合适的增量$\Delta x$就能保证函数值下降，负梯度方向是其中的一个特例接下来证明：增量的模一定时，在负梯度方向，函数值是下降最快的由于其中$\theta$为$\nabla f(\boldsymbol{x})$与$\Delta \boldsymbol{x}$之间的夹角，因此，如果$\theta&lt;\frac{\pi}{2}$, 则$\cos \theta&gt;0$，从而有此时函数值增大$\Delta \boldsymbol{x}$沿着正梯度方向是其特例，如果$\theta&gt;\frac{\pi}{2}$，则$\cos \theta&lt;0$，从而有此时函数值下降$\Delta x$沿着负梯度方向即$\theta=\pi$是其特例，由于$-1 \leqslant \cos \theta \leqslant 1$，因此，如果向量$\Delta x$的模大小一 定，则$\Delta x=-\nabla f(\boldsymbol{x})$，即在梯度相反的方向函数值下降最快，此时$\cos \theta=-1$。梯度下降法每次的迭代增量为其中$\alpha$为人工设定的接近于0的正数，称为步长或学习率，其作用是保证$\boldsymbol{x}+\Delta \boldsymbol{x}$在$\boldsymbol{x}$的邻域内、从而可以忽略泰勒公式中的$o(|\Delta x|)$项，否则不能保证每次达代时函数值下降，使用该增量则有函数值下降，由此得到梯度下降法的迭代公式，从初始点$x_{0}$开始，反复使用如下迭代公式只要没有到达梯度为0的点，函数值会沿序列$\boldsymbol{x}_{k}$递减，最终收敛到梯度为0的点从出发，用上式进行迭代，会形成一个函数值递减的序列迭代终止的条件是函数的梯度值为$\mathbf{0}$(实际实现时是接近于 0 即可)，此时认为已经达到极值点。梯度下降法的流程如算法$4.1$所示🧮算法梯度下降法初始化$x_{0}$，$k=0$while $\left|\nabla f\left(\boldsymbol{x}_{k}\right)\right|&gt;$eps and $k&lt;N$ do​    迭代​    $k=k+1$end while$x_{0}$可初始化为固定值，如$\mathbf{0}$，或随机数(通常为均匀分布或正态分布)，后者在训练神经网络时经常被采用eps为人工指定的接近于0的正数，用于判定梯度是否已经接近于$0$；$N$为最大迭代次数，防止死循环的出现梯度下降法在每次迭代时只需要计算函数在当前点处的梯度值，具有计算量小、实现简单的优点只要未到达驻点处且学习率设置恰当，每次达代时均能保证函数值下降下图为用梯度下降法求解$x^{2}+y^{2}$极值的过程，迭代初始值(图中的大圆点)设置为$(0,4)$，学习率设置为$0.1$每次迭代时的值$\boldsymbol{x}_{i}$以小圆点显示学习率$\alpha$的设定也是需要考虑的问题，一般情况下设置为固定的常数，如$10^{-5}$在深度学习中，采用了更复杂的策略，可以在迭代时动态调整其值最速下降法梯度下降法中步长$\alpha$是固定的，或者根据某种人工指定的策略动态调整最速下降法(Steepest Descent Method)是对梯度下降法的改进，它用算法自动确定步长值最速下降法同样沿着梯度相反的方向进行迭代，但每次需要计算最佳步长$\alpha$最速下降法的搜索方向与梯度下降法相同，也是负梯度方向在该方向上寻找使得函数值最小的步长，通过求解如下一元函数优化问题实现优化变量是$\alpha$，实现时有两种方案将的取值离散化，取典型值，分别计算取这些值的目标函数值，然后确定最优值直接求解上式目标函数的驻点，对于有些情况可得到解析解这类方法也称为直线搜索(Line Search)，它沿着某一确定的方向在直线上寻找最优步长梯度下降法的改进梯度下降法在某些情况下存在收敛速度慢、收敛效果差的同题，因此出现了大量改进方案标准的梯度下降法可能存在振荡问题，具体表现为在优化变昰的某些分量方向上来回振荡，导致收敛速度慢下图显示了用梯度下降法求解的极值时的逄代过程，可以看到， 迭代序列在方向来回振荡xxx图梯度下降法振荡问题动量项梯度下降法动量项梯度下降法通过引入动量项解决此问题，类似于物理中的动量，依靠惯性保持迭代时的前进方向动量项的计算公式为它是上次迭代时的动量项与本次负梯度值的加权和，其中$\alpha$是学习率，其作用与标准的梯度下降法相同，$\mu$是动量项系数如果按照时间线展开，则第$k$次迭代时使用了从1到$k$次迭代时的所有负梯度值，且负梯度值按系数$\mu$指数级衰减，即使用了移动指数加权平均反复利用上式，展开之后的动量项为更新优化变量值时使用动量项代替负梯度项，梯度下降更新公式为动量项加快了梯度下降法的收敛速度，它使用历史信息对当前梯度值进行修正，消除病态条件问题上的来回振荡下图显示了用动量项梯度下降法求解极值时的迭代过程，与上图相比，迭代代序列更为平滑，且收敛更快xxx图使用动量项后的迭代轨迹AdaGrad标准梯度下降法的步长值难以确定，且优化变量的各个分量采用了相同的步长，AdaGrad(Adaptive Gradient)算法根据前几轮迭代时的历史梯度值动态计算步长值，且优化向量的每一个分量都有自已的步长，梯度下降迭代公式为其中是人工设定的全局学习率，$g_{k}$是第$k$次迭代时的梯度向量，$\varepsilon$是为避免除0操作而增加的接近于0的正数，$i$为向量的分量下标，这里的计算针对向量的每个分量分别进行与标准梯度下降法相比，上式多了分母项，它累积了到本次迭代为止的梯度的历史值信息，用于计算步长值历史导数值的绝对值越大，在该分量上的学习率越小，反之越大虽然实现了自适应学习率，但这种算法还存在问题：需要人工设置全局学习率$\alpha$；随着时间的累积，上式中的分母会越来越大，导致学习率趋向于0，优化变量无法有效更新RMSPropRMSProp算法是对AdaGrad的改进，避免了长期累积梯度值所导致的学习率趋向于0的问题算法维持一个梯度平方累加值的向量$E\left[\boldsymbol{g}^{2}\right]$，其初始值为$\mathbf{0}$，更新公式为这里的$g^{2}$是对梯度向量的每个分量分别进行平方，$\delta$是人工设定的衰减系数不同于AdaGrad直接累加所有历史梯度的平方和，RMSProp将历史梯度平方值按照系数$\delta$指数级衰减之后再累加，即使用了移动指数加权平均梯度下降法更新公式为$\alpha$是人工设定的全局学习率，标准梯度下降方法相比，这里也只多了一个分母项AdaDeltaAdaDelta算法也是对AdaGrad的改进，避免了长期累积梯度值所导致的学习率趋向于0的问题，还去掉了对人工设置全局学习率的依赖算法定义了两个向量，初始值均为0$E\left[g^{2}\right]$是梯度平方(对每个分量分别平方)的累计值，与RMSProp算法相同，更新公式为$g^{2}$是向量每个元素分别计算平方，后面所有的计算公式都是对向量的每个分量分别进行计算，接下来计算RMS向量然后计算优化变量的更新值这里根据计算，计算公式与相同这个更新值同样通过梯度来构造，但学习率是通过梯度的历史值确定的$E\left[\Delta x^{2}\right]$是优化变量更新值的平方累加值，它们的更新公式为在这里，是对的每个分量进行平方。梯度下降的达代公式为AdamAdam(Adaptive Moment Estimation)算法整合了自适应学习率与动量项算法用梯度构造了两个向量$m$和$v$，初始值为$0$，更新公式为这里、是人工置顶的参数，梯度下降的迭代公式为$m$的作用相当于于动量项，$v$用于构造学习率随机梯度下降法在机器学习中，目标函数通常定义在一个训练样本集上假设训练样本集有$N$个样本，机器学习模型在训练时优化的目标是这个数据集上的平均损失函数其中是对单个训练样本的损失函数，是机哭学习模型需要学习的参数，是优化变量显然，因此计算目标函数梯度时需要计算对每个训 练样本损失函数的梯度，然后求均值如果训练时每次达代都用所有样本，那么计算成本太高作为改进，可以在每次迭代时选取一批样本，将损失函数定义在这些样本上，作为整个样本集的损失函数的近似值小批量随机梯度下降法(Mini Batch Gradient Descent Method在每次迭代时使用上面目标函数的随机逼近值，只使用$M \ll N$个样本来近似计算损失函数，在每次迭代时，要优化的目标函数变为随机梯度下降法在数学期望的意义下收敛，随机采样产生的梯度的期望值是真实的梯度在具体实现时，每次先对所有训练样本进行随机洗牌，打乱顺序；然后将其均匀分成多份，每份$M$个样本；接下来依次用每一份执行梯度下降法迭代一种特殊情况是$M=1$，每次迭代只使用一 个训练样本随机梯度下降法并不能保证每次迭代后目标函数值下降，事实上，每次迭代时使用的是不同的目标函数但通常情况下目标函数的整体趋势是下降的，能够收敛到局部极值点处下图是用随机梯度下降法训练神经网络时损失函数的曲线，横轴为迭代次数，纵轴为损失函数的值可以看到，迭代时函数的值会出现振荡，但整体趋势是下降的，最后收敛xxx图用随机梯度下降法训练神经网络时的损失函数曲线除具有实现效率高的优点之外，随机梯度下降法还会影响收敛的效果对于深度神经网络，随机梯度下降法比批量梯度下降法更容收敛到一个好的极值点处应用一工神经网络假设有个训练样本，其中为输入向量，为标签向量训练的目标是最小化样本标签值与神经网络预测值之间的误差，如果使用均方误差，则优化的目标为其中$w$为神经网络所有参数的集合，包括各层的权重和偏置，$h(x)$是神经网络实现的映射这 个最优化问题是一个不带约束条件的问题，可以用梯度下降法求解如果计算出了损失函数对参数的梯度值，梯度下降法第$k+1$次迭代时参数的更新公式为梯度值的计算通过反向传播算法实现参数的初始化是一个需要考虑的问题，一般用随机数进行初始化如果训练样本数很大，那么通常采用随机梯度下降法，通常情况下，随机梯度下降法有很好的收敛效果二阶优化算法梯度下降法只利用了一阶导数信息，收敛速度慢，通常情况下，利用二阶导数信息可以加快收敘速度，典型代表是牛顿法和拟牛顿法牛顿法在每个迭代点处将目标函数近似为二次函数，然后通过求解梯度为0的方程得到迭代方向牛顿法在每次迭代时需要计算梯度向量与黑塞矩阵，并求解一个线性方程组，计算量大且面临黑塞矩阵不可逆的问题拟牛顿法是对它的改进，算法构造出一个矩阵作为黑塞矩阵或其逆矩阵的近似牛顿法牛顿法(Newton Method)寻找目标函数作二阶近似后梯度为$\mathbf{0}$的点，逐步逼近极值点根据费马引理，函数在点$x$处取得极值的必要条件是梯度为0直接计算函数的梯度然后解上面的方程组通常很困难，和梯度下降法类似，可以采用迭代法近似求解对目标函数在$x_{0}$处作二阶泰勒展开忽略二次以上的项，将目标函数近似成二次函数，等式两边同时对$\boldsymbol{x}$求梯度，可得其中为在处的黑塞矩阵从上面可以看出，这里至少要展开到二阶，如果只有一阶，那么无法建立梯度为$\mathbf{0}$的方程组，因为此时一次近似函数的梯度值为常数令函数的梯度为$\mathbf{0}$，有解这个线性方程组可以得到如果将梯度向量简写为$\boldsymbol{g}$，黑塞矩阵简记为$\boldsymbol{H}$，上式可以简写为由于在泰勒公式中忽略了高阶项将函数进行了近似，因此这个解不一定是目标函数的驻点，需要反复用上式进行迭代从初始点$x_{0}$处开始，计算函数在当前点处的黑塞矩阵和梯度向量，然后用下面的公式进行迭代直至收敛到驻点处即在毎次迭代之后，在当前点处将目标函数近似成二次函数，然后寻找梯度为0的点$-H^{-1} g$称为牛顿方向，迭代终止的条件是梯度的模接近于0，或达到指定的达代次数牛顿法的流程如下算法所示🧮牛顿法初始化$x_{0}$，$k=0$while $k&lt;N$ do​    计算当前点处的梯度值$g_k$以及黑塞矩阵$H_k$​    if $\left|g_{k}\right|&lt;$ eps then​        停止迭代​    end if​    迭代​    迭代​    $k=k+1$end while其中$\alpha$是人工设置的学习率，需要学习率的原因与梯度下降法相同，是为了保证能够忽略泰勒公式中的高阶无穷小项如果目标函数是二次函数，则黑塞矩阵是一个常数矩阵，且泰靿公式中的高阶项为0对于任意给定的初始点$\boldsymbol{x}_{0}$，牛顿法只需要一次迭代即可收敛到驻点与梯度下降法不同，牛顿法无法保证每次迭代时目标函数值下降。为了确定学习率的值，通常使用直线搜索技术具体做法是让$\alpha$取一些典型的离散值，如下面的值选择使得最小化的步长值作为最优步长，保证迭代之后的函数值充分下降与梯度下降法相比，牛顿法有更快的收敛速度，但每次迭代的成本也更高1️⃣每次迭代时需要计算梯度向量与黑塞矩阵，并计算黑塞矩阵的逆矩阵，最后计算矩阵与向量乘积实现时通常不直接求黑塞矩阵的逆矩阵，而是求解如下方程组求解线性方程组可使用迭代法，如共轭梯度法2️⃣另一个问题是黑塞矩阵可能不可逆，从而导致其失效拟牛顿法拟牛顿法(Quasi-Newton Methods)对牛顿法存在的问题进行了改进其核心思路是不精确计算目标函数的黑塞矩阵然后求逆矩阵，而是通过其他手段得到黑塞矩阵的逆具体做法是构造一个近似黑塞矩阵或其逆矩阵的正定对称矩阵，用该矩阵进行牛顿法迭代由于要推导下一个迭代点的黑塞矩阵需要满足的条件，并建立与上一个迭代点处的函数值、导数值之间的关系，以指导近似矩阵的构造，因此需要在点处作泰勒展开，并将的值代入泰勒公式将函数在$x_{k+1}$点处作二阶泰勒展开，忽略高次项，有上式两边同时对$x$取梯度，可以得到如果令$x=x_{k}$，则有将梯度向量与黑塞矩阵简写，则有如果令则上式可简写为这里的和都可以根据之前的迭代结果直接算出，如果可逆，那么上式等价于上两式称为拟牛顿条件，用于近似代替黑塞矩阵和它的逆矩阵的矩阵需要满足该条件利用该条件，根据上一个迭代点和当前迭代点的值以及这两点处的梯度值，就可以近似计算出当前点的黑塞矩阵或其逆矩阵由于黑塞矩阵与它的逆矩阵均对称，因此它们的近似矩阵也要求是对称的此外，通常还要求近似矩阵正定，拟牛顿法通过各种方法构造出满足上述条件的近似矩阵下面介绍典型的实现：DFP算法以及BFGS算法DFP算法问题的核心是构造黑塞矩阵或其逆矩阵的近似矩阵$H_{k}$，保证满足拟牛顿条件首先为该矩阵设定初始值，然后在每次迭代时更新此近似矩阵其中称为校正矩阵，现在的任务变为寻找该矩阵，根据上式，如果以充当黑塞矩阵逆矩阵的近似，有上式变形后得到DFP算法采用了这种思路，DFP(Davidon-Fletcher-Powell)算法以其3位发明人的名字命名算法构造黑塞矩阵逆矩阵的近似(Inverse Hessian Approximation)，其初始值为单位矩阵$I$，每次迭代时按照下式更新该矩阵即校正矩阵为其中和为待定的维向量，和为待定的系数显然，按照上式构造的$\boldsymbol{H}_{k}$是一个对称矩阵根据上几个式子，校正矩阵必须满足即此方程的解不唯一，可以取某些特殊值从而简化问题的求解，这里令同时令将这两个解代人上面的两个方程，可以得到以及上面两个结果利用了矩阵乘法的结合律以及是对称矩阵这一条件，在这里与均为标量，从而解得将上面的解代人式 (4.16)，由此得到矩阵$H_{k}$的更新公式此更新公式可以保证$H_{k}$的对称正定性每次迭代时，得到矩阵$H_{k}$之后用牛顿法进行更新，由于构造的是黑塞矩阵逆矩阵的近似，因此可以直接将其与梯度向量相乘从而得到牛顿方 向DFP算法的流程如下算法所示🧮DFP算法初始化$x_{0}$，$H_0$，$k=0$while $k&lt;N$ do​    迭代​    用直线搜索得到步长$\lambda_{k}$​    迭代​    if $\left|g_{k+1}\right|&lt;eps$ then​        结束循环​    end if​    迭代​    迭代​    $k=k+1$end while如果用单位矩阵初始化$\boldsymbol{H}_{k}$，则第一次迭代时这相当于使用梯度下降法，后面逐步细化$H_{k}$，使其更精确地通近当前点处黑塞矩阵的逆矩阵。BFGS算法BFGS(Broyden-Fletcher-Goldfarb-Shanno)算法以其4位发明人的名字命名算法构造黑塞矩阵的一个近似矩阵$B_{k}$并用下式迭代更新这个矩阵该矩阵的初始值为单位阵，要解决的问题就是每次的校正矩阵的构造根据前面的式子，黑塞矩阵的近似矩阵$B_{k}$需要满足与DFP算法相同，校正矩阵构造为如下形式将其代人式 (4.20)，可以得到整理后可得同样，可以取这个方程的一组特殊解，这里直接令同时令两个向量为将它们的代入上面两个方程，可得以及从而解得两个系数为由此得到校正矩阵为如果初始值是正定矩阵，且在每次迭代时，则每次更新后得到的都是正定的由于BFGS算法构造的是黑塞矩阵的近似，因此还需要求解方程组以得到牛顿方向而$B_{k}$是正定对称矩阵，可以采用高效的方法求解此线性方程组比较DFP算法与BFGS算法可以发现，二者的校正矩阵计算公式互为对偶，将与的角色进行了对换BFGS算法的流程如下算法所示🧮BFGS算法初始化$x_{0}$，$B_0=I$，$k=0$while $k&lt;N$ do​    迭代​    用直线搜索得到步长$\lambda_{k}$​    迭代​    if $\left|g_{k+1}\right|&lt;eps$ then​        结束循环​    end if​    迭代​    迭代​    $k=k+1$end whileBFGS 算法在每次达代时需要计算$n \times n$的矩阵$\boldsymbol{B}_{k}$，当$n$很大时，存储该矩阵将耗费大量内存为此，提出了改进方案L-BFGS算法(有限存储的BFGS算法)，其思想是不存储完整的矩阵，只存储向量和对于大多数目标函数，BFGS 算法有很好的收敛效果下图是用L-BFGS算法求解$x^{2}+y^{2}$极值的迭代过程，算法只需要达代4次即可收敛到极小值点处xxx图L-BFGS算法求解$x^{2}+y^{2}$极值的迭代过程分治法分治法是算法设计中常用的思路，它把一个问题拆分成多个子问题，通常情况下，子问题更容易求解在求得子问题的解之后，将其合并得到整个问题的解在用于最优化方法时的通行做法是每次只优化部分变量，将高维优化问题分解为低维优化问题坐标下降法坐标下降法(Coordinate Descent)是分治法的典型代表对于多元函数的优化问题，坐标下降法每次只对一个分量进行优化，将其他分量固定不动算法依次优化每一个变量，直至收敛，假设要求解的优化问题为算法在每次迭代时依次选择进行优化，求解单个变量的优化问题完整的算法流程如下算法所示🧮坐标下降法初始化$x_{0}$while 没有收敛 do​    for i=1,2, $\cdots$, n do​        求解 $min_{x_i}{f(x)}$​    end forend while算法每次迭代时在当前点处沿一个坐标轴方向进行一维搜索，固定其他坐标轴方向对应的分量，求解一元函数的极值在整个过程中依次循环使用不同坐标轴方向对应的分量进行达代，更新这些分量的值，一个周期的一维搜索迭代过程相当于一次梯度下降法造代完成对优化变量每个分量的一次更新坐标下降法的求解过程如下图所示，这里是二维优化问题xxx图坐标下降法求解二维优化问题的迭代过程在每次连代时，首先固定$y$轴分量，优化$x$轴分量；然后固定$x$轴分量，优化$y$轴分量，整个优化过程在各个坐标轴方向之间轮换 坐标下降法具有计算量小、实现效率高的优点，在机器学习领域得到了成功的应用典型的是求解线性模型的训练问题，在开源库liblinear中有实现，此外，在求解非负矩阵分解问题中也有应用缺点坐标下降法的缺点是对非光滑(不可导)的多元目标函数可能无法进行有效处理，以及难以并行化考虑下面的目标函数f(x, y)=|x+y|+3|y-x|如果当前迭代点为$(-2,-2)$，单独在$x$轴和$y$轴方向进行迭代均无法保证目标函数值下降，此时坐标下降法失效如下图所示，图中画出了目标函数的等高线，$(0,0)$是极小值点，在$(-2,-2)$点处，单独改变$x$或$y$的值均不能使目标函数值下降xxx图 4.16 坐标下降法失效的例子SMO算法SMO(Sequential Minimal Optimization)算法是求解支持向量机对偶问原的高效算法算法的核心思想是每次从优化变量中挑出两个分量进行优化，让其他分量固定，这样能保证满足等式约束条件假设训练样本集为，其中为样本的特征向量，为标签值在使用核函数之后，支持向量机在训练时求解的对偶问题为$C$为惩罚因子，是人工设定的正常数，核矩阵的元素为这里为核函数，将两个向量映射为一个实数该对偶问题是二次规划问题，问题规模由训练样本数$l$决定，其值很大时，常规的求解算法将面临计算效率低和存储空间占用太大的问题SMO算法每次选择两个变量进行优化，假设选取的两个分量为和，其他分量都固定，当成常数由于、，对这两个变量的目标函数可以写成其中$c$是一个常数。这里定义这里的$\alpha^{*}$为$\alpha$在上一轮迭代后的值子问题的目标函数是二元二次函数，可以直接给出最小值 的解析解，这个问题的约束条件为利用上面的等式约束可以消掉，从而只剩下变量，目标函数是的二次函数，直接求得解析解分阶段优化AdaBoost算法在训练时同样采取了分治法的策略，每次迭代时先训练弱分类器，然后确定弱分类器的权重系数AdaBoost算法在训练时的目标是最小化指数损失函数假设强分类器为$F(x)$，$x \in \mathbb{R}^{n}$为特征向量，$y=\pm 1$为标签值，单个训练样本的指数损失函数为强分类器是弱分类器的加权组合，定义为其中是第个弱分类器，是其权重，为弱分类器的数量训练时依次训练每个弱分类器，将其加人强分类器中将强分类器的计算公式代人上面的损失函数中，得到训练第$j$个弱分类器时对整个训练样本集的训练损失函数为这里将强分类器拆成两部分第一部分是之前的迭代已经得到的强分类器$F_{j-1}$第二部分是当前要训练的弱分类器$f$与其权重$\beta$的乘积对训练样本的损失函数前者在之前的迭代中已经求出，因此可以看成常数，上式目标函数可以简化为其中它只和前面迭代得到的强分类器有关，与当前的弱分类器、弱分类器权重无关，这就是样本权重上上式的问题可以分两步求解，首先将$\beta$看成常数由于和的取值只能为或，且样本权重非负，要让上上式的目标函数最小化，必须让二者相等因此损失函数对$f(x)$的最优解为其中$I$是指标函数，如果括号里的条件成立，其值为1，否则为0上式的最优解是使得对样本的加权误差最小的弱分类器，在得到弱分类器之后，式子的优化目标可以表示成$\beta$的函数上式前半部分是被当前的弱分类器正确分类的样本，此时与同号且，，后半部分是被当前的弱分类器错误分类的样本，这种情况有，目标函数可以进一步写成具体推导过程为函数在极值点的导数为0，对$\beta$求导并令其为0上式两边同除以，由此得到关于的方程最后得到最优解为其中$err_{j}$为当前弱分类器对训练样本集的加权错误率在得到当前弱分类器之后，对强分类器进行更新下次迭代时样本的权重为即AdaBoost训练算法中的样本权重更新公式应用logistic回归下面介绍坐标下降法在求解logistic回归训练问题中的应用，给定$l$个训练样本，，其中为特征向量，为标签值L2正则化logistic回归的对偶问题为其中$C$为惩罚因子，矩阵$Q$定义为如果定义它与下面的极限是一致的最小化式可以简化为上式的目标函数中带有对数函数，可以采用坐标下降法求解与其他最优化方法(如共轭梯度法、拟牛顿法)相比，坐标下降法有更快的迭代速度，更适合大规模问题的求解下面我们介绍带约束条件的坐标下降法求解思路在用坐标下降法求解时，采用了一个技巧，不是直接优化一个变量，而是优化该变量的增量假设本次迭代时要优化，将其他的固定不动假设本次迭代之后的值为，在这里为一个常数，是上一次迭代后的值用替换，最小化式子的目标函数以及不等式约束可以写成的函数其中所有常数定义为因为目标函数含有对数函数，上面的函数是一个超越函数，无法给出极值的公式解采用牛顿法求解上面的问题，不考虑不等式约束条件，迭代公式为这是牛顿法对一元函数的情况，梯度为一阶导数，黑塞矩阵为二阶导数，子问题目标函数的一阶导数和二阶导数分别为为了保证牛顿法收敛，还需要使用直线搜系，检查迭代之后的函数值是否充分下降</content>
	</entry>
	<entry>
		<title>高斯分布相关算法</title>
		<url>/article/8896.html</url>
		<content>学派对概率的诠释有两大学派，一种是频率派另一种是贝叶斯派，后面我们对观测集采用下面记号：$X$这个矩阵展开如下表示有$N$个样本，每个样本都是$p$维向量，其中假设每个观测都是由 $p(x|\theta)$生成的频率派频率派认为$p(x|\theta)$中的$\theta$是一个未知的常量，而数据是一个随机变量，关心的是数据，目标是估计未知的常量$\theta$常用的方法是最大似然估计其中每个样本$x_{i}$独立同分布于$P(x \mid \theta)$，因此上面的累乘可以改写成累加的形式贝叶斯派贝叶斯派认为$p(x|\theta)$中的$\theta$是一个随机变量，且$\theta$服从一定的概率分$\theta\sim p(\theta)$，通常将$p(\theta)$成为先验根据贝叶斯定理，将参数的先验和后验用似然函数联系起来其中先验是$ p(\theta)$，后验是$p(\theta|X)$，似然函数为$p(\theta){p(X)}$最大后验估计为了估计$\theta$的值，我们使用最大后验估计MAP进行求解，其目的是找到一个$\theta$使用估计出来的结果最大其中$\propto$是因为对于分类问题而言$p(X)$都是一样的，可以看成一个常数，严格意义上MAP并不是标准的贝叶斯方法贝叶斯估计真正的贝叶斯估计是实实在在的去求解后验概率其中$\int _{\theta}p(X|\theta)\cdot p(\theta)d\theta$通常求解难度较大，因此可以随机采样算法，如蒙特卡洛MCMC算法进行近似求解求出来的后验概率可以在贝叶斯预测时使用，假设这时候来了个新样本$\tilde{x}$，预测问题就是求$p(\tilde{x}|X)$通过$\theta$这个桥梁，即$X \rightarrow \theta \rightarrow \tilde{x}$，上式可以变换如下其中$p(\theta | X)$就是贝叶斯估计求出来的后验小结频率派和贝叶斯派分别给出了一系列的机器学习算法频率派的观点导出了一系列的统计机器学习算法，而贝叶斯派导出了概率图理论success@频率派主要对应的问题是优化问题 频率派主要对应的问题是优化问题，通常的步骤是定义模型定义loss function优化算法贝叶斯派主要是求积分的问题条件概率与似然函数在极大似然估计相关博文中，出现频率最高的就是这个公式:公式的输入分别为参数$\theta$以及结果$x$根据$\theta$和$x$的已知或者末知的情况，该公式有两个不同的意义:当$\theta$是已知的并且保持不变，$x$是变量时，该公式描述的是在参数确定的情况下，某一事件(结果)$x$出现的概率，是概率函数当$\theta$是变量，$x$是已知的并且保持不变，该公式描述的是事件(结果)在不同$\theta$下出现的概率，是似然函数在后面极大似然估计中，用到的是就是似然函数在似然的意义下，$P(x \mid \theta)$还可以写成$p\left(x_{i} ; \theta\right)$，一般情况下为了特别的区分似然函数和概率函数，都会采用后面的写法高斯分布笔记整理自：Bilibili站上shuhuai008强势手推讲解的白板推导CRF系列课程，课程质量很高！B站scyw读者整理的笔记高斯分布在机器学习中占有举足轻重的作用，尤其在统计机器学习中，比如线性高斯模型线性高斯模型是一个体系，比如卡曼滤波，隐变量服从线性高斯分布，即，其中是一个高斯噪声其中，独立同分布于$N(\mu, \Sigma)$参数估计高斯分布下的MLE可以表述为这里先写出一维的高斯分布的概率密度函数高维的高斯分布密度函数为一维情况假设$p=1$，$\theta=(\mu, \sigma ^2)$，将一维高斯分布的概率密度函数带入到MLE中，可以得到求解$\mu$我们的目标是求解参数$\theta$，这里先求解$\mu$，可以得到对$\mu$求偏导可以得到这里的$\mu$是无偏估计，因为如果对$\mu$求期望，可以得到求解$\Sigma$同理对$\sigma$求偏导，可以得到这里的$\mu$实际上应该时$\mu _{MLE}$，所以继续推导可以得到上式中的$\mu$严格意义上应该是，此时是有偏估计，我们对$\sigma_{M L E}^{2}$求期望可以得到注意以上公式推导中用到了以下的公式变换其中$E[x_i^2]-\mu^2 = Var(x_i)$的变换过程如下$E[\mu {MLE}^2] - E^2[\mu {MLE}] = Var(\mu _{MLE})$的变换过程如下$Var(\mu _{MLE}) = \frac {\sigma ^2}{N}$的变换过程如下至此可以看到有偏体现在了$\frac {N-1}{N}$，其实方差被往小的方向估计了，其中因此真实的无偏估计$\hat{\sigma}$应该是多维情况高维的高斯分布高维的高斯分布密度函数为其中$x \in \mathbb{R}^p$，是一个$p$维随机变量，$\mu$也是$p$维变量，$\Sigma$是$p \times p$维的矩阵，为协方差矩阵一般情况下$\Sigma$是半正定的，并且是对称的，这里假设$\Sigma$是正定的，即$\lambda \gt 0$，方便后面叙述这里$(x-\mu)^{T} \Sigma^{-1}(x-\mu)$计算出来的结果是一个数，可以看作$x$和$\mu$的马氏距离马氏距离例子设此时马氏距离为当$\Sigma=I$为单位矩阵时，此时马氏距离就变成了欧氏距离，即协方差矩阵计算对于对称的协方差矩阵$\Sigma$可进行特征值分解其中$U = (u{1},u{2},\cdots,u_{p})$，开始推导同理可以推得二次型推导接下来对二次型$\Delta=(x-\mu)^{T}\Sigma^{-1}(x-\mu)$进行推导令所以有那么这里的马氏距离怎么理解呢，这里先令$p=2$进行分析，此时这里的1是我们给定的，此时表示的一个椭圆图(1)是$(x-\mu)^{T} \Sigma^{-1}(x-\mu)=r$不同值的情况，图(1)是三维下的示例图，z轴取不同值表示在xy轴上的投影，也就是图(1)的情况如果特征值分解$\Sigma = U\Lambda U^{T}$中的$\Lambda=c$，等于一个常量时，就是图(3)的圆，当$\Sigma$是一个对角矩阵时，椭圆就变成了图(3)中的椭圆，即轴平行于xy轴局限性参数多$\Sigma_{p \times p}$的参数的个数是$p^2$，由于是对称的，所以参数的个数减少约一半，变为了即参数个数为$O(p^2)$级别因此需要对协方差矩阵做简化协方差矩阵为对角矩阵时，此时就不需要做特征值分解此时的椭圆轴平行于原来的轴对角矩阵，并对角元素都相等此时的椭圆轴不仅平行于原来的轴，还变成了标准的圆，这种情况被称为各向同性分布是单峰的单个高斯分布是单峰的，对有多个峰的数据分布不能得到好的结果，解决方案：高斯混合GMM模型</content>
	</entry>
	<entry>
		<title>机器学习_概率论(1)</title>
		<url>/article/8271.html</url>
		<content>写在前面，本系列主要是对下面这本书做的学习笔记常用数学符号的 LaTeX 表示方法Markdown 常用数学符号和公式随机事件与概率概率论同样在机器学习和深度学习中有至关重要的作用。如果将机器学习算法的输入数据和输出数据看作随机变量，则可用概率论的方法对数据进行计算，以此对不确定性进行建模使用概率模型，可以输出概率值而非确定性的值，这对某些应用是至关重要的对于某些应用问题，需要对变量之间的概率依赖关系进行建模，也需要概率论的技术，概率图模型是典型代表随机数生成算法，即采样算法，需要以概率论作为理论指导某些随机算法，如蒙特卡洛算法、遗传算法，同样需要以概率论作为理论或实现依据。随机事件和概率是概率论中基本的概念，也是理解随机变量的基础随机事件概率随机事件是可能发生也可能不发生的事件，这种事件每次出现的结果具有不确定性例如，天气可能是晴天、雨天、阴天；考试分数可能为0与100之间的整数；掷骰子，1到6这几种点数都可能出现以集合论作为工具，给出随机事件的定义对于一个随机试验，其所有可能结果组成的集合称为样本空间记为$\Omega$随机试验可能的结果称为样本点，记为$\omega$，它是样本空间$\Omega$中的元素对于天气，样本空间为，每种天气均为一个样本点对于考试成绩，样本空间为，每个分数均为一个样本点对于掷骰子，样本空间为离散事件和连续事件样本空间可以是有限集，也可以是无限集对于无限的样本空间，可以是可数集(离散的)，也可以是不可数集(连续的)有限样本空间与无限可数样本空间中定义的随机事件称为离散型无限可数样本空间: 抛一枚硬币，如果令$n$为第一次出现正面时所试验的次数，则其取值为$[1,+\infty)$内的整数这种情况的样本空间是无限可数集，如果记事件$A_{n}$为直到扔到第$n$次才第一次出现正面朝上，则这样的事件有无穷多个无限不可数样本空间: 在区间$[0,1]$内随机扔一个点，这个点的取值为此区间内所有实数，是无限不可数集此时, 我们无法将样本空间中所有的样本点列出样本空间$\Omega$ 元素构成的集合称为随机事件，通常用大写斜体字母表示，如记为$A$显然$\Omega$也是随机本件，它一定会发生，称为必然事件；空集$\varnothing$则不可能发生，称为不可能事件衡量随机事件的可能性随机事件发生的可能性用概率进行度量，随机事件$A$发生的概率记为$p(A)$，表示此事件发生的可能性，其值满足概率值越大则事件越可能发生。一般情况下，假设样本空间中每个样本点发生的概率是相等的(称为等概率假设)因此事件$A$发生的概率是该集合的基数与整个样本空间基数的比值:根据定义，所有单个样本点构成的随机事件的概率之和为其中为样本空间中所有单个样本点构成的随机事件对于有限样本空间中的随机事件，可直接根据集合的基数计算的概率值抛硬币问题: 记正面朝上为事件$A$，反面朝上为事件$B$，则有$p(A)=p(B)=\frac{1}{2}$表示正面朝上和反面朝上的概率是相等的掷骰子问题: 记事件$A$为出现的点数为 1 , 则有$p(A)=\frac{1}{6}$1至6点出现的概率相等，均为$\frac{1}{6}$ 显然，不可能事件发生的概率为0；必然事件发生的概率为1两个随机事件对应集合的交运算与并运算，可以定义两个随机事件同时发生的概率，以及两个随机事件至少有一个发生的概率🍁两个随机事件$A$和$B$同时发生即为它们的交集，记为$A \cap B$，其概率为以掷骰子为例，记$A$为出现的点数为奇数，$B$为出现的点数不超过3，则有两个事件同时发生的概率也可以简记为$p(A, B)$ 可以将两个事件同时发生的概率推广到多个事件如果两个随机事件满足$A \cap B=\varnothing$则称为互不相容事件，即两个事件不可能同时发生，因此互不相容事件同时发生的概率为0🍒两个随机事件$A$和$B$至少有一个发生即为它们的并集，记为$A \cup B$称为加法公式，两个集合的并集元素数等于它们元素数之和减掉它们重复的部分因为重复的部分被算了两次，所以有考虑拼骰子问题，定义事件$A$为点数大于或等于2，定义事件$B$为点数小于或等于4因此有$p(A \cup B)=\frac{6}{6}=1$如果用加法公式进行计算，则有如果两个事件$A$和$B$是互不相容的，则加法公式变为这一结论可以推广到多个随机事件的情况完备事件组完备事件组是对样本空间的一个划分，显然，对于完备事件组，有考虑集合的补运算，对应于对立事件，事件$A$的补集称为它的对立事件，记为$\bar{A}$，即$A$不发生，显然有对于掷羖子问题，如果记$A$为出现的点数为偶数，则其对立事件为出现的点数为奇数，因为点数不是偶数就是奇数考虑无限可数的样本空间: 做一个试验，每次成功的概率为$p$，假设各次试验之间无关，事件$A_{n}$定义为试验$n$次才取得第一次成功，即前面$n-1$次都失败，第$n$次成功，显然，整个样本空间为可以得到概率值其中$(1-p)^{n-1}$是前面$n-1$次都失败的概率，$p$是第$n$次时成功的概率，显然有满足所有基本事件的概率之和为1的要求，这里利用了下面的幂级数求和结果几何型概率前面介绍的概率均针对有限的或无限可数的样本空间，下面介绍无限不可数样本空间概率的计算，称为几何型概率几何型概率定义在无限不可数集上，根据积分值(也称为测度值，如长度、面积、体积)定义事件$A$发生的概率为区域$A$的测度值与$\Omega$测度值的比值，即其中$s(A)$为集合$A$的测度这里同样假设落在区域内任意点处的可能性相等，同时保证整个样本空间的概率为1一维几何在$[0,1]$区间内随机扔一个点，计算该点落在$[0,0.7]$内的概率假设点的坐标为$x$，落在区间$[0,0.7]$内，即$0 \leqslant x \leqslant 0.7$，由于落在区间中任意点处的可能性相等，因此概率值为是短线段与长线段的长度之比二维几何推广到二维的情况，可用面积计算概率在单位正方形$0 \leqslant x, y \leqslant 1$内部随机地扔一个点，计算该点落在区域$x \leqslant 0.2, y \leqslant 0.3$内的概率由于落在任意点处的可能性相等，因此是两个矩形区域的面积之比考虑一个更复杂的例子，在圆周上随机选择两个点，计算这两个点与圆心的连线之间沿着逆时针方向的夹角是锐角的概率假设点落在圆周上任意位置处的可能性相等如图所示，假设圆周上的两个点 $A 、 B$ 与圆心的连线和$x$轴正半轴的夹角(按照逆时针方向计算)分别为与显然有，这里采用弧度作为计量单位两个点与圆心的连线之间的夹角为，夹角为锐角，即，这等价于夹角为锐角的区域为直线之下、直线之上的区域，因此是夹在这两条直线之间的区域，两点之间逆时针方向夹角为锐角的概率为三维几何对于三维的情况，可用体积值来计算概率值，对于更高维的情况，则借助多重积分的值进行计算条件概率条件概率主要描述的是多个随机件的概率关系条件概率定义对于随机事件$A$和$B$，在$A$发生的条件下$B$发生的概率称为(条件概率)，记为$p(B \mid A)$如果事件$A$的概率大于0，则条件概率可按下式计算根据定义，条件概率是$A$和$B$同时发生的概率与$A$发生的概率的比值条件概率计算的例子下面用一个例子说明条件概率的计算对于掷骰子问题，假设事件$A$为点数是奇数，事件$B$为点数小于或等于3，则二者的条件概率为类似地，有对条件概率公式进行变形，可以得到乘法公式两个以上的随机事件下的条件概率将条件概率推广到两个以上的随机事件，对于两组随机事件与, 它们的条件概率定义为将乘法公式推广到 3 个随机事件，可以得到需要注意的是，这种分解的顺序不是唯一的，推广到$n$个随机事件，有随机事件的独立性如果$p(B \mid A)=p(B)$，或$p(A \mid B)=p(A)$，则称随机事件$A$和$B$独立随机事件独立意味着一个事件是否发生并不影响另外一个事件如果随机事件$A$和$B$独立，根据式 (5.4), 有将上面的定义进行推广，如果$n$个随机事件$A_{i}, i=1, \cdots, n$ 相互独立，则对所有可能的组合$1 \leqslant i&lt;j&lt;k&lt;\cdots \leqslant n$，都有全概率公式定义如果随机事件是一个完备事件组，且，是任意随机事件，则有称为全概率公式，借助于条件概率，全概率公式将对复杂事件的概率计算问题转化为在不同情况下发生的简单事件的概率的求和问题例子箱子红球白球箱子164箱子255箱子328先随机抽取一个箱子，然后从中随机抽取一个球，计算抽中红球的概率令$A_{i}$为抽中第$i$个箱子，$B$为抽中红球根据全概率公式，有贝叶斯公式贝叶斯公式由数学家贝叶斯(Bayes)提出，它阐明了随机事件之间的因果概率关系，根据条件概率的定义，有变形可得贝叶斯公式先后验和似然函数它描述了先验概率和后验概率之间的关系，如果事件$A$是因，事件件$B$是果先验概率: 称$p(A)$为先验概率(Prior Probability)，意为事先已经知道其值后验概率: $p(A \mid B)$ 称为后验概率(Posterior Probability)，意为事后才知道其值似然函数: 条件概率$p(B \mid A)$则称为似然函数先验概率是根据以往经验和分析得到的概率，在随机事件发生之前已经知道，是原因发生的概率后验概率是根据结果信息所计算出的导致该结果的原因所出现的概率后验概率用于在事情已经发生的条件下，分析使得这件事情发生的原因概率根据贝叶斯公式可以实现这种因果推理，这在机器学习中是常用的似然函数与概率给定联合样本值$x$关于$\theta$的函数其中$f(x|\theta)$是密度函数，表示给定$\theta$下的联合概率密度函数，而似然函数的关于$\theta$的函数，密度函数是关于$x$的概率表达了在给定参数$\theta$下$X=x$的可能性，而似然表示的是给定样本$X=x$时，参数的可能性如果下式成立，则表示参数$\theta_1$下随机变量$X$取到$x$值的可能性大于$\theta_2$ 一个形象的似然例子是，NBA总决赛时，已知球进了，问谁去投球的可能性大全概率公式与贝叶斯公式如果事件构成一个完备事件组，且，根据全概率公式与贝叶斯公式，可以得到例子箱子红球黑球箱子155箱子273箱子391首先随机地抽取一个箱子，然后从中随机抽取一个球，如果抽中的是红球，计算这个球来自每个箱子的概率令$A_{i}, i=1,2,3$表示抽中第$i$个箱子，$B$表示抽中红球则这个红球来自第1个箱子的概率为来自第2个箱子的概率为来自第3个箱子的概率为条件独立定义将随机事件的独立性与条件概率相结合，可以得到条件独立的概念如果随机事件$A, B, C$满足$p(A \mid B, C)=p(A \mid C)$，则称$A$和$B$关于事件$C$条件独立直观含义是在$C$发生的情况下，$B$是否发生并不影响$A$，它们之间相互独立，这意味着事件$C$的发生使得$A$和$B$相互独立$A$和$B$关于$C$条件独立可以记为$A \perp B \mid C$，条件独立的概念在概率图模型中被广泛使用随机变量普通的变量只允许取值可变，随机变量(Random Variable)是取值可变并且取每个值都有一 个概率的变量从另外一个角度来看，随机变量是用于表示随机试验结果的变量随机变量通常用大写斜体字母表示，如$X$， 随机变量的取值一般用小写斜体字母表示，如$x_{i}$，随机变量可分为离散型和连续型两种离散型: 取值集合为有限集或者无限可数集，对应离散型随机事件连续型: 取值集合为无限不可数集, 对应几何型随机事件随机变量相关概念频率分布直方图下图左边表示的是频率分布直方图，$x$轴是组距，而$y$轴是频数而右边表示的是也是频率分布直方图，只不过$y$轴是频率/组距，于是此时每个小长方形的面积是频率右图中所有小长方形的面积之和记为$S=1$概率密度函数大学的概率论就是引入了微积分，而微积分很重要的一个就是极限，当我们把这里的组距变得无限小，就会变成图中蓝色的线，此时蓝线与$x$轴围成的面积始终等于1，并将这条蓝线命名为概率密度函数，记为$f(x)$$X$在任意区间$(a,b]$上的概率可以表示为如下公式，代表的是$a$到$b$区间的面积累积分布函数(分布函数)微积分中另一个知识是积分，当我们对下面曲线求积分时，所得到的面积记为$F(x)=\int _{-\infty}^{x} f(t) dt$，也就是累积分布函数其中积分上限$x$为自定义的值，如下图中的橘色的线，此时$F(x)$对应的是下图中阴影部分的面积连续型与离散型随机变量连续型随机变量的一维情况如上面所描述，是一条线而离散型随机变量使用表来表示，如掷骰子出现的点数即为离散型随机变量，取值集合为1和6之间的整数X123456p1/61/61/61/61/61/6此时这个表可以叫做分布表，这个表的前身是分布率(概率分布、概率函数)，分布率每个取值记为$p(X=x)=p_i$分布表对应的是连续型中的概率密度函数，分布表也可以叫做概率质量函数，对其做累积，其累积分布函数如表所示随机变量取值123456概率质量函数值1/62/63/64/65/61累积分布函数也称为分布函数，是概率质量函数的累加，定义为二维随机变量对于二维的情况，从原来的一根线变为了一个面，如下图所示，表示的是联合概率密度，记为$f(x,y)$对这个面切两刀，求剩余的体积，就是联合分布函数，记为$F(x,y) = \int {- \infty}^{x}{\int {- \infty}^{y} {f(u,v)dudv}}$，或者记为$p(X \leq x , Y \leq y)$对于二维离散型随机变量，给出的是联合分布表，前身是联合分布律，联合分布率每个取值记为$p(X=xi,Y=y_j)=p{ij}$X/Y56X的边缘分布表11/41/83/831/81/25/8Y的边缘分布表3/85/81 边缘概率密度(一维下没有这个概念)对于微积分而言，有个知识点是偏导数，在这里可以引申出边缘概率，即在计算是把另外的变量看作常数记为$fX(x) = \int {- \infty}^{\infty} f(x,y) dy$，这里的$X$表示的是$X$的边缘概率边缘分布函数对应于二维的联合分布函数，这里连续型还有一个概念叫边缘分布函数，记为$F_X(x)=p(X \leq x, Y \leq +\infty) = p(X \leq x)$ 对于联合分布表，边缘化之后就是叫做边缘分布表，上表的$X$和$Y$的边缘分布表如表所示离散型随机变量离散型随机变量的取值集合是离散集合，为有限集或无限可数集，可以将所有取值列举出来例如，掷骰子出现的点数即为离散型随机变量，取值集合为1和6之间的整数概率质量函数(Probability Mass Function, PMF)描述离散型随机变量取值概率的是概率质量函数，函数由随机变量取每个值的概率排列组成，可以将简记为 $p\left(x_{i}\right)$ 这里的质量与物理学中的质量相对应，可看作是一些有质量的质点，概率质量函数值对应这些点的质量概率质量函数必须满足以下约束条件下表是一个离散型随机变量的概率质量函数，其取值集合为${1,2,3,4}$随机变量取值概率质量函数值10.120.530.240.2离散型随机变量的取值可能为无限可数集，此时要保证下面的级数收敛，并且其值为1累积分布函数(Cumulative Distribution Function, CDF)累积分布函数也称为分布函数，是概率质量函数的累加，定义为上表所示的随机变量，其累积分布函数如表所示随机变量取值累积分布函数值10.120.630.841.0连续型随机变量几何型随机事件对应的是连续型随机变量，连续型随机变量的取值集合为无限不可数集一般为实数轴上的一个或多个区间，或者是整个实数集$\mathbb{R}$例如，我们要观测每个时间点的温度，是一个连续值，为连续型随机变量考虑计算一维几何型随机事件概率的例子，随机点落在$[0, x]$区间内的概率就是$x$，这是$X \leqslant x$这一随机事件的概率其中$X$是一个连续型随机变量，是随机点的一维坐标，其允许的取值范围为$[0,1]$累积分布函数对上面的函数进行扩充，使得$X$的取值范围为整个实数集$\mathbb{R}$，可以得到如下的函数该函数称为累积分布函数概率密度函数对累积分布函数进行求导，即可得到概率密度函数， 表示连续型随机变量在每一个取值点处的概率密度值除去0和1这两点，上面的累积分布函数是可导的，其导数为此函数在区间$[0,1]$内所有点处的取值相等，这意味着点$x$落在$[0,1]$内所有点处的可能性相等概率密度函数与累积分布函数的定义概率密度函数(Probability Density Function, PDF)定义了连续型随机变量的概率分布其函数值表示随机变量取该值的可能性(注意，不是概率)概率密度函数必须满足如下约束条件这可以看作是离散型随机变量的推广，积分值为1对应取各个值的概率之和为1连续型随机变量落在某一点处的概率值为0落在某一区间内的概率值为概率密度函数在该区间内的定积分其中$F(x)$是$f(x)$的一个原函数，也称为分布函数，近似地有其中$\xi$是$[x, x+\Delta x]$内的一个点概率密度函数中的密度可类比物理学中的密度，概率质量函数在每一点处的值为随机变量取该值的概率而概率密度函数在每一点处的值并不是概率值，只有区间上的积分值才是随机变量落入此区间的概率随机变量$X$服从概率分布$f(x)$，一般可以简记为对于连续型随机变量，分布函数是概率密度函数的变上限积分，定义为显然这是增函数，分布函数的意义是随机变量$X \leqslant y$的概率根据分布函数的定义有根据定义，分布函数单调递增考虑指数分布，其概率密度函数为其中$\lambda&gt;0$ ，下面计算它的分布函数，如果$x&lt;0$，则有如果$x \geqslant 0$，则有因此其分布函数为下面以logistic回归为例说明概率密度函数与分布函数在机器学习中的应用，logistic回归的分布函数为logistic函数，定义为其形状如图所示，该函数单调递增，且有满足分布函数的定义要求，其概率密度函数为$F(x)(1-F(x))$，概率密度函数的图像如图所示数学期望定义数学期望(Mathematical Expectation)是平均值的推广，是加权平均值的抽象，对于随机变量，是其在概率意义下的均值普通的均值没有考虑权重或概率，对于$n$个变量，它们的算术平均值为这可看作变量取每个值的可能性相等，或者每个取值的权重相等但对于很多应用，变量取每个值有不同的概率，因此这种简单的均值无法刻画出变量的性质表为买彩票时各种奖的中多金额以及对应的概率值，中奖金额可看作离散型随机变量如果要计算买一张彩票的平均中奖金额，直接用各种奖的中奖金额计算平均值显然是不合理的正确的做法是考虑中各种奖的概率，以其作为权重来计算均值这种计算方式就是求数学期望离散型随机变量的数学期望对于离散型随机变量数学期望定义为数学期望也可以写成或，表示用概率分布对随机变量$X$计算数学期望如果式$E[X]$的级数收敛，则称数学期望存在连续型随机变量的数学期望对于连续型随机变量，数学期望通过定积分定义假设连续型随机变量$X$的概率密度函数是$f(x)$，它的数学期望为根据定积分的定义，连续型是离散型数学期望的极限情况对于连续型随机变量，其数学期望是一个泛函根据定义，常数的数学期望为其自身，即$E[c]=c$根据数学期望的定义, 下面的公式成立其中$k$为常数如果$g(X)$是随机变量$X$的函数，则由它定义的随机变量的数学期望为一般简记为$E_{X \sim p(x)}[g(X)]$对于连续型椭机变量$X$，其函数$g(X)$的数学期望为根据这种定义, 下面的公式成立以下表中的随机变量为例，$X^{2}$的数学期望为随机变量取值概率质量函数值10.120.530.240.2下面以指数分布为例计算连续型概率分布的数学期望，如果$X$服从参数为$\lambda$的指数分布，则其数学期望为物理意义如果将随机变量看作物体各点在空间中的坐标，概率密度函数是其在空间各点处的密度，则数学期望的物理意义是物体的质心方差与标准差定义方差(Variance, var)反映随机变量取值的波动程度，是随机变量与其数学期望差值平差的数学期望方差也可记为$D[X]$，如果不使用平方，则随机变量所有值与其数学期望的差值之和为0离散型随机变量的方差对于离散随机变量，方差定义为对于下表中的随机变量，它的方差为随机变量取值概率质量函数值10.120.530.240.2连续型随机变量的方差对于连续型随机变量，方差同样通过积分定义其中，$f(x)$为概率密度函数，根据定义，方差是非负的计算指数分布的方差，其数学期望为$1 / \lambda$，其方差为方差反映了随机变量偏离均值的程度，方差越小，随机变量的变化幅度越小，反之则越大标准差(Standard Deviation)标准差定义为方差的平方根根据方差的定义，下面公式成立实际计算方差时经常采用此式，在机器学习中被广泛使用根据方差的定义，有$\operatorname{var}[k X]=k^{2} \operatorname{var}[X]$，这是因为这意味着将随机变量的取值扩大$k$倍，其方差扩大$k^{2}$倍物理意义如果将随机变量看作物体各点在空间中的坐标，概率密度函数是其在空间各点处的密度，则方差的物理意义是物体的转动惯量Jensen不等式介绍数学期望的一个重要不等式，Jensen不等式(Jensen’s Inequality)，它在机器学习某些算法的推导中起着至关重要的作用回顾凸函数的定义，如果$f(x)$是一个凸函数，$0 \leqslant \theta \leqslant 1$，则有将上式从两个点推广到$m$个点，如果可以得到，对于有如果将$x$看作是一个随机变量，是其概率分布，则有从而得到Jensen不等式对于凹函数，上面的不等式反号可以根据定义式用归纳法证明式多点情况下的公式成立首先考虑$m=2$的情况，且，即，根据凸函数的定义，对于有假设$m=n$时不等式成立，则当$m=n+1$时有上面第2步利用了凸函数的定义，第3步成立是因为根据归纳法的假设，$m=n$时不等式成立如果$f(x)$是严格凸函数且$x$不是常数，则有这同样可以用归纳法证明，与前面的证明过程类似如果$f(x)$是严格凸函数，当且仅当随机变量$x$是常数时，不等式取等号下面给出证明，如果随机变量$x$是常数，则有因此以及因此有接下来证明如果不等式取等号，则有可用反证法证明，如果，由于是严格凸函数，根据前面的结论有Jensen不等式可以推广到随机向量的情况，在后面将利用此不等式推导出求解含有隐变量的最大似然估计问题的EM算法常用概率分布离散型概率分布包括均匀分布、伯努利分布、 二项分布、多项分布、几何分布连续型概率分布包括均匀分布、正态分布, 以及$t$分布均匀分布定义对于离散型随机变量$X$，如果服从均匀分布(Uniform Distribution)，则其取每个值的概率相等，即对于连续型随机变量$X$，如果服从区间$[a, b]$上的均匀分布，则其概率密度函数为分段常数函数，定义为在允许取值的区间内，概率密度函数值相等，等于区间长度的倒数下面计算它的分布函数，如果$x&lt;a$，则有如果$a \leqslant x \leqslant b$，则有如果$x&gt;b$，则有因此，其分布函数为随机变量$X$服从区间$[a, b]$上的均匀分布，简记为$X \sim U(a, b)$例子下面计算服从区间$[a, b]$上均匀分布的随机变量的数学期望和方差，根据数学期望的定义因此，均匀分布的均值为区间的中点，根据方差的定义均匀分布的方差与区间长度的平方成正比均匀分布是最简单的概率分布, 在程序设计中各种概率分布的随机数一般通过均匀分布随机 数构造。算法生成的随机数不是真随机而是伪随机。通常情况下, 基本的随机数生成算法生成的 是某一区间 $\left[0, n_{\max }\right]$ 上均匀分布的随机整数。对均匀分布随机整数进行变换，可以将其变为另外一个区间上的均匀分布随机数例如，对上均匀分布随机整数$x$除以其最大值可以将其变换为区间$[0,1]$上的连续型均匀随机数如果要生成某一区间上均匀分布的整数, 则可借助于取余运算实现如果要生成$[0, k]$上均匀分布的整数，则可以将$x$除以$k+1$取余数这里假设$k&lt;n_{\max }$，显然，该余数在$[0, k]$上均匀分布伯努利分布定义服从伯努利分布(Bernoulli Distribution)的随机变量$X$的取值为0或1两种情况，该分布也称为$0-1$分布取值为1的概率为$p$，取值为0的概率为$1-p$，其中$p$为$(0,1)$内的实数，是此概率分布的参数，即概率质量函数可统一写成其中$x \in{0,1}$，如果$x=0$，则有如果$x=1$，则有如果$p=\frac{1}{2}$，那么此时的伯努利分布为离散型均匀分布，为方便起见，通常将$1-p$简记为$q$如果将随机变量取值为1看作试验成功，取值为0看作试验失败，则伯努利分布是描述试验结果的一种概率分布随机变量$X$服从参数为$p$的伯努利分布简记为机器学习中的二分类问题可以用伯努利分布描述，logistic回归拟合的是这种分布根据定义，伯努利分布的数学期望为方差为可以看到，当$p=\frac{1}{2}$时，方差有极大值根据服从均匀分布的随机数可以生成服从伯努利分布的随机数，对于伯努利分布将$[0,1]$区间划分成两个子区间，第1个子区间的长度为$p$，第二个子区间的长度为$1-p$如果有一个$[0,1]$上均匀分布的随机数，则它落在第1个子区间内的概率即为$p$，落在第2个子区间内的概率即为$1-p$因此，可以先生成$[0,1]$上均匀分布的随机数$\zeta$，然后判定其所在的子区间，如果它落在第1个子区间，即则输出1，否则输出0算法输出的值即服从伯努利分布，深度学习中的Dropout机制、稀疏自动编码器都使用了伯努利分布二项分布定义$n$个独立同分布的伯努利分布随机变量之和服从$n$重伯努利分布，也称为二项分布(Binomial Distribution)此时，随机变量$X$表示$n$次试验中有$k$次成功的概率，其概率质量函数为其中$\mathrm{C}_{n}^{k}$为组合数，是$n$个数中有$k$个取值是1的所有可能情况数，$p$是每次试验时成功的概率，取值范围为$(0,1)$二项分布的概率$p(X=k)$也是对$(p+(1-p))^{n}$进行二项式展开时第$k+1$个展开项，因此而得名在$n$次试验中，$k$次成功的概率为$p^{k}$，$n-k$次失败的概率为$(1-p)^{n-k}$，而$k$次成功可以是$n$次试验中的任意$k$次，因此有组合系数$\mathrm{C}_{n}^{k}$ 随机变量$X$服从参数为$n, p$的二项分布，可以简记为下图是一个二项分布的图像，其中$n=5, p=0.5$，横轴为随机变量的取值，纵轴为取各离散值的概率此时二项分布的展开项是对称的，因此其概率质量函数的图像是对称的从图可以看出，二项分布在中间位置的概率值更大，在两端处的概率值更小，随着$n$的值增大，它将以正态分布为极限分布计算二项分布的数学期望各种均值和方差的正态分布的概率密度函数下面计算二项分布的数学期望，根据定义有上式第6步进行了换元，令$a=k-1, b=n-1, n-k=b-a$，方差为而上式第2步利用了，因此由于二项分布是多个相互独立同分布的伯努利分布之和，因此其均值与方差和伯努利分布刚好为$n$倍的关系多项分布定义多项分布(Multinomial Distribution)是伯努利分布的推广，随机变量$X$的取值有$k$种情况假设取值为${1, \cdots, k}$内的整数，则有对$X$的取值进行One-Hot向量编码，由$k$个分量组成，如果$X$取值为$i$，则第$i$个分量为1，其余分量均为0假设One-Hot编码结果为，则概率质量函数可以统一写成如果$p_{i}=\frac{1}{k}, i=1, \cdots, k$，则多项分布为离散型均匀分布，多项分布对应多分类问题，softmax回归拟合的就是这种分布根据服从均匀分布的随机数可以生成服从多项分布的随机数，其方法与生成伯努利分布随机数相同，对于多项分布将$[0,1]$区间划分成$k$个子区间，第$i$个子区间的长度为$p_{i}$如果有一个$[0,1]$上均匀分布的随机数，则它落在第$i$个子区间内的概率为$p_{i}$因此，可以先生成$[0,1]$上均匀分布的随机数$\zeta$，然后判定其所在的子区间，如果它落在第$i$子区间，即则输出$i$，算法输出的数即服从多项分布对多个样本按照权重抽样借助于多项分布随机数可以实现对多个样本按照权重抽样，有$n$个样本，它们对应的归一化权重为现在要对这些数进行抽样，保证抽中每个样本的概率为$w_{i}$，借助于内多项分布的随机数，即可实现此功能在粒子滤波器、遗传算法中均有这样的需求几何分布定义前面已经说明了无限可数样本空间的概率值，几何分布是这种取值为无限种可能的离散型概率分布做一个试验，每次成功的概率为$p$，假设各次试验之间相互独立，事件$A_{n}$定义为试验$n$次才取得第一次成功，与其对应，定义随机变量$X$表示第一次取得成功所需要的试验次数，其概率为如果令$q=1-p$，则上式可以写成这就是几何分布(Geometric Distribution)，几何分布因其分布函数为几何级数而得名随机变量$X$服从参数为$p$的几何分布，可以简记为$X \sim \mathrm{Geo}(p)$分布函数计算几何分布的分布函数，根据定义，有几何分布的数学期望为上式利用了幂级数求导公式，当$0&lt;q&lt;1$时，幂级数$\sum_{n=0}^{+\infty} q^{n}$收敛于$\frac{1}{1-q}$根据方差的定义，有而同样，这里利用了幂级数求导公式，因此几何分布的方差为几何分布的随机数也可以借助于均匀分布的随机数生成，方法与伯努利分布类似算法循环进行尝试，每次生成一个伯努利分布随机数，如果遇到1，则结束循环，返回尝试的次数正态分布定义正态分布(Normal Distribution)也称为高斯分布(Gaussian Distribution)，它的概率密度函数为其中$\mu$和$\sigma^{2}$分别为均值和方差该函数在$(-\infty,+\infty)$上的积分为1，令$t=\frac{x-\mu}{\sqrt{2} \sigma}$，则有显然，概率密度函数关于数学期望$x=\mu$对称，且在该点处有极大值在远离数学期望时，概率密度函数的值单调递减具体地，在$(-\infty, \mu)$内单调递增，在$(\mu,+\infty)$内单调递减，该函数的极限为现实世界中的很多数据，例如人的身高、体重、寿命等，近似服从正态分布随机变量$X$服从均值为$\mu$ 、方差为$\sigma^{2}$的正态分布，简记为$X \sim N\left(\mu, \sigma^{2}\right)$标准正态分布如果正态分布的均值为0，方差为1，则称为标准正态分布，此时的概率密度函数为该函数是一个偶函数，下图红色为标准正态分布的概率密度函数图像，其形状像钟，因此也称为钟形分布正态分布的均值决定了其概率密度函数峰值出现的位置，方差则决定了曲线的宽和窄，方差越大，曲线越宽，反之则越窄正态分布$N\left(\mu, \sigma^{2}\right)$的分布函数为由于$\mathrm{e}^{-x^{2}}$的不定积分不是初等函数，因此该函数无解析表达式假设随机变量$Z$服从标准正态分布$N(0,1)$，则随机变量$X=\sigma Z+\mu$服从正态分布$N\left(\mu, \sigma^{2}\right)$相反，如果随机变量$X$服从正态分布$N\left(\mu, \sigma^{2}\right)$，则随机变量$Z=\frac{X-\mu}{\sigma}$服从标准正态分布$N(0,1)$置信区间正态分布的$k-\sigma$置信区间定义为$[\mu-k \sigma, \mu+k \sigma]$，其中$k$为一个正整数随机变量落入该区间的概率为随机变量在$\sigma, 2 \sigma, 3 \sigma$区间内的概率分别为数学期望下面计算正态分布的数学期望，使用换元法，令则有根据数学期望的定义，有上式第5步成立是因为$z \mathrm{e}^{-\frac{x^{2}}{2}}$是奇函数，它在$(-\infty,+\infty)$上的积分为0第6步利用了下面的结论这可以用式换元法证明方差下面计算方差，同样令$z=\frac{x-\mu}{\sigma}$，则有上式第5步利用了分部积分法，第6步成立是因为正态分布的概率密度函数由均值和方差决定，这是非常好的一个性质，通过控制这两个参数，即可接制均值和方差中心极限定理指出正态分布是某些概率分布的极限分布正态分布具有$(-\infty,+\infty)$的支撑区间，且在所有定义于此区间内的连续型概率分布中，正态分布的熵最大，这些优良的性质使得正态分布在机器学习中得到了大量的使用多个正态分布的加权组合可形成高斯混合模型，是混合模型的一种，它可以逼近任意连续型t分布定义$t$分布其概率密度函数为其中$\Gamma$为伽马函数，$\nu$为自由度，是一个正整数显然，当$x=0$时，概率密度函数有极大值且函数是偶函数，伽马函数是阶乘的推广，将其从正整数推广到正实数，通过积分定义此函数的定义域为$(0,+\infty)$且在该定义域内连续根据定义，有伽马函数满足与阶乘相同的递推关系这可以通过分部积分验证根据这两个结果，对于$n \in \mathbb{N}$，有$t$分布概率密度函数的形状与正态分布类似，如下图所示啧啧啧其分布函数为其中为超几何函数(Hypergeometric Function)，定义为上图显示了各种自由度取值时的$t$分布概率密度函数曲线以及标准正态分布的概率密度函数曲线$t$分布具有长尾的特点，在远离中心点的位置依然有较大的概率密度函数值，且自由度越小，长尾性越强随着自由度的增加，它以标准正态分布为极限分布考虑$\nu \rightarrow+\infty$时的极限情况，有上面第2步进行了换元，令$t=\frac{1}{\nu}$，第4步利用了等价无穷小，同时因此$\nu \rightarrow+\infty$时$t$分布的极限分布是标准正态分布$t$分布具有长尾的特点，远离概率密度函数中心点的位置仍然有较大的概率密度函数值，因此更易于产生远离均值的样本，它在机器学习中的典型应用是$t$-SNE降维算法</content>
	</entry>
	<entry>
		<title>隐马尔可夫模型HMM</title>
		<url>/article/4987.html</url>
		<content>隐马尔可夫HMM定义一站式解决：隐马尔可夫模型（HMM）全过程推导及实现关于HMM、MEMM、CRF的一些知识整理整理图解隐马尔可夫模型HMM_通俗易懂隐马尔可夫模型(hidden Markov model, HMM)是可用于标注问题的统计学习模型描述由隐藏的马尔可夫链随机生成观测序列的过程，属于生成模型隐藏的马尔可夫链随机生成的状态的序列，称为状态序列(state sequence)每个状态生成一个观测而由此产生的观测的随机序列，称为观测序列(observation sequence)序列的每一个位置又可以看作是一个时刻这里的$Z$就是状态序列，$x$是观测序列，设Q是所有可能的状态的集合，V是所有可能的观测的集合，$N$是可能的状态数，$M$是可能的观测数三要素隐马尔可夫三要素隐马尔可夫模型由初始状态概率$\pi$、状态转移矩阵$A$、观测概率矩阵(或者叫发射矩阵)$B$决定，记作其中$\pi$的维度为$N$，$A$的维度为$N \times N$，$B$的维度为$N \times M$例子🌰🌱假设有4个盒子，每个盒子里面有不同数量的红、白两种颜色的球，具体如下表盒子编号1234红球数5368白球数5742按照下面的方法抽球，产生一个球的颜色的观测序列：开始，从4个盒子里以等概率随机选取1个盒子，从这个盒子里随机抽出1个球，记录其颜色后，放回然后，从当前盒子随机转移到下一个盒子，规则如下：如果当前盒子是盒子1，那么下一盒子一定是盒子2如果当前是盒子2或3，那么分别以概率0.4和0.6转移到左边或右边的盒子如果当前是盒子4，那么各以0.5的概率停留在盒子4或转移到盒子3确定转移的盒子后，再从这个盒子里随机抽出1个球，记录其颜色，放回如此下去，重复进行5次，得到一个球的颜色的观测序列：$O＝{红,红,白,白,红}$根据所给条件，可以明确状态集合、观测集合、序列长度以及模型的三要素盒子对应状态，状态的集合是$Q＝{盒子1,盒子2,盒子3,盒子4}$，$N=4$球的颜色对应观测。观测的集合是$V＝{红,白}$，$M＝2$状态序列和观测序列长度$T＝5$则初始概率分布为状态转移概率分布为观测概率分布为 (最终盒子红白球的概率)三个假设马尔科夫性假设: $t$时刻的状态出现的概率只和$t-1$时刻的状态有关齐次马尔可夫假设(假设1)观测独立性假设(假设2)假设1和假设2在前后向算法推导上有用到观测序列生成输入：隐马尔可夫模型 $\lambda=(A ,B ,\pi )$ ，观测序列长度$T$输出：观测序列按照初始状态分布 $\pi$ 产生状态$i_1$令t $=1$按照状态$\mathrm{t}$的观测概率分布 生成 按照状态  的状态转移概率分布  产生状态 ，令 $\mathrm{t}=\mathrm{t}+1$; 如果 $\mathrm{t}&lt;\mathrm{T}$ ，转步 $(3)$ ；否则，终止概率基础联合概率联合概率指的是包含多个条件且所有条件同时成立的概率，记作边缘概率仅与单个随机变量有关的概率，记作条件概率条件概率表示在条件$Y=b$成立的情况下，$X=a$的概率，记作具有性质：在条件$Y=b$下$X$的条件分布，也是一种$X$的概率分布，穷举$X$的可取值之后，所有这些值对应的概率之和为$1$，即:🌿联合概率、边缘概率与条件概率之间的关系: 联=边*条可以理解成在$Y$选中$b$的情况下，同时$X$选中$b$的概率就是联合概率，表示两个条件同时满足🌰举个例子，假设有3个盒子，每个盒子中有白色和红色两种颜色的球，那么从第二个盒子中取出白球的概率，等价于抽到第二个盒子的概率$\times$第二个盒子中抽到白色求的概率贝叶斯公式先验概率：知道原因推结果的，P(原因)、P(结果|原因)等后验概率：根据结果推原因的，P(原因|结果)等HMM的三个问题HMM的基本问题一共有三个:概率计算：给定参数$ \lambda =( \pi , A, B) $和观测序列$ X=(x_1,x_2, \cdots ,x_T) $，计算观测序列$ X $的条件概率$ P(X|\lambda)$参数学习：给定观测序列$ X $，反推参数$\pi$、$A$、$B$解码问题：给定参数$\lambda$和观测序列$ X=(x_1,x_2, \cdots ,x_T) $，求可能性最大的$ Z=(z_1, \cdots ,z_T)$概率计算HMM模型定义🌱假设有4个盒子，每个盒子里面有不同数量的红、白两种颜色的球，具体如下表盒子编号1234红球数5368白球数5742现在从这些盒子中抽取$T$个球，每次抽取后记录颜色，再放回原盒子序列如下图所示其中$z_i$代表第$i$个隐序列值，$q_i$代表第$i$个隐状态，$x_i$代表第$i$个观测序列值，注意隐序列值指的是第几个位置，比如第几个盒子、或者第几个单词隐状态指的是第几个状态，比如红球还是白球，或者名词还是动词此时$z_t = q_i$表达的含义就是$t$时刻，抽取第$q_i$个盒子，因为有$4$个盒子，所以在这里$i={1,2,3,4}$对应取到的球的颜色可以记为$x_i$，其中$x_i \in {白色, 红色}$直接计算给定模型 $\lambda=(A, B, \pi)$ 和观测序列 ，算观测序列 $O$ 出现的 概率  ，最直接的方法是按概率公式直接计算.通过列举所有可能的长度为$T$的状态序列，求各个状态序列$I$与观测序列 的联合概率 ，然后对所有可能的状态序列求和，得到 状态序列  的概率是对固定的状态序列 ，观测序列  的概率是 $O$ 和 $I$ 同时出现的联合概率为然后, 对所有可能的状态序列 $I$ 求和, 得到观测序列 $O$ 的概率 $P(O \mid \lambda)$, 即但是，利用上述公式计算量很大，是 $O\left(T N^{T}\right)$ 阶的，这种算法不可行🐜前向概率定义前向概率$ \alpha _t (i) $，它是$t$时刻的状态以及$1, 2, \cdots , t$时刻的观测在给定参数下的联合概率也就是上图中标记的那一部分，$ \alpha _t (i) $表示的是在$t$时刻，隐状态为$q_i$的概率前向概率初值根据定义可以得到初值:其中$P(x_1|z_1=q_i, \lambda)$表达的是，给定$\lambda$和状态$z_1=q_i$的情况下，观测值是$x_1$的概率假设此时$x_1=白色$，这里可以通俗理解为$z_1$表示的第$1$个盒子，即从第一个盒子中取出白球的概率同时$ b_i(x) $表示由状态$ q_i $生成给定观测数据的概率，例如设$ t $时刻观测数据$ x_t=v_j $，有上述用到的公式为为概率基础部分的知识推导递推公式由前向概率$ \alpha _t (i) $继续推导可以得到$ \alpha _T (i) $根据这个公式，遍历$ z_T $的取值求和，可以得到$X$的边际概率其中$N$表示的是状态数量，比如盒子中只有红球和白球，即状态集合为${白球, 红球}$，此时$N=2$由前向概率推导可以得到引入变量$z_t=q_i$，注意($t+1$时刻$=$$z_t$所有状态求和)，所以有找到了$t$到$t+1$时刻的递推公式，分别看下公式的两个部分前半部分根据观测独立假设后半部分根据齐次马尔可夫假设将上述结果代入$\alpha _{t+1}(j)$可以得到条件概率例子HMM前向和后向算法在计算什么？考虑盒子和球模型$\lambda (\pi,A,B)$，状态集合$Q＝{1,2,3}$，观测集合$V＝{红,白}$观测序列 $O={ 红, 白, 红 }$输出: 观测序列 $O={ 红, 白, 红 }$ 的概率$t=1$时刻的前向概率隐藏状态为盒子1且为红色球的概率为: 隐藏状态为盒子 2 且为红色球的概率为: 隐藏状态为盒子3且为红色球的概率为: $t=2$时刻的前向概率(在前一个观测状态为红色球的基础下)隐藏状态是盒子1且为白色球的概率为 关键 : 这里可以理解成，在$t=2$的时候，且盒子为1时，它的$t=1$时刻可能是盒子1或盒子2或盒子3也就是$t=2$的路径有(盒子1-&gt;盒子1、盒子2-&gt;盒子1、盒子3-&gt;盒子1)，由公式表述就是$$\sum_{i=1}^{3} \alpha_{1}(i) a_{i 1}$$隐藏状态是盒子2且为白色球的概率为:$$\alpha_{2}(2)=\left[\sum_{i=1}^{3} \alpha_{1}(i) a_{i 2}\right] b_{2}\left(o_{2}\right)=[0.1 * 0.2+0.16 * 0.5+0.28 * 0.3] \times 0.6=0.1104$$隐藏状态是盒子 3 且为白色球的概率为:$$\alpha_{2}(3)=\left[\sum_{i=1}^{3} \alpha_{1}(i) a_{i 3}\right] b_{3}\left(o_{2}\right)=[0.1 * 0.3+0.16 * 0.2+0.28 * 0.5] \times 0.3=0.0606$$* $t=3$时刻的前向概率(在1时刻为红色球，2时刻为白色球的基础下)隐藏状态是盒子1且为红色球的概率为:$$\begin{array}{l}\alpha_{3}(1)=\left[\sum_{i=1}^{3} \alpha_{2}(i) a_{i 1}\right] b_{1}\left(o_{3}\right)=[0.077 * 0.5+0.1104 * 0.3+0.0606 * 0.2] \times 0.5 = 0.04187\end{array}$$隐藏状态是盒子2且为红色球的概率为:$$\begin{array}{l}\alpha_{3}(2)=\left[\sum_{i=1}^{3} \alpha_{2}(i) a_{i 2}\right] b_{2}\left(o_{3}\right)=[0.077 * 0.2+0.1104 * 0.5+0.0606 * 0.3] \times 0.4 =0.03551\end{array}$$隐藏状态是盒子3且为红色球的概率为:$$\begin{array}{l}\alpha_{3}(3)=\left[\sum_{i=1}^{3} \alpha_{2}(i) a_{i 3}\right] b_{3}\left(o_{3}\right)=[0.077 * 0.3+0.1104 * 0.2+0.0606 * 0.5] \times 0.7 =0.05284\end{array}$$最终观测序列 $$O=\{ 红, 白, 红 \}$$ 的概率为: $$P(O \mid \lambda)=\sum_{i=1}^{3} \alpha_{3}(i)= 0.04187+0.03551+0.05284 = 0.13022$$123456789101112131415161718192021222324252627282930313233343536import numpy as npdef cal_step(pa, pb, state, last):    pa = np.array(pa)    pb = np.array(pb)    res = []    for idx, s in enumerate(np.array(pb)[:, state]):        tmp = sum([last[i] * a for i, a in enumerate(pa[:, idx])]) * s        res.append(round(tmp, 4))    return resdef cal_forward(pa, pb, ppi, po):    &quot;&quot;&quot; 前向算法 &quot;&quot;&quot;    # 初始化状态    init_state = [round(p * pb[idx][0], 4) for idx, p in enumerate(ppi)]    print(init_state)    state = init_state    for step in range(1, len(po)):        state = cal_step(pa=pa, pb=pb, state=po[step], last=state)        print(state)    print(round(sum(state), 4))if __name__ == '__main__':    pi = [0.2, 0.4, 0.4]    B = [[0.5, 0.5], [0.4, 0.6], [0.7, 0.3], ]    A = [[0.5, 0.2, 0.3], [0.3, 0.5, 0.2], [0.2, 0.3, 0.5], ]    # 红 白 红    O = [0, 1, 0]    cal_forward(pa=A, pb=B, ppi=pi, po=O)    print(&quot;结束&quot;)🐝后向概率定义也就是下图中标记的那一部分，$ \beta _t (i) $表示的是在$t$时刻，隐状态为$q_i$的概率后向概率初值，定义为1根据后向概率定义可以推出然后来看上式和要计算的概率$ P(X| \lambda ) $之间的关系上面$b_i(x_1) \pi _i$也就是$ \alpha _1(i) $的定义，实际上，对于任意时刻$ t $，存在以下等式接着，假设已知所有的，来推导观察上式，后部分实际上就是$\alpha _{ij}$，而前半部分，根据前向概率中的观测独立性假设，$z_t$与$x_1, \cdots ,x_T$都是无关的，即相互独立，可以省去，因此第二部分可以变为将结论代入$\beta _t{i}$得到条件概率例子题目仍为前向概率小节所描述$t=2$时刻的后向概率2时刻隐藏状态为盒子1的条件下， 3观测为红的概率为 $\beta_{2}(1)=0.5 \times 0.5+0.2 \times 0.4+0.3 \times 0.7=0.54$2时刻隐藏状态为盒子2的条件下， 3观测为红的概率为 $\beta_{2}(2)=0.3 \times 0.5+0.5 \times 0.4+0.2 \times 0.7=0.49$2时刻隐藏状态为䀂子3的条件下， 3观测为红的概率为 $\beta_{2}(3)=0.2 \times 0.5+0.3 \times 0.4+0.5 \times 0.7=0.57$$t=1$时刻的后向概率1时刻隐藏状态为盒子1的条件下，2为白3为红的概率:1时刻隐藏状态为盒子2的条件下，2为白3为红的概率:1时刻隐藏状态为盒子2的条件下，2为白3为红的概率:最终观测序列$O={ 红, 白,  红 }$ 的概率为:参数学习解码问题</content>
	</entry>
	<entry>
		<title>条件随机场CRF</title>
		<url>/article/12117.html</url>
		<content>前置知识笔记整理自：Bilibili站上shuhuai008强势手推讲解的白板推导CRF系列课程，课程质量很高！条件随机场【NLP】从隐马尔科夫到条件随机场分类问题根据输出的类型，可以将其划分为硬模型和软模型硬输出: 输出为0或1软输出: 引入概率，不直接计算边界，而是计算取各类别的概率软硬模型硬模型svm支持向量机(最大间隔)几何角度出发，求的是max margin classifier，式子最终的形式为PLA感知机，可以看作最基本的神经元LDA线性判别分析(误分类驱动)核心思想: 类间大、类内小软模型概率判别模型建模核心是对进行建模例子_逻辑回归(Logistics Regression, LR)，或者也可以叫SoftMax Regression例子_最大熵模型(Maximum Entropy Model, MEM)利用最大熵思想(Maximum Entropy Model)驱动模型，从熵模型来看，LG是熵模型的一个特例在最大熵原则下，如果给定均值和方差，那么Gaussian Dist熵最大例子_最大熵马尔科夫模型(Maximum Entropy Markov Model, MEMM)结合了MEM和HMM形成了MEMM，图模型可以表达成黄线部分(全局影响)或绿线部分(局部影响)，表达成绿线是为了和HMM对比潜在问题: 标注偏差问题标注偏差问题(Label Bias Problem): 原因是因为局部归一化，上图中灰色虚线可以表达为，这部分要求符合概率分布，因此需要归一化，强制变为概率分布与其他模型的关系与HMM的关系HMM直接求联合概率分布，对于标注问题而言，显然不需要求联合概率分布，求条件概率就可以了，因此MEMM比HMM更简单打破了HMM的观测独立假设，比如的标注不仅和有关，也和有关，甚至可以说和整个都有关系，这个假设更加合理，观测间不应该是相互独立的CRF将MEMM的有向调整为了无向，因此此时的归一化变为了全局归一化，解决了标注偏差问题概率生成模型建模核心是对进行建模例子_朴素贝叶斯(Naive Bayes, NB)朴素贝叶斯假设公式如下朴素贝叶斯模型简化为下图:该图可以公式表达为，意思是在给定的情况下，独立于或者可以解释为和之间受到的阻隔，因此在给定的情况下，独立于例子_隐马尔科夫(Hidden Markov Model, HMM)朴素贝叶斯中的是分类问题，如果这里把看作是一个标注问题，此时演化为序列预测HMM有两个重要假设齐次Markov假设：观测独立假设：公式表达为，意思是在给定的情况下，独立于例子_高斯混合模型(Gaussian mixture model, GMM)NB-(0/1 变 seq)-&gt; HMMGMM-(+times)-&gt; HMM概率图模型结构D-Separation是一种用来判断变量是否条件独立的图形化方法。换言之，对于一个DAG(有向无环图)E，D-Separation方法可以快速的判断出两个节点之间是否是条件独立的head-to-head(a-&gt;c&lt;-b)graph TB    A --&gt; C    B --&gt; C在c未知的条件下，a、b被阻断(blocked)，是独立的。反过来讲，在c已知的条件下，a、b是连通的，不独立。数学表示是tail-to-tailgraph TB    C --&gt; A    C --&gt; B在c给定的条件下，a，b被阻断(blocked)，是独立的。数学表示是head-to-tailgraph LR    A --&gt; C --&gt; B在c给定的条件下，a，b被阻断(blocked)，是独立的。数学表示是化简后可得无向图因子分解这里的无向图以马尔科夫随机场MRF举例其中，是最大团的个数，是第个最大团，根据因子分解定理，要求势函数，是最大团的表达通常指定势函数是一个指数函数，是一个能量函数，是的简化表达，都是一个函数团的定义是子图中每个点都能相互连通多个变量之间的联合概率分布能基于团分解为多个因子的乘积，每个因子仅与一个团相关演化以下是从朴素贝叶斯到CRF的演变，阴影高亮部分是每次改变的地方朴素贝叶斯的序列形态(或者说引入时间)就变成了HMM模型，朴素贝叶斯的观察独立假设，这里仍存在，同时HMM还多了一阶马尔科夫假设从HMM到MEMM，主要是改变了观测变量为输入，此时$x_2$不仅受$y_2$的影响，也会受到$y_1$的影响，从而打破了观测独立假设，但由于$x_2,y_1,y_2$需要做局部归一化，因此会产生标注偏差问题ps: HMM在预测时，还可以分为预测问题和滤波问题预测问题定义为$P(i_{t+1}| o_1,o_2, \cdots, o_t)$，即已知当前和历史时刻，预测下一时刻的隐状态滤波问题定义为$P(i_{t}| o_1,o_2, \cdots, o_t)$，即已知当前和历史时刻，求当前时刻的隐状态CRF的就能解决标注偏差问题，它将$y$之间的顺序，改为了无向图，这意味着，$y$之间的归一化是全局的，因此解决了标注偏差问题HMM马尔科夫随机场(MRF)在图像处理中的应用-图像分割、纹理迁移HMM是一个生成模型，可以看成是NB或GMM的泛化参数定义为，其中是初始状态分布、是y之间的状态转移矩阵(到的概率)、是从发射到的发射矩阵HMM有两个重要假设(非必须，折中的方式，减少计算量)一阶齐次Markov假设：一阶: 含义为只和有关系，或者表述为给定的情况下和是无关的，二阶的话就是给定和的情况下和是无关的齐次: 含义为每个的概率是是一致，也可以表述为和是相等的，也就是与时间无关术语化表达: 观测独立假设：公式表达为，意思是在给定的情况下，独立于术语化表达: 建模对象如下计算复杂度太高，于是引入了前向后向算法MEMM与HMM比较，将变为了的输入，此时给定的情况下，和不是相互独立的，从而打破观测独立性假设此时的MEMM建模对象变为了条件概率，为判别模型在这里，判别式的模型比生成式模型要好因为：任务驱动的，在这样的序列标注任务上，在给定一个待标注序列，我们更关注的是标注的是什么，因此，对于这样的任务我们只需要对条件概率建模就足矣了，而去求联合概率分布就将问题复杂化了(计算也复杂化了)这里把整个输入分成了2个部分，，即对的影响分成两个部分：全局的影响+局部的影响主要缺点是会造成Label Bias Problem，（在John Lafferty论文中，指出了MEMM会造成Label Bias Problem问题）看上方灰色框内的局部，可以简单看成是逻辑回归，在这个最小单元我们把它当成一个系统，系统从到的这样一个转移用数学可以表示成一个函数，是受、与共同作用的一个函数，系统对外是有一定能量的，这个函数被称为Mass Score，而且是大于0的。但是问题出现在：这个Mass Score被局部归一化了。条件概率熵越小，那么对于观测值的考虑越少为了能够更好地理解它的问题所在，这里举一个比较极端的例子比如，我们训练好了这样的一个模型，现有一个观测序列是rib在状态0时，由于，所以我们选择从转移路径；问题出现在了第二步转移上：由于，是受到了局部归一化的影响，从4状态转移仅有1条路径选择，其转移的条件概率为1，其所具有的熵值为0为什么从4状态向5转移的条件概率为1正是因为MEMM在每一步转移的概率都是要做局部归一化的，也就是说，从4状态向外转移的所有的情况加和要为1，在这里，仅有1中转移情况，所以在这种情况下，无论观测值为什么，都不会影响的转移，但是，这样标注出来的序列，即rob就与客观事实不符了，这也就是问题的所在CRF所以，为了解决局部归一化的问题，CRF取消了Y序列的转移方向，也就是取消了局部归一化，改变为了全局归一化 CRFCRF是马尔科夫随机场MRF的特例，它假设马尔科夫随机场中只有𝑋和𝑌两种变量，𝑋一般是给定的，而𝑌一般是在给定𝑋的条件下我们的输出。这样马尔科夫随机场就特化成了条件随机场条件体现在它是判别式模型，计算条件概率随机场体现在是无向图模型，或者说是马尔科夫随机场MRF图中和是为了表达上的方便通常在没有说明的情况下，CRF指的就是线性链条件随机场概率密度函数参数化根据无向图因子分解公式，概率密度函数可以表达为其中最大团展开之后就是因为这里是线性链，每个结构都一样，没有必要为每一个都设一个函数，因此这里表达为而不是接着，我们图中灰色框部分举例，这里可以把拆成三部分来看，、和它们之间边的关系前两个可以看成状态函数，第三个可以看成是状态转移函数同时第一个可以忽略掉，原因在于目前是在时刻，而在时刻时，已经有了，也就是上面的第二部分这里的为什么可以把F拆成三部分呢，从图上看可能有些突兀，这里可以带入到标注问题来理解比如这里有一句话: 小明 是 中国人假设这里已经分过词了，这里要去判断词性，这里和是属于词性{名词、形容词、动词、…}的集合因此这里前两个可以看成状态函数而第三个就表示两个状态之间的转移函数状态函数和转移函数其中是一个特征函数，一般是指示函数，比如\Delta{y{t}, x{1:T}} = \sum {l=1}^{L}{\eta l g_l (y_t, x{1:T})}p(Y|X) = \frac {1}{Z} exp (\sum {i=1}^{K} {F_i(x{ci})}) = \frac {1}{Z} exp (\sum {t=1}^{T} {F(y{t-1},y_t, x{1:T})}) = \\frac {1}{Z} exp (\sum {t=1}^{T} [\sum {k=1}^{K}{\lambda k f_k(y{t-1}, t{t}, x{1:T})} + \sum {l=1}^{L}{\eta _l g_l (y_t, x{1:T})}]$$写出概率密度函数之后，就可以求解了机器学习基础（七）：概率图模型（HMM、MRF、CRF、话题模型、推断方法）https://blog.csdn.net/weixin_44949421/article/details/120523212概率图模型（PGM）（或简称图模型）在形式上是由图结构组成的。图的每个节点（node）都关联了一个随机变量，而图的边（edge）则被用于编码这些随机变量之间的关系。根据图是有向的还是无向的，我们可以将图的模式分为两大类——贝叶斯网络（ Bayesian network）和马尔可夫网络（Markov networks）0.概率图：用图来表达变量之前相关关系的概率模型。其中图中的点表示一组随机变量，边表示随机变量之间的概率相关关系 1.有向图模型/贝叶斯网：有向无环图。当变量之间有明确的因果关系时使用 2.无向图模型/马尔科夫网：无向图。当变量之间有关系，但难以获得明确的因果关系时使用 3.生成式模型：对联合概率进行建模，如隐马尔可夫模型、马尔可夫随机场 4.判别式模型：对条件概率进行建模，如条件随机场马尔可夫性1.全局马尔可夫性：给定两个变量子集的分离集，则这两个变量子集条件独立。2.局部马尔可夫性：给定某变量的邻接变量，则该变量相对其他变量条件独立。3.对称马尔可夫性：给定所有其他变量，两个非邻接变量条件独立。对比MRF和CRF，都是使用团上的势函数来定义概率，只是一个是联合概率，一个是条件概率7.1隐马尔可夫模型Hidden Markov Model，HMM结构最简单的动态贝叶斯网dynamic Bayesian network，生成式有向图模型，主要用于时序数据建模（语音识别、自然语言处理等领域）7.2马尔可夫随机场Markov Random Field，MRF典型的马尔可夫网，生成式无向图模型为满足非负性，指数函数常被用于定义势函数每个状态的转移只依赖于之前的n个状态，这个过程被称为1个n阶的模型，其中n是影响转移状态的数目。最简单的马尔可夫过程就是一阶过程，每一个状态的转移只依赖于其之前的那一个状态，这个也叫作马尔可夫性质。马尔可夫性质：一句话总结就是“未来只与现在有关”，即给定一个过程当前状态及历史的所有状态，其未来状态仅依赖于当前状态，与历史状态无关，这种性质叫做马尔科夫性质。这里比较有意思的事情是，有些非马尔可夫过程可以通过扩展“现在”和“未来”状态的概念来构造一个马尔可夫过程，这种情况称为二阶马尔可夫过程。以此类推，还可以构造更高阶的马尔可夫过程。马尔可夫链：是一种最简单的马尔可夫过程，专指离散指数集的马尔可夫过程。经典的马尔可夫链主要是研究当前状态和未来状态之间的转移概率，并可以计算出多次试验之后的每个状态的概率分布，从而将看起来毫无规律的一些随机现象变成了整体有序的状态变化。马尔可夫决策过程，是将马尔可夫性质应用于时序决策建模的方法，设定智能体的随机性策略和回报符合马尔可夫性质，这样就将智能体在与环境交互中的状态转移计算过程定义为马尔可夫性质的状态转移过程计算。通过使用动态规划、随机采样等方法，MDP可以求解使回报最大化的智能体策略。当今人工智能研究中火热的强化学习方向，方法的基石就是马尔可夫决策过程（Markov Decision Processes, MDP），这个方法是Bellman通过离散随机最优控制模型首次提出的，对于时序决策问题具有很好的建模能力。隐马尔可夫模型，是对马尔可夫模型的扩展，这种模型的思想核心是把马尔科夫的状态转移设定为未知的隐含量，通过可观测的状态转移过程来估计隐含的状态，然后再用隐含状态来预计未来的变化，利用这种方法发现很多实际问题能够得到有效的建模，典型的应用包括了语音识别、生物信息科学的DNA分析和故障诊断等领域。马尔可夫随机场，给随机场定义一种马尔可夫性质，即随机场中每个位置的属性定义是马尔可夫性的，简单理解就是随机场中每个位置的属性只与邻近的位置有关，与其他位置无关。这种方法被应用于图像分割取得较好效果。CRF和MRF区别：MRF和CRF建模思路是不一样的，MRF是一种generative模型。所谓generative模型就是需要先得出变量的概率模型参数，然后在进行推断任务。如bayes 推断中，可以先假定变量服从高斯分布，然后利用似然函数方法求取高斯分布的均值和方差这些参数，从而得出变量的概率分布模型，后期推断时依据该模型得出相应的结论。对于MRF模型而言，由于需要考虑观察变量y和标号变量x概率分布问题，这本身就是一个非常困难的问题。因为一般自然图片像素和其标号分布，很难采用一个简单的模型加以描述，MRF是表达这种模型的一种方法。对于某一张图片我们的兴趣点在于其标号，而不关心原始像素y的分布，所以没有必要花费资源考虑y的概率分布问题。CRF便是基于这一点，提出一种discriminative模型。所谓discriminative模型，就是不用考虑原始变量实际的概率分布问题，直接研究标号推断问题。比如对于二分问题，我们可以直接获取其判别式h，当h&gt;0时属于A类，反之属于B类。这样在不知道变量在A，B两类中的实际概率分布，便可以得出我们关心的分类结果。MRF和CRF都是属于图理论中的问题，区别在于他们能量函数建立细节不同。MRF理论上需要显得出观察变量y和标号变量x的实际概率分布。但是由于计算能力的限制，这一点无法满足，所以目前研究中，采用了近似的方法。假定x的先验分布和y没有关系，只和相邻两节点有关。而y中某一个节点yi的分布只与当前标号变量xi有关，一般假定满足高斯分布。这样的假设是一种妥协，削弱了MRF的能力，但是便于计算实现。在自然图片中这些假设很难成立。针对上面的问题，为了提高MRF的能力，同时保持其可计算性，CRF被提了出来。在CRF中xi和所有的观察变量y有关，这个构成节点能量，xi和xj也和所有的观察变量y有关，构成边缘能量。这样提高了MRF的适用性。CRF中节点能量和边缘能量均是通过discriminative方式实现，其中节点能量通过线性分类器实现。而在MRF中节点能量是通过generative模型实现，如高斯分布。马尔可夫网络一般指马尔可夫随机场。 马尔可夫随机场（Markov Random Field），也有人翻译为马尔科夫随机场，马尔可夫随机场是建立在马尔可夫模型和贝叶斯理论基础之上的，它包含两层意思：一是什么是马尔可夫，二是什么是随机场条件随机场（Conditional random field，CRF）是条件概率分布模型 P(Y|X) ，表示的是给定一组输入随机变量 X 的条件下另一组输出随机变量 Y 的马尔可夫随机场，也就是说 CRF 的特点是假设输出随机变量构成马尔可夫随机场。条件随机场可被看作是最大熵马尔可夫模型在标注问题上的推广给出一个样本x，判别猫和狗，判别式模型直接可以用 arg⁡maxyP(y|x) 来进行判断，这也是判别式模型的建模目标；对于生成式模型，是通过看 ，猫P(x，Y=猫) 和 狗P(x,Y=狗) 谁大来进行判断。可以看出，判别模型能更直接应用到分类任务中，而生成模型用于分类任务时，还需要”绕”一下自然语言处理序列模型——MRF和CRF条件随机场</content>
	</entry>
	<entry>
		<title>贪心算法</title>
		<url>/article/51855.html</url>
		<content>贪心算法概念案例</content>
	</entry>
	<entry>
		<title>动态规划</title>
		<url>/article/55046.html</url>
		<content>动态规划概念动态规划(英语：Dynamic programming，简称 DP)，是一种在数学、管理科学、计算机科学、经济学和生物信息学中使用的，通过把原问题分解为相对简单的子问题的方式求解复杂问题的方法动态规划常常适用于有重叠子问题和最优子结构性质的问题适用情况动态规划有几个典型特征，最优子结构、状态转移方程、边界、重叠子问题最优子结构: 如果某条路径是起点到终点的最短路径，则起点到该路径上的每一点都是最短路径，一个最优化策略的子策略总是最优的，最优化原理是动态规划的基础无后效性: 当前选择任何一条边都不会影响以后选择其他边，如果当前选择会影响后面的选择则不适用，每个状态都是过去历史的一个完整总结动态规划解题的步骤找出最优子结构写出动态规划方程使用自底向上或者自顶向下的方法求解逆推求解小结动态规划的适用条件任何思想方法都有一定的局限性，超出了特定条件，它就失去了作用同样,动态规划也并不是万能的，适用动态规划的问题必须满足最优化原理和无后效性动态规划的实质是分治思想和解决冗余，因此，动态规划是一种将问题实例分解为更小的、相似的子问题，并存储子问题的解而避免计算重复的子问题，以解决最优化问题的算法策略动态规划思想设计的算法从整体上来看基本都是按照得出的递推关系式进行递推，这种递推相对于计算机来说，只要设计得当，效率往往是比较高的，这样在时间上溢出的可能性不大相反地，动态规划需要很大的空间以存储中间产生的结果，这样可以使包含同一个子问题的所有问题共用一个子问题解，从而体现动态规划的优越性但这是以牺牲空间为代价的，为了有效地访问已有结果，数据也不易压缩存储，因而空间矛盾是比较突出的另一方面，动态规划的高时效性往往要通过大的测试数据体现出来(以与搜索作比较)，因而，对于大规模的问题如何在基本不影响运行速度的条件下，解决空间溢出的问题，是动态规划解决问题时一个普遍会遇到的问题时空矛盾一个思考方向是尽可能少占用空间。如从结点的数据结构上考虑，仅仅存储必不可少的内容，以及数据存储范围上精打细算(按位存储、压缩存储等)，当然这要因问题而异，进行分析，另外，在实现动态规划时，一个我们经常采用的方法是用一个与结点数一样多的数组来存储每一步的决策，这对于倒推求得一种实现最优解的方法是十分方便的，而且处理速度也有一些提高但是在内存空间紧张的情况下，我们就应该抓住问题的主要矛盾。省去这个存储决策的数组，而改成在从最优解逐级倒推时，再计算一次，选择某个可能达到这个值的上一阶段的状态，直到推出结果为止。这样做,在程序编写上比上一种做法稍微多花一点时间，运行的时效也可能会有一些(但往往很小)的下降，但却换来了很多的空间。因而这种思想在处理某些问题时,是很有意义的案例🐸青蛙跳阶题目动态规划详解思路递归: 青蛙🐸跳阶可以使用递归求解带记忆的递归: 而递归求解中存在大量的重复计算，可以引入状态记忆，避免大量的重复计算递推: 除了递归方式，也可以使用递推的方式计算，目标是算出F(1)、F(2)…F(N)，只需要两个状态变量存储动态规划: 使用动态规划的方式，拆分子问题，记住过往，减少重复计算，关键在于状态转移转矩阵计算状态转移求解打家劫舍https://leetcode.cn/problems/house-robber/1234567891011121314151617181920212223242526def run():    samples = [[1, 2, 3, 1], [2, 7, 9, 3, 1]]    for sample in samples:        one, two = sample[0], sample[1]  # 表示n-2位置处的最大价值 和 n-1位置处的最大价值        res_sum = max(one, two)        for idx, s in enumerate(sample[2:], start=2):            # n-2位置处的最大价值+n位置处的价值 和 n-1位置处的最大价值            res_sum = max(one + s, two)            one, two = two, res_sum        print(res_sum)        # 暴力法验证    for sample in samples:        idx_lst = gen_bin(n=len(sample))        res_sum = max([sum([sample[idx] for idx, i in enumerate(bin(idx)[2:][::-1]) if int(i) == 1]) for idx in idx_lst])        print(res_sum)def gen_bin(n: int):    &quot;&quot;&quot; 列出所有可能性 &quot;&quot;&quot;    n = 1 &lt;&lt; n    res = []    while n := n - 1:        if '11' not in bin(n):            res.append(n)    return res</content>
	</entry>
	<entry>
		<title>递推和递归</title>
		<url>/article/54551.html</url>
		<content>递推和递归概念递归: 从问题的最终目标出发，逐渐将复杂问题化为简单问题，最终求得问题，是逆向的递推: 是从简单问题出发，一步步的向前发展，最终求得问题。是正向的一般来说，递推的效率高于递归(当然是递推可以计算的情况下)，比如：斐波那契数列案例递归实现指数型枚举题目描述从 $1∼n$ 这 $n$ 个整数中随机选取任意多个，输出所有可能的选择方案难度简单输入格式输入一个整数 $n$输出格式每行输出一种方案同一行内的数必须升序排列，相邻两个数用恰好 $1$ 个空格隔开。对于没有选任何数的方案，输出空行本题有自定义校验器（SPJ），各行（不同方案）之间的顺序任意数据范围输入样例13输出样例1234567322 311 31 21 2 3求解递归方式1234567891011n = input()def dfs(u, state):    if u == n:        res = [str(i + 1) for i in range(n) if state &gt;&gt; i &amp; 1]        print(&quot; &quot;.join(res))        return    dfs(u + 1, state)    dfs(u + 1, state | 1 &lt;&lt; u)    dfs(0, 0)   位运算方式给定n，求出$[1,n]$的组合，比如$n=5$，其组合数为$C_5^1+C_5^2+C_5^3+C_5^4+C_5^5 = 32$这里可以把每位是否选择用$n$位的二进制表达，比如$11 \cdots 11$，一直减1，减到0时结束，期间每个二进制就是选择的结果，输出对应结果就好1234567891011def solution(n):    a = 1 &lt;&lt; n    ct = 0    import time    t0 = time.time()    res = []    while a := a - 1:        ct += 1        res.append(bin(a))    print('\n'.join(res))    print(&quot;ct&quot;, ct, time.time() - t0)递归实现组合型枚举题目描述从 $1∼n$ 这 $n$ 个整数中随机选取$m$个，输出所有可能的选择方案难度简单输入格式两个整数 $n$，$m$在同一行用空格隔开输出格式按照从小到大的顺序输出所有方案，每行 1 个首先，同一行内的数升序排列，相邻两个数用一个空格隔开其次，对于两个不同的行，对应下标的数一一比较，字典序较小的排在前面（例如 1 3 5 7 排在 1 3 6 8 前面）数据范围输入样例15 3输出样例123456789101 2 3 1 2 4 1 2 5 1 3 4 1 3 5 1 4 5 2 3 4 2 3 5 2 4 5 3 4 5 求解递归方式123456789101112131415n = input()m = input()def dfs(u, sum_, state):    # 不满足要求的剪枝    if sum_ + n - u &lt; m:        return    if u == n:        res = [str(i + 1) for i in range(n) if state &gt;&gt; i &amp; 1]        print(&quot; &quot;.join(res))    dfs(u + 1, sum_, state)    dfs(u + 1, sum_ + 1, state | 1 &lt;&lt; u)dfs(0, 0, 0)位运算方式123456789101112131415161718192021222324def lowbit(x: int):    return x &amp; -x    def cal_one(v):    ct = 0    while v:        v -= lowbit(v)        ct += 1    return ctdef solution_m(n, m):    # 获取m位的组合    a = 1 &lt;&lt; n    ct = 0    while a := a - 1:        if cal_one(a) == m:            ct += 1            print(bin(a))    print(&quot;ct&quot;, ct)if __name__ == '__main__':    solution_m(5, 3)递归实现排列型枚举题目描述从 $1∼n$ 这 $n$ 个整数排成一行后随机打乱顺序，输出所有可能的次序难度简单输入格式一个整数 $n$输出格式按照从小到大的顺序输出所有方案，每行 1 个首先，同一行相邻两个数用一个空格隔开。其次，对于两个不同的行，对应下标的数一一比较，字典序较小的排在前面。数据范围输入样例13输出样例1234561 2 31 3 22 1 32 3 13 1 23 2 1思路每次选取没有选过的数求解12345678910111213141516n = input()path = []def dfs(u, state):    if u == n:        print(&quot; &quot;.join(path))        return    for i in range(n):        if not (state &gt;&gt; i &amp; 1):            path.append(str(i + 1))            dfs(u + 1, state | (1 &lt;&lt; i))            path.pop()dfs(0, 0)费解的开关题目描述求a的b次方对p取模的值难度简单输入格式三个整数a,b,p，在同一行用空格隔开输出格式输出一个整数，表示$(a^b) \% p$的结果数据范围输入样例123 2 7123456789 0 1输出样例1220思路快速幂12345678910113 ^ 10000000# 先计算出3 ^ 1 = 33 ^ 2 = 93 ^ 4 = 813 ^ 8 = 3 ^ 16 = 3 ^ 32 = ...3 ^ (2^19) = xxx🥕核心: 再看下10000000的二进制表示，把对应为1的位乘起来就可以10000000的二进制表示为0b100110001001011010000000，因此只要对应位置的值乘起来就是3 ^ 10000000🥕根据数学常识，每一个正整数可以唯一表示为若干指数不重复的2的次幕的和12345678910111213141516171819def ck_sum_of_power(in_n: int):    &quot;&quot;&quot;    检验整数可以由若干个不重复的2的次幂的和    @param in_n:    @return:    &quot;&quot;&quot;    nb = in_n    res = 0    v = 1    while nb:        if nb &amp; 1:            res += v        v *= 2        nb &gt;&gt;= 1    return resif __name__ == '__main__':    print(all([ck_sum_of_power(in_n=i) == i for i in range(100)]))也就是说，如果b在二进制表示下有$k$位，其中第$i(0&lt;i&lt;k)$位的数字是$c_i$，则于是因为$k= \lceil log2(b+1) \rceil $，所以上式乘积项的数量不多于$\lceil log2(b+1) \rceil$个又因为$a^{2^i} = {a^{(2^{i-1})}}^2$，所以我们很容易通过$k$次递推求出每个乘积项，当$c_i=1$时，把该乘积项累积到答案中b&amp;1运算可以取出b在二进制表示下的最低位，而b&gt;&gt;1运算可以舍去最低位，在递推的过程中将二者结合，就可以遍历b在二进制表示下的所有数位$c_i$，整个算法的时间复杂度为O($log2(b)$)求解123456789101112def solution(a: int, b: int, p: int) -&gt; int:    res = 1 % p    while b:        # 如果b的个位是1        if b &amp; 1:            res = res *  a % p        # 准备十位        a = a * a % p        # 去掉各位        b &gt;&gt;= 1    return res三层汉诺塔12345678910111213141516171819202122232425262728293031def hanoi(n, from_, temp, to):    &quot;&quot;&quot;    @param n: 剩余盘子数量    @param from_: 当前位置    @param temp: 临时位置    @param to: 目标位置    @return:    &quot;&quot;&quot;    if n == 1:        print(from_ + &quot; -&gt; &quot; + to)        return    hanoi(n - 1, from_, to, temp)    print(from_ + &quot; -&gt; &quot; + to)    hanoi(n - 1, temp, from_, to)def solution(n: int, from_, temp, to):    hanoi(n, from_, temp, to)if __name__ == '__main__':    solution(n=3, from_='A', temp='B', to='C')    # 输出A -&gt; CA -&gt; BC -&gt; BA -&gt; CB -&gt; AB -&gt; CA -&gt; C🐸青蛙跳阶题目思路递归: 青蛙跳阶可以使用递归求解带记忆的递归: 而递归求解中存在大量的重复计算，可以引入状态记忆，避免大量的重复计算递推: 除了递归方式，也可以使用递推的方式计算，目标是算出F(1)、F(2)…F(N)，只需要两个状态变量存储动态规划: 使用动态规划的方式，拆分子问题，记住过往，减少重复计算，关键在于状态转移转矩阵计算状态转移求解1234567891011121314151617181920212223from functools import lru_cache@lru_cache()def fab(n: int):    # 带记忆的递归    return 1 if n &lt;= 1 else fab(n - 1) + fab(n - 2)def fab_no_stock(n: int):    # 递推    res = 0    one, two = 1, 1    while n := n - 1:        res = one + two        one, two = two, res    return resif __name__ == '__main__':    p_t = 10    res = fab(n=p_t)    print(res)    res_no = fab_no_stock(n=p_t)    print(res_no)</content>
	</entry>
	<entry>
		<title>位运算</title>
		<url>/article/60366.html</url>
		<content>位运算基础位运算基础可以见python基础知识计算位运算计算时间为O(1)移位运算左移:逻辑右移: 就是不考虑符号位，右移一位，左边补零即可算术右移: 考虑符号位，右移一位，若符号位为1，就在左边补1；否则，就补0算术右移也可以进行有符号位的除法，右移n位就等于除2的n次方🍆n&gt;&gt;1是向下取整，n/2是整数除法，是向0取整1234567[(i // 2, i &gt;&gt; 1) for i in range(10)]# 输出[(0, 0), (0, 0), (1, 1), (1, 1), (2, 2), (2, 2), (3, 3), (3, 3), (4, 4), (4, 4)]位运算小技巧用异或实现配偶123456789100, 12, 34, 56, 70 ^ 1 = 1, 1 ^ 1 = 02 ^ 1 = 3, 3 ^ 1 = 2...4 ^ 1 = 5, 5 ^ 1 = 4...12345678[i ^ 1 for i in range(10)]Out[3]: [1, 0, 3, 2, 5, 4, 7, 6, 9, 8][i ^ 2 for i in range(10)]Out[4]: [2, 3, 0, 1, 6, 7, 4, 5, 10, 11][i ^ 3 for i in range(10)]Out[5]: [3, 2, 1, 0, 7, 6, 5, 4, 11, 10][i ^ 4 for i in range(10)]Out[6]: [4, 5, 6, 7, 0, 1, 2, 3, 12, 13]lowbit运算: 求1最低的位数12345lowbit = lambda n: (-n) &amp; n或lowbit = lambda n: (~n+1 ) &amp; nlowbit(11110010000) = 10000判断奇偶只要根据最未位是0还是1来决定，为0就是偶数，为1就是奇数。因此可以用if ((a &amp; 1) == 0)代替if (a % 2 == 0)来判断a是不是偶数取一个数中指定位取X的低4位，用 X &amp; 0000 1111 = 00001110 即可得到对一个数的某些位置1找到一个数，对应X要置1的位，该数的对应位为1，其余位为零。此数与X相或可使X中的某些位置1使特定位翻转使特定位翻转，该数对应X要翻转的对应位为1，其余位为零，此数与X对应位异或即可例：X=10101110，使X低4位翻转，用X ^0000 1111 = 1010 0001即可得到异或的几条性质：异或其实就是不进位加法1、交换律 a ^ b = c ===&gt; a ^ c = b , b ^ c = a2、结合律 即 (a ^ b) ^ c == a ^ (b ^ c)3、对于任何数x，都有x ^ x = 0，x ^ 0 = x4、自反性: a ^ b ^ b = a ^ 0 = a1000放在含有1001个元素的数组中，只有唯一的一个元素值重复，其它均只出现一次。每个数组元素只能访问一次，设计一个算法，将它找出来；不用辅助存储空间将所有的数全部异或，得到的结果与1 ^ 2 ^ 3 ^ … ^ 1000的结果进行异或，得到的结果就是重复数枚举一个无重复字符串所有子集的技巧用二进制数组表示单词中那些字母出现（1表示），那些没出现（0表示），找出该单词的所有子集。ace 对应二进制 int origin = 21 (二进制10101)，包含的子集(n代表子集,初始 n = origin)有：123456789ace 10101ce 10100ae 10001ac 00101e 10000c 00100a 00001遍历核心代码：n = (n - 1) &amp; origin;// n-1 AND puzzleBit，生成一个puzzleBit的新的子集合可以自己手动推算一下，是对的，n刚好遍历完所有情况找map中 存在的 字符串str = ace（必含有首字母a）子集的个数12345678910int first = 1 &lt;&lt; (str[0] - 'a');while (n &gt; 0) { // 遍历origin 的所有字母组合，当n=0时终止遍历           // 按位都是1才为1，否则为0，即n这个组合包含origin 的首字母           // 而且n这个组合在map中有值，即有单词长n这样，值累加给res[i]           if ((n &amp; first) != 0 &amp;&amp; map[n] &gt; 0) {               res[i] += map[n];           }           // n-1 AND puzzleBit，生成一个puzzleBit的新的子集合           n = (n - 1) &amp; puzzleBit;       }常用位运算公式a &gt;&gt; b &amp; 1 代表检查 a 的第 b 位是否为 1，有两种可能性 0 或者 1a += 1 &lt;&lt; b 代表将 a 的第 b 位设置为 1 (当第 b 位为 0 的时候适用)状态压缩定义二进制状态压缩，是指将一个长度为$m$的bool数组用一个$m$位二进制整数表示并存储的方法利用下列位运算操作可以实现原bool数组中对应下标元素的存取操作运算取出整数$n$在二进制表示下的第$k$位n &gt;&gt; k &amp; 1取出整数$n$在二进制表示下的第$0 \sim k-1$位(后$k$位)n &amp; ((1 &lt;&lt; k) - 1)把整数$n$在二进制表示下的第$k$位取反n ^ (1 &lt;&lt; k)对整数$n$在二进制表示下的第$k$位赋值$1$n $\vert$ (1 &lt;&lt; k)对整数$n$在二进制表示下的第$k$位赋值$0$n &amp; (~(1 &lt;&lt; k))这种方法运算简便，并且节省了程序运行的时间和空间当$m$不太大时，可以直接使用一个整数类型存储当$m$较大时，可以使用若干个整数类型(int数组)，也可以直接利用C++ STL为我们提供的bitset实现案例a^b题目描述求a的b次方对p取模的值难度简单输入格式三个整数a,b,p，在同一行用空格隔开输出格式输出一个整数，表示$(a^b) \% p$的结果数据范围输入样例123 2 7123456789 0 1输出样例1220思路快速幂12345678910113 ^ 10000000# 先计算出3 ^ 1 = 33 ^ 2 = 93 ^ 4 = 813 ^ 8 = 3 ^ 16 = 3 ^ 32 = ...3 ^ (2^19) = xxx🥕核心: 再看下10000000的二进制表示，把对应为1的位乘起来就可以10000000的二进制表示为0b100110001001011010000000，因此只要对应位置的值乘起来就是3 ^ 10000000🥕根据数学常识，每一个正整数可以唯一表示为若干指数不重复的2的次幕的和12345678910111213141516171819def ck_sum_of_power(in_n: int):    &quot;&quot;&quot;    检验整数可以由若干个不重复的2的次幂的和    @param in_n:    @return:    &quot;&quot;&quot;    nb = in_n    res = 0    v = 1    while nb:        if nb &amp; 1:            res += v        v *= 2        nb &gt;&gt;= 1    return resif __name__ == '__main__':    print(all([ck_sum_of_power(in_n=i) == i for i in range(100)]))也就是说，如果b在二进制表示下有$k$位，其中第$i(0&lt;i&lt;k)$位的数字是$c_i$，则于是因为$k= \lceil log2(b+1) \rceil $，所以上式乘积项的数量不多于$\lceil log2(b+1) \rceil$个又因为$a^{2^i} = {a^{(2^{i-1})}}^2$，所以我们很容易通过$k$次递推求出每个乘积项，当$c_i=1$时，把该乘积项累积到答案中b&amp;1运算可以取出b在二进制表示下的最低位，而b&gt;&gt;1运算可以舍去最低位，在递推的过程中将二者结合，就可以遍历b在二进制表示下的所有数位$c_i$，整个算法的时间复杂度为O($log2(b)$)求解123456789101112def solution(a: int, b: int, p: int) -&gt; int:    res = 1 % p    while b:        # 如果b的个位是1        if b &amp; 1:            res = res *  a % p        # 准备十位        a = a * a % p        # 去掉各位        b &gt;&gt;= 1    return resa*b%p题目描述求a乘b次方对p取模的值难度简单输入格式三个整数a,b,p，在同一行用空格隔开输出格式输出一个整数，表示$(a*b) \% p$的结果数据范围输入样例123 4 5111 999 1000000输出样例122110889思路12345678a * ba * 1 = aa * 2 = 2aa * 4 = 4aa * 8 = 8a...a * (2^k) = (2^k) * a求解123456789def solution(a: int, b: int, p: int) -&gt; int:    res = 0    while b:        if b &amp; 1:            res = (res + a) % p        a = a * 2 % p        b &gt;&gt;= 1    return res最短Hamilton路径哈密顿路径哈密顿路径也称作哈密顿链，指在一个图中沿边访问每个顶点恰好一次的路径。寻找这样的一个路径是一个典型的NP-完全(NP-complete）问题。后来人们也证明了，找一条哈密顿路的近似比为常数的近似算法也是NP完全的旅行商问题旅行商问题 (Traveling Salesman Problem，TSP)，又叫货郎担问题，它是图论中一个经典的组合优化问题。经典的TSP可以描述为：一个商品推销员要去若干个城市推销商品，该推销员从一个城市出发，需要经过所有城市一次并且仅一次之后后，回到出发城市。问他应如何选择在城市之间的行程路线，以使他走过的总路程最短。从图论的角度来看，该问题其实就是在一个赋权的无向图中，去找一个哈密尔顿回路，并且使得该回路的总权值最小。容易看出，旅行商问题的可行解是N个顶点的全排列(把[1,2,3,4，…,N]打乱顺序随机排列)，其可行解的个数有N!个，也就是路线有N!个。如果按照穷举法将N!个可行解一一找出来，然后从中找出行程最短的路线。随着顶点数N的增加，会产生组合爆炸。从计算复杂性的角度来看，它是NP-完全问题拓展P问题，NP问题，NPC问题，NP-Hard问题题目描述给定一张 $n$ 个点的带权无向图，点从 $0∼n−1$ 标号，求起点 $0$ 到终点 $n−1$ 的最短 Hamilton 路径。Hamilton 路径的定义是从 $0$ 到 $n−1$ 不重不漏地经过每个点恰好一次。难度中等输入格式第一行输入整数 $n$接下来 $n$ 行每行 $n$ 个整数，其中第 $i$ 行第 $j$ 个整数表示点 $i$ 到 $j$ 的距离（记为 $a[i,j]$]）对于任意的 $x,y,z$，数据保证 $a[x,x]=0, a[x,y]=a[y,x]$ 并且 $a[x,y]+a[y,z] \geq a[x,z]$输出格式输出一个整数，表示最短 Hamilton 路径的长度数据范围输入样例12345650 2 4 5 12 0 6 5 34 6 0 8 35 5 8 0 51 3 3 5 0输出样例118思路考虑两个点哪些点被用过目前停在哪个点上第一维有$n$个点，取值为0或1，表示用或不用第二维有$n$个点，所以状态矩阵大小为使用暴力方法，状态数为$2^{20}$接下来考虑每个状态是如何计算的$state_k$是$state$去掉$j$之后的集合，$state_k$要包含$k$关键点这里可以用二进制的整数表示$state$比如包含$0,1,4$，则$state=10011$求解12345678910111213141516171819202122232425262728293031323334353637383940def solution(n, f, weight):    # 一开始停在0,0被选中故状态位为00..0001, 此时的距离为0    f[1][0] = 0    # 枚举    for i in range(1 &lt;&lt; n):        # 最后停留的位置        for j in range(n):            # i的二进制下第j位是不是1            if i &gt;&gt; j &amp; 1:                for k in range(n):                    # 此时 i^(1 &lt;&lt; j) == n &amp; (~(1 &lt;&lt; k))表示i的第j位取反                    # 并且第k位为1(表示有路径到达j)                    if i ^ (1 &lt;&lt; j) &gt;&gt; k &amp; 1:                        # 更新最短路径                        f[i][j] = min(f[i][j], f[i ^ (1 &lt;&lt; j)][k] + weight[k][j])if __name__ == '__main__':    N, M = 20, 1 &lt;&lt; 20    # f[i][j]表示状态是i的情况下，停在点j的最小路径是多少    f, weight = [], []    for i in range(M):        f.append([1e10] * N)    for i in range(N):        weight.append([0] * N)    # 输入n和权重    # n: int = int(input())    # weight = input()    n = 5    s = &quot;&quot;&quot;0 2 4 5 1           2 0 6 5 3           4 6 0 8 3           5 5 8 0 5           1 3 3 5 0&quot;&quot;&quot;    weight = [[int(j) for j in i.strip().split(' ')] for i in s.split('\n')]    solution(n=n, f=f, weight=weight)    print(f[(1 &lt;&lt; n) - 1][n - 1])强调一下(1&lt;&lt;n)-1的意思：1&lt;&lt;n是将1左移n位也就是10000…(n个0)，减去1以后就会是1111…(n个1)，理解一下，前一个数是n+1位，后一个数是n位，所以说此时(1&lt;&lt;n)-1也就是所有位都是1</content>
	</entry>
	<entry>
		<title>算法综述</title>
		<url>/article/39101.html</url>
		<content>算法算法定义算法是一系列解决问题的清晰指令算法代表着用系统的方法描述解决问题的策略机制也就是说，能够对一定规范的输入，在有限时间内获得所要求的输出如果一个算法有缺陷，或不适合于某个问题，执行这个算法将不会解决这个问题不同的算法可能用不同的时间、空间或效率来完成同样的任务算法的优劣可以用空间复杂度与时间复杂度来衡量分类有限的，确定性算法: 这类算法在有限的一段时间内终止。他们可能要花很长时间来执行指定的任务，但仍将在一定的时间内终止，这类算法得出的结果常取决于输入值有限的，非确定算法: 这类算法在有限的时间内终止。然而，对于一个(或一些)给定的数值，算法的结果并不是唯一的或确定的无限的算法: 是那些由于没有定义终止定义条件，或定义的条件无法由输入的数据满足而不终止运行的算法通常，无限算法的产生是由于未能确定的定义终止条件算法复杂度定义算法复杂度，即算法在编写写成可执行程序后，运行时所需要的资源，资源包括时间资源和内存资源复杂度表示我们记一个算法的复杂度为O(n)，表示数据规模为n的情况下，算法使用的时间和空间资源也可以用O(n)描述，代码执行花费的时间和空间随着n增大变化的趋势算法复杂度从时间上考虑是时间复杂度(快不快)，从空间上考虑是空间复杂度(占的内存多不多)时间复杂度时间复杂度的四种类型最好时间复杂度指的是在理想情况(最好情况)下，执行这段代码的时间复杂度最坏时间复杂度指的是在最坏情况下，执行这段代码的时间复杂度平均时间复杂度指的是概率论中的加权平均值，也叫作期望值，所以平均时间复杂度的全称应该叫加权平均时间复杂度或者期望时间复杂度其中$A(n)$表示平均时间复杂度，$S$是规模为$n$的实例集，实例$ I \in S$的概率为$P_I$，算法对实例$I$执行的基本运算次数是$t_I$均摊时间复杂度均摊时间复杂度(amortized time complexity)，它对应的分析方法为摊还分析或者平摊分析听起来与平均时间复杂度有点类似，比较容易弄混，平均复杂度只在某些特殊情况下才会用到而均摊时间复杂度应用的场景比它更加特殊、更加有限对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间存在前后连贯的时序关系，这个时候，我们就可以将这一组操作放在一块儿分析，看是否能将较高时间复杂度那次操作的耗时，平摊到其他那些时间复杂度比较低的操作上在能够应用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度空间复杂度时间复杂度的全称是渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系类比一下，空间复杂度全称就是渐进空间复杂度，表示算法的存储空间与数据规模之间的增长关系它的分析规则与时间复杂度一样，也是只要高阶项，不要低阶项算法种类🌿暂时留坑，后续总结算法时再慢慢补基本算法枚举贪心递归和分治递推排序算法图算法哈夫曼编码，树的遍历，最短路径算法，最小生成树算法数据结构代数算法</content>
	</entry>
	<entry>
		<title>md数学公式和emoji表情</title>
		<url>/article/21745.html</url>
		<content>表情⚓表情网站⛪emoji表情官网⛺timwhitlock表情🌴常用表情😀 😃 🥰 😍 😘❤️ ✨ ⭐ ❗❓ ❕❔✊ ✌️ ✋ ✋ ☝️ 👏 🤝☀️ ☔ ☁️ ❄️ ⛄ ⚡ ⛅⛪ ⛺ ⛲ ⛵ ⛵ ⚓ ✈️ ⛽ ⚠️ ♨️1️⃣ 2️⃣ 3️⃣ 4️⃣ 5️⃣ 6️⃣ 7️⃣ 8️⃣ 9️⃣ 0️⃣️⃣ ◀️ ⬇️ ▶️ ⬅️ ↙️ ↘️ ➡️ ⬆️ ↖️ ↗️⏬ ⏫ ⤵️ ⤴️ ↩️ ↪️ ↔️ ↕️ ⏪ ⏩ ℹ️ ️ ️ ️♿ ㊙️ ㊗️ Ⓜ️ ⛔ ✳️ ❇️ ✴️♈ ♉ ♊ ♋ ♌ ♍ ♎ ♏ ♐ ♑ ♒ ♓ ⛎❎ ️ ️ ️ ♻️ ©️ ®️ ™️ ❌ ❗ ‼️ ⁉️ ⭕ ✖️ ➕ ➖ ➗ ✔️☑️ ➰ 〰️ 〽️ ▪️ ▫️ ◾ ◽ ◼️ ◻️ ⬛ ⬜ ✅ ⚫ ⚪🌿 🍃 🌾 🌴 🌲 🥝 🥑 🍇 🍒 🍐 🌼 🍁 🥭 🌸 🍄🐋 🐬 🐶 🐳 🐰 🦀☝️ ⭐ 👉 👀🌚 🌝 🧡ྀི 💛ྀི ❤️ྀི❤️ 🌈 💯 🐾 🍭 🥂 🎼 🏀 👏Latex公式Markdown 数学公式总结🌈以下部分公式，hexo无法解析，可能需要装什么插件支持或者是修改源码，不太清楚，有谁知道的可以评论留言~通用符号上下标算式markdowna0, a{pre}$ a^0, a^{[0]} $a^0, a^{[0]}括号算式markdown(, )[, ]\lang, \rang 或 \langle, \rangle\lvert, \rvert\lVert, \rVert\lbrace, \rbrace 或 {, }增大括号方法如下表：算式markdown(x)\big( x \big)\Big( x \Big)\bigg( x \bigg)\Bigg( x \Bigg)取整符号算式markdown结束$\lfloor x \rfloor$\lfloor x \rfloor向下取整$\lceil x \rceil$\lceil x \rceil向上取整分数算式markdown\frac{a}{b}开方算式markdown\sqrt{a + b}\sqrt[n]{a + b}累加/累乘算式markdown\sum_{i = 0}^{n}\frac{1}{i^2}\prod_{i = 0}^{n}\frac{1}{x^2}符号堆叠算式markdown说明\overset {@} {ABC}上堆叠\underset {@} {ABC}下堆叠X \overset {a} { \underset{b} { \to } }Y上下堆叠三角函数算式markdown\sin\cos\tan\cot\sec\csc\bot\angle40^\circ对数函数算式markdown\ln{a + b}\log_{a}^{b}\lg{a + b}二元运算符算式markdown描述\pm正负号\mp负正号\times乘号\div除号\ast星号\star\mid竖线\nmid\circ圈\bullet\cdot点\wr\diamond\Diamond\triangle\bigtriangleup\bigtriangledown\triangleleft\triangleright\lhd\rhd\unlhd\unrhd\circ\bigcirc\odot\bigodot点积\oslash\ominus\otimes\bigotimes克罗内克积\oplus\bigoplus异或\dagger\ddagger\amalg关系符号算式markdown描述\leq小于等于\geq大于等于\equiv全等于\models\prec\succ\sim\perp\preceq\succeq\simeq\mid\ll\gg\asymp\parallel\approx\cong\neq不等于\doteq\propto\bowtie\Join\smile\frown\vdash\dashv极限算式markdown\lim\rightarrow\infty\lim_{n\rightarrow+\infty}n向量算式markdown\vec{a}箭头算式markdown\uparrow\downarrow\updownarrow\Uparrow\Downarrow\Updownarrow\rightarrow\leftarrow\leftrightarrow\Rightarrow\Leftarrow\Leftrightarrow\longrightarrow\longleftarrow\longleftrightarrow\Longrightarrow\Longleftarrow\Longleftrightarrow\mapsto\longmapsto\hookleftarrow\hookrightarrow\rightharpoonup\leftharpoondown\rightleftharpoons\leftharpoonup\rightharpoondown\leadsto\nearrow\searrow\swarrow\nwarrow集合算式markdown描述 、\emptyset或\varnothing空集\in属于\ni\notin不属于\subset子集\supset\not\subset非子集\subseteq真子集\supseteq\cup并集\bigcup并集\cap交集\bigcap交集\uplus多重集\biguplus多重集\sqsubset\sqsupset\sqcap\sqsubseteq\sqsupseteq\vee\wedge\setminus集合中的减法微积分算式markdown描述\prime\int积分\iint双重积分\iiint三重积分\oint曲线积分\nabla梯度\int_0^2 x^2 dx其他的积分符号类似逻辑运算算式markdown描述\because因为\therefore所以\forall任意$ \exist $\exist存在\vee逻辑与\wedge逻辑或\bigvee逻辑与\bigwedge逻辑或上下标符号算式markdown\bar{a}\acute{a}\breve{a}\grave{a}\dot{a}\ddot{a}\hat{a}\check{a}\breve{a}\tilde{a}\vec{a}\overline{a + b + c + d}\underline{a + b + c + d}\overbrace{a + b + c + d}\underbrace{a + b + c + d}\overbrace{a + \underbrace{b + c}_{1.0} + d}^{2.0}希腊字母大写markdown小写markdown\Alpha\alpha\Beta\beta\Gamma\gamma\Delta\delta\Epsilon\epsilon\varepsilon\Zeta\zeta\Eta\eta\Theta\theta\Iota\iota\Kappa\kappa\Lambda\lambda\Mu\mu\Nu\nu\Xi\xi\Omicron\omicron\Pi\pi\Rho\rho\Sigma\sigma\Tau\tau\Upsilon\upsilon\Phi\phi\varphi\Chi\chi\Psi\psi\Omega\omega省略号算式markdown描述\dots一般用于有下标的序列\ldots\cdots纵向位置比\dots稍高\vdots竖向\ddots空格算式markdown描述123!123空格距离：-3/18 em123,123空格距离：3/18 em123:123空格距离：4/18 em123;123 or 123 \ 123空格距离：5/18 em123 \quad 123空格距离：1 em123 \qquad 123空格距离：2 em上表中的 em 是指当前文本中文本的字体尺寸其他符号算式markdown\aleph\hbar\imath\jmath\ell\wp\Re\Im\mho\nabla\surd\top\bot\neg\flat\natural\sharp\backslash\partial\Box\clubsuit\diamondsuit\heartsuit\spadesuit分支公式12345678$$y=\begin{cases}-x,\quad x\leq 0\\x, \quad x&gt;0\end{cases}\tag{1}$$矩阵不带括号12345678$$\begin{matrix}1 &amp; 2 &amp; 3\\4 &amp; 5 &amp; 6 \\7 &amp; 8 &amp; 9\end{matrix}\tag{1}$$括号，left后的括号 可以换为[或{12345678\left(\begin{matrix}1 &amp; 2 &amp; 3\\4 &amp; 5 &amp; 6 \\7 &amp; 8 &amp; 9\end{matrix}\right)\tag{2}分块矩阵1234567\left(\begin{array}{c|cc}     1  &amp;  0   &amp; 0 \\     \hline   -5  &amp;  2   &amp; 3\\ \end{array}\right)字体与颜色调整大小调整大小时，只需要在公式之前添加相应的标识，如 \Large1$$ \Large E=mc^2 $$取值有Huge、huge、LARGE、Large、large、normalsize(默认)、samll、footnotesize、scriptsize、tiny粗体斜体编码字体效果\rm罗马体\it意大利体(默认)\bf粗体\boldsymbol黑体\Bbb黑板粗体修改颜色注意typora里支持，但编译到hexo会有问题123$$P \left ( x_{l} \mid y_{l} \right) = \frac {P(x_l,y_l)} {P(y_l)} = \frac { \textcolor{#FF0000} {P(y_l \mid x_l)} P(x_l) } {P(y_l)}$$常用颜色：red：红色green：绿色blue：蓝色yellow：黄色花体字母123456789\mathbb{R}\mathcal{R}\mathscr{R}\mathrm{R}\mathbf{R}\mathit{R}\mathsf{R}\mathtt{R}\mathfrak{R}</content>
	</entry>
	<entry>
		<title>机器学习_线性代数与矩阵论(1)</title>
		<url>/article/47032.html</url>
		<content>写在前面，本系列主要是对下面这本书做的学习笔记向量及其运算基本概念线性代数是多元函数微积分的基础定义向量(Vector)是具有大小和方向的量，是由多个数构成的一维数组，每个数称为向量的分量，向量分量的数量称为向量的维数向量的表示物理中的力、速度以及加速度是典型的向量，$n$维向量$x$有$n$个分量，可以写成行向量的形式$(x_1 \cdots x_n)$通常将向量写成小写黑体斜体字符，如果写成列的形式则称为列向量，这些分量在列方向排列这些向量的分量是实数，则成为实向量，如果是复数，则成为复向量，$n$维实向量的集合记为$\mathbb{R}^n$与向量相对的是标量(Scalar)，标量只有大小而无方向，物理中的时间、质量以及电流是典型的标量在数学中通常把向量表示成列向量，而计算机中通常按行存储二维平面内的一个向量，其在$x$轴方向和$y$轴方向的分量分别为$3$和$1$，写成行向量形式为$\left[ \begin{matrix} 3 &amp; 1 \end{matrix} \right]$图中的向量以虚线箭头表示，起点为原点，终点是以向量的分量为坐标的点，三维空间中的力是三维向量，写成向量形式为力的加法遵守平行四边形法则零向量所有分量全为$0$的向量称为零向量，即为$0$，它的方向是不确定的向量与空间的点是一一对应的，向量$x$是以原点为起点，以$x$点为终点在机器学习中，样本数据通常用向量的形式表达，称为特征向量(Feature Vectos)，用于描述样本的特征但是这里的特征向量和矩阵的特征向量是不同的概念，不要混淆基本运算转置运算(Transpose)将列向量变成行向量，将列向量转行向量，向量$x$的转置记为$x^T$加法两个向量的加法定义为对应分量相加，要求参与运算的两个向量维数相等向量$x$和$y$相加记为$x+y$，比如$\left[ \begin{matrix} 1 &amp; 0 &amp; 0 \end{matrix} \right] + \left[ \begin{matrix} 4 &amp; 0 &amp; 1 \end{matrix} \right] = \left[ \begin{matrix} 5 &amp; 2 &amp; 4 \end{matrix} \right]$这与力的加法的平行四边形法则一致，是其在高维空间的推广向量满足交换律和结合律减法两个向量的减法为它们对应分量相减，同样要求参与运算的两个向量维数相等与向量加法的平行四边形法则相对应，向量减法符合三角形法则，$x-y$的结果是以$y$为起点，以$x$为终点的向量乘积向量$x$与标量$k$的乘积$kx$定义为标量与向量的每个分量相乘，比如乘积运算可以改变向量的大小和方向加法和数乘满足分配律内积两个向量$x$和$y$内积(Inner Product)定义为它们对应分量乘积之和内积可以记为$x \cdot y$两个$n$维向量的内积运算需要执行$n$次乘法运算和$n-1$次加法运算内积运算满足下面的规律利用内积可以简化线性函数(一次函数)的表述对于机器学习中广泛使用的线性模型的预测函数$\omega _1x_1 + \cdots \omega _nx_n + b$定义系数(权重)向量$\omega = (\omega _1 \cdots \omega _n)^T$，输入向量$x = (x_1 \cdots x_n)^T$，$b$为偏置项，预测函数写成向量内积的话为向量与自身内积的结果为其所有分量的平方和，即$x^Tx=\sum _{i=1}^{n}{x_i^2}$两个向量的内积为$0$，则称它们正交正交是几何垂直这一概念在高维空间的推广阿达玛积两个向量的阿达玛(Hadamard)积定义为它们对应分量相乘，结果为相同维数的向量，记为$x \odot y$对于两个向量它们的阿达玛积为阿达玛积可以简化问题的表述，在反向传播算法、各种梯度下降法中被使用向量的范数定义向量的范数(Norm)是向量的模(长度)这一概念的推广，向量的$L-p$范数是一个标量，定义为$p$为整数，常用的是$L1$和$L2$范数，$p$的取值分别为$1$和$2$L1范数L2范数无穷范数$L1$范数是所有分量的绝对值之和对于向量$x = \left[ \begin{matrix} 1 &amp; -1 &amp; 2 \end{matrix} \right]$的$L1$范数为$||x||_1 = |1|+|-1|+|2| = 4$$L2$范数也称为向量的模。即向量的长度，定义为长度为$1$的向量称为单位向量，向量$x = \left[ \begin{matrix} 1 &amp; -1 &amp; 2 \end{matrix} \right]$的$L1$范数为$||x||_2 = \sqrt {1^2+(-1)^2+2^2} = \sqrt {6}$$L1$范数和$L2$范数被用于构造机器学习的正则化项向量范数默认指$L2$范数当$p=\infty$时，称为$L - \infty$范数，其定义为即向量分量绝对值的最大值，向量$x = \left[ \begin{matrix} 1 &amp; -1 &amp; 2 \end{matrix} \right]$的$L1$范数为$||x||_1 = 2$$L - \infty$范数是$L-p$范数的极限性质向量数乘之后的范数为$||kx|| = |k| \cdot ||x||$，显然有$x^Tx = ||x||_2^2$对于非$0$向量，通过数乘向量模的倒数，可以将向量单位化(标准化)，使其长度为$1$对于上面的$L2$范数，归一化之后为$\left[ \begin{matrix} \frac {1}{\sqrt 6} &amp; \frac {-1}{\sqrt 6} &amp; \frac {2}{\sqrt 6} \end{matrix} \right]$向量内积和$L2$范数满足著名的柯西-施瓦茨(Cauchy-Schwarz)不等式可以通过构造一元二次方程证明由于$(x+ty)^T (x+ty) = y^Tyt^2 + 2x^Tyt + x^Tx \geq 0$对于$t$的一元二次方程$y^Tyt^2+2x^Tyt+x^Tx = 0$，只有$x+ty=0$时才有实数解，根据二次方程的判别法则有即$(x^Ty)^2 \leq ||x||^2 ||y||^2$，当且仅当$x+ty=0$即两个向量成比例时不等式取等号向量内积、向量模与向量夹角之间的关系可以表示为$x^Ty = ||x|| \cdot ||y|| \cdot cos \theta$其中$\theta$为两个向量之间的夹角，其取值范围为$[0, \pi]$，变形后得到向量夹角计算公式当向量之间的夹角超过$\frac {\pi}{2}$时，它们的内积为负对于两个长度确定的向量，当夹角为$0$时它们的内积最大，此时$cos \theta=1$；夹角为$\pi$时它们的内积最小，此时$cos \theta=-1$这一结论常在梯度下降法和最速下降法的推导中被使用对于向量$x = \left[ \begin{matrix} 1 &amp; 1 &amp; 0 \end{matrix} \right]$、$y=\left[ \begin{matrix} 0 &amp; 1 &amp; 1 \end{matrix} \right]$，它们夹角的余弦为因此它们的夹角为$\frac {\pi}{3}$对于向量$x = \left[ \begin{matrix} 1 &amp; 0 &amp; 0 \end{matrix} \right]$、$y=\left[ \begin{matrix} 0 &amp; 1 &amp; 0 \end{matrix} \right]$，它们夹角的余弦为因此它们的夹角为$\frac {\pi}{2}$，两个向量正交，正好是$x$轴和$y$轴范数满足三角不等式，是平面几何中三角不等式的抽象将三角不等式两边同时平方，有以及欧氏距离两个向量相减之后的$L2$范数是它们对应的点之间的距离，称为欧氏距离，即$||x-y||$对于三维空间中的两个点$x_1 = \left[ \begin{matrix} 1 &amp; 2 &amp; 1 \end{matrix} \right]$与$x_2 = \left[ \begin{matrix} 1 &amp; 2 &amp; 3 \end{matrix} \right]$，它们之间的距离为除了欧氏距离还可以定义其他的距离一个将两个向量映射为实数的函数$d(x_1,x_2)$只要满足下面的性质，均可以作为距离函数非负性: 距离必须是非负的，对于$ \forall x_1,x_2 \in \mathbb {R}^n$，均有$d(x_1,x_2) \geq 0$对称性: 距离是对称的，对于$ \forall x_1,x_2 \in \mathbb {R}^n$，均有$d(x_1,x_2) = d(x_2, x_1)$三角不等式: 对于$ \forall x_1,x_2,x_3 \in \mathbb {R}^n$，均有$d(x_1,x_2) + d(x_2,x_3) \geq d(x_1, x_2)$这些性质是欧式几何中距离特性的抽象解析几何定义介绍下线性代数在解析几何中的应用，结论可以从二维平面和三维空间平面解析几何中直线方程为$ax+by+x=0$，空间解析几何中平面方程为$ax+by+cz+d = 0$将其推广到$n$维空间，得到超平面(Hyperplane)方程$\omega ^Tx+b = 0$法向量超平面中的$\omega$称为法向量，它与超平面内任意两个不同点之间连成的直线垂直图中黑色虚线为平面的法向量，它与平面垂直，对于平面内任意两点$x_1$和$x_2$，它们的连线(平面上虚线)均与法向量垂直事实上，如果这两个点在平面内，则它们满足平面方程，有$\omega ^Tx_1 + b = 0$和$\omega ^Tx_2 + b = 0$两式相减可以得到$\omega ^T(x_1-x_2) = 0$，因此法向量$\omega$与平面内任意两点之间的连线$x_1x_2$正交将线性方程式的两侧同时乘以一个非$0$的系数，表示的还是同一个超平面点到超平面的距离平面解析空间解析超平面在平面解析几何中，点$(x,y)$到直接的距离为在空间解析几何中，点到平面的距离为将其推广到$n$维空间，根据向量内积和范数可以计算出点到超平面的距离，对于上面定义的超平面，点$x$到它的距离为这与二维平面、三维空间中点到直线和平面的距离公式在形式上是统一的，在支持向量机的推导过程中会用到计算点$\left[ \begin{matrix} 1 &amp; 1 &amp; 1 &amp; 1 \end{matrix} \right]$到超平面$x_1-2x_2+x_3-3x_4+1=0$的距离线性相关性线性相关根据数乘和加法运算定义线性组合的概念，有向量组$x_1, \cdots ,x_l$，如果存在一组实数$k_1, \cdots ,k_l$使得则称向量$x$可由向量组$x_1, \cdots ,x_l$线性表达右侧称为向量组$x_1, \cdots ,x_l$的线性组合，$k_1, \cdots ,k_l$为组合系数对于向量组$x_1 = \left[ \begin{matrix} 1 &amp; 2 &amp; 3 \end{matrix} \right] \qquad x_2 = \left[ \begin{matrix} 1 &amp; 0 &amp; 2 \end{matrix} \right] \qquad x_3 = \left[ \begin{matrix} 0 &amp; 0 &amp; 1 \end{matrix} \right] $向量$x= x_1+2_x2+x_3 = \left[ \begin{matrix} 3 &amp; 2 &amp; 8 \end{matrix} \right] $，可由该向量组线性表达，组合系数为$\left[ \begin{matrix} 1 &amp; 2 &amp; 1 \end{matrix} \right]$对于向量组$x_1, \cdots ,x_l$，如果存在一组不全为$0$的数$kx_1, \cdots ,k_l$，使得则称这组向量线性相关，如果不存在一组不全为$0$的数使得上式成立，则称为这组向量线性无关，也称为线性独立线性无关线性相关意味着这组向量存在冗余，至少有一个向量可以由其他向量线性表达，如果$x_1 \neq 0$，则有比如下面的行向量线性无关$x_1 = \left[ \begin{matrix} 1 &amp; 0 &amp; 0 \end{matrix} \right] \qquad x_2 = \left[ \begin{matrix} 0 &amp; 1 &amp; 0 \end{matrix} \right] \qquad x_3 = \left[ \begin{matrix} 0 &amp; 0 &amp; 1 \end{matrix} \right] $给定组合系数$k_1$、$k_2$、$k_3$，有$k_1 \left[ \begin{matrix} 1 &amp; 0 &amp; 0 \end{matrix} \right] + k_2 \left[ \begin{matrix} 0 &amp; 1 &amp; 0 \end{matrix} \right] + k_3 \left[ \begin{matrix} 0 &amp; 0 &amp; 1 \end{matrix} \right] = \left[ \begin{matrix} k_1 &amp; k_2 &amp; k_3 \end{matrix} \right]$欲使该向量为$0$，则有$k_1=k_2=k_3=0$，因此这组向量线性无关下面的行向量线性相关，因为存在系数$\left[ \begin{matrix} 1 &amp; 1 &amp; -1 \end{matrix} \right]$是的向量为$0$$x_1 = \left[ \begin{matrix} 1 &amp; 1 &amp; 0 \end{matrix} \right] \qquad x_2 = \left[ \begin{matrix} 2 &amp; 2 &amp; 0 \end{matrix} \right] \qquad x_3 = \left[ \begin{matrix} 3 &amp; 3 &amp; 0 \end{matrix} \right] $极大线性无关组一个向量组数量最大的线性无关向量子集称为极大线性无关组给定向量组，如果线性无关，但任意加入一个向量之后线性相关则是极大线性无关组，极大线性无关组不唯一$n$维向量的极大线性无关组最多有$n$个向量，这意味着任意一个向量均可以由$n$个线性无关的$n$维向量线性表达向量空间定义有$n$维向量的集合$X$，如果在其上定义了加法和数乘运算，且对两种计算封闭，即运算结果仍属于此集合，则称$X$为向量空间(Vector Sapce)，也称为线性空间，对于任意的向量$x,y \in X$都有$x+y \in X \qquad kx \in X$，则集合$X$为向量空间根据线性组合的定义，向量空间中任意向量的线性组合仍属于此空间设$S$是向量空间$X$的子集，如果$S$对加法和数乘运算都封闭，则称$S$为$X$的子空间例如，由三维实向量构成的集合$\mathbb {R}^3$是一个线性空间，显然对于任意$x,y \in \mathbb {R}^3$以及$k \in \mathbb {R}^3$，都有集合$S = { x \in \mathbb {R}^3, x_i &gt; 0 }$，即分量全为正的三维向量的集合不是线性空间，因为它对数乘不封闭$S$中的向量$x$数乘一个负数，结果向量的分量为负，不再属于该集合基(维数)向量空间的极大线性无关组称为空间的基，基所包含的向量数称为空间的维数如果$u_1, \cdots ,u_n$是空间的一组基，空间中的任意向量$x$均可由这组基线性表达$x = k_1u_1 + cdots + k_nu_n$则$k_1, \cdots k_n$称为向量$x$在这组基下的坐标正交基如果基向量$u_1, \cdots ,u_n$相互正交则称为正交基，如果基向量相互正交且长度均为$1$则称为标准正交基向量组$\left[ \begin{matrix} 1 &amp; 0 &amp; 0 \end{matrix} \right] \qquad \left[ \begin{matrix} 0 &amp; 1 &amp; 0 \end{matrix} \right] \qquad \left[ \begin{matrix} 0 &amp; 0 &amp; 1 \end{matrix} \right]$为$\mathbb {R}^3$的一组标准正交基，其方向对应三维空间的$3$个坐标轴方向需要强调的是，空间的基和标准正交基不唯一格拉姆-施密特(Gram-Schmidt)正交化给定一组线性无关的向量，可以根据它们构造出标准正交基，用的是格拉姆-施密特(Gram-Schmidt)正交化具体方法: 给定一组非$0$且线性无关的向量$x_1, \cdots ,x_l$，格拉姆-施密特正交化先构造出一组正交基$u_1, \cdots ,u_l$然后将这组正交基进行标准化得到标准正交基$e_1, \cdots ,e_l$首先选择向量$x_1$作为一个正交基方向，令$u_1=x_1$然后加入，构造和的线性组合，使得它与正交，即由于与正交，因此有解得$\alpha _{21} = \frac {x_2^Tu_1}{u_1^Tu_1}$解释下这种做法的几何意义，由于$x_2^Tu_1 = ||x_2||||u_1|| cos \theta$因此$\frac {x_2^Tu_1}{||u_1||} = ||x_2|| cos \theta$就是$x_2$在$u_1$方向上投影向量的长度，是图中直角三角形$ABC$的直角边$AB$的长度，这里$x_2$是三角形的斜边$AC$由于$\frac {u_1}{||u_1||}$是$u_1$方向的单位向量，$\frac {x_2^Tu_1}{||u_1||} \frac {u_1}{||u_1||} = \frac {x_2^Tu_1}{u_1^Tu_1} u_1$就是$x_2$在$u_1$方向上的投影向量，是图中的向量$AB$根据向量减法的三角形法则，$x_2- \frac {x_2^Tu_1}{u_1^Tu_1} u_1$就是图中的向量$BC$，与$u_1$垂直加下来加入$x_3$，构造出$u_3$，是$u_1$、$u_2$和$x_3$的线性组合，使得它与$u_1$及$u_2$均正交由于$u_3$与$u_1$正交，因此有而与正交，，因此可以解得由于$u_3$与$u_2$正交，因此有而与正交，，因此可以解得以此类推，在加入$x_k$时构造下面的线性组合由于它与均正交，因此而$u_j$与$u_i,i=1, \cdots, k-1 ,i \neq j$均正交，从而解得反复执行上述步骤，可以得到一组正交基$u_1, \cdots , u_l$将它们分别标准化，得到标准正交基$ \frac {u_1}{||u_1||} , \cdots , \frac {u_l}{||u_l||}$格拉姆-施密特正交化的几何意义首先考虑二维的情况图中向量$\frac {x_2^Tu_1}{u_1^Tu_1}u_1$与$u_1$同向，是向量$x_2$在$x_1$方向的投影，显然$x_2$减掉该投影之后的向量，即向量$u_2$，与$u_1$垂直下面考虑三维的情况首先构造出$u_2$，与二维平面的方法相同，保证$u_2$与$u_1$垂直，然后处理$x_3$，首先减掉其在$u_1$方向的投影，保证相减之后与$u_1$垂直，然后减掉在$u_2$方向的投影，保证与$u_2$垂直下面举例说明，有如下的向量组首先生成$u_1=x_2= \left[ \begin{matrix} 1 \ 0 \ 1 \end{matrix} \right]$然后生成$u_2$，组合系数为因此最后生成$u_3$，组合系数为以及因此最后对$u_1$、$u_2$和$u_3$进行单位化即为一组标准正交基应用之线性回归应用之线性分类器与支持向量机矩阵及其运算基本概念定义矩阵$A$是二维数组，一个$m \times n$的矩阵有$m$行和$n$列，每个位置$(i,j)$处的元素$a_{i,j}$是一个数，记为矩阵通常用大写的黑体、斜体字母表示矩阵的元素可以是实数，称为实矩阵，元素为复数，称为复矩阵，全体$m \times n$实矩阵的集合记为$\mathbb R ^{m \times n}$方阵如果矩阵行数和列数相等，则称为方阵，$n \times n$的方阵称为$n$阶方阵对称矩阵如果一个方阵的元素满足，则称为对称矩阵，比如对角矩阵矩阵所有行号和列号相等的元素$a_{ii}$的全体称为主对角线，如果一个矩阵出主对角线之外所有的元素均为$0$，则称为对角矩阵该对角矩阵可以简记为$diag(1,2,3)$，通常将对角矩阵记为$A$单位矩阵如果矩阵的主对角线的元素为$1$，其他元素为$0$，则称为单位矩阵，记为$I$单位矩阵的作用类似于实数中的$1$，在矩阵乘法中会说明，$n$阶单位矩阵记为$I_n$零矩阵如果矩阵的所有元素都为$0$，则称为零矩阵，记为$0$，其作用类似于实数中的$0$上三角矩阵如果矩阵的主对角线下方的元素全为$0$，则称为上三角矩阵下三角矩阵如果矩阵的主对角线上方的元素全为$0$，则称为下三角矩阵格拉姆(Gram)矩阵一个向量组的格拉姆(Gram)矩阵是一个$n \times  n$的矩阵，其每一个矩阵元素为向量与的内积，即由于$x_i^Tx_j = x_j^Tx_i$，因此格拉姆矩阵是一个对称矩阵对于向量，其格拉姆矩阵为在机器学习中该矩阵常被使用，比如主成分分析、核主成分分析、线性判别分析、线性回归、logisitic回归以及支持向量机的推导和证明基本运算转置矩阵的转置(Transpose)定义为行和列下标相互交换，一个$m \times n$的矩阵转置之后为$n \times m$的矩阵，矩阵$A$的转置记为$A^T$加法矩阵的加法为对应位置的元素相加，需要保证两个矩阵有相同的尺寸，矩阵$A$和$B$相加记为$A+B$加法和转置满足$(A+B)^T = A^T+B^T$加法满足交换律和结合律$A+B=B+A \qquad A+B+C = A+(B+C)$数乘矩阵和标量的乘法即数乘，定义为标量和矩阵每个元素相乘，矩阵$A$和$k$数乘记为$kA$数乘和加法满足分配律$k(A+B) = kA + kB$乘法矩阵乘法定义为第一个矩阵的每个行向量和第二个矩阵的每个列向量做内积，形成结果矩阵的每个元素，矩阵相乘记为$AB$要求第一个矩阵的列数要等于第二个矩阵的行数结果矩阵第$i$行第$j$列位置处的元素为$A$的第$i$行与$B$的第$j$列的内积结果矩阵的每个元素需要$p$次乘法运算、$p-1$次加法运算得到，结果矩阵有$m \times n$个元素因此，矩阵乘法需要$m \times n \times p$次乘法和$m \times n \times (p-1)$次加法使用矩阵乘法可以简化线性方程组的表述，对于如下的线性方程组定义系数矩阵为定义解向量和常数向量为既可以将方程组写成矩阵的形式$Ax=b$这种表示可以与一元一次方程$ax=b$达成形式上的统一，系数矩阵和常数向量合并之后称为增广矩阵，比如以上的增广矩阵为阿达玛积矩阵的阿达玛积定义为对应位置元素乘积形成的矩阵，记为$A \odot B$矩阵分块表示对于下面的矩阵可以将其分块为其中如果矩阵的子矩阵为$0$矩阵，或者单位矩阵等特殊类型的矩阵，这边表示会非常有效如果矩阵$A,B$分块后各块的尺寸以及水平、垂直方向的块数量相容，那可以将块当做标量来计算乘积$AB$如果各个位置处对应的两个字块尺寸相容，那么可以进行矩阵乘积运算🌰举个分块乘法的例子将$A$分为4块将$B$分块为因此它们的乘积为其中因此在多态正态分布中，将会对协方差矩阵进行分块特性1️⃣单位矩阵与任意矩阵的左乘和右乘都等于该矩阵本身，即2️⃣矩阵$A$左乘对角矩阵$\Lambda = diag(k_1, \cdots, k_n)$相当于将$A$的第$i$行的所有元素都乘以$k_i$3️⃣矩阵$A$右乘对角矩阵$\Lambda = diag(k_1, \cdots, k_n)$相当于将$A$的第$i$列的所有元素都乘以$k_i$4️⃣向量组的格拉姆矩阵可以写成一个矩阵与其转置的乘积其中是所有向量按列形成的矩阵5️⃣矩阵的乘法满足结合律这些由标量乘法的结合律可推得6️⃣矩阵乘法和加法满足左分配律和右分配律注意矩阵的乘法不满足交换律，即一般情况下$AB \neq BA$7️⃣矩阵乘法和转置满足穿脱原则逆矩阵定义逆矩阵对应标量的倒数运算，对于$n$阶矩阵$A$，如果存在另一个$n$阶矩阵$B$，使得它们的乘积为单位矩阵对于$AB=I$，$B$称为$A$的右逆矩阵，对于$BA=I$，$B$称为$A$的左逆矩阵如果矩阵的左逆矩阵和右逆矩阵存在，则它们相等，统称为矩阵的逆，记为$A^{-1}$假设$B_1$是$A$的左逆，$B_2$是$A$的右逆，则有因此$B_1=B_2$非奇异矩阵和奇异矩阵如果矩阵的逆矩阵存在，则称其可逆(Invertable)。可逆矩阵也称为非奇异矩阵，不可逆矩阵也称为奇异矩阵如果矩阵可逆，则其逆矩阵唯一假设$B$和$C$都是$A$的逆矩阵，则有$AB=BA=I$和$AC=CA-I$从而有$CAB=(CA)B=IB=B$和$CAB=C(AB)=CI=C$，因此B=C对于线性方程组，如果能得到系数矩阵的逆矩阵，方程两边同乘以该逆矩阵，可以得到方程的解这与一元一次方程的求解形式上是统一的$ax=b \Rightarrow x=a^{-1b}$如果对角矩阵$A$的主对角线非$0$，则其逆矩阵存在，且逆矩阵为对角矩阵，主对角线元素为矩阵$A$的主对角线元素的逆可以推出，上三角矩阵的逆矩阵仍然是上三角矩阵第1个等式与矩阵乘法的转置类似因此第1个等式成立，这里利用了矩阵乘法的结合律由于$AA^{-1}=I$根据逆矩阵的定义，第2个等式成立由于$(A^{-1})^TA^T = (AA^{-1})^T = I^T = I$根据逆矩阵的定义，第3个等式成立该等式可以证明对称矩阵的逆矩阵也是对称矩阵，用类似的方法可以证明第4个等式成立矩阵的秩矩阵的秩定义为矩阵线性无关的行向量或列向量的最大数量，记为$r(A)$该矩阵秩为$2$，该矩阵的极大线性无关组为矩阵的前两个行向量或列向量如果$n$阶方阵的秩为$n$，则称其满秩，矩阵可逆的充分必零条件是满秩对于$m \times n$的矩阵$A$，其秩满足$r(A) \leq min(m,n)$，即矩阵的秩不超过其行数和列数的较小值矩阵的秩相关结论初等行变换所谓矩阵的初等行变换是指以下3种变换用一个非零的数$k$乘矩阵的某一行把矩阵的某一行的$k$倍加到另一行，这里的$k$是任意实数互换矩阵的两行🌴初等变换是单位矩阵$I$经过一次初等变换之后得到的矩阵对于第一种初等行变化，对应的初等矩阵和逆矩阵分别如下，这意味着将单位矩阵的第$i$行乘以$k$，然后再乘以$\frac{1}{k}$，将变为单位矩阵对于第二种初等行变化，对应的初等矩阵和逆矩阵分别如下，意味着将单位矩阵的第$i$行乘以$k$之后加到第$j$行，然后再将第$i$行乘以$-k$之后加到第$j$行，将变为单位矩阵对于第三种初等行变化，对应的初等矩阵和逆矩阵相等，这意味着，将单位矩阵的第$i$行和$j$行互换，然后再互换一次，将变为单位矩阵🍒对矩阵做初等变换，等价于左乘对应的初等矩阵对于第一种行变换对于第二种行变换对于第三种行变换初等行变换计算逆矩阵如果矩阵$A$可逆，则可以利用初等行变换将其变换为单位矩阵，对应于一次左乘初等矩阵$P1,P2,\cdots,P_s$两侧同时右乘$A^{-1}$可以得到这意味着同样的初等行变换序列，在将矩阵$A$化为单位矩阵的同时，可将矩阵$I$化为$A^{-1}$，这就是逆矩阵🌰用初等行变换求解$A$的逆矩阵求解过程如下python的linalg库中的inv函数实现了逆矩阵的计算123import numpy as npA = np.array([[1,0,0], [0,1,0], [0,0,5]])B = np.linalg.inv(A)正交矩阵如果一个方阵满足则称为正交矩阵，正交矩阵的行向量均为单位向量且相互正交，构成标准正交基，对于矩阵按行分块，有因此有如果一个矩阵是正交矩阵，根据逆矩阵的定义，有下面是一个正交矩阵的例子可以验证其行向量和列向量均为单位向量，且相互正交正交矩阵的乘积仍然是正交矩阵，如果则正交矩阵的逆矩阵仍然是正交矩阵。如果有则正交矩阵的转置仍然是正交矩阵，因为而$A^{-1}$是正交矩阵，因此$A^{\mathrm{T}}$也是正交矩阵矩阵的范数诱导范数矩阵$\boldsymbol{W}$的范数定义为该范数通过向量的L-p范数定义，因此也称为诱导范数(Induced Norm)式中右侧分母为向量$x$的L-p范数，分子是经过矩阵对应的线性映射作用之后的向量的L-p范数因此诱导范数的几何意义是矩阵所代表的线性变换对向量进行变换后，向量长度的最大拉伸倍数谱范数如果$p=2$，此时诱导范数你为谱范数(Spectral Norm)F范数矩阵的Frobenius范数(F范数)定义为这等价于向量的L2范数，将矩阵按行或列展开之后形成向量，然后计算L2范数对于下面的矩阵其$\mathrm{F}$范数为根据柯西不等式，对于任意的$\boldsymbol{x}$，下面不等式成立如果$x \neq 0$，上式两边同时除以$|x|$可以得到因此$(\mathrm{F}$范数)是谱范数的一个上界矩阵的范数对于分析线性映射函数的特性有重要的作用，典型的应用是深度神经网络稳定性与泛化性能的分析线性变换定义矩阵与向量的乘法可以解释为线性变换(Linear Transformation)，它将一个向量变成另外一个向量对于线性空间$X$，如果在其上定义了一种变换(即映射)$A$，对任意$\boldsymbol{x} 、 y \in X$以及数域中的数$k$均满足以及即对加法和数乘具有线性关系，则称这种映射为线性变换线性变换对向量的加法与数乘运算具有线性矩阵乘法是一种线性变换，它满足线性变换的定义要求，对任意的向量$x, y \in \mathbb{R}^{n}$以及实数$k$有几何中的旋转变换是一种线性变换，下面以二维平面的旋转为例进行说明对于二维平面内的向量，其在极坐标系下的坐标为，从极坐标系到直角坐标系的转换公式为将极坐标为$(r \quad \alpha)^{\mathrm{T}}$的向量逆时针旋转$\alpha$度之后的结果向量$\boldsymbol{x}^{\prime}$的极坐标为，其直角坐标为因此旋转变换的变换矩阵为正交变换如果一个线性变换能保持向量之间的角度以及向量的长度不变，即变换之后两个向量的夹角不变，且向量的长度不变，则称为正交变换，正交变换对应的矩阵是正交矩阵下面给出证明: 如果$\boldsymbol{A}$是正交矩阵，使用它对向量$\boldsymbol{x}$进行变换之后的向量长度为变换之后向量长度不变，对向量$\boldsymbol{x}$和$\boldsymbol{y}$变换之后的内积为根据向量夹角公式内积和向量长度均不变，因此保持向量夹角不变旋转变换是正交变换，以二维平面的旋转矩阵为例，有旋转变换矩阵是正交矩阵，因此旋转变换是正交变换几何中的缩放变换也是一种线性变换，对于二维平面的向量，如果有下面的缩放变换矩阵则变换之后的向量为这相当于在方向拉伸2倍，在方向拉伸3倍缩放变换对应的矩阵为对角矩阵，主对角线元素为在该方向上的拉伸倍数，如果为负，则表示反向缩放变换和旋转变换被广泛应用于数字图像处理、计算机图形学，以及机器视觉等领域，实现对几何体和图像的旋转和缩放等操作行列式行列式(Determinant, det)是对矩阵的一种运算，它作用于方阵，将其映射成一个标量行列式的定义与性质$n$阶方阵$\boldsymbol{A}$的行列式记为$|\boldsymbol{A}|$或$\operatorname{det}(\boldsymbol{A})$，称为$n$阶行列式。计算公式为其中为正整数$1,2, \cdots, n$的一个排列，$S_{n}$是这$n$个正整数所有排列构成的集合， 显然有 种排列这里为排列的逆序数对于一个排列，如果，但，则称为一个逆序排列中所有逆序的数量称为排列的逆序数下面举例说明，对于3个正整数$1,2,3$，其所有排列的集合$S_{n}$为排列$3,2,1$的所有逆序为因此其逆序数为3排列$2,1,3$的所有逆序为因此其逆序数为1根据定义，$n$阶行列式的求和项有$n!$ 项，每个求和项中的表示按行号递增的顺序从$\boldsymbol{A}$的每一行各抽取一个元素相乘且这些元素的列号不能重复，它们的列号是的一个排列这里决定了求和项的符号，它意味着如果这些元素的列号排列的逆序数为偶数，则其值为1；如果为奇数，则为-1$n!$ 种排列中逆序数为奇数的排列和逆序数为偶数的排列各占一半，此求和项中正号和负号各占一半下面按照定义计算3阶行列式的值下面推导2阶和3阶行列式的计算公式，2阶矩阵的行列式的计算公式为下面的2阶行列式值为3阶矩阵的行列式的计算公式为下面的3阶行列式值为行列式可以表示平行四边形与平行六面体的有向面积和体积，也是线性变换的伸缩因子如果将方阵看作线性变换，则其行列式的绝对值表示该变换导致的体积元变化系数雅克比行列式被广泛应用于多元函数微分与积分的计算，代表了多元换元后的比例量按照定义，一个行列式可以按照行或列进行递归展开，称为拉普拉斯展开(Laplace Expan-sion)其中是去掉矩阵$\boldsymbol{A}$的第$i$行和第$j$ 后的$n-1$阶矩阵的行列式，并且带有符号$(-1)^{i+j}, i+j$为行号和列号之和，称为的代数余子式，不带符号的子行列式则称为余子式下面的行列式可以按第一行展开为特殊行列式的值某一行(列)全为0的行列式值为0，根据拉普拉斯展开可以得到此结论，根据行列式的定义也可以直接得到此结果，$n!$ 个求和项中每一项都必然包含某一 行列的一个元素，根据此结论，下面的行列式值为0根据定义，如果一个矩阵为对角矩阵，则其行列式为矩阵主对角线元素的乘积，这是因为$n !$ 个求和项中，除了全部由主对角线元素构成的项之外，其他的项的乘积中都含有0下面的对角矩阵的行列式值为单位矩阵的行列式为1上三角矩阵和下三角矩阵的行列式为其主对角线元素的乘积，这是因为$n$ ! 个求和项中，除了全部由主对角线元素构成的项之外，其他的项的乘积中都含有0根据这一结论有行列式的重要性质行列式具有多线性，可以按照某一行或列的线性组合拆分成两个行列式之和因为按照这一结论有如果行列式的两行或列相等，那么行列式的值为0，即下面给出证明，假设行列式的第$i$行和第$k$行相等，$n!$ 个求和项可以分成两组，即与由于且排列与的逆序数的奇偶性相反(二者通过一次置换可以互相得到)，因此这两项的符号相反，故行列式的值为0根据这一结论，下面的行列式为0根据这一结论可以构造出可逆矩阵的逆矩阵，对于矩阵假设是的代数余子式，利用它们构造如下的伴随矩阵根据拉普拉斯展开，第$i$行与其代数余子式的内积为行列式本身第$i$行与第$j, j \neq i$行的代数余子式的内积为0，这是因为它是第$j$行与第$i$行相等的行列式的拉普拉斯展开，其值为0因此有如果$|\boldsymbol{A}| \neq 0$，则有因此这也证明了矩阵$\boldsymbol{A}$可逆的充分必要条件是$|\boldsymbol{A}| \neq 0$如果把行列式的某一行元素都乘以$k$，则行列式变为之前的$k$倍，即如果将行列式的两行交换，行列式反号如果一个行列式的两行成比例关系，其值为0行列式的一行加上另一行的$k$倍，行列式值不变按照这一结论，下面两个行列式的值相等可以通过这种变换将矩阵化为三角矩阵，然后计算其行列式的值根据拉普拉斯展开可以证明下面的结论成立如果矩阵$A$和$B$是尺寸相同的$n$阶矩阵，则有即矩阵乘积的行列式等于矩阵行列式的乘积，下面给出证明，由于将$n+1$行乘以加到第1行，第行乘以加到第1行，… ， 将第行乘以加到第1行，可以得到对第$2 \sim n$行执行类似的操作，将上式右侧行列式的左上角全部消为0，最后可以得到上式第3步将行列式左侧的$n$列与右侧的$n$列对换，因此出现$(-1)^{n}$，第4步利用了拉普拉斯展开，第5步利用了对角矩阵的行列式计算公式，通常使用它计算矩阵乘积的行列式根据上式可以直接得到下面的结论: 如果矩阵可逆，则其行列式不为0，且其逆矩阵的行列式等于行列式的逆，即这是因为$A A^{-1}=I$，因此矩阵与标量乘法的行列式为其中$n$为矩阵的阶数，这可以根据行列式的定义直接证明，所有求和项中均变为，因此最后出现$\alpha^{n}$，根据这一结论有矩阵转置之后行列式不变这可以根据行列式的定义以及行列对换进行证明正交矩阵的行列式为$\pm 1$，如果$A$是正交矩阵，则有因此$|\boldsymbol{A}|=\pm 1$计算方法行列式的计算分为手动计算与编程计算两种方式手动计算对于手动计算，重点介绍将矩阵化为上三角矩阵的方法上三角矩阵或下三角矩阵的行列式是易于许算的，等于其主对角线元素的乘积根据下面的初等行变换将行列式的两行交换将行列式的某一行乘以$k$倍之后加到另外一行可以将行列式化为上三角形式，根据前面介绍的行列式的性质，第一种变换使得行列式的值反号，第二种变换保证行列式的值不变下面举例说明，对于下面的行列式将其化为上三角矩阵，然后计算行列式的值编程计算Python中linalg的det函数实现了计算方阵行列式的功能，下面是计算矩阵行列式的示例代码1234import numpy  A = np.array([[1,0,0],[0,1,0],[0,0,5]])d = np.linalg.det(A)print(d)程序运行结果为5，对角矩阵的行列式为主对角线元素的乘积</content>
	</entry>
	<entry>
		<title>docker基本使用</title>
		<url>/article/50622.html</url>
		<content>dockerdocker官方文档Docker 快速入门docker是什么Docker 是一个应用打包、分发、部署的工具你也可以把它理解为一个轻量的虚拟机，它只虚拟你软件需要的运行环境，多余的一点都不要而普通虚拟机则是一个完整而庞大的系统，包含各种不管你要不要的软件跟普通虚拟机的对比特性普通虚拟机Docker跨平台通常只能在桌面级系统运行，例如 Windows/Mac，无法在不带图形界面的服务器上运行支持的系统非常多，各类 windows 和 Linux 都支持性能性能损耗大，内存占用高，因为是把整个完整系统都虚拟出来了性能好，只虚拟软件所需运行环境，最大化减少没用的配置自动化需要手动安装所有东西一个命令就可以自动部署好所需环境稳定性稳定性不高，不同系统差异大稳定性好，不同系统都一样部署方式打包、分发、部署打包：就是把你软件运行所需的依赖、第三方库、软件打包到一起，变成一个安装包分发：你可以把你打包好的“安装包”上传到一个镜像仓库，其他人可以非常方便的获取和安装部署：拿着“安装包”就可以一个命令运行起来你的应用，自动模拟出一摸一样的运行环境，不管是在 Windows/Mac/Linuxdocker安装容器是由镜像实例化而来，简单来说，镜像是文件，容器是进程容器是基于镜像创建的，即容器中的进程依赖于镜像中的文件docker 的镜像概念类似虚拟机的镜像。是一个只读的模板，一个独立的文件系统，包括运行容器所需的数据，可以用来创建新的容器镜像运行起来就是容器，容器服务运行的过程中，基于原始镜像做了改变，比如安装了程序，添加了文件，也可以提交回去 (commit)成为镜像docker安装分为安装包和命令行两种win11安装包centos7命令行下载docker安装包，按照指示安装即可windows需要打开启用或关闭Windows功能下的Hyper-V、容器和适用于Linux的Windows子系统选项安装完要打开Docker Desktop软件，打开会启动引擎，这样才能使用docker重要：打开Docker Desktop软件，打开Settings下的Resources，选择Advanced下的Disk image location，选择合适的文件夹存放docker的镜像和容器，注意后期容器多的话，这个文件夹很占空间的1curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun配置镜像加速进入阿里云，登陆后点击左侧的镜像加速，生成自己的镜像加速地址执行阿里云推荐的终端命令，即可更新docker的镜像源为阿里云镜像12345678sudo mkdir -p /etc/dockersudo tee /etc/docker/daemon.json &lt;&lt;-'EOF'{  &quot;registry-mirrors&quot;: [&quot;https://xk5lrhin.mirror.aliyuncs.com&quot;]}EOFsudo systemctl daemon-reloadsudo systemctl restart docker代理配置为docker服务创建一个内嵌的systemd目录1mkdir -p /etc/systemd/system/docker.service.d创建/etc/systemd/system/docker.service.d/http-proxy.conf文件，并添加HTTP_PROXY环境变量，其中[proxy-addr]和[proxy-port]分别改成实际情况的代理地址和端口12[Service]Environment=&quot;HTTP_PROXY=http://192.168.4.xx:xx&quot; &quot;HTTPS_PROXY=http://192.168.4.xx:xx&quot;如果还有内部的不需要使用代理来访问的Docker registries，那么还需要制定NO_PROXY环境变量12[Service]Environment=&quot;HTTP_PROXY=http://192.168.4.xx:xx&quot; &quot;HTTPS_PROXY=http://192.168.4.xx:xx&quot; &quot;NO_PROXY=localhost,127.0.0.1,docker-registry.somecorporation.com&quot;更新配置并重启Docker服务12systemctl daemon-reloadsystemctl restart docker加速格式：”registry-mirrors”: [“https://registry.docker-cn.com“]镜像加速器镜像加速器地址Docker 中国官方镜像https://registry.docker-cn.comDaoCloud 镜像站http://f1361db2.m.daocloud.ioAzure 中国镜像https://dockerhub.azk8s.cn科大镜像站https://docker.mirrors.ustc.edu.cn阿里云https://ud6340vz.mirror.aliyuncs.com七牛云https://reg-mirror.qiniu.com网易云https://hub-mirror.c.163.com腾讯云https://mirror.ccs.tencentyun.com基本命令学习基本命令123456789101112131415161718192021# 查看版本号docker version# 启动dockersystemctl start docker# 关闭dockersystemctl stop docker# 重启dockersystemctl restart docker# docker设置随服务启动而自启动systemctl enable docker# 查看docker 运行状态systemctl status docker# 查看volume列表docker volume ls# 查看网络列表docker network ls镜像Docker默认拉取的架构是与主机的架构相同，当你在一个特定的架构上运行Docker命令时，Docker将自动尝试拉取与该架构匹配的镜像。例如，如果你在x86架构的主机上运行Docker命令，Docker将尝试拉取x86架构的镜像这意味着Docker会自动适配主机的架构，并拉取相应的镜像供使用。如果需要指定架构，可以加上--platform=arm64参数1234567891011121314151617181920212223# 查看本地镜像docker images# 搜索镜像docker search neo4j# 拉取镜像  如果需要指定架构，可以加上`--platform=arm64`参数docker pull 镜像名:tag# 删除镜像 ------当前镜像没有被任何容器使用才可以删除docker rmi -f 镜像名/镜像ID# 删除多个 其镜像ID或镜像用用空格隔开即可docker rmi -f 镜像名/镜像ID 镜像名/镜像ID# 保存镜像，将我们的镜像保存为tar压缩文件，这样方便镜像转移和保存# 然后可以在任何一台安装了docker的服务器上加载这个镜像docker save 镜像名/镜像ID -o 镜像保存在哪个位置与名字docker save -o my_ubuntu_v3.tar runoob/ubuntu:v3# 加载镜像docker load --input my_ubuntu_v3.tar# 镜像重命名docker tag &lt;IMAGE_ID&gt; &lt;NEW_REPOSITORY&gt;:&lt;TAG&gt;容器1234567891011121314151617181920212223242526# 创建一个全新的容器，不会立即运行docker create  --name my_container nginx:latest# 创建和启动的组合，创建了一个新容器并立即启动它，-d表示后台docker run --name my_container -d nginx:latest# 启动已终止容器docker container start# 停止容器docker stop 容器ID或者容器名# 重启容器docker restart 容器ID或者容器名# 列出所有在运行的容器信息docker ps# 查看本地所有容器docker ps -a# 删除指定容器docker rm -f &lt;container_id0&gt; &lt;container_id1&gt;# 删除未启动成功的容器docker rm $(docker ps -a|grep Created|awk '{print $1}')# 删除所有未运行的容器docker rm $(docker ps -a|grep Created|awk '{print $1}')# 容器重命名docker rename CONTAINER NEW_NAME镜像打包工具  Buildpacks、DockerfileDocker, Dockerfile, 和Docker Compose区别 | BaeldungDocker Compose允许我们定义容器共享的共同对象。例如，我们可以一次性定义一个卷，然后把它挂在每个容器里，这样它们就可以共享一个共同的文件系统。或者，我们可以定义一个或多个容器用来通信的网络。Docker Compose只是一个协调多个容器的工具。其他选择包括Kubernetes、Openshift和Apache MesosDocker容器迁移到其他服务器的5种方法详解_docker目录挂载使用 Docker 运行后，我们改了项目代码不会立刻生效，需要重新build和run，很是麻烦容器里面产生的数据，例如 log 文件，数据库备份文件，容器删除后就丢失了目录挂载解决以上问题bind mount 直接把宿主机目录映射到容器内，适合挂代码目录和配置文件。可挂到多个容器上volume 由容器创建和管理，创建在宿主机，所以删除容器不会丢失，官方推荐，更高效，Linux 文件系统，适合存储数据库数据。可挂到多个容器上tmpfs mount 适合存储临时文件，存宿主机内存中。不可多容器共享演示12345# bind mount 方式docker run -p 8080:8080 --name test-hello -v D:/code:/app -d test:v1# volume 方式docker run -p 8080:8080 --name test-hello -v db-data:/app -d test:v1多容器通信虚拟网络要想多容器之间互通，从 Web 容器访问 Redis 容器，我们只需要把他们放到同个网络中就可以了创建一个名为test-net的网络：1docker network create test-net运行 Redis 在 test-net 网络中，别名redis12# network-alias指的是网络的别名docker run -d --name redis --network redis-net --network-alias redis redis:latest使用时url填写redis：//redis:6379运行 Web 项目，使用同个网络1docker run -p 8080:8080 --name test -v D:/test:/app --network redis-net -d test:v0Docker-Compose假设运行了两个容器：Web 项目 + Redis如果项目依赖更多的第三方软件，我们需要管理的容器就更加多，每个都要单独配置运行，指定网络我们可以使用 docker-compose 把项目的多个服务集合到一起，一键运行安装 Docker Compose如果你是安装的桌面版 Docker，不需要额外安装，已经包含了如果是没图形界面的服务器版 Docker，你需要单独安装 安装文档运行docker-compose检查是否安装成功编写脚本要把项目依赖的多个服务集合到一起，我们需要编写一个docker-compose.yml文件，描述依赖哪些服务容器默认时间不是北京时间，增加 TZ=Asia/Shanghai 可以改为北京时间1234567891011121314151617181920version: &quot;3.7&quot;services:  app:    build: ./  # 或者直接使用镜像  image: narutohyc/ubuntu:v0    ports:      - 80:8080    volumes:      - ./:/app    environment:      - TZ=Asia/Shanghai  redis:    image: redis:5.0.13    volumes:      - redis:/data    environment:      - TZ=Asia/Shanghaivolumes:  redis:在docker-compose.yml 文件所在目录，执行：docker-compose up就可以跑起来了，文档在这在后台运行只需要加一个 -d 参数docker-compose up -d动作命令查看运行状态docker-compose ps停止运行docker-compose stop重启docker-compose restart重启单个服务docker-compose restart service-name进入容器命令行docker-compose exec service-name sh查看容器运行logdocker-compose logs [service-name]hello world创建dockfile，新建一个Dockerfile文件123456789# 使用基础镜像，选择适合您的环境FROM ubuntu:20.04# 安装依赖库和工具RUN apt updateRUN apt-get install -y wget# 设置工作目录WORKDIR /app构建镜像，在Dockerfile目录下，执行以下命令，-t后面跟的是镜像名和版本，这里会构建一个fastchat镜像，版本号为v0，1docker build -t fastchat:v0 .如果出现网络问题，需要配置代理，可以带上--build-arg参数1docker build -t fastchat:v0 --build-arg http_proxy=http://192.168.0.xx:xx --build-arg https_proxy=http://192.168.0.xx:xx .如果需要指定dockfile文件，可以使用-f参数1docker build -f Q:\pyCharmWS\chatgpts\dockfiles\Dockerfile-ubuntu -t ubuntu:v0 . 除了自己写dockfile，还可以使用docker pull拉取现成的镜像查看镜像列表1234docker imagesREPOSITORY   TAG       IMAGE ID       CREATED          SIZEfastchat     v0        20ee81939779   20 seconds ago   115MB启动镜像123docker run -itd -p 8080:80 --name ubuntu-test fastchat:v0&gt;&gt;&gt; 918a3b2ccc42436e01c5033021ba97d5bd4c56df48b2c549bf01855d9979333cdocker run -itd表示在后台运行容器，并允许与容器进行交互，同时为容器分配一个伪终端将容器的80端口映射到主机的8080端口--name ubuntu-test用于指定容器的名称为`ubuntu-test，返回一个容器id如果已执行就退出’Exit(0)’，可以加上参数--entrypoint=/bin/bash，这里会自动退出是因为Docker容器后台运行，就必须有一个前台进程。容器运行的命令如果不是那些一直挂起的命令(比如运行top，tail)就是会自动退出的查看容器列表1234docker psCONTAINER ID   IMAGE         COMMAND       CREATED          STATUS          PORTS     NAMES918a3b2ccc42   fastchat:v0   &quot;/bin/bash&quot;   20 seconds ago   Up 19 seconds             ubuntu-testCONTAINER ID指的是容器ID进入容器exec方式attach方式1docker exec -it ubuntu-test bash输入exit退出，container是不会被关闭的1docker attach ubuntu-test使用快捷键退出：按下键盘上的Ctrl + P，然后按下Ctrl + Q，container是不会被关闭的或者输入exit退出，container是会被关闭的这个组合键可以分离终端与容器的连接，但不会停止容器的运行，将返回到宿主机的终端，而容器会继续在后台运行停止容器1docker stop ubuntu-test当您执行docker stop命令停止容器后，容器的名称可能仍然保留在Docker中。如果您尝试使用相同的容器名称再次运行容器，会出现”重名”的错误123Q:\pyCharmWS\chatgpts\dockfiles&gt;docker run -itd --name ubuntu-test fastchat:v0docker: Error response from daemon: Conflict. The container name &quot;/ubuntu-test&quot; is already in use by container &quot;918a3b2ccc42436e01c5033021ba97d5bd4c56df48b2c549bf01855d9979333c&quot;. You have to remove (or rename) that container to be able to reuse that name.See 'docker run --help'.这是因为Docker要求容器名称在给定的命名空间中必须是唯一的。当您停止容器时，该容器的名称不会立即从Docker中删除，以便您可以查看已停止容器的状态和日志等信息这时候可以更改原来容器的名称或删除已停止的容器docker rename ubuntu-old ubuntu-test123Q:\pyCharmWS\chatgpts\dockfiles&gt;docker ps -aCONTAINER ID   IMAGE         COMMAND       CREATED          STATUS                        PORTS     NAMES918a3b2ccc42   fastchat:v0   &quot;/bin/bash&quot;   41 minutes ago   Exited (130) 11 minutes ago             ubuntu-testdocker rm ubuntu-test重启容器1docker restart ubuntu-test将容器保存为镜像，以便稍后再次使用或与其他人共享要将容器保存为镜像，可以使用docker commit命令。以下是保存容器为镜像的步骤：确保您的容器处于停止状态。如果容器正在运行，请先停止它：1docker stop &lt;container_name_or_id&gt;使用docker commit命令将容器保存为镜像。在命令中指定容器的名称或ID以及要为新镜像指定的名称和标签：1docker commit &lt;container_name_or_id&gt; &lt;new_image_name:tag&gt;例如：1docker commit mycontainer myimage:v1这将创建一个名为myimage，标签为v1的新镜像等待docker commit命令完成，它会将容器的文件系统和元数据保存为新的镜像。一旦完成，您可以使用docker images命令查看新创建的镜像现在，您已经将容器保存为新的镜像。您可以使用该镜像创建新的容器，或将其推送到镜像仓库以供其他人使用请注意，使用docker commit命令保存容器为镜像时，镜像将包含容器中当前的文件系统状态和配置。这意味着如果容器中有任何不必要的文件或敏感信息，它们也会包含在保存的镜像中。因此，在保存容器之前，最好确保容器中不包含不必要的文件，并遵循最佳实践来保护敏感信息值得一提的是，docker commit 命令是将容器的状态保存为镜像，而不是推荐的方法更好的做法是使用 Dockerfile 来定义容器的配置和状态，并使用 docker build 命令构建镜像。这样可以更好地跟踪和管理镜像的变更dockfile构建一个基于ubuntu的anaconda环境123456789101112131415161718192021222324252627# 使用基础镜像，选择适合您的环境# FROM arm64v8/ubuntu:20.04FROM ubuntu:20.04# 安装依赖库和工具RUN apt updateRUN apt-get install -y wget# 下载并安装 Anaconda  https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/RUN wget -qO ~/anaconda.sh https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2023.03-0-Linux-x86_64.sh# RUN wget -qO ~/anaconda.sh https://mirrors.tuna.tsinghua.edu.cn/anaconda/archive/Anaconda3-2023.03-0-Linux-aarch64.shRUN /bin/bash ~/anaconda.sh -b -p /opt/anaconda3RUN rm ~/anaconda.sh# 将 Anaconda 添加到环境变量ENV PATH=&quot;/opt/anaconda3/bin:${PATH}&quot;RUN conda create -y -n chat38 python=3.8 anaconda# 设置工作目录WORKDIR /app# 指定全局 shell 环境，这样才能用sourceSHELL [&quot;/bin/bash&quot;, &quot;-c&quot;]# 切换py环境CMD source activate chat38RUN pip install simplejson除了SHELL [&quot;/bin/bash&quot;, &quot;-c&quot;]，还可以每次指定bash环境，CMD /bin/bash -c &quot;source activate chat38&quot;发布和部署要将镜像发布到 Docker Hub 上，需要按照以下步骤进行操作：注册 Docker Hub 账号，如果你还没有 Docker Hub 账号，首先需要在Docker Hub的官方网站上进行注册登录 Docker Hub，使用以下命令在终端中登录到 Docker Hub123Q:\pyCharmWS\chatgpts\dockfiles&gt;docker login -u narutohycPassword:Login Succeeded标记镜像，在发布到 Docker Hub 之前，需要为镜像添加一个适当的标签，以便将其与你的 Docker Hub 用户名和存储库关联起来，使用以下命令为镜像添加标签1docker tag &lt;IMAGE_ID&gt; &lt;DOCKER_HUB_USERNAME&gt;/&lt;REPOSITORY_NAME&gt;:&lt;TAG&gt;其中，&lt;IMAGE_ID&gt; 是你要发布的镜像的 ID，&lt;DOCKER_HUB_USERNAME&gt; 是你的 Docker Hub 用户名，&lt;REPOSITORY_NAME&gt; 是你想要为镜像设置的存储库名称，&lt;TAG&gt; 是一个可选的标签，用于标识镜像的版本号发布镜像，使用以下命令将标记后的镜像发布到 Docker Hub1docker push &lt;DOCKER_HUB_USERNAME&gt;/&lt;REPOSITORY_NAME&gt;:&lt;TAG&gt;这将把镜像推送到 Docker Hub 上的指定存储库中确认发布，登录到 Docker Hub 的网站，访问你的 Docker Hub 账号，你应该能够在相应的存储库中看到已发布的镜像请注意，发布到 Docker Hub 的镜像将成为公开可访问的，任何人都可以从 Docker Hub 上获取并使用该镜像。如果你希望限制访问权限，可以考虑使用私有仓库，或者在 Docker Hub 上创建一个私有的组织来管理镜像的访问权限示例演示 Docker 安装 Redis一个命令跑起来：docker run -d -p 6379:6379 --name redis redis:latest命令参考：https://docs.docker.com/engine/reference/commandline/run/安装 Wordpress定义docker-compose.yml文件12345678910111213141516171819202122232425262728293031version: '3.1'services:  wordpress:    image: wordpress    restart: always    ports:      - 8080:80    environment:      WORDPRESS_DB_HOST: db      WORDPRESS_DB_USER: exampleuser      WORDPRESS_DB_PASSWORD: examplepass      WORDPRESS_DB_NAME: exampledb    volumes:      - wordpress:/var/www/html  db:    image: mysql:5.7    restart: always    environment:      MYSQL_DATABASE: exampledb      MYSQL_USER: exampleuser      MYSQL_PASSWORD: examplepass      MYSQL_RANDOM_ROOT_PASSWORD: '1'    volumes:      - db:/var/lib/mysqlvolumes:  wordpress:  db:执行docker-compose up -d实战neo4jdocker安装部署neo4j安装neo4j拉取neo4j镜像从docker官方镜像中找合适的镜像1docker search neo4j拉取镜像源1docker pull neo4j(:版本号) # 缺省 &quot;:版本号&quot; 时默认安装latest版本的查看本地镜像，检验是否拉取成功1docker images图算法插件neo4j提供了一系列常用的图算法，该算法库需要单独安装，注意版本对应关系点击上方官网链接，下载Neo4j Graph Data Science下的文件，放到$NEO4J_HOME/plugins目录中将以下内容添加到您的$NEO4J_HOME/conf/neo4j.conf文件中，参考这里1dbms.security.procedures.unrestricted=apoc.*,gds.*此配置条目是必需的，因为 GDS 库访问 Neo4j 的低级组件以最大化性能重启 Neo4j，在docker下就是重启为了验证您的安装，可以输入以下命令1RETURN gds.version()要列出所有已安装的算法，请运行以下gds.list()过程1CALL gds.list()例子: neo4j实现PageRank算法启动neo4j找一个目录存放docker的各镜像运行目录，比如我这里选的是/home/huangyc/docker然后再在这个目录新建具体的镜像对应的文件夹/home/huangyc/docker/neo4j，在下面新建四个文件夹(实际上好像不需要手动建，执行命令时自动会新建)data: 数据存放的文件夹logs: 运行的日志文件夹conf: 数据库配置文件夹(在配置文件neo4j.conf中配置包括开放远程连接、设置默认激活的数据库)import: 为了大批量导入csv来构建数据库，需要导入的节点文件nodes.csv和关系文件rel.csv需要放到这个文件夹下plugins: 存放jar插件启动命令格式如下123456789docker run -d --name container_name \  //-d表示容器后台运行 --name指定容器名字-p 7474:7474 -p 7687:7687 \  //映射容器的端口号到宿主机的端口号-v /home/huangyc/docker/neo4j/data:/data \  //把容器内的数据目录挂载到宿主机的对应目录下-v /home/huangyc/docker/neo4j/logs:/logs \  //挂载日志目录-v /home/huangyc/docker/neo4j/conf:/var/lib/neo4j/conf   //挂载配置目录-v /home/huangyc/docker/neo4j/import:/var/lib/neo4j/import \  //挂载数据导入目录-v /home/huangyc/docker/neo4j/plugins:/var/lib/neo4j/plugins \  //挂载数据导入目录--env NEO4J_AUTH=neo4j/password \  //设定数据库的用户名和和密码neo4j //指定使用的镜像也可以写成单行命令1docker run -d --name neo4j_hyc -p 7474:7474 -p 7687:7687 -v /home/huangyc/docker/neo4j/data:/data -v /home/huangyc/docker/neo4j/logs:/logs -v /home/huangyc/docker/neo4j/conf:/var/lib/neo4j/conf -v /home/huangyc/docker/neo4j/import:/var/lib/neo4j/import -v /home/huangyc/docker/neo4j/plugins:/var/lib/neo4j/plugins --env NEO4J_AUTH=neo4j/hyc neo4jpy库使用Python访问图数据库，主要使用的库时py2neo和neo4j，py2neo通过操作python变量，达到操作neo4j的目的，同时也支持cypher语法neo4j主要时执行CQL(cypher)语句cypher 语法py2neoneo4j安装1pip install py2neo使用例子12345678910111213141516171819202122232425262728293031323334# coding:utf-8from py2neo import Graph, Node, Relationship # 连接neo4j数据库，输入地址、用户名、密码graph = Graph(&quot;http://192.168.1.106:7474&quot;, name=&quot;neo4j&quot;)graph.delete_all()# 创建结点test_node_1 = Node('ru_yi_zhuan', name='皇帝') # 修改的部分test_node_2 = Node('ru_yi_zhuan', name='皇后') # 修改的部分test_node_3 = Node('ru_yi_zhuan', name='公主') # 修改的部分 graph.create(test_node_1)graph.create(test_node_2)graph.create(test_node_3) # 创建关系# 分别建立了test_node_1指向test_node_2和test_node_2指向test_node_1两条关系，关系的类型为&quot;丈夫、妻子&quot;，两条关系都有属性count，且值为1。node_1_zhangfu_node_1 = Relationship(test_node_1, '丈夫', test_node_2)node_1_zhangfu_node_1['count'] = 1node_2_qizi_node_1 = Relationship(test_node_2, '妻子', test_node_1)node_2_munv_node_1 = Relationship(test_node_2, '母女', test_node_3) node_2_qizi_node_1['count'] = 1 graph.create(node_1_zhangfu_node_1)graph.create(node_2_qizi_node_1)graph.create(node_2_munv_node_1) print(graph)print(test_node_1)print(test_node_2)print(node_1_zhangfu_node_1)print(node_2_qizi_node_1)print(node_2_munv_node_1) 安装1pip install neo4j使用例子12345678910111213141516171819from neo4j import GraphDatabasedriver = GraphDatabase.driver(&quot;bolt://localhost:7687&quot;, auth=(&quot;neo4j&quot;, &quot;password&quot;))def add_friend(tx, name, friend_name):    tx.run(&quot;MERGE (a:Person {name: $name}) &quot;           &quot;MERGE (a)-[:KNOWS]-&gt;(friend:Person {name: $friend_name})&quot;,           name=name, friend_name=friend_name)def print_friends(tx, name):    for record in tx.run(&quot;MATCH (a:Person)-[:KNOWS]-&gt;(friend) WHERE a.name = $name &quot;                         &quot;RETURN friend.name ORDER BY friend.name&quot;, name=name):        print(record[&quot;friend.name&quot;])with driver.session() as session:    session.write_transaction(add_friend, &quot;Arthur&quot;, &quot;Guinevere&quot;)    session.write_transaction(add_friend, &quot;Arthur&quot;, &quot;Lancelot&quot;)    session.write_transaction(add_friend, &quot;Arthur&quot;, &quot;Merlin&quot;)    session.read_transaction(print_friends, &quot;Arthur&quot;)实战nebula基本介绍NebulaGraph 由三种服务构成：Graph 服务、Meta 服务和 Storage 服务，是一种存储与计算分离的架构Meta 服务Meta 服务是由 nebula-metad 进程提供的，用户可以根据场景配置 nebula-metad 进程数量：测试环境中，用户可以在 NebulaGraph 集群中部署 1 个或 3 个 nebula-metad 进程。如果要部署 3 个，用户可以将它们部署在 1 台机器上，或者分别部署在不同的机器上。生产环境中，建议在 NebulaGraph 集群中部署 3 个 nebula-metad 进程。请将这些进程部署在不同的机器上以保证高可用。所有 nebula-metad 进程构成了基于 Raft 协议的集群，其中一个进程是 leader，其他进程都是 follower。leader 是由多数派选举出来，只有 leader 能够对客户端或其他组件提供服务，其他 follower 作为候补，如果 leader 出现故障，会在所有 follower 中选举出新的 leader。Graph 服务Graph 服务是由nebula-graphd 进程提供，服务主要负责处理查询请求，包括解析查询语句、校验语句、生成执行计划以及按照执行计划执行四个大步骤，查询请求发送到 Graph 服务后，会由如下模块依次处理：Parser：词法语法解析模块。Validator：语义校验模块。Planner：执行计划与优化器模块。Executor：执行引擎模块。Storage 服务NebulaGraph 的存储包含两个部分，一个是 Meta 相关的存储，称为 Meta 服务，在前文已有介绍。另一个是具体数据相关的存储，称为 Storage 服务。其运行在 nebula-storaged 进程中环境搭建以nebula 3.2.1为例下载并安装NebulaGraph12wget https://oss-cdn.nebula-graph.com.cn/package/3.2.1/nebula-graph-3.2.1.el7.x86_64.rpmsudo rpm -ivh --prefix=/home/huangyc/nebula nebula-graph-3.2.1.el7.x86_64.rpm--prefix为可选项，用于指定安装路径如不设置，系统会将 NebulaGraph 安装到默认路径/usr/local/nebula/集群配置集群配置需要保证集群机器配置ssh免密修改每个服务器上的 NebulaGraph 配置文件NebulaGraph 的所有配置文件均位于安装目录的etc目录内，包括nebula-graphd.conf、nebula-metad.conf和nebula-storaged.conf，用户可以只修改所需服务的配置文件。各个机器需要修改的配置文件如下。机器名称待修改配置文件Anebula-graphd.conf、nebula-storaged.conf、nebula-metad.confBnebula-graphd.conf、nebula-storaged.conf、nebula-metad.confCnebula-graphd.conf、nebula-storaged.conf、nebula-metad.confDnebula-graphd.conf、nebula-storaged.confEnebula-graphd.conf、nebula-storaged.conf机器 A 配置nebula-graphd.conf1234567891011########## networking ########### Comma separated Meta Server Addresses--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559# Local IP used to identify the nebula-graphd process.# Change it to an address other than loopback if the service is distributed or# will be accessed remotely.--local_ip=192.168.10.111# Network device to listen on--listen_netdev=any# Port to listen on--port=9669nebula-storaged.conf123456789########## networking ########### Comma separated Meta server addresses--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559# Local IP used to identify the nebula-storaged process.# Change it to an address other than loopback if the service is distributed or# will be accessed remotely.--local_ip=192.168.10.111# Storage daemon listening port--port=9779nebula-metad.conf123456789########## networking ########### Comma separated Meta Server addresses--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559# Local IP used to identify the nebula-metad process.# Change it to an address other than loopback if the service is distributed or# will be accessed remotely.--local_ip=192.168.10.111# Meta daemon listening port--port=9559机器 D 配置nebula-graphd.conf1234567891011########## networking ########### Comma separated Meta Server Addresses--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559# Local IP used to identify the nebula-graphd process.# Change it to an address other than loopback if the service is distributed or# will be accessed remotely.--local_ip=192.168.10.114# Network device to listen on--listen_netdev=any# Port to listen on--port=9669nebula-storaged.conf123456789########## networking ########### Comma separated Meta server addresses--meta_server_addrs=192.168.10.111:9559,192.168.10.112:9559,192.168.10.113:9559# Local IP used to identify the nebula-storaged process.# Change it to an address other than loopback if the service is distributed or# will be accessed remotely.--local_ip=192.168.10.114# Storage daemon listening port--port=9779ABC三台配置除了local_ip，其他都一致，DE两台配置除了local_ip，其他都一致启动服务命令学习使用脚本nebula.service管理服务，包括启动、停止、重启、中止和查看nebula.service的默认路径是/usr/local/nebula/scripts，如果修改过安装路径，请使用实际路径语法1234sudo /usr/local/nebula/scripts/nebula.service[-v] [-c &lt;config_file_path&gt;]&lt;start | stop | restart | kill | status&gt;&lt;metad | graphd | storaged | all&gt;比如对于非容器部署的 NebulaGraph，执行如下命令启动服务：1234567sudo /usr/local/nebula/scripts/nebula.service start all[INFO] Starting nebula-metad...[INFO] Done[INFO] Starting nebula-graphd...[INFO] Done[INFO] Starting nebula-storaged...[INFO] Done执行如下命令查看 NebulaGraph 服务状态1sudo /usr/local/nebula/scripts/nebula.service status all如果返回如下结果，表示 NebulaGraph 服务正常运行12345[INFO] nebula-metad(33fd35e): Running as 29020, Listening on 9559[INFO] nebula-graphd(33fd35e): Running as 29095, Listening on 9669[WARN] nebula-storaged after v3.0.0 will not start service until it is added to cluster.[WARN] See Manage Storage hosts:ADD HOSTS in https://docs.nebula-graph.io/[INFO] nebula-storaged(33fd35e): Running as 29147, Listening on 9779正常启动 NebulaGraph 后，nebula-storaged进程的端口显示红色。这是因为nebula-storaged在启动流程中会等待nebula-metad添加当前 Storage 服务，当前 Storage 服务收到 Ready 信号后才会正式启动服务。从 3.0.0 版本开始，在配置文件中添加的 Storage 节点无法直接读写，配置文件的作用仅仅是将 Storage 节点注册至 Meta 服务中。必须使用ADD HOSTS命令后，才能正常读写 Storage 节点。注册具体是通过nebula-console命令行或者studio页面执行，用 java client 也可以的, 底层逻辑都是通过客户端发送一条 add hosts xxxxx query1ADD HOSTS 192.168.40.39:9779, 192.168.40.40:9779, 192.168.40.41:9779;比如使用python客户端12345678910111213141516from nebula3.gclient.net import ConnectionPoolfrom nebula3.Config import Config# define a configconfig = Config()config.max_connection_pool_size = 10# init connection poolconnection_pool = ConnectionPool()# if the given servers are ok, return true, else return falseok = connection_pool.init([('192.168.123.xx', 9669)], config)# option 1 control the connection release yourself# get session from the poolsession = connection_pool.get_session('root', 'nebula')session.execute(&quot;ADD HOSTS 192.168.123.24:9779&quot;)等待20s后，重新查看服务状态，发现警告没有就说明成功了12345[root@localhost nebula]# sudo scripts/nebula.service status all[WARN] The maximum files allowed to open might be too few: 1024[INFO] nebula-metad(bb2e684): Running as 11660, Listening on 9559[INFO] nebula-graphd(bb2e684): Running as 11727, Listening on 9669[INFO] nebula-storaged(bb2e684): Running as 11764, Listening on 9779清空图数据清除后，如无备份，无法恢复。使用该功能务必谨慎CLEAR SPACE不是原子性操作。如果执行出错，请重新执行，避免残留数据图空间中的数据量越大，CLEAR SPACE消耗的时间越长如果CLEAR SPACE的执行因客户端连接超时而失败，可以增大 Graph 服务配置中storage_client_timeout_ms参数的值在CLEAR SPACE的执行过程中，向该图空间写入数据的行为不会被自动禁止这样的写入行为可能导致CLEAR SPACE清除数据不完全，残留的数据也可能受到损坏1CLEAR SPACE [IF EXISTS] &lt;space_name&gt;;保留的数据¶图空间中，CLEAR SPACE不会删除的数据包括：Tag 信息Edge type 信息原生索引和全文索引的元数据DROP SPACE语句用于删除指定图空间以及其中的所有信息1DROP SPACE [IF EXISTS] &lt;graph_space_name&gt;;客户端NebulaGraph 支持多种类型的客户端，包括命令行客户端、可视化界面客户端和流行编程语言客户端详情参见客户端列表原生CLI客户端python客户端前提条件¶官方文档NebulaGraph 服务已启动运行 NebulaGraph Console 的机器和运行 NebulaGraph 的服务器网络互通NebulaGraph Console 的版本兼容 NebulaGraph 的版本使用命令arch，查看系统结构，去官网下载安装包，重命名文件为nebula_console为用户授予 nebula-console 文件的执行权限1chmod 111 nebula-console在命令行界面中，切换工作目录至 nebula-console 文件所在目录，执行如下命令连接 NebulaGraph1./nebula-console -addr &lt;ip&gt; -port &lt;port&gt; -u &lt;username&gt; -p &lt;password&gt; [-t 120] [-e &quot;nGQL_statement&quot; | -f filename.nGQL]例如，要连接到部署在 192.168.10.8 上的 Graph 服务，运行以下命令1./nebula_console -addr 192.168.10.8 -port 9669 -u root -p nebula已安装 Python，版本为 3.6 及以上，版本对照表¶NebulaGraph 版本NebulaGraph Python 版本3.2.13.1.02.6.x2.6.0pip安装1$ pip install nebula3-python==&lt;version&gt;初始化一些数据执行如下语句创建名为basketballplayer的图空间1CREATE SPACE basketballplayer(partition_num=15, replica_factor=1, vid_type=fixed_string(32));执行命令SHOW HOSTS检查分片的分布情况，确保平衡分布12345678nebula&gt; SHOW HOSTS;+-------------+-----------+-----------+-----------+--------------+----------------------------------+------------------------+---------+| Host        | Port      | HTTP port | Status    | Leader count | Leader distribution              | Partition distribution | Version |+-------------+-----------+-----------+-----------+--------------+----------------------------------+------------------------+---------+| &quot;storaged0&quot; | 9779      | 19669     | &quot;ONLINE&quot;  | 5            | &quot;basketballplayer:5&quot;             | &quot;basketballplayer:5&quot;   | &quot;3.1.0&quot; || &quot;storaged1&quot; | 9779      | 19669     | &quot;ONLINE&quot;  | 5            | &quot;basketballplayer:5&quot;             | &quot;basketballplayer:5&quot;   | &quot;3.1.0&quot; || &quot;storaged2&quot; | 9779      | 19669     | &quot;ONLINE&quot;  | 5            | &quot;basketballplayer:5&quot;             | &quot;basketballplayer:5&quot;   | &quot;3.1.0&quot; |+-------------+-----------+-----------+-----------+--------------+----------------------------------+------------------------+---------+选择图空间basketballplayer1nebula[(none)]&gt; USE basketballplayer;用户可以执行命令SHOW SPACES查看创建的图空间123456nebula&gt; SHOW SPACES;+--------------------+| Name               |+--------------------+| &quot;basketballplayer&quot; |+--------------------+创建 Tag 和 Edge type语法:12345678CREATE {TAG | EDGE} [IF NOT EXISTS] {&lt;tag_name&gt; | &lt;edge_type_name&gt;}    (      &lt;prop_name&gt; &lt;data_type&gt; [NULL | NOT NULL] [DEFAULT &lt;default_value&gt;] [COMMENT '&lt;comment&gt;']      [{, &lt;prop_name&gt; &lt;data_type&gt; [NULL | NOT NULL] [DEFAULT &lt;default_value&gt;] [COMMENT '&lt;comment&gt;']} ...]     )    [TTL_DURATION = &lt;ttl_duration&gt;]    [TTL_COL = &lt;prop_name&gt;]    [COMMENT = '&lt;comment&gt;'];创建 Tag:player和team，以及 Edge type:follow和serve。说明如下表。| 名称   | 类型      | 属性                             || ——— | ————- | ———————————————— || player | Tag       | name (string), age (int)         || team   | Tag       | name (string)                    || follow | Edge type | degree (int)                     || serve  | Edge type | start_year (int), end_year (int) |1234CREATE TAG player(name string, age int);CREATE TAG team(name string);CREATE EDGE follow(degree int);CREATE EDGE serve(start_year int, end_year int);插入点和边插入点12345678INSERT VERTEX [IF NOT EXISTS] [tag_props, [tag_props] ...]VALUES &lt;vid&gt;: ([prop_value_list])tag_props:  tag_name ([prop_name_list])prop_name_list: [prop_name [, prop_name] ...]prop_value_list: [prop_value [, prop_value] ...]  vid是 Vertex ID 的缩写，vid在一个图空间中是唯一的。参数详情请参见 INSERT VERTEX插入边123456INSERT EDGE [IF NOT EXISTS] &lt;edge_type&gt; ( &lt;prop_name_list&gt; ) VALUES &lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;] : ( &lt;prop_value_list&gt; )[, &lt;src_vid&gt; -&gt; &lt;dst_vid&gt;[@&lt;rank&gt;] : ( &lt;prop_value_list&gt; ), ...];&lt;prop_name_list&gt; ::= [ &lt;prop_name&gt; [, &lt;prop_name&gt; ] ...]&lt;prop_value_list&gt; ::= [ &lt;prop_value&gt; [, &lt;prop_value&gt; ] ...]参数详情请参见 INSERT EDGE插入代表球员和球队的点1234nebula&gt; INSERT VERTEX player(name, age) VALUES &quot;player100&quot;:(&quot;Tim Duncan&quot;, 42);nebula&gt; INSERT VERTEX player(name, age) VALUES &quot;player101&quot;:(&quot;Tony Parker&quot;, 36);nebula&gt; INSERT VERTEX player(name, age) VALUES &quot;player102&quot;:(&quot;LaMarcus Aldridge&quot;, 33);nebula&gt; INSERT VERTEX team(name) VALUES &quot;team203&quot;:(&quot;Trail Blazers&quot;), &quot;team204&quot;:(&quot;Spurs&quot;);插入代表球员和球队之间关系的边1234nebula&gt; INSERT EDGE follow(degree) VALUES &quot;player101&quot; -&gt; &quot;player100&quot;:(95);nebula&gt; INSERT EDGE follow(degree) VALUES &quot;player101&quot; -&gt; &quot;player102&quot;:(90);nebula&gt; INSERT EDGE follow(degree) VALUES &quot;player102&quot; -&gt; &quot;player100&quot;:(75);nebula&gt; INSERT EDGE serve(start_year, end_year) VALUES &quot;player101&quot; -&gt; &quot;team204&quot;:(1999, 2018),&quot;player102&quot; -&gt; &quot;team203&quot;:(2006,  2015);查询数据¶GO 语句可以根据指定的条件遍历数据库。GO语句从一个或多个点开始，沿着一条或多条边遍历，返回YIELD子句中指定的信息。FETCH 语句可以获得点或边的属性。LOOKUP 语句是基于索引的，和WHERE子句一起使用，查找符合特定条件的数据。MATCH 语句是查询图数据最常用的，可以灵活的描述各种图模式，但是它依赖索引去匹配 NebulaGraph 中的数据模型，性能也还需要调优。🌶注意无属性索引的时候，在没有指定条件的时候，可以通过数据扫描的sample下推去扫得 limit 的数据而带有filter条件时，需要全扫才行，这个情况在nebula里是被禁止的，需要创建这个tag上的索引才能查询1234567891011# 在 Tag player 的 name 属性和 Edge type belong_to 上创建索引CREATE TAG INDEX IF NOT EXISTS name ON player(name(20));CREATE EDGE INDEX IF NOT EXISTS belong_to_index on belong_to();# 重建索引使其生效REBUILD EDGE INDEX name;REBUILD EDGE INDEX belong_to_index;# 开始查询MATCH ()&lt;-[e:belong_to]-() WHERE e.industry_name=='水力发电'RETURN eLIMIT 3;图模式模式(pattern)和图模式匹配，是图查询语言的核心功能单点模式1(a)示例为一个简单的模式，描述了单个点，并使用变量a命名该点多点关联模式多个点通过边相连是常见的结构，模式用箭头来描述两个点之间的边。例如：1(a)-[]-&gt;(b)示例为一个简单的数据结构：两个点和一条连接两个点的边，两个点分别为a和b，边是有方向的，从a到b这种描述点和边的方式可以扩展到任意数量的点和边，例如：1(a)-[]-&gt;(b)&lt;-[]-(c)这样的一系列点和边称为路径(path)只有在涉及某个点时，才需要命名这个点。如果不涉及这个点，则可以省略名称，例如：1(a)-[]-&gt;()&lt;-[]-(c)Tag模式nGQL 中的Tag概念与 openCypher 中的Label有一些不同。例如，必须创建一个Tag之后才能使用它，而且Tag还定义了属性的类型模式除了简单地描述图中的点之外，还可以描述点的 Tag。例如：1(a:User)-[]-&gt;(b)模式也可以描述有多个 Tag 的点，例如：1(a:User:Admin)-[]-&gt;(b)属性模式点和边是图的基本结构。nGQL 在这两种结构上都可以增加属性，方便实现更丰富的模型。在模式中，属性的表示方式为：用花括号括起一些键值对，用英文逗号分隔。例如一个点有两个属性：1(a {name: 'Andres', sport: 'Brazilian Ju-Jitsu'})在这个点上可以有一条边是：1(a)-[{blocked: false}]-&gt;(b)边模式描述一条边最简单的方法是使用箭头连接两个点。可以用以下方式描述边以及它的方向性。如果不关心边的方向，可以省略箭头，例如：1(a)-[]-(b)和点一样，边也可以命名。一对方括号用于分隔箭头，变量放在两者之间。例如：1(a)-[r]-&gt;(b)和点上的 Tag 一样，边也可以有类型。描述边的类型，例如：1(a)-[r:REL_TYPE]-&gt;(b)和点上的 Tag 不同，一条边只能有一种 Edge type。但是如果我们想描述多个可选 Edge type，可以用管道符号（|）将可选值分开，例如：1(a)-[r:TYPE1|TYPE2]-&gt;(b)和点一样，边的名称可以省略，例如：1(a)-[:REL_TYPE]-&gt;(b)变长模式在图中指定边的长度来描述多条边（以及中间的点）组成的一条长路径，不需要使用多个点和边来描述。例如：1(a)-[*2]-&gt;(b)该模式描述了 3 点 2 边组成的图，它们都在一条路径上（长度为 2），等价于：1(a)-[]-&gt;()-[]-&gt;(b)也可以指定长度范围，这样的边模式称为variable-length edges，例如：1(a)-[*3..5]-&gt;(b)*3..5表示最小长度为 3，最大长度为 5。该模式描述了 4 点 3 边、5 点 4 边或 6 点 5 边组成的图。也可以忽略最小长度，只指定最大长度，例如：1(a)-[*..5]-&gt;(b)必须指定最大长度，不支持仅指定最小长度（(a)-[*3..]-&gt;(b)）或都不指定（(a)-[*]-&gt;(b)）。路径变量一系列连接的点和边称为路径。nGQL 允许使用变量来命名路径，例如：1p = (a)-[*3..5]-&gt;(b)可以在 MATCH 语句中使用路径变量注释支持四种注释方式：#、//、/* */nGQL 语句中的反斜线（\）代表换行数据类型字符串的表示方式为用双引号或单引号包裹，例如&quot;Hello, Cooper&quot;或'Hello, Cooper'日期和时间的类型，包括DATE、TIME、DATETIME、TIMESTAMP和DURATION12345678910111213141516171819202122232425262728nebula&gt; RETURN DATE({year:-123, month:12, day:3});+------------------------------------+| date({year:-(123),month:12,day:3}) |+------------------------------------+| -123-12-03                         |+------------------------------------+nebula&gt; RETURN DATE(&quot;23333&quot;);+---------------+| date(&quot;23333&quot;) |+---------------+| 23333-01-01   |+---------------+# 传入当前时间。nebula&gt; RETURN timestamp();+-------------+| timestamp() |+-------------+| 1625469277  |+-------------+# 传入指定时间。nebula&gt; RETURN timestamp(&quot;2022-01-05T06:18:43&quot;);+----------------------------------+| timestamp(&quot;2022-01-05T06:18:43&quot;) |+----------------------------------+| 1641363523                       |+----------------------------------+date()支持的属性名称包括year、month和day。date()支持输入YYYY、YYYY-MM或YYYY-MM-DD，未输入的月份或日期默认为01time()支持的属性名称包括hour、minute和seconddatetime()支持的属性名称包括year、month、day、hour、minute和second列表（List）是复合数据类型，一个列表是一组元素的序列，可以通过元素在序列中的位置访问列表中的元素nGQL 的下标支持从前往后查询，从 0 开始，0 表示第一个元素，1 表示第二个元素，以此类推；也支持从后往前查询，从-1 开始，-1 表示最后一个元素，-2 表示倒数第二个元素，以此类推。[M]：表示下标为 M 的元素。[M..N]：表示M ≤ 下标 ＜ N的元素。N为 0 时，返回为空。[M..]：表示M ≤ 下标的元素。[..N]：表示下标 ＜ N的元素。N为 0 时，返回为空。可视化NebulaGraph Studio（简称 Studio）是一款可以通过 Web 访问的开源图数据库可视化工具，搭配 NebulaGraph 内核使用，提供构图、数据导入、编写 nGQL 查询等一站式服务。前提条件¶在部署 RPM 版 Studio 之前，用户需要确认以下信息：NebulaGraph 服务已经部署并启动。详细信息，参考 NebulaGraph 安装部署使用的 Linux 发行版为 CentOS ，已安装 lsof确保以下端口未被占用，7001是Studio 提供 web 服务使用的从官网下载nebula-graph-studio-3.4.1.x86_64.rpm，使用以下命令安装到指定路径1sudo rpm -i nebula-graph-studio-3.4.1.x86_64.rpm --prefix=&lt;path&gt; 安装完，自动会启动，当屏幕返回以下信息时，表示 PRM 版 Studio 已经成功启动。123Start installing NebulaGraph Studio now...NebulaGraph Studio has been installed.NebulaGraph Studio started automatically.启动成功后，在浏览器地址栏输入 http://&lt;ip address&gt;:7001注意: host处填写为 ip:port格式用户名和密码：根据 Nebula Graph 的身份验证设置填写登录账号和密码如果未启用身份验证，可以填写默认用户名 root 和任意密码。如果已启用身份验证，但是未创建账号信息，用户只能以 GOD 角色登录，必须填写 root 及对应的密码 nebula。如果已启用身份验证，同时又创建了不同的用户并分配了角色，不同角色的用户使用自己的账号和密码登录。如果在浏览器窗口中能看到以下登录界面，表示已经成功部署并启动 Studio异常处理¶如果在安装过程中自动启动失败或是需要手动启动或停止服务，请使用以下命令：手动启动服务1bash /usr/local/nebula-graph-studio/scripts/rpm/start.sh手动停止服务1bash /usr/local/nebula-graph-studio/scripts/rpm/stop.sh如果启动服务时遇到报错报错 ERROR: bind EADDRINUSE 0.0.0.0:7001，用户可以通过以下命令查看端口 7001 是否被占用。1lsof -i:7001如果端口被占用，且无法结束该端口上进程，用户可以通过以下命令修改 Studio 服务启动端口，并重新启动服务。123456//修改 studio 服务启动端口vi etc/studio-api.yam//修改Port: 7001 // 修改这个端口号，改成任意一个当前可用的即可//重启服务systemctl restart nebula-graph-studio.service集群监控NebulaGraph Dashboard(简称Dashboard)是一款用于监控NebulaGraph集群中机器和服务状态的可视化工具数据导入Nebula Importer 数据导入实践Nebula 目前作为较为成熟的产品，已经有着很丰富的生态。数据导入的维度而言就已经提供了多种选择。有大而全的Nebula Exchange，小而精简的Nebula Importer, 还有为 Spark / Flink 引擎提供的Nebula Spark Connector 和 Nebula Flink Connector。使用场景介绍：Nebula Exchange需要将来自 Kafka、Pulsar 平台的流式数据, 导入 Nebula Graph 数据库需要从关系型数据库（如 MySQL）或者分布式文件系统（如 HDFS）中读取批式数据需要将大批量数据生成 Nebula Graph 能识别的 SST 文件Nebula ImporterImporter 适用于将本地 CSV 文件的内容导入至 Nebula Graph 中Nebula Spark Connector:在不同的 Nebula Graph 集群之间迁移数据在同一个 Nebula Graph 集群内不同图空间之间迁移数据Nebula Graph 与其他数据源之间迁移数据结合 Nebula Algorithm 进行图计算Nebula Flink Connector在不同的 Nebula Graph 集群之间迁移数据在同一个 Nebula Graph 集群内不同图空间之间迁移数据Nebula Graph 与其他数据源之间迁移数据总体来说，Exchange 大而全，可以和大部分的存储引擎结合，导入到 Nebula 中，但是需要部署Spark 环境。Importer 使用简单，所需依赖较少，但需要自己提前生成数据文件，配置好 schema 一劳永逸，但是不支持断点续传，适合数据量中等。Spark / Flink Connector 需要和流数据结合。不同的场景选择不同的工具，如果作为新人使用 Nebula 在导入数据时，建议使用 Nebula Importer 工具，简单快速上手。k8sk8s官网中文文档Kubernetes（K8S）教程Kubernetes Clusters-狂乱的贵公子简介Kubernetes (K8S) 是什么它是一个为 容器化 应用提供集群部署和管理的开源工具，由 Google 开发Kubernetes 这个名字源于希腊语，意为“舵手”或“飞行员”。k8s 这个缩写是因为 k 和 s 之间有八个字符的关系。 Google 在 2014 年开源了 Kubernetes 项目主要特性：高可用，不宕机，自动灾难恢复灰度更新，不影响业务正常运转一键回滚到历史版本方便的伸缩扩展（应用伸缩，机器加减）、提供负载均衡有一个完善的生态部署方案传统部署方式：应用直接在物理机上部署，机器资源分配不好控制，出现Bug时，可能机器的大部分资源被某个应用占用，导致其他应用无法正常运行，无法做到应用隔离虚拟机部署在单个物理机上运行多个虚拟机，每个虚拟机都是完整独立的系统，性能损耗大容器部署所有容器共享主机的系统，轻量级的虚拟机，性能损耗小，资源隔离，CPU和内存可按需分配Kubernetes 可以为你提供集中式的管理集群机器和应用，加机器、版本升级、版本回滚，那都是一个命令就搞定的事，不停机的灰度更新，确保高可用、高性能、高扩展重要概念 Pod豆荚，K8S 调度、管理的最小单位，一个 Pod 可以包含一个或多个容器，每个 Pod 有自己的虚拟IP。一个工作节点可以有多个 pod，主节点会考量负载自动调度 pod 到哪个节点运行Kubernetes 组件Kubernetes 是一个开源的容器编排和管理平台，由多个组件组成。下面是 Kubernetes 的一些核心组件：kube-apiserver：作为 Kubernetes API 的前端组件，处理来自用户、kubectl 命令行工具以及其他组件的 API 请求，并将其存储在 etcd 中etcd：是一个高度可用且一致性的分布式键值存储，用于保存 Kubernetes 集群的配置数据、元数据和状态信息kube-scheduler：负责监视新创建的 Pod，并根据可用的计算资源、硬件和软件要求选择合适的节点进行调度kube-controller-manager：包含多个控制器，用于管理集群中的各种资源，例如节点控制器、副本控制器、服务控制器等。它负责保持期望状态和实际状态之间的一致性kubelet：在每个节点上运行的代理组件，负责管理节点上的容器，与容器运行时进行交互，接收来自 kube-apiserver 的指令，并确保容器按照期望状态运行kube-proxy：负责为 Service 对象提供网络代理和负载均衡功能，将网络流量路由到正确的容器上Container Runtime：Kubernetes 支持多种容器运行时，如 Docker、containerd、CRI-O 等，用于创建和管理容器kube-dns/coredns：提供集群内 DNS 服务，用于解析 Kubernetes 集群中的服务名称Dashboard：提供可视化的 Kubernetes 集群管理界面，用于查看和操作集群中的资源除了这些核心组件之外，还有其他的辅助组件和插件，如Ingress Controller（用于管理入口流量）、Heapster（用于集群监控和性能分析）、Prometheus（用于指标收集和监控）、Fluentd（用于日志收集和聚合）等，这些组件可以根据具体需求选择安装和配置安装和部署占位…</content>
	</entry>
	<entry>
		<title>厦门方特梦幻王国一日攻略</title>
		<url>/article/63632.html</url>
		<content>方特王国园区地图打卡厦门方特梦幻王国 夏天去的话带好遮阳伞、帽子、防晒霜等，防晒工作要做好,相机更是必不可少水尽量少带。所有液体、食品(含600ml以上饮料)、易燃易爆品、违禁品、化学品禁止带入园。园区的水较其他景区也不贵，是统一价，矿泉水8-10元，其他汽水果汁10元，几乎每一个场馆旁边都有水吧很方便的。园区小火车是免费的，会一直在园区里转，走累了看到可以招手随停随上。记得带：身份证，饮料，晚餐，带书包，耳机，充电宝，充电器丛林的故事适宜人群：所有人；身高要求：无；项目特色：大型主题舞蹈秀时间安排202221.30-22.30打道回府21.00-21.30看烟花表演19.30-21.00吃饭时间(景区内物价比外面稍高，正餐人均30~40左右)+自由安排ps餐饮：乐园内有非常多的餐饮小吃店，正餐像童话餐厅、熊记汉堡、欢乐餐厅、开心肉夹馍、小淮娘鸭血粉丝等等，口味众多丰俭由人；除了正餐，更多的是小吃驿站，比如围炉锅盔、降龙爪爪、杯中的鱼、正新鸡排、金山客栈、老塞咖啡、快乐冰室等等，很容易管不住嘴，最后预算和卡路里双双超标18.30-19.30 室内场馆魔法城堡魔法城堡，塔罗牌浮空，冒失盗贼的变成青蛙，血腥的惨像迎面袭来，这究竟是哪？大型DARK RIDE表演项目，魔法城堡将以不可思议的表现手段，为你呈现一个不可思议的魔法世界。宇宙小勇士儿童项目，大人就不用浪费时间尝试了（带小孩另说）。特别是宇宙小勇士造型显眼很有迷惑性，其实是一个儿童射击项目童梦天地童梦天地，这是一个儿童的天堂，各个年龄段的小朋友都可以在里面找到适合自己的游玩项目，里面设有动画剧场，低龄游玩区，大型攀爬游戏区和互动投影区。宇宙博览会19.00-19.30一定要去看的地方飞越极限飞跃极限，作为一款高空飞翔体验项目，一秒天上一秒深渊真的不是在说笑，想体验从没有过的凌空飞翔体验，来到这里准没有错。生命之光讲述了地球生命演化的历史，生命的诞生、恐龙的灭绝、哺乳动物的崛起、人类的登顶，非常棒的科普类特效剧场。《生命之光》对大人来说可看可不看，但是它对小朋友的吸引力却非常大，恐龙、生物进化、陨石袭击完全抓住了他们的注意力， 让他们津津有味看完一场‘科普大电影’。18.00-19.00次深处景区游玩：唐古拉雪山唐古拉雪山，惊险指数五颗星，欢乐指数五颗星，奇幻指数五颗星，你准备好从雪山钻入岩浆中了吗?唐古拉小镇观光类型园区中心双层转马：最浪漫的项目-粉色双层转马。坐在色彩斑斓、富丽堂皇的木马上跳跃旋转，宛如置身于童话世界之中。音乐飘起，点亮梦幻般的霓虹灯，与心中的公主王子结伴策骑，满足每一个小仙女的粉色童话梦。秦陵历险秦陵历险，将现实场景和立体电影灯光效果相结合，充分还原出千年秦陵的幽深黑暗，寻宝的环节趣味十足，让人深陷其中，欲罢不能，是一个大型的室内游历项目。16.30-18.00深处景区游玩：太空飞梭太空飞梭，这个项目能让你深刻明白，什么是失重!瞬间来到40米高空再瞬间下落，不怕脚软就来挑战吧。熊出没儿童项目，大人就不用浪费时间尝试了（带小孩另说）。特别是宇宙小勇士造型显眼很有迷惑性，其实是一个儿童射击项目暴风眼扭曲的轨道，720度后半空骤停，这才是真正的刺激，真正的过瘾。极地快车极地快车，这款过山车是真正的过山车中的战斗机!集合了战斗机特效动作的它，将让你充分感受心悬在嗓子眼是一种什么样的感受!逃出恐龙岛逃出恐龙岛，吴哥窟式的建筑风格，在这里，你将乘上小船进入侏罗纪时代，在茂密的雨林中经历一场惊心动魄的冒险，勇士，你准备好了吗?16.00 准备入园啦~，门口会有玩偶可以合影，入场后，路旁的室内先不要玩，直奔园区最深处12.00-14.00吃完午餐，从天伦医院出发，前往方特乐园入口的彩虹桥项目开放时间类  别项目名称身高要求项目开放时间 （平时）项目开放时间 （周末）飞越极限1.2M以上10:10 10:30 11:3012:30 13:30 14:30 15:30 16:3009:40 10:30 11:3012:30 13:3014:30 15:30 16:30 17:10生命之光儿童需成人陪同13:00 15:00 17:0010:30 13:00 15:00 16:00宇宙博览会1.2M以上暂不开放暂不开放熊熊乐园儿童需成人陪同11:00-17:0009:30-17:30宇宙小勇士1M以上11:30-17:0009:30-17:30魔法城堡1.2M以上10:00-17:0009:30-17:30秦陵历险1.2M以上11:00-17:0010:30-17:30聊斋1M以上13:30 15:3012:30 13:30 14:30 15:30 16:30决战金山寺儿童需成人陪同11:00 12:00 13:0016:00 16:3011:00 12:00 14:0015:00 16:00 17:00丛林的故事儿童需成人陪同暂不开放暂不开放室外项目类  别项目名称身高要求项目开放时间 （平时）项目开放时间 （周末）海盗船1.1M以上10:00-17:0009:30-17:30UFO1.4M以上10:00-17:0009:30-17:30双层转马儿童需成人陪同10:00-17:0009:30-17:30转转杯1M以上10:00-17:0009:30-17:30果虫滑车0.9M-1.4M10:00-17:0009:30-17:30波浪翻滚1.3M以上12:30-17:0009:30-17:30大摆锤1.2M以上10:00-17:0009:30-17:30太空飞梭1.4M以上10:00-17:0009:30-17:30空中飞舞1.2M以上12:30-17:0009:30-17:30极地快车1.2M-2.05M10:00-17:0009:30-17:30暴风眼1.3M-2M10:00-17:0009:30-17:30唐古拉雪山1.1M以上10:00-17:0009:30-17:30逃出恐龙岛1.2M以上12:30-17:0009:30-17:30</content>
	</entry>
	<entry>
		<title>阿里云部署</title>
		<url>/article/14386.html</url>
		<content>阿里云部署阿里云部署笔记(centos版本)，购买阿里云主机，开启安全组云主机实例配置安全组环境安装安装git、jdk和nodejs安装jdk12# 安装jdkrpm -ivh jdk-8u241-linux-x64.rpm配置jdk环境变量vi /etc/profile1234export JAVA_HOME=/usr/java/jdk1.8.0_241-amd64export JRE_HOME=/usr/java/jdk1.8.0_241-amd64/jreexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libps: 若原先存在openjdk，需要先卸载1234# 查看jdk列表rpm -qa|grep java# 卸载rpm -e --nodeps java-1.8.0-openjdk-1.8.0.181-7.b13.el7.x86_64安装和配置nodejs12345678910111213141516# 安装和配置nodejscp node-v12.16.1-linux-x64.tar.xz /usr/local/cd /usr/local/tar -xvf node-v12.16.1-linux-x64.tar.xzmv node-v12.16.1-linux-x64/ nodejscd nodejsmkdir node_globalmkdir node_cachenpm config set prefix &quot;/usr/local/nodejs/node_global&quot;npm config set cache &quot;/usr/local/nodejs/node_cache&quot;# 配置nodejsvim /etc/profile# 在末尾添加export PATH=$PATH:/usr/local/nodejs/bin # config nodejs# 执行生效命令source /etc/profile安装和配置git123456# 安装和配置gityum install gitgit config --global user.name &quot;yourname&quot;git config --global user.mail &quot;xxxxxxxxxx@mail.com&quot;# 配置远程仓库 将pub_key黏贴到gitssh-keygen -t rsa -C &quot;xxxxxxxxxx@mail.com&quot;安装和配置git123456# 安装和配置gityum install gitgit config --global user.name &quot;narutohyc&quot;git config --global user.mail &quot;1832044043@qq.com&quot;# 配置远程仓库 将pub_key黏贴到gitssh-keygen -t rsa -C &quot;1832044043@qq.com&quot;安装gitbook123456# 安装gitbooknpm install gitbook-cli -g# 添加gitbook软链接ln -s /usr/local/nodejs/node_global/bin/gitbook /usr/bin/gitbook# gitbook指令测试gitbook -Vgit访问慢配置Github下载速度太慢怎么办？完美解决服务器查询github hosts下载, 实时更新通过修改 hosts文件，重定向网站的访问到代理服务器上| 系统        | hosts文件路径                             || :——————— | :—————————————————————— || Window      | %systemroot%\system32\drivers\etc\hosts || Mac / Linux | /etc/hosts                              |1234# github219.76.4.4 github-cloud.s3.amazonaws.com140.82.113.4 github.com151.101.185.194 github.global.ssl.fastly.net刷新DNS缓存，Linux/Mac/Windows-刷新DNS的方法centos: 方式一: /etc/rc.d/init.d/nscd restart方式二: yum install -y nscdsystemctl restart nscdwindows: ipconfig /flushdns常用命令netstat -atunlp查看端口占用杀死指定进程gitbook指定端口并后台执行12# 指定端口为4001，并放在后台执行setsid gitbook serve --lrport=35729 --port=4001若启动gitbook报错1234Starting server ...Serving book on http://localhost:4000Error: ENOSPC: System limit for number of file watchers reached, watch '/home/hyc/github/bk_python/node_modules/entities/mathjax/jax/output/HTML-CSS/fonts/Neo-Euler/Symbols'则执行以下命令即可12echo fs.inotify.max_user_watches=524288 | sudo tee -a /etc/sysctl.conf &amp;&amp; sudo sysctl -psudo sysctl --system批量杀死进程查看进程号1234[root@iZ2zeh6nc313zs4azjijjnZ ~]# ps -ef | grep gitbook | grep -v grep|awk '{print $2}'240642938730425方式一：batch_shell12345678#!/bin/shdsp_admin_id=`ps -ef | grep gitbook | grep -v grep|awk '{print $2}'`echo $dsp_admin_idfor id in $dsp_admin_iddo    kill -9 $id      echo &quot;killed $id&quot; done方式二：单行执行12#!/bin/shps -ef | grep gitbook | grep -v grep|grep 4000|  awk '{print $2}' | xargs kill -9</content>
	</entry>
	<entry>
		<title>github协作</title>
		<url>/article/51084.html</url>
		<content>github协作GitHub 多人协作开发三种方式Fork 方式暂无Organizations方式准备项目新建组织(创建免费的即可，有每月免费2000分钟)创建team(不建也是可以，建了容易管理)添加team成员，在组织下选择team，给team拉队友，队友收到邮件后，同意了就可以加入仓库转入(原来已有的仓库，进入仓库选择settings，下滑选Transfer)或在新建项目为组织下的仓库添加合作者，配置权限设置主分支保护，不让其他人直接push到主分支，打开仓库下的settings，选择branchs添加Branch protection rules开始协作协作者在组织下找到对应的仓库，克隆仓库到本地1git clone git@github.com:hycBook/bk_python.git开始编写，完成后提交至新分支，一般命名为dev_xxx新分支创建一次就可以，commit之后，再push到自己的dev_xxx分支注意：在修改文件之前一般是先从主分支拉取最新内容，修改完成后尽早提交，较少冲突commit和push操作都是在自己的dev_xxx分支，pull操作是从主分支来到github界面，选择Pull requests，在选择New pull request，选择分支提交合并请求后，等待合并即可合作者暂无githun actionsGithub Actions 是 Github 推出的自动化构建工具，一般来说，CI / CD （持续集成 / 持续部署）都需要自己的计算资源，但 Github Actions 提供免费的计算资源，这是它的优势之一，用户只用操心 workflow 文件，不用关心环境相关的琐碎，便可部署 CI / CD，加速了流程。在 GitHub Actions 中，一次执行过程被称为一个 workflow，一个 workflow 中可以有一个或者多个 job，job 又由 step 组成，step 中可以执行 action，就是一条条命令github同步giteeHub Mirror Action一个用于在hub间（例如Github，Gitee）账户代码仓库同步的action准备步骤本地生成公钥和秘钥1ssh-keygen -t -C &quot;1832044043@qq.com&quot;公钥放在giteegithub项目下新建：name=GITEE_PRIVATE_KEY，value=秘钥gitee生成tokengithub项目下新建：name=GITEE_TOKEN，value=gitee生成的token基本用法1234567891011121314steps:    - name: Sync Github Repos To Gitee  # 名字随便起      uses: Yikun/hub-mirror-action@master  # 使用Yikun/hub-mirror-action      with:        src: github/hycBook  # 源端账户名(github),如果是组织的话要用组织名,此时account_type=org        dst: gitee/hycBook  # 目的端账户名(gitee),如果是组织的话要用组织名,此时account_type=org        dst_key: ${{ secrets.GITEE_PRIVATE_KEY }}  # SSH密钥对中的私钥        dst_token:  ${{ secrets.GITEE_TOKEN }}  # Gitee账户的私人令牌        account_type: org  # 账户类型,        # clone_style: &quot;ssh&quot;  # 使用https方式进行clone，也可以使用ssh        debug: true  # 启用后会显示所有执行命令        force_update: true  # 启用后，强制同步，即强制覆盖目的端仓库        static_list: &quot;bk_index&quot;  # 静态同步列表，在此填写需要同步的仓库名称，可填写多个        timeout: '660s'  # git超时设置，超时后会自动重试git操作必选参数src 需要被同步的源端账户名，如github/kunpengcompute，表示Github的kunpengcompute账户。dst 需要同步到的目的端账户名，如gitee/kunpengcompute，表示Gitee的kunpengcompute账户。dst_key 用于在目的端上传代码的私钥(默认可以从~/.ssh/id_rsa获取），可参考生成/添加SSH公钥或generating SSH keys生成，并确认对应公钥已经被正确配置在目的端。对应公钥，Github可以在这里配置，Gitee可以这里配置。dst_token 创建仓库的API tokens， 用于自动创建不存在的仓库，Github可以在这里找到，Gitee可以在这里找到。可选参数account_type 默认为user，源和目的的账户类型，可以设置为org（组织）或者user（用户），该参数支持同类型账户（即组织到组织，或用户到用户）的同步。如果源目的仓库是不同类型，请单独使用src_account_type和dst_account_type配置。src_account_type 默认为account_type，源账户类型，可以设置为org（组织）或者user（用户）。dst_account_type 默认为account_type，目的账户类型，可以设置为org（组织）或者user（用户）。clone_style 默认为https，可以设置为ssh或者https。当设置为ssh时，你需要将dst_key所对应的公钥同时配置到源端和目的端。cache_path 默认为’’, 将代码缓存在指定目录，用于与actions/cache配合以加速镜像过程。black_list 默认为’’, 配置后，黑名单中的repos将不会被同步，如“repo1,repo2,repo3”。white_list 默认为’’, 配置后，仅同步白名单中的repos，如“repo1,repo2,repo3”。static_list 默认为’’, 配置后，仅同步静态列表，不会再动态获取需同步列表（黑白名单机制依旧生效），如“repo1,repo2,repo3”。force_update 默认为false, 配置后，启用git push -f强制同步，注意：开启后，会强制覆盖目的端仓库。debug 默认为false, 配置后，启用debug开关，会显示所有执行命令。timeout 默认为’30m’, 用于设置每个git命令的超时时间，’600’=&gt;600s, ‘30m’=&gt;30 mins, ‘1h’=&gt;1 hoursmappings 源仓库映射规则，比如’A=&gt;B, C=&gt;CC’, A会被映射为B，C会映射为CC，映射不具有传递性。主要用于源和目的仓库名不同的镜像。部署github pagesGitHub Pages Deploy Action自动发布到github pages分支(可以自己设置分支)配置tokengithub项目下新建：name=TOKEN，value=github生成的token基本用法123456- name: Deploy #步骤4：把编译好的静态目录上传到gh-pages分支上  uses: JamesIves/github-pages-deploy-action@3.1.5 #这里调用第三方依赖，用于上传代码到指定分支上  with:    ACCESS_TOKEN: ${{ secrets.TOKEN }}    BRANCH: gh-pages #github pages 默认的分支    FOLDER: _book #上传文件的目录，我这个目录在根目录下，如果是子目录可以这么写  docs/.vuepress/dist发布releaseaction-automatic-releases@latest自动创建release，并发布基本用法123456789- name: Create GitHub release  uses: marvinpinto/action-automatic-releases@latest  with:    repo_token: ${{ secrets.TOKEN }}    automatic_release_tag: 'ebook_index' # 发布标签    title: ${{ env.ReleaseVersion }}    prerelease: false    files: |      _ebook/* # 发布的目录build gitee pagesGitee 如何自动部署 Pages？还是用 GitHub Actions!使用 GitHub Actions 解决了 GitHub 代码自动同步 Gitee 的问题，但我们的博客仓库代码同步到 Gitee 后，并不能像 GitHub 一样自动部署 Pages，如果不使用付费的 Gitee Pages Pro 服务，那我们该怎么实现 Gitee 自动部署 Pages 呢基本用法新建一个name=GITEE_PASSWORD, value=gitee登陆密码的secrets1234567891011- name: Build Gitee Pages  uses: yanglbme/gitee-pages-action@main  with:    # 注意替换为你的 Gitee 用户名    gitee-username: narutohyc    # 注意在 Settings-&gt;Secrets 配置 GITEE_PASSWORD    gitee-password: ${{ secrets.GITEE_PASSWORD }}    # 注意替换为你的 Gitee 仓库，仓库名严格区分大小写，请准确填写，否则会出错    gitee-repo: hycBook/bk_index    # 要部署的分支，默认是 master，若是其他分支，则需要指定（指定的分支必须存在）    branch: gh-pagesgithub基本命令强制覆盖本地文件12345# 单条执行git fetch --all &amp;&amp;  git reset --hard origin/master &amp;&amp; git pull# 或者试下git pull --forcegitlab runnerGitLab Runner 介绍GitLab Runner是一个开源项目，用于运行您的作业并将结果发送回GitLab。它与GitLab CI一起使用，GitLab CI是GitLab随附的开源持续集成服务，用于协调作业GitLab Runner是用Go编写，可以作为单个二进制文件运行，不需要语言特定的要求GitLab Runner的三种类型shared：运行整个平台项目的作业(gitlab)group：运行特定group下的所有项目的作业(group)specific：运行指定的项目作业(project)下载安装下载安装 Install GitLab Runner manually on GNU/Linux(依赖docker)linux手动123curl -LJO &quot;https://gitlab-runner-downloads.s3.amazonaws.com/latest/rpm/gitlab-runner_amd64-fips.rpm&quot;chomd 777 gitlab-runner_amd64-fips.rpmrpm -i gitlab-runner_amd64-fips.rpmdocker方式Run GitLab Runner in a container注册runnergitlab runner 注册点击用户管理—左边点击runner，可以看到界面右边有gitlab的地址和token。这个需要用于后面runner的注册使用1234567891011121314151617181920212223(base) [root@FDBL ~]# sudo gitlab-runner registerINFO[0000] Binary was compiled with FIPS mode, but an external SSL library was not enabled. Runtime platform                                    arch=amd64 os=linux pid=5898 revision=8ec04662 version=16.3.0Running in system-mode.                                                                               Enter the GitLab instance URL (for example, https://gitlab.com/):http://192.168.50.128Enter the registration token:iqxKz5XTz4w_2RxiSQ5SEnter a description for the runner:[FDBL]: for python projectEnter tags for the runner (comma-separated):py310Enter optional maintenance note for the runner:WARNING: Support for registration tokens and runner parameters in the 'register' command has been deprecated in GitLab Runner 15.6 and will be replaced with support for authentication tokens. For more information, see https://gitlab.com/gitlab-org/gitlab/-/issues/380872 Registering runner... succeeded                     runner=tKejdsiTEnter an executor: instance, kubernetes, custom, parallels, shell, virtualbox, docker+machine, docker, docker-windows, ssh, docker-autoscaler:dockerEnter the default Docker image (for example, ruby:2.7):python:3.10Runner registered successfully. Feel free to start it, but if it's running already the config should be automatically reloaded!如果是docker的方式123[root@localhost config]# docker exec -it gitlab-runner bashroot@24dc60abee0b:/# gitlab-runner register# 同上挂载gitlab runner高级设置打开/etc/gitlab-runner/config.toml，添加volumes配置1234567891011121314151617181920212223242526concurrent = 4check_interval = 0shutdown_timeout = 0[session_server]  session_timeout = 1800[[runners]]  name = &quot;for python project&quot;  url = &quot;http://192.168.0.143&quot;  id = 947  token = &quot;xxx&quot;  token_obtained_at = 2023-08-21T02:17:13Z  token_expires_at = 0001-01-01T00:00:00Z  executor = &quot;docker&quot;  [runners.cache]    MaxUploadedArchiveSize = 0  [runners.docker]    tls_verify = false    image = &quot;centos:7&quot;    privileged = false    disable_entrypoint_overwrite = false    oom_kill_disable = false    disable_cache = false    volumes = [&quot;/cache&quot;, &quot;/home/pypi_packages:/var/pypi_packages&quot;]    shm_size = 0volumes是将/home/pypi_packages映射到/var/pypi_packages中，concurrent = 4表示并行的数量修改成立即生效sudo gitlab-runner verify.gitlab-ci.yml.gitlab-ci.yml keyword referencegitlab—内置的环境变量、自定义环境变量验证您的GitLab CI配置(语法检验)实现一个自动化编译python，并将打包的好的发布到仓库，如果需要自定义变量可以用$env_name访问在CI/CD配置的变量1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253image: python:3.10stages:  - buildbuild:  stage: build  script:    - python setup.py sdist bdist_wheel    - cp -R dist/* /var/pypi_packages/    - touch ~/.pypirc    - 'echo &quot;[distutils]&quot; &gt;&gt; ~/.pypirc'    - 'echo &quot;index-servers&quot; = &gt;&gt; ~/.pypirc'    - 'echo &quot;  local&quot; &gt;&gt; ~/.pypirc'    - 'echo &quot;&quot; &gt;&gt; ~/.pypirc'    - 'echo &quot;[local]&quot; &gt;&gt; ~/.pypirc'    - 'echo &quot;repository: 192.168.xx.xx:8282&quot; &gt;&gt; ~/.pypirc'    - 'echo &quot;username: $u_name&quot; &gt;&gt; ~/.pypirc'    - 'echo &quot;password: $u_pd&quot; &gt;&gt; ~/.pypirc'#    - python setup.py sdist bdist_wheel upload -r http://192.168.xx.xx:8282#    - pip install --extra-index-url http://192.168.xx.xx:8282/simple/ --trusted-host 192.168.xx.xx:8282 twine==4.0.2#    - twine upload dist/*  artifacts:    paths:      - dist/  only:    - dev  # 或者您的主分支的名称，比如 &quot;master&quot;  # 告诉 Runner 我们应用哪个标签  tags:    - global_py310#方式一#    - python setup.py sdist bdist_wheel#    - cp -R dist/* /var/pypi_packages/#方式二#  variables:#    TWINE_USERNAME: $TWINE_USERNAME#    TWINE_PASSWORD: $TWINE_PASSWORD#  script:#    - touch ~/.pypirc#    - 'echo &quot;[distutils]&quot; &gt;&gt; ~/.pypirc'#    - 'echo &quot;index-servers&quot; = &gt;&gt; ~/.pypirc'#    - 'echo &quot;  local&quot; &gt;&gt; ~/.pypirc'#    - 'echo &quot;&quot; &gt;&gt; ~/.pypirc'#    - 'echo &quot;[local]&quot; &gt;&gt; ~/.pypirc'#    - 'echo &quot;repository: 192.168.xx.xx:8282&quot; &gt;&gt; ~/.pypirc'#    - 'echo &quot;username: $u_name&quot; &gt;&gt; ~/.pypirc'#    - 'echo &quot;password: $u_pd&quot; &gt;&gt; ~/.pypirc'#    - python setup.py sdist bdist_wheel upload -r http://192.168.135.201:8282#方式三#    - python setup.py sdist bdist_wheel#    - pip install --extra-index-url http://192.168.135.201:8282/simple/ --trusted-host 192.168.135.201:8282 twine==4.0.2#    - twine upload dist/*</content>
	</entry>
	<entry>
		<title>Linux常用命令</title>
		<url>/article/48230.html</url>
		<content>linux常用指令linux指令入门文件类文件拷贝cp命令是Unix和Linux系统中用于复制文件和目录的命令命令语法1cp [选项] 源文件或目录[可多个] 目标文件或目录命令选项-a：归档模式，保留源文件或目录的所有属性，包括权限、所有者、时间戳等-f：强制模式，不提示确认即覆盖目标文件或目录-i：交互模式，覆盖目标文件或目录前询问是否确认-r：递归模式，复制目录及其下所有文件和子目录-v：详细模式，显示复制的每个文件或目录名称本地拷贝cp 命令用于复制文件或目录，以下是一些 cp 命令的示例，涵盖了各种常见用法：将文件从一个位置复制到另一个位置1234567891011121314151617# 将file.txt复制到目标目录cp file.txt /path/to/destination/# 复制多个文件到目标目录cp file1.txt file2.txt /path/to/destination/# 保留源文件的元数据cp -a file.txt /path/to/destination/# 强制复制，覆盖目标文件，即覆盖目标文件(如果目标已存在)，`-f` 选项用于强制复制，即使目标文件已存在cp -f file.txt /path/to/destination/# 交互式复制，提示是否覆盖，`-i` 选项用于交互式复制，会提示是否覆盖目标文件cp -i file.txt /path/to/destination/# 复制并显示进度条，`-v` 选项用于显示详细的复制信息，包括进度cp -v file.txt /path/to/destination/复制整个目录及其内容到目标目录12# `-r` 选项用于递归地复制目录及其内容cp -r /path/to/source_directory /path/to/destination_directory/  将多个文件复制到一个目录并合并为一个文件12# 合并为一个文件cp file1.txt file2.txt /path/to/destination/singlefile.txt将文件复制到另一个目录并重命名12# 复制并重命名cp file.txt /path/to/destination/newfile.txt这些示例涵盖了 cp 命令的常见用法，您可以根据您的具体需求进行调整。请注意，在进行文件操作时，请小心操作，以避免不必要的数据丢失或覆盖远程拷贝scp(Secure Copy Protocol)是一个用于在本地和远程系统之间进行安全文件传输的命令行工具它使用 SSH 协议来加密传输数据，通常用于在不同的计算机之间复制文件和目录。以下是 scp 命令的基本语法：语法1scp [选项] [源] [目标]选项：可以是多个选项，用于指定 scp 的不同行为。以下是一些常用选项：-r：递归复制目录及其内容-p：保留文件的权限和时间戳-i：指定要使用的身份验证密钥文件-P：指定 SSH 服务器的端口号(默认为 22)-q：安静模式，减少输出信息源：指定要复制的源文件或目录的路径可以是本地文件/目录或远程文件/目录，远程文件/目录的格式通常为 [user@]host:source，其中 user 是远程用户，host 是远程主机，source 是源文件/目录路径目标：指定要将文件/目录复制到的目标位置可以是本地路径或远程路径，远程路径的格式通常为 [user@]host:destination，其中 user 是远程用户，host 是远程主机，destination 是目标路径远程拷贝文件(-P 是指定端口，-r 是循环复制)1scp -P 8002 -r root@192.168.2.1:/home/hyc/test /home/hyc/文件权限一文带你学习Linux 中的文件权限概念和相关命令在 Linux 中，每个文件和目录都有相应的权限设置，用于确定哪些用户或组可以对其进行访问、读取、写入或执行操作文件权限定义了对文件的访问级别，确保只有经过授权的用户才能对文件进行操作，从而提高系统的安全性权限类别文件权限可以分为三个部分：用户权限、组权限和其他权限：用户权限: 用户权限指的是文件的所有者对文件的权限。文件的所有者可以是系统中的任何用户。以下是用户权限的字符表示：r（读取）：允许所有者读取文件内容或查看目录内容w（写入）：允许所有者修改文件内容或在目录中创建、删除文件x（执行）：对于文件，允许所有者执行文件；对于目录，允许所有者进入该目录-（无权限）：表示没有相应的权限组权限: 组权限指的是文件所属组中的用户对文件的权限。每个文件都会关联一个所属组。以下是组权限的字符表示：r（读取）：允许组成员读取文件内容或查看目录内容w（写入）：允许组成员修改文件内容或在目录中创建、删除文件x（执行）：对于文件，允许组成员执行文件；对于目录，允许组成员进入该目录-（无权限）：表示没有相应的权限其他权限: 其他权限指的是不属于文件所有者和所属组的用户对文件的权限。以下是其他权限的字符表示：r（读取）：允许其他用户读取文件内容或查看目录内容w（写入）：允许其他用户修改文件内容或在目录中创建、删除文件x（执行）：对于文件，允许其他用户执行文件；对于目录，允许其他用户进入该目录-（无权限）：表示没有相应的权限权限表示权限的表示方式有如下几种：文件权限的表示方式: 在 Linux 中，文件权限用一串字符来表示，共有 10 个字符，可以分为四个部分：文件类型、用户权限、组权限和其他权限文件权限的表示方式如下1- rwxrwxrwx其中，每个字符的含义如下：第一个字符表示文件类型。常见的文件类型包括 -(普通文件)和 d(目录)后面的三个字符表示用户权限，即文件所有者对文件的权限接下来的三个字符表示组权限，即文件所属组的用户对文件的权限最后的三个字符表示其他权限，即其他用户对文件的权限每个权限字符可以是以下四个字符之一：r(读取)：允许读取文件内容、查看目录内容w(写入)：允许修改文件内容、在该目录中创建、删除文件x(执行)：对于文件，允许执行文件；对于目录，允许进入该目录-(无权限)：表示没有相应的权限权限数字表示法: 权限数字表示法使用数字来表示权限设置。每个权限字符都被赋予一个特定的数字值：r：4w：2x：1-：0要设置文件权限，可以将这些数字相加。例如，要为文件所有者设置读取和写入权限，可以运行以下命令：1chmod 6 file_name这将为文件所有者设置读取和写入权限(4+2=6)权限符号表示法: 权限符号表示法使用符号来指定权限设置。以下是常用的权限符号表示法：+：添加权限-：移除权限=：设置权限使用权限符号表示法时，可以将符号与权限字符组合使用。例如，要为文件所有者添加写入权限，可以运行以下命令：1chmod u+w 文件名这将为文件所有者添加写入权限，可以使用以下符号来表示不同的权限：u：文件所有者g：文件所属组o：其他用户a：所有用户(包括所有者、所属组和其他用户)例如，要为文件所有者和文件所属组添加读取和写入权限，可以运行以下命令：1chmod ug+rw 文件名这将为文件所有者和文件所属组添加读取和写入权限语法格式语法格式：chmod [选项] 权限 文件，其中，权限可以使用权限符号表示法或权限数字表示法1chomd -R 777 /home/hyc创建文件夹在 Linux 中，你可以使用 mkdir 命令来创建文件夹(目录)1mkdir -p -m 755 OUTPUT_DIRmkdir: 表示创建目录(make directory)-p: 表示递归创建目录，如果指定的目录路径中的某些父目录不存在，-p 选项会自动创建这些父目录，而不会报错-m: 使用 -m 选项可以指定创建目录时的权限创建多个目录可以用1mkdir dir1 dir2 dir3删除文件(夹)可以使用 rm 命令来删除文件和目录，请谨慎使用此命令，因为删除操作是不可逆的，删除后的文件和目录无法恢复1rm [选项] 文件名可以使用不同的选项来改变删除行为。一些常用的选项包括：-f：强制删除，无需确认-i：交互式删除，删除前会进行确认-r 或 -R：递归删除，用于删除目录及其内容删除文件1rm example.txt删除文件夹1rm -r 文件夹名移动和更名12345# 移动文件mv file.txt /path/to/destination/# 更名文件mv file.txt /path/to/destination/newfile.txt查找搜索这三个命令适用于不同的用途。如果您需要按照文件属性、大小、权限等条件来搜索文件，并且需要搜索用户文件夹中的文件，find 是更适合的选择如果您只需要快速搜索文件名，并且可以接受不太详细的搜索结果，locate 可能更适合而 whereis 主要用于查找系统命令的位置findfind 命令是一种强大的文件搜索工具，可以按照文件属性、名称、大小等多个条件来搜索文件可以使用 find 来搜索整个文件系统，包括子目录它支持使用通配符和正则表达式来指定搜索条件find 的搜索是实时的，即每次运行都会进行完整的文件系统扫描，因此可能比较慢，特别是在大型文件系统上12# 在指定目录中搜索文件名为 &quot;file.txt&quot; 的文件find /path/to/search -name &quot;file.txt&quot;按照文件类型搜索12# 语法: find 搜索路径 [选项] 搜索内容find /home -type d -name huangyc-type d：查找目录-type f：查找普通文件-type l：查找软链接文件locatelocate 命令使用数据库来快速搜索文件，因此通常比 find 快得多它默认搜索整个文件系统，但通常限于搜索文件名locate 不支持通配符和正则表达式，只能搜索文件名的完全或部分匹配由于使用数据库，因此需要定期更新数据库以确保搜索结果的准确性，可以使用 updatedb 命令来手动更新数据库12345# 搜索文件名为 &quot;file.txt&quot; 的文件locate file.txt# ==&gt; 等价于find / -type f -name &quot;file.txt&quot;whereiswhereis 命令用于查找可执行文件、源代码和帮助文档的位置它通常用于查找系统命令或程序的二进制文件、源代码和手册页whereis 只返回命令的位置，不搜索用户文件或其他文件12# 查找命令 ls 的位置whereis ls磁盘类查看某个文件或目录占用磁盘空间的大小(需要进入到当前目录)1du -ah --max-depth=1查看全局文件系统磁盘占用1df -h查看当前路径下 各文件夹的磁盘占用1du -sh *防火墙端口类防火墙1234567891011121314151617# 重启防火墙firewall-cmd --reload# 开机启动防火墙systemctl enable firewalld# 开启防火墙systemctl start firewalld# 查看防火墙状态firewall-cmd --state# 禁止防火墙开机启动systemctl disable firewalld# 停止防火墙systemctl stop firewalld查看端口占用12345678910# 查看已开放的端口firewall-cmd --list-ports# 或# yum install net-tools# apt-get install net-tools  #适用于Debian和Ubuntu等发行版netstat -atunlp# 查看程序端口占用lsof -i:6006开启和关闭端口12345678# 开放端口（开放后需要要重启防火墙才生效）firewall-cmd --zone=public --add-port=3338/tcp --permanent# 关闭端口（关闭后需要要重启防火墙才生效）firewall-cmd --zone=public --remove-port=3338/tcp --permanent# 配置立即生效firewall-cmd --reloadwin端口拓展下windows下的端口查找和kill操作1234567891011C:\Users\Administrator&gt;netstat -aon | findstr &quot;4000&quot;  TCP    0.0.0.0:4000           0.0.0.0:0              LISTENING       12128  TCP    127.0.0.1:4000         127.0.0.1:57174        TIME_WAIT       0  TCP    127.0.0.1:4000         127.0.0.1:57175        TIME_WAIT       0  TCP    [::]:4000              [::]:0                 LISTENING       12128C:\Users\Administrator&gt;tasklist|findstr &quot;12128&quot;node.exe                     12128 Console                    3    141,152 KC:\Users\Administrator&gt;taskkill /T /F /PID 12128成功: 已终止 PID 12128 (属于 PID 24224 子进程)的进程。进程类杀死指定进程批量杀死进程查看进程号1234[root@iZ2zeh6nc313zs4azjijjnZ ~]# ps -ef | grep gitbook | grep -v grep|awk '{print $2}'240642938730425方式一：batch_shell12345678#!/bin/shdsp_admin_id=`ps -ef | grep gitbook | grep -v grep|awk '{print $2}'`echo $dsp_admin_idfor id in $dsp_admin_iddo    kill -9 $id      echo &quot;killed $id&quot; done方式二：单行执行12#!/bin/shps -ef | grep gitbook | grep -v grep|grep 4000|  awk '{print $2}' | xargs kill -9解压缩压缩123456tar -cvf jpg.tar *.jpg //将目录里所有jpg文件打包成jpg.tartar -czf jpg.tar.gz *.jpg //将目录里所有jpg文件打包成jpg.tar后，并且将其用gzip压缩rar a jpg.rar *.jpg //rar格式的压缩，需要先下载rar for linuxzip jpg.zip *.jpg //zip格式的压缩，需要先下载zip for linux解压缩12345678910tar -xvf file.tar -C DestPath 1.txt //解压 tar包  1.txt表示只需要解压出这个文件,默认解压所有tar -xzvf file.tar.gz //解压tar.gztar -xjvf file.tar.bz2 //解压 tar.bz2tar -xZvf file.tar.Z //解压tar.Zunrar e file.rar DestPath //解压rarunzip file.zip //解压zipbzip2 -d file.bz2 在命令行工具中，-d 是 bzip2 命令的一个选项，表示解压缩（decompress）。当你使用 bzip2 命令来处理 Bzip2 压缩的文件时，-d 选项告诉命令执行解压缩操作。具体来说，bzip2 -d 文件名.bz2 命令告诉系统将名为 “文件名.bz2” 的 Bzip2 压缩文件解压缩，还原为原始文件。如果不使用 -d 选项，bzip2 命令将默认执行压缩操作。所以，-d 是一个用于指示解压缩操作的命令行选项。日期时间显示日期12[root@iZ2zeh6nc313zs4azjijjnZ ~]# date2022年 03月 12日 星期六 10:57:52 CST显示日历12345678[root@iZ2zeh6nc313zs4azjijjnZ ~]# cal      三月 2022     日 一 二 三 四 五 六       1  2  3  4  5 6  7  8  9 10 11 1213 14 15 16 17 18 1920 21 22 23 24 25 2627 28 29 30 31环境变量在CentOS操作系统中，您可以使用几种方式来设置环境变量。以下是其中一些常见的方法：/etc/profile文件通过/etc/profile文件：在CentOS中，可以编辑/etc/profile文件来设置系统范围的环境变量。打开该文件并在文件末尾添加您想要设置的环境变量，例如：1export MY_VARIABLE=/path/to/my/directory保存文件后，重新登录或运行source /etc/profile以使更改生效~/.bash_profile或~/.bashrc通过~/.bash_profile或~/.bashrc文件：每个用户可以在其主目录中的.bash_profile或.bashrc文件中设置个人环境变量。这些文件会在用户登录时自动加载。打开其中一个文件，并在文件末尾添加类似的行：1export MY_VARIABLE=/path/to/my/directory保存文件后，注销并重新登录，或者使用命令source ~/.bash_profile（或source ~/.bashrc）来加载更改/etc/environment文件通过/etc/environment文件：可以编辑/etc/environment文件来设置系统范围的全局环境变量。该文件包含逐行的”key=value”对。例如：1MY_VARIABLE=/path/to/my/directory保存文件后，重新登录或重启系统以使更改生效命令行临时设置可以在命令行中临时设置环境变量，仅对当前会话有效。使用以下语法：1export MY_VARIABLE=/path/to/my/directory这将在当前会话中设置环境变量，但在注销或重新启动后不会保留请注意，设置环境变量后，您可能需要重新启动相关应用程序或终端窗口，以便它们能够读取新的环境变量值后台执行systemctl方式cloudreve私有云盘配置后台运行12345678910111213141516171819202122vi /usr/lib/systemd/system/cloudreve.service# 输入以下内容，注意最好别带中文[Unit]Description=CloudreveDocumentation=https://docs.cloudreve.orgAfter=network.targetWants=network.target[Service]Type=simpleWorkingDirectory=/home/software/cloudreveExecStart=/home/software/cloudreve/cloudreveRestart=on-abnormalRestartSec=5sKillMode=mixedStandardOutput=nullStandardError=syslog[Install]WantedBy=multi-user.target更新重启服务12345678# 更新配置systemctl daemon-reload# 启动服务systemctl start cloudreve# 设置开机启动systemctl enable cloudreve管理命令1234567891011# 启动服务systemctl start cloudreve# 停止服务systemctl stop cloudreve# 重启服务systemctl restart cloudreve# 查看状态systemctl status cloudrevenohup方式1nohup python run.pt &gt; log.log &amp;svn基本使用centos安装svn客户端12345# 安装svn客户端yum install mod_dav_svn# 查看当前svn的版本svnserve --version拉取项目(已存在)1svn checkout svn://10.10.13.13提交上传文件123svn add &lt;文件路径&gt;svn commit -m &quot;提交说明&quot; &lt;文件路径&gt;如果要提交整个目录下的所有文件，可以将 &lt;文件路径&gt; 替换为目录的路径更新文件checkout只在第一次链接时使用一次，以后如果进行更新操作请使用update指令sh脚本简述在 Linux 中，.sh 文件通常表示一个 Shell 脚本。Shell 脚本是一种文本文件，包含一系列 Shell 命令，这些命令按照脚本文件的顺序依次执行。Shell 脚本通常用来自动执行一系列任务，可以包括文件操作、系统配置、运行程序等。Shell 脚本的文件扩展名 .sh 表示这是一个 Shell 脚本文件，而在文件的开头一般会包含指定解释器的声明，例如：1#!/bin/bash上述声明告诉系统使用 Bash 解释器来执行该脚本。#!/bin/bash 称为 “shebang” 行，用于指定脚本的解释器。以下是一个简单的 Shell 脚本的示例，该脚本用于输出当前用户的用户名和主目录：1234567891011#!/bin/bash# 获取当前用户的用户名username=$(whoami)# 获取当前用户的主目录home_directory=$(echo ~)# 输出信息echo &quot;当前用户：$username&quot;echo &quot;主目录：$home_directory&quot;你可以将上述脚本保存为一个 .sh 文件，例如 myscript.sh，然后通过在终端中运行 ./myscript.sh 来执行它。Shell 脚本是 Linux 中强大而灵活的工具，用于简化和自动化许多系统任务。不同的 Shell（如 Bash、Zsh、Sh等）提供了各种功能和语法，使得脚本编写更加灵活。用法示例基本使用12345678910111213141516171819# 获取当前日期时间DATESTR=`date +%Y%m%d-%H%M%S`# 获取当前日期DATESTR=`date +%Y%m%d`# 定义变量MAX_STEP=50000# Number of GPUs per GPU workerGPUS_PER_NODE=$(python -c 'import torch; print(torch.cuda.device_count())')# 函数定义function usage() {    echo 'Usage: bash finetune/finetune_lora_ds.sh [-m MODEL_PATH] [-d DATA_PATH] [--deepspeed DS_CONFIG_PATH]'}# 获取当前路径DIR=`pwd`花括号{}的使用在 Bash 脚本中，花括号 {} 的使用有几个不同的场景：变量扩展： 示例中，${RUN_NAME}、${PRE_SEQ_LEN}、${LR}、${DATESTR} 都是变量使用花括号可以帮助明确变量名的边界，防止解释器将变量名错误地理解为其他标识符的一部分例如，${RUN_NAME}-${PRE_SEQ_LEN}-${LR}-${DATESTR} 确保这四个变量名的边界清晰，不会被错误地解释1OUTPUT_DIR=output/${RUN_NAME}-${PRE_SEQ_LEN}-${LR}-${DATESTR}数组的索引： 在数组操作时，花括号也用于指定数组元素的索引。12my_array=(&quot;value1&quot; &quot;value2&quot; &quot;value3&quot;)echo ${my_array[0]}  # 输出数组的第一个元素代码块： 花括号还可用于创建代码块在这种情况下，花括号将一系列命令组合在一起，以便它们可以被视为一个单独的实体。这在条件语句、循环等结构中很常见12345678910if [ condition ]; then    # commandsfi# 或者使用花括号创建代码块if [ condition ]; then{    # commands}fi总的来说，在你的示例中，花括号主要用于变量扩展，确保将变量名包含在明确的边界内。这在字符串拼接时尤其有用，以避免歧义其他命令查看文件夹及子目录下文件数量12[root@uslave02 datasets]# ls -lR /home/hyc/ | grep -E &quot;.jpg|.png|.jpeg|.jpe&quot; |wc -l6599Linux 手动清除NVIDIA显存1234567apt-get install psmisc#查找占用GPU资源的PIDfuser -v /dev/nvidia*# 解除显存占用kill -9 PID系统级版本信息12345678[root@iZ2zeh6nc313zs4azjijjnZ ~]# uname -r3.10.0-1062.12.1.el7.x86_64[root@iZ2zeh6nc313zs4azjijjnZ ~]# lsb_release -aLSB Version:    :core-4.1-amd64:core-4.1-noarchDistributor ID: CentOSDescription:    CentOS Linux release 7.7.1908 (Core)Release:        7.7.1908Codename:       Core软链接ln是linux中又一个非常重要命令，它的功能是为某一个文件在另外一个位置建立一个同步的链接当我们需要在不同的目录，用到相同的文件时，我们不需要在每一个需要的目录下都放一个必须相同的文件我们只要在某个固定的目录，放上该文件，然后在 其它的目录下用ln命令链接(link)它就可以，不必重复的占用磁盘空间这个命令最常用的参数是-s,具体用法是：ln -s 源文件 目标文件报错例子1ImportError: /lib64/libstdc++.so.6: version `GLIBCXX_3.4.21' not found (required by /home/anaconda/install/envs/stk37/lib/python3.7/site-packages/pandas/_libs/window/aggregations.cpython-37m-x86_64-linux-gnu.so)解决方案1234567891011# 查找lib库sudo find / -name &quot;libstdc++.so.6*&quot;# 从上述列表里找一个路径，查看是否存在我们需要的'GLIBCXX_3.4.21'strings /home/anaconda/lib/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6 |grep GLIBCXX_3.4.21# 如果存在的话，建立软链接到我们需要的地方，这里例子里是/lib64/libstdc++.so.6ln -s /home/anaconda/lib/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6 /lib64/libstdc++.so.6# 注意如果/lib64/libstdc++.so.6 已经存在，此时上述命令会报错，需要先备份后删除，再执行ln，(但是里面没有我们需要的'GLIBCXX_3.4.21'，可以用strings查看)cp /lib64/libstdc++.so.6 /lib64/libstdc++.so.6_bakrm /lib64/libstdc++.so.6ln -s /home/anaconda/lib/x86_64-conda_cos6-linux-gnu/sysroot/lib/libstdc++.so.6 /lib64/libstdc++.so.6# 至此完成软链接配置代理配置系统代理yum代理wget代理修改配置文件/etc/profile，即可配置centos通过代理进行上网123456789# 编辑配置文件vi /etc/profile# 添加代理服务器ip地址和端口号export http_proxy=http://192.168.xx.xx:portexport https_proxy=http://192.168.xx.xx:portexport ftp_proxy=$http_proxy# 使配置文件生效source /etc/profile在/etc/yum.conf后面添加以下内容123456# 编辑配置文件vi /etc/yum.confproxy=http://127.0.0.1:8080# 使配置文件生效source /etc/yum.conf编辑配置文件/etc/wgetrc添加下面两行1234567# 编辑配置文件vi /etc/wgetrchttp_proxy=http://127.0.0.1:8080https_proxy=http://127.0.0.1:8080# 使配置文件生效source /etc/wgetrc包管理安装 RPM 包1sudo rpm -ivh --prefix=&lt;installation_path&gt; &lt;package_name&gt;--prefix为可选项，用于指定安装路径。如不设置，系统会将安装到默认路径</content>
	</entry>
	<entry>
		<title>gitbook博客搭建</title>
		<url>/article/44835.html</url>
		<content>gitbook使用 Gitbook 打造你的电子书gitbook官网环境搭建nodejs官网下载，这里可以选择12.16.1版本，太新的版本后面可能会出现奇怪的问题，这是笔者使用的版本执行以下命令安装gitbook命令123456npm install -g gitbook-cli# 查看是否安装完成, 能正确显示版本号就行，这里推荐gitBook==3.2.3gitbook -VCLI version: 2.3.2GitBook version: 3.2.3初始化电子书初始化电子书1gitbook init这时候会生成下面两个文件README.md和SUMMARY.mdREADME.md 书籍的介绍写在这个文件里；SUMMARY.md书籍的目录结构在这里配置安装插件有配置book.json时，执行gitbook install会安装插件到node_modules目录使用以下命令启动gitbook，打开http://localhost:4000就可以访问了，1gitbook serve重要文件book.json，没有的话，自己新建一个就好用于配置插件和书籍的基本信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116{    &quot;title&quot;: &quot;书籍目录&quot;,    &quot;author&quot;: &quot;narutohyc&quot;,    &quot;description&quot;: &quot;书籍目录，记录一些工作和学习中的笔记&quot;,    &quot;language&quot;: &quot;zh-hans&quot;,    &quot;structure&quot;: {        &quot;readme&quot;: &quot;README.md&quot;    },    &quot;links&quot;: {        &quot;sidebar&quot;: {            &quot;我の小窝~&quot;: &quot;https://github.com/narutohyc&quot;        }    },    &quot;plugins&quot;: [        &quot;-sharing&quot;,        &quot;splitter&quot;,        &quot;expandable-chapters-small&quot;,        &quot;anchors&quot;,        &quot;github&quot;,        &quot;github-buttons&quot;,        &quot;donate&quot;,        &quot;sharing-plus&quot;,        &quot;anchor-navigation-ex&quot;,        &quot;mathjax&quot;,        &quot;mermaid-gb3&quot;,        &quot;tbfed-pagefooter&quot;,        &quot;code&quot;,        &quot;search-plus&quot;, &quot;-lunr&quot;, &quot;-search&quot;,        &quot;lightbox&quot;,        &quot;theme-comscore&quot;,        &quot;valine&quot;,        &quot;pageview-count&quot;,        &quot;favicon-absolute&quot;,        &quot;copyright-v&quot;    ],    &quot;pluginsConfig&quot;: {        &quot;theme-default&quot;: {            &quot;showLevel&quot;: true        },        &quot;github&quot;: {            &quot;url&quot;: &quot;https://github.com/narutohyc&quot;        },        &quot;tbfed-pagefooter&quot;: {            &quot;copyright&quot;: &quot;Copyright © narutohyc.com 2021&quot;,            &quot;modify_label&quot;: &quot;该文件修订时间：&quot;,            &quot;modify_format&quot;: &quot;YYYY-MM-DD HH:mm:ss&quot;        },        &quot;github-buttons&quot;: {            &quot;buttons&quot;: [{                &quot;user&quot;: &quot;narutohyc&quot;,                &quot;repo&quot;: &quot;bk_index&quot;,                &quot;type&quot;: &quot;star&quot;,                &quot;size&quot;: &quot;small&quot;,                &quot;count&quot;: true                }            ]        },        &quot;donate&quot;: {            &quot;wechat&quot;: &quot;https://s2.loli.net/2022/03/23/WDiTVSamQBJdEA4.png&quot;,            &quot;alipay&quot;: &quot;https://s2.loli.net/2022/03/23/dEYjkaSGXwe7rnu.png&quot;,            &quot;title&quot;: &quot;&quot;,            &quot;button&quot;: &quot;欢迎打赏&quot;,            &quot;alipayText&quot;: &quot;alipay打赏&quot;,            &quot;wechatText&quot;: &quot;wechat打赏&quot;        },        &quot;sharing&quot;: {            &quot;douban&quot;: true,            &quot;facebook&quot;: true,            &quot;google&quot;: true,            &quot;hatenaBookmark&quot;: false,            &quot;instapaper&quot;: false,            &quot;line&quot;: false,            &quot;linkedin&quot;: false,            &quot;messenger&quot;: false,            &quot;pocket&quot;: false,            &quot;qq&quot;: true,            &quot;qzone&quot;: true,            &quot;stumbleupon&quot;: false,            &quot;twitter&quot;: true,            &quot;viber&quot;: false,            &quot;vk&quot;: false,            &quot;weibo&quot;: true,            &quot;whatsapp&quot;: false,            &quot;all&quot;: [                &quot;google&quot;, &quot;facebook&quot;, &quot;weibo&quot;, &quot;twitter&quot;,                &quot;qq&quot;, &quot;qzone&quot;, &quot;linkedin&quot;, &quot;pocket&quot;            ]        },        &quot;anchor-navigation-ex&quot;: {            &quot;showLevel&quot;: false        },        &quot;valine&quot;: {            &quot;appId&quot;: &quot;leancloud申请的appId&quot;,            &quot;appKey&quot;: &quot;leancloud申请的appKey&quot;,            &quot;avatar&quot;: &quot;wavatar&quot;,            &quot;placeholder&quot;: &quot;欢迎留下评论交流~&quot;,            &quot;pageSize&quot;: 15        },        &quot;favicon-absolute&quot;:{            &quot;favicon&quot;: &quot;./chapters/res/other/favicon.ico&quot;,            &quot;appleTouchIconPrecomposed152&quot;: &quot;./chapters/res/other/favicon.ico&quot;        },        &quot;fontSettings&quot;: {            &quot;family&quot;: &quot;msyh&quot;,            &quot;size&quot;: 2        },        &quot;copyright-v&quot;: {            &quot;site&quot;: &quot;https://hycbook.github.io/bk_python/&quot;,            &quot;author&quot;: &quot;narutohyc&quot;,            &quot;website&quot;: &quot;python元知识驿站&quot;,            &quot;image&quot;: &quot;https://s2.loli.net/2022/03/24/pbMd1BCgUNzi7mG.png&quot;,            &quot;copyProtect&quot;: true,        &quot;enableFooter&quot;: false        }    }}Gitbook 默认带有 5 个插件：highlightsearchsharingfont-settingslivereload重要插件说明:github: 右上角添加github图标donate: 文章打赏插件，在文末点击展示付款码sharing-plus: 比默认的分享插件，多了更多的分享按钮mathjax: LaTeX公式显示插件，可以支持绝大多数的数据公式，个别的公式还是会有问题mermaid-gb3: Mermaid渲染插件，可以支持画图code: 为代码块添加行号和复制按钮，复制按钮可关闭，单行代码无行号search-plus: 文章内容搜索插件，支持中文，使用这个插件时要移除默认的lunr和search插件lightbox: 当前页显示大图，点击图片会放大显示，不支持切换图片theme-comscore: 主题插件，默认主题是黑白的，comscore 主题是彩色的，标题和正文颜色有所区分valine: 诞生于2017年8月7日，是一款基于LeanCloud的快速、简洁且高效的无后端评论系统pageview-count: 页面浏览量统计，但好像用不了，自己结合cutercounter进行了改造favicon-absolute: 网页icon图标定义copyright-v: 版权插件，复制文章内容时会附带上定义好的版权信息，和知乎复制的效果一样.bookignore，存放不想被打包的资源123456.gitignore.bookignorebook_default.json_book/chapters/*.md*.bat*.pdf_book使用gitbook build命令，会在项目的目录下生成一个 _book 目录，里面的内容为静态站点的资源文件发布出来的就是这个目录下的资源gitbook build只负责生成静态文件，gitbook serve不仅生成文件，还会启动网站服务使用选项 --log=debug 和 --debug 来获取更好的错误消息1gitbook build ./ --log=debug --debugSUMMARY.md，用于存放数据目录12345678# Summary* [Introduction](README.md)* [alicloud_deployment](./chapters/1.alicloud_deployment.md)* [reinstall_the_system](./chapters/2.reinstall_the_system.md)* [Linux常用命令](./chapters/3.Linux常用命令.md)* [github协作](./chapters/4.github协作.md)* [gitbook博客搭建](./chapters/5.gitbook博客搭建.md)node_modules终端执行gitbook install可以安装book.json配置的插件，下载的插件会在node_modules文件夹markdown语法基本撰写和格式语法Markdown是一种轻量级标记语言，创始人是约翰·格鲁伯(John Gruber)它允许人们使用易读易写的纯文本格式编写文档，然后转换成有效的 HTML 文档推荐配合Typora编辑器一起食用改造访客数待完成…valine博客Valine评论样式美化美化待完成…部署打造完美写作系统：Gitbook+Github Pages+Github ActionsTravis CI简介什么是持续集成(CI)/持续交付(CD)管道？GitHub ActionsGitHub Actions 是什么？前面部分的内容主要讲解了gitbook的构建和美化优化等内容这部分讲一下如何借助github pages和gitee pages部署自己的博客网站，正文开始前，先讲解下Github Actions的基础概念Github ActionsCI/CD:  持续集成(CI)/持续交付(CD)管道是一系列步骤，其中包括从CI/CD流程开始的所有阶段，并负责创建自动化和无缝的软件交付。而使用CI/CD管道，软件发布工件可以从代码检入阶段到测试、构建、部署和生产阶段一直在管道中前进这一概念之所以强大，是因为一旦指定了管道，就可以将其部分或全部实现自动化，从而加快了流程，并减少了错误。换句话说，CI/CD管道使组织每天更轻松地自动多次交付软件Travis CI: 提供的是持续集成服务（Continuous Integration，简称 CI）。它绑定 Github 上面的项目，只要有新的代码，就会自动抓取。然后，提供一个运行环境，执行测试，完成构建，还能部署到服务器Github Actions: GitHub Actions是你在GitHub库中创建自定义的工作流，工作流指的就是自动化的流程，比如构建、测试、打包、发布、部署等等，可以设置触发条件或定时执行，由github提供服务器环境查看个人帐户的 GitHub Actions 使用情况在任何页面的右上角，单击个人资料照片，然后单击“设置”在侧边栏的“访问”部分，单击“ 计费和计划”，然后单击“计划和使用情况” 在“本月使用情况”下的“操作”部分中，查看所用分钟数的详细信息可以展开此部分，按运行器类型显示使用情况的明细在“用于操作和包的存储”下，查看用于 GitHub Actions 和 GitHub Packages 的存储使用详情。 存储使用数据每小时同步一次（可选）在“本月使用情况”旁边，单击“获取使用情况报告”向帐户主电子邮件地址发送关于 GitHub Actions 和 GitHub Packages 存储使用情况的 CSV 报告持续集成(CI)/持续交付(CD)是一种自动化管理流程，而Travis CI和Github Actions都是具体的实现，后者是github官方提供的githun actions的使用案例可以自己百度或参考github协作github pagesGitHub Pages Deploy Action自动发布到github pages分支(可以自己设置分支)配置tokengithub项目下新建：name=TOKEN，value=github生成的token基本用法123456- name: Deploy #步骤4：把编译好的静态目录上传到gh-pages分支上  uses: JamesIves/github-pages-deploy-action@3.1.5 #这里调用第三方依赖，用于上传代码到指定分支上  with:    ACCESS_TOKEN: ${{ secrets.TOKEN }}    BRANCH: gh-pages #github pages 默认的分支    FOLDER: _book #上传文件的目录，我这个目录在根目录下，如果是子目录可以这么写  docs/.vuepress/distgitee pagesGitee 如何自动部署 Pages？还是用 GitHub Actions!使用 GitHub Actions 解决了 GitHub 代码自动同步 Gitee 的问题，但我们的博客仓库代码同步到 Gitee 后，并不能像 GitHub 一样自动部署 Pages，如果不使用付费的 Gitee Pages Pro 服务，那我们该怎么实现 Gitee 自动部署 Pages 呢基本用法新建一个name=GITEE_PASSWORD, value=gitee登陆密码的secrets1234567891011- name: Build Gitee Pages  uses: yanglbme/gitee-pages-action@main  with:    # 注意替换为你的 Gitee 用户名    gitee-username: narutohyc    # 注意在 Settings-&gt;Secrets 配置 GITEE_PASSWORD    gitee-password: ${{ secrets.GITEE_PASSWORD }}    # 注意替换为你的 Gitee 仓库，仓库名严格区分大小写，请准确填写，否则会出错    gitee-repo: hycBook/bk_index    # 要部署的分支，默认是 master，若是其他分支，则需要指定（指定的分支必须存在）    branch: gh-pages多吉云cdn融合CDN是一种稳定易用、低成本的内容分发服务，为网页 Web、应用 App、文件下载、音视频点播等场景提供加速服务，提升用户体验，降低服务端带宽压力，节省成本的方案多吉云不自建节点，而是融合阿里云、腾讯云、华为云、网宿等国内一线 CDN 厂商的节点，质量可靠，实时调整调度策略，精准保障访问质量添加网站使用多吉云需要网站备案后才可以使用，配置步骤如下：注册多吉云账号添加域名管理，详细的配置见下方添加解析，需要在域名网站添加CNAME解析(多吉云会提供)| 字段              | 填写说明                                                     || ————————- | —————————————————————————————— || 记录类型          | 选择 CNAME                                                 || 主机记录          | 填写完整域名除去主域名后的部分，填写示例： 加速域名为 example.com，则主机记录填写 @加速域名为 cdn.example.com，则主机记录填写 cdn加速域名为 doge.cdn.example.com，则主机记录填写 doge.cdn || 线路类型/请求来源 | 建议保持默认                                               || 记录值            | 填写控制台域名管理中该域名的 CNAME 值，一般以 .s2-xxx.dogedns.com（xxx 根据业务类型会有不同）结尾 || TTL               | 建议保持默认值 10 分钟                                       |点击确认保存记录，等待生效即可验证生效要验证 CNAME 是否配置成功，可以使用命令行工具测试：在 Windows 系统下，可以使用系统自带的nslookup工具进行测试：运行CMD程序，输入指令：1nslookup -qt=cname 加速域名请将加速域名替换为你添加的加速域名，并运行上述指令。 如果程序显示的canonical name和多吉云控制台提供的CNAME值一致，则表示你的加速域名已经成功配置 CNAME，加速生效了在MacOS或Linux系统下，请使用dig命令测试：在终端内运行命令：1dig 加速域名请将加速域名替换为你添加的加速域名，并运行上述指令。 如果程序显示的CNAME和多吉云控制台提供的CNAME值一致，则表示你的加速域名已经成功配置CNAME，加速生效了详细配置网站加速详细配置，可以参考如下tomcat部署网站tomcat安装下载Linux版本的tomcat下载地址：Apache Tomcat® - Apache Tomcat 9 Software Downloads比如：apache-tomcat-9.0.80.tar.gz，将其解压到/usr/local/tomcat启动1234567cd /usr/soft/apache-tomcat-9.0.80/bin# 启动./startup.sh# 停止./shutdown.sh注意：tomcat依赖了jdk静态网站文件将静态网站文件添加软连接到/usr/local/tomcat/apache-tomcat-9.0.80/webapps1ln -s 静态网站文件 /usr/local/tomcat/apache-tomcat-9.0.80/webapps/静态网站文件同时修改配置文件/usr/local/tomcat/apache-tomcat-9.0.80/conf/server.xml123456789101112131415161718192021222324252627&lt;!-- 端口 --&gt;&lt;Connector port=&quot;8080&quot; protocol=&quot;HTTP/1.1&quot;           connectionTimeout=&quot;20000&quot;           redirectPort=&quot;443&quot;           maxParameterCount=&quot;1000&quot; /&gt;&lt;!-- 项目路径 --&gt;&lt;Host name=&quot;localhost&quot;  appBase=&quot;webapps&quot;    unpackWARs=&quot;true&quot; autoDeploy=&quot;true&quot;&gt;&lt;!-- SingleSignOn valve, share authentication between web applications     Documentation at: /docs/config/valve.html --&gt;&lt;!--&lt;Valve className=&quot;org.apache.catalina.authenticator.SingleSignOn&quot; /&gt;--&gt;&lt;!-- Access log processes all example.     Documentation at: /docs/config/valve.html     Note: The pattern used is equivalent to using pattern=&quot;common&quot; --&gt;&lt;Valve className=&quot;org.apache.catalina.valves.AccessLogValve&quot; directory=&quot;logs&quot;       prefix=&quot;localhost_access_log&quot; suffix=&quot;.txt&quot;       pattern=&quot;%h %l %u %t &amp;quot;%r&amp;quot; %s %b&quot; /&gt;        &lt;!-- 添加项目的路径，不然会出现资源目录找不到的情况 --&gt;&lt;Context path=&quot;&quot; docBase=&quot;/usr/local/tomcat/apache-tomcat-9.0.80/webapps/hycBookPage&quot; reloadale=&quot;false&quot; /&gt;&lt;/Host&gt;   这里主要修改端口和项目路径端口开放放行指定端口号123456789101112131415# 开放8080端口firewall-cmd --add-port=8080/tcp --zone=public --permanent# 参数解释--add-port=被放行的端口号/tcp--zone=作用域--permanent=永久有效# 重启防火墙让其生效systemctl restart firewalld# 查看防火墙状态systemctl status firewalld# 查看防火墙放行的端口号firewall-cmd --list-portnginxyum方式以下是在CentOS系统中安装Nginx的步骤：更新系统软件包1sudo yum update安装EPEL存储库1sudo yum install epel-release安装Nginx1sudo yum install nginx启动Nginx服务1sudo systemctl start nginx设置Nginx服务自启动1sudo systemctl enable nginx验证Nginx是否运行1sudo systemctl status nginx如果一切正常，输出应该是Active: active (running)或者类似的信息现在，你已经成功在CentOS系统上安装和启动了Nginx源码方式直接访问Nginx官网，下载对应的安装包，本次案例选择的是nginx-1.6.3.tar.gz版本，安装环境是centos712#下载nginx-1.6.3.tar.gzwget -c https://nginx.org/download/nginx-1.6.3.tar.gz提示wget命令找不到，使用如下命令，进行安装，之后再次执行上述下载命令1yum install wget安装gcc 环境1yum install gcc-c++安装PCRE依赖库1yum install -y pcre pcre-devel安装zlib 依赖库1yum install -y zlib zlib-devel安装OpenSSL安全套接字层密码库1yum install -y openssl openssl-devel解压Nginx: 安装完以上环境库之后，接着进行解压操作12# 解压文件夹tar -zxvf nginx-1.6.3.tar.gz执行配置命令12345cd nginx-1.6.3./configure --prefix=/usr/local/nginx --with-http_stub_status_module --with-http_ssl_modulemake install查找安装路径1whereis nginx启动服务12345678910111213141516cd /usr/local/nginx/sbin/# 启动./nginx# 停止，此方式相当于先查出nginx进程id再使用kill命令强制杀掉进程./nginx -s stop# 停止，此方式停止步骤是待nginx进程处理任务完毕进行停止./nginx -s quit# 重新加载配置文件，Nginx服务不会中断./nginx -s reload# 查看nginx进程ps -ef|grep nginx修改配置文件: 比如，修改端口号，默认端口号为801234567cd /usr/local/nginx/conf# 备份原始配置文件cp nginx.conf nginx.conf.bak# 修改配置文件vi nginx.conf到此，nginx安装基本完成，直接在浏览器上访问服务器地址ip:80，就可以进入页面配置tomcat打开/usr/local/nginx/conf/nginx.conf123456789101112131415161718server {    listen       80;    server_name  localhost;    #charset koi8-r;    #access_log  logs/host.access.log  main;    location / {        root   html;        proxy_pass http://127.0.0.1:8080;        index  index.html index.htm;        #proxy_redirect off;        proxy_set_header Host $host;        proxy_set_header X-Real-IP $remote_addr;        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;    }}自定义域名配置ssl证书：解压下载的证书文件压缩包，那么你会得到两个文件一个.key一个.pem结尾的文件，这就是SSL配置最主要的两个文件打开/usr/local/nginx/conf/nginx.conf，添加如下配置1234567891011121314151617181920212223# HTTPS serverserver {    listen       443 ssl;    server_name  localhost;    ssl on;    ssl_certificate  ../ssl/hycbook.com_nginx/hycbook.com_bundle.pem;    ssl_certificate_key ../ssl/hycbook.com_nginx/hycbook.com.key;    ssl_session_timeout 5m;    ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;    ssl_protocols TLSv1 TLSv1.1 TLSv1.2;    ssl_prefer_server_ciphers on;    location / {        proxy_pass http://127.0.0.1:8080;        proxy_set_header   Host             $host;        proxy_set_header   X-Real-IP        $remote_addr;        proxy_set_header   X-Forwarded-For  $proxy_add_x_forwarded_for;        proxy_http_version 1.1;        proxy_set_header Upgrade $http_upgrade;        proxy_set_header Connection &quot;upgrade&quot;;    }}</content>
	</entry>
	<entry>
		<title>重装系统</title>
		<url>/article/41279.html</url>
		<content>重装系统安装系统不区分windows和linux，方式都是一样的u盘启动盘制作官网工具自己做镜像百度搜索微软官网工具下载后，使用管理员身份运行按步骤操作，其中下载镜像会比较耗时(好像只能下载win10)制作启动盘完成后，就可以开始安装系统了百度搜索itellyou，下载系统镜像下载u盘启动盘制作工具UltraISO，安装后执行制作启动盘完成后，就可以开始安装系统了磁盘格式化以 UEFI 启动的 Windows 磁盘必须是 GPT 格式，这里介绍如何在安装 Windows 期间将磁盘从 MBR 转换成 GPT。在安装界面按Shift+F10或Fn+Shift+F10进入cmd界面输入diskpart，进入管理界面输入list disk查看所有的磁盘情况12345磁盘 ###  状态           大小     可用     Dyn  Gpt--------  -------------  -------  -------  ---  ---磁盘 0    联机              119 GB   118 GB        *磁盘 1    联机              465 GB  1024 KB磁盘 2    联机               28 GB      0 B        *选择磁盘select disk 1注意这里选择的是磁盘，而不是分区！不是通常所说的 C 盘/D 盘，而是一块 SSD，或一块机械硬盘。执行clean或clean all清除所有数据操作会清除选中磁盘中的所有数据，是所有数据，毫无保留！清除结束，然后敲入 convert gpt 命令完成转换操作结束之后输入exit切换到 Windows 安装程序继续安装即可激活工具(点击下载)主板启动热键表中没有的机型尝试：ESC、下F8、9、F10、11、F2台式机品牌启动按键笔记本品牌启动按键主板品牌启动按键联想F12联想F12华硕F8惠普F12戴尔F12微星F11华硕F8华硕ESC戴尔ESC惠普F9神舟F12联想F12有些联想电脑有NOVO键是一键恢复按键。在电脑的侧面有个很小的按钮，旁边有标号——“一个逆时针箭头”，作用是：关机状态下，按此按键可启动联想一键恢复系统或,BIOS，设置程序，或者进入启动菜单分区合并工具使用磁盘管理工具合并分区步骤1. 按Win+R输入diskmgmt.msc并按Enter键打开磁盘管理工具。步骤2. 在主界面中右键单击与目标分区（C分区）相邻右侧的分区，然后选择“删除卷”步骤3. 右键单击与刚刚删除产生的未分配空间相邻左侧的目标分区（C分区），然后选择“扩展卷”步骤4. 在弹出页面中设置您想要扩展的空间量，然后依次单击“下一步”&gt;“完成”即可完成Windows11合并分区任务，轻松将未分配磁盘合并到C盘使用傲梅分区助手合并分区(推荐)系统内置的磁盘管理工具想要实现Win11合并分区需求，需要用到扩展卷功能。而扩展卷功能存在限制条件，又需要目标分区右侧相邻处有未分配空间，否则扩展卷选项变灰，无法使用。而对于我们日常工作生活中正常使用的硬盘而言，通常来说是不会出现未分配空间的，因此我们需要删除已有分区来创造未分配磁盘，这会导致分区上的数据丢失，很不友好。那么问题来了，是否有一种分区合并软件能够实现无损合并硬盘分区呢？答案是肯定的。傲梅分区助手即为符合以上要求的专业磁盘分区管理工具，可以帮助用户轻松合并同一硬盘上处于任意位置的未分配空间，即使没有未分配空间也可以轻松将两个相邻的正在使用的正常分区合并为一个分区，而不需要删除分区，并且兼容性强，在大多数Windows PC和Windows Server操作系统中皆可稳定运行DiskGenius是一款专业级的 数据恢复软件 ,算法精湛、功能强大,用户群体广泛;支持各种情况下的文件恢复、分区恢复,恢复效果好;文件预览、扇区编辑、加密分区恢复、Ext4分区恢复、RAID恢复等高级功能其他高级用法禁用更新要完全禁止 Win11 更新，您必须使用服务窗口，操作方法如下：按Win + R打开 RUN 对话框，键入services.msc，然后按Enter现在，向下滚动并双击Windows Update如果它正在运行，请单击停止然后单击启动类型旁边的下拉菜单选择禁用单击确定软连接mklink命令(需要管理员)1格式：MKLINK [[/D]|[/H]|[/J]] 创建的链接名 源目标地址/d 给目录创建符号链接，简称符号链接、软链接/h 创建硬链接，简称硬链接/j 给目录创建联接点，简称软链接例子：创建目录软连接到指定目录1mklink /j &quot;D:\壁纸&quot; &quot;S:\实际壁纸文件夹&quot;类型能否链接到文件能否链接到目录能否跨磁盘能否指向不存在的目标能否指向相对目录如何删除快捷方式能能能能否del file硬链接能否否否否del file软链接否能能能否rd folder符号链接能能能能能rd folder or del file固定ipbat脚本(推荐)图形化界面自动获取ip12345678910111213@echo //-=-=-=-=-=-=-=-=-=-=-=-=-=-=@echo //  【自动获取】@echo //  设置IP，子网掩码，网关@echo //-=-=-=-=-=-=-=-=-=-=-=-=-=-=%1 mshta vbscript:CreateObject(&quot;Shell.Application&quot;).ShellExecute(&quot;cmd.exe&quot;,&quot;/c %~s0 ::&quot;,&quot;&quot;,&quot;runas&quot;,1)(window.close)&amp;&amp;exitcd /d &quot;%~dp0&quot;netsh interface ip set address name=&quot;WLAN&quot; source=dhcp @echo //-=-=-=-=-=-=-=-=-=-=-=-=-=-=@echo //  设置DNS@echo //-=-=-=-=-=-=-=-=-=-=-=-=-=-=netsh interface ip set dns name = &quot;WLAN&quot; static addr = none@echo //  自动获取IP设置完成！@pause固定指定ip1234567891011121314@echo //-=-=-=-=-=-=-=-=-=-=-=-=-=-=@echo //  【固定设置】@echo //  设置IP，子网掩码，网关@echo //-=-=-=-=-=-=-=-=-=-=-=-=-=-=%1 mshta vbscript:CreateObject(&quot;Shell.Application&quot;).ShellExecute(&quot;cmd.exe&quot;,&quot;/c %~s0 ::&quot;,&quot;&quot;,&quot;runas&quot;,1)(window.close)&amp;&amp;exitcd /d &quot;%~dp0&quot;netsh interface ip set address name=&quot;WLAN&quot; source=static addr=192.168.0.16 mask=255.255.255.0 gateway=192.168.0.1 @echo //-=-=-=-=-=-=-=-=-=-=-=-=-=-=@echo //  设置DNS@echo //-=-=-=-=-=-=-=-=-=-=-=-=-=-=netsh interface ip set dns &quot;WLAN&quot; static 192.168.0.1 primarynetsh interface ip add dns &quot;WLAN&quot; 192.168.0.1 @echo //  固定IP设置完成！@pause这里的addr是你要固定的ip，mask是掩码，gateway是网关，还有DNS，这三个参考图形化界面标签下适配器ip详细情况查询打开网络适配器，右键查看适配器ip详细情况将ip对应的地址、掩码和网关填下在自定义的IP地址下(注意这里只能固定IP的最后一位，不知道为什么，也就是前三位地址不能变)蓝屏和进不去系统如果您的计算机出现蓝屏并且无法进入系统，或者系统一直在启动循环中转圈，这可能是由于多种原因引起的。以下是一些常见的解决方法：安全模式启动：尝试进入安全模式来解决问题。重启计算机，在引导过程中按下F8键（或适用于您的计算机的其他功能键），然后选择”安全模式”。安全模式会以最小的驱动程序和服务启动系统，有助于排除可能导致问题的驱动程序或软件。系统还原：如果您的计算机之前正常工作，您可以尝试使用系统还原将系统恢复到之前的工作状态。在安全模式下，打开”控制面板”，找到”系统和安全”或”系统”选项，然后选择”系统保护”。在系统保护选项卡中，选择”系统还原”，按照提示选择之前的还原点并执行还原过程。修复启动：使用Windows安装媒体或恢复分区，您可以尝试修复启动问题。插入Windows安装媒体（例如安装光盘或USB驱动器），重新启动计算机并从安装媒体引导。选择”修复您的计算机”或类似选项，然后按照屏幕上的指示选择”启动修复”或”自动修复”。检查硬件问题：硬件问题（如内存故障或硬盘问题）也可能导致系统无法启动。您可以尝试以下步骤来排除硬件问题：检查内存：打开计算机并重新安装内存模块，或尝试使用一条已知工作的内存模块进行测试。检查硬盘：确保硬盘连接正常，并尝试使用硬盘测试工具来检查硬盘的健康状况。专业技术支持：如果上述方法无法解决问题，您可能需要寻求专业的技术支持。联系计算机制造商或技术支持专家，他们可以提供更进一步的帮助和指导。请注意，以上方法可能因具体情况而异。在尝试任何解决方法之前，请确保您了解可能产生的影响，并在可能的情况下备份重要数据。某些操作系统（如Windows）在连续发生启动问题时，可能会尝试自动修复。通过连续开关机三次，系统可能会检测到启动异常，并尝试自动修复启动问题，此时进去后就能选到安全模式启动了个人实际操作，当您的计算机出现问题时，可以按照以下有条理的步骤进行故障排除和修复：尝试安全模式：重启计算机，并在引导过程中按下F8键或适用于您的计算机的其他功能键选择”安全模式”以最小的驱动程序和服务启动系统进入安全模式后，进行以下操作：运行杀毒软件扫描以检查恶意软件或病毒卸载最近安装的软件，特别是与问题发生时间相关的软件检查系统日志或事件查看器，以了解是否有任何错误或警告提示自动修复启动：如果安全模式无法解决问题，您可以尝试自动修复启动连续开关机三次，以触发自动修复启动操作系统会尝试自动修复启动问题，如修复损坏的启动文件或配置卸载最新的更新：如果自动修复启动失败或问题仍然存在，您可以考虑卸载最近的功能更新或质量更新进入计算机的控制面板或设置寻找”程序和功能”或”应用和功能”选项选择”安装的更新”或”更新历史记录”卸载最近安装的功能更新或质量更新重置电脑：如果问题仍然存在，最后的选择是重置电脑进入计算机的设置或恢复选项找到”恢复”或”重置此电脑”选项根据提示选择重置选项。您可以选择保留个人文件或完全恢复出厂设置请注意，重置电脑将清除所有个人文件和安装的软件，需要重新安装您需要的软件手动重装系统：如果以上方法都无法解决问题，最后一步是手动重装操作系统获取适用于您的操作系统的安装媒体启动计算机并从安装媒体引导按照屏幕上的指示进行操作系统的重新安装请注意，在执行这些步骤之前，请务必备份重要的数据。如果您不确定如何操作或遇到困难，请考虑寻求专业技术支持或咨询设备制造商的支持软件安装必装软件微软Office办公软件是指可以进行文字处理、表格制作、幻灯片制作、图形图像处理、简单数据库的处理等方面工作的软件。办公软件朝着操作简单化，功能细化等方向发展。链接下载(点我下载2016版)删除软件IOBitIObit Uninstaller最新版是款功能高效的卸载工具。IObit Uninstaller官方版可轻松卸载并消除您电脑中的所有程序及文件，并在卸载后极速搜索注册表，找出残余信息并删除。并且IObit Uninstaller中还有一键移除工具列，批次移除和强制移除等功能。点我下载chrome浏览器百度搜索chrome浏览器，下载最新版本安装即可可牛看图一个功能强大且体积小巧的电脑看图工具，拥有丰富多样的图片浏览方式以及炫酷动感3D图片墙，支持批量图片上传、支持图片批量下载(官网下载，微云下载)极速PDF阅读器极速PDF阅读器(点我下载)是一款闪电般好用的PDF阅读器,体积小、启动速度快,十分轻巧且占用内存极少;提供单页、书本模式个性阅读,又有全屏、幻灯片等功能布谷鸟配音(收费)布谷鸟配音(点我下载)提供一站式智能配音的专业工具，输入文字转语音，一键将文字合成语音，真人情感配音重启还原(免费版、专业版)重启还原点我下载是一款不影响电脑性能，不占CPU使用率的还原工具，真正做到零内存占用，全程使用无感知，进入虚拟系统速度和普通系统速度相差无几进阶软件效率三件套AnyTXT Searcher(点我下载)是一款免费的功能强大的本地文档和文本搜索应用程序,就像本地磁盘谷歌搜索引擎,速度比Windows 搜索快得多,是您理想的桌面文件内容全文搜索引擎[搜文件内容]Everything(点我下载)是voidtools开发的一款文件搜索工具，它体积小巧，界面简洁易用，快速建立索引，快速搜索，同时占用极低的系统资源，实时跟踪文件变化，并且还可以通过http或ftp形式分享搜索[搜文件]uTools(点我下载)是一个极简、插件化、跨平台的现代化桌面软件。通过自由选配丰富的插件,打造你得心应手的工具集合。 当你熟悉它后,能够为你节约大量时间,让你可以更加专注地改变世界[搜软件和插件]TreesizeTreeSize Pro是一款优秀的磁盘空间管理工具。凭借其多样化的功能深受用户喜爱。通过该软件能够有效管理和清理磁盘空间，可视化磁盘空间的使用情况，详细分析目录级别，显示目录大小，包括所有子目录，直观呈现为饼图或条形状，所有信息一目了然(官网-需要自己找注册码、百度云下载-提取码: wqd1)visioMicrosoft Visio是Windows 操作系统下运行的流程图软件，它现在是Microsoft Office软件的一个部分。Visio可以制作的图表范围十分广泛，有些人利用Visio的强大绘图功能绘制地图、企业标志等，同时Visio支持将文件保存为svg、dwg等矢量图形通用格式，因此受到广泛欢迎。链接下载(点我下载2016版)GowGow(点我下载)是一个轻量级的Cygwin替代软件，它能做的事情是给Liunx软件提供一个Windows系统上的运行环境安装后可以直接在windows上执行linux的大部分命令VmwareVMware Workstation Pro 点我下载是行业标准桌面 Hypervisor，使用它可在 Windows 或 Linux 桌面上运行 Windows、Linux 和 BSD 虚拟机。激活码全家桶，历史版本下载将上面的Name项内容，拼接到下面链接后面https://download3.vmware.com/software/wkst/file/，复制到浏览器下载，例如:https://download3.vmware.com/software/wkst/file/VMware-workstation-full-16.2.1-18811642.exetyporaTypora(点我下载)是一款跨平台的Markdown编辑器软件,拥有Windows、macOS客户端neatreaderNeat Reader(点我下载)率先在业界实现了主流平台的全面支持，让你无需再在各个软件间来回切换，真正实现了无拘无束，随时随地阅读体验，致力于打造一个满足现代需求的 EPUB/TXT 阅读器PandaOCRPandaOCR(普通版，专业版)是一款专注于 Windows 桌面端 OCR 文字识别的免费软件，支持多功能 OCR 识别、即时翻译和朗读等。而且识别和翻译等引擎都是可以自由调换的，界面简洁，使用起来也很方便PandaOCR.Pro同样是免费使用，但它有一个激活专业版本状态，在未激活状态下您可以使用除高级功能外的全部功能，同时识别或翻译等功能可使用演示接口和百度API版接口，普通使用基本足够IT软件gitgit官网下载12345git config --global user.name &quot;narutohyc&quot;git config --global user.mail &quot;1832044043@qq.com&quot;# 配置远程仓库 将pub_key黏贴到gitssh-keygen -t rsa -C &quot;1832044043@qq.com&quot;如果需要指定的文件名，可以在ssh-keygen -t rsa -C &quot;xxxxxxxxx@xx.com&quot;命令后输入自己的名字(如: /root/.ssh/hyc_github)但是 ssh 命令默认只会读取id_rsa这个私钥文件，所以如果是其它的名字需要添加配置文件，这里新建一个配置文件12touch ~/.ssh/configvi ~/.ssh/config输入以下内容12345Host www.github.comIdentityFile /root/.ssh/hyc_githubHost www.gitlab.comIdentityFile C:\Users\hyc\.ssh\gitlab_id_rsaps: 这里建议github的key直接默认名字(因为换名字的话，后面配置有点难，我没试成功，测试命令ssh -T git@github.com)，其他key的可以换个名字Git问题：解决“ssh:connect to host github.com port 22: Connection timed out”如果报错，可以试下ssh -T -p 443 git@ssh.github.com,如果没问题，再在config中配置端口1234Host github.comPort 443Hostname ssh.github.comIdentityFile C:\Users\hyc\.ssh\id_rsa在config中配置端口为443比如ssh-keygen -t rsa -C &quot;huangyc@gitlab.com.cn&quot;，名字输入gitlab_id_rsa，然后在config里配置12Host www.gitlab.comIdentityFile C:\Users\hyc\.ssh\gitlab_id_rsaTortoiseGit(海龟 Git)是TortoiseSVN的 Git 版本，用于迁移 TortoiseSVN 到TortoiseGit，TortoiseGit 是一个 Git 版本控制客户端，作为 Microsoft Windows 的外壳扩展实现，配置教程安装TortoiseGit安装语言包执行PuTTYgen，生成密钥保存公钥和密钥公钥配置到git打开 Pageant，添加公钥(设置开机自动启动并加载密钥)按win + r 打开windows的运行命令框，输入shell:startup，会打开windows开机启动的文件夹，里面放的所有exe程序就会开机启动找到pageant.exe文件，鼠标右键新建快捷方式或发送到桌面快捷方式复制到启动文件夹，右键属性，修改目标为”C:\Program Files\TortoiseGit\bin\pageant.exe” “C:\Users\.ssh\github_tor.ppk”tortoisegit 常见错误：disconnected no supported authentication methods available(server sent: publickey)多半是因为git和小乌龟有冲突，解决方法：右键—小乌龟—-settings—network—修改ssh client为git的ssh.exegit的ssh.exe在Git\usr\bin\ssh.exe下或者是打开编辑本地git/config(L),添加puttykeyfile1234[remote &quot;origin&quot;]url = git@github.com:hycBook/hycBlog.gitfetch = +refs/heads/*:refs/remotes/origin/*puttykeyfile = C:\\Users\\hyc\\.ssh\\tor_pri.ppkjdk官网下载，jdk1.8官网地址主要配置参考1234export JAVA_HOME=/usr/java/jdk1.8.0_241-amd64export JRE_HOME=/usr/java/jdk1.8.0_241-amd64/jreexport PATH=$PATH:$JAVA_HOME/bin:$JRE_HOME/binexport CLASSPATH=.:$CLASSPATH:$JAVA_HOME/lib:$JRE_HOME/libpycharm官网下载，IDE全家桶激活码，验证码输入lookdiv.comnavicat提供简便的管理方法，设计和操作MySQL、MariaDB、SQL Server、Oracle、PostgreSQL和SQLite的数据(点我下载-提取码: 6sp9)Power Designer是Sybase公司的CASE工具集，使用它可以方便地对管理信息系统进行分析设计，它几乎包括了数据库模型设计的全过程。利用PowerDesigner可以制作数据流程图、概念数据模型(ConceptualDataModel)、物理数据模型(PhysicalDataModel)，还可以为数据仓库制作结构模型，也能对团队设计模型进行控制。(官网-需要自己找注册码、百度云下载-提取码: n3xz)finalshell免费的国产的集 SSH 工具、服务器管理、远程桌面加速的良心软件，同时支持 Windows，macOS，Linux，它不单单是一个 SSH 工具，完整的说法应该叫一体化的服务器，网络管理软件，在很大程度上可以免费替代 XShell，是国产中不多见的良心产品，具有免费海外服务器远程桌面加速，ssh 加速，双边 tcp 加速，内网穿透等特色功能(点我下载)Officebox完全免费的PDF工具/万能格式转换/识别工具/图片处理，多达60+个小工具，提供绿色版(点我下载)Wolfram Mathematica一款数学计算软件，具有编程语言、文本系统、计算引擎、图形系统等多种特色功能，并且还完美支持支持高性能计算，让用户科学计算的过程中，充分的发挥出自身的优势等。可以用于画函数图像(点我下载-提取码: 52qa)内网穿透cpolar速度一般，官网网址操作简单，直接执行命令启动特点：随机URL/端口上的HTTP/TCP隧道1个在线cpolar进程4个隧道/cpoloar进程40个连接数/分钟每次地址会一直变starryfrp星空内网穿透，官网网址免费版通过签到可以随机获得 1 ~ 10 GiB 流量5 条隧道（实名后赠送 5 条可用隧道数）8Mbps 限速(实际下载速度 200k左右)每日签到获取 1 ~ 5 GiB 随机流量支持 TCP/UDP/HTTP/HTTPS/STCP 隧道大带宽版  ￥10 /月   15 条隧道   10Mbps 限速每日签到获取 1 ~ 5 GiB 随机流量支持 TCP/UDP/HTTP/HTTPS/STCP 隧道解锁VIP节点natfrp樱花内网穿透，官网网址10Mbps (速度快  流量消耗挺快的)普通用户   完全免费隧道限速10 Mibps隧道数量2 条每月流量5 GiB解锁节点普通节点青铜VIP   ￥68 ￥10 / 月隧道限速24 Mibps隧道数量10 条每月流量5 GiB + 66 GiB解锁节点+ VIP节点文件共享下载客户端(点我下载)，查看python源码文件，可以依据自己的需求修改使用方法：将noup_server_pig.exe放在需要共享的目录下，双击启动就好，默认端口为8282如果需要修改端口或文件夹时，可以通过以下命令执行1noup_server_pig.exe --port 8282 -d S:\port表示开放的端口，默认为8282d表示是共享的文件夹，默认为H盘端口开放，没有开放端口的话，其他设备是无法访问你的(注意到这一步，只能在局域网内访问)windows：命令行打开输入Firewall.cpl，打开高级设置   点击入站规则后再新建规则   最后输入名称就可以开放8282端口了   如果开放后没有效果，直接关闭防火墙试试内网穿透为了让局域网外的设备访问到，需要安装内网穿透软件，可以参考上文的软件安装部分至此完成文件共享Chrome扩展程序没有插件的浏览器无异于没有灵魂！以下扩展程序的离线包微云下载插件下载地址crx4chrome: 下载指定扩展id的crx文件extfanshuajiakejicrxdown应用商店翻墙类Astar VPNAstar VPN- 适合所有人的免费且快速的 VPNGhelper官网需要翻墙脚本管理工具扩展油猴Tampermonkey暴力猴Violentmonkey脚本下载与推荐脚本下载地址openuserjsgreasyfork脚本推荐百度网盘直接下载助手网盘直链下载助手解锁B站大会员番剧、B站视频解析下载；全网VIP视频免费破解去广告；全网音乐直接下载；油管、Facebook等国外视频解析下载；网盘搜索引擎破解无限下载等破解VIP会员视频集合下载类Chrono下载管理器: 做Chrome浏览器中最好的下载管理器nternet Download Manager: 您可以使用Internet下载管理器下载Stream Recorder: 可以下载99%在线视频，包括直播流、m3u8流Fatkun图片批量下载: 可以下载当前标签或者是所有标签的图片，提供按分辨率、链接等筛选图片效率类书签管理: 在任何页面都可以通过书签关键字快速搜索打开书签,管理您的书签书签地球助手: 一款很优秀好用的网页浏览器上的书签管理助手，帮助用户一键整理网页标签，自定义标签设置方法，同时还可以查询搜索历史记录广告拦截AdGuard 广告拦截器是一款无与伦比的广告拦截扩展，用以对抗各式广告与弹窗。可以拦截 Facebook、YouTube 和其它所有网站的广告广告终结者能清除网页上的所有广告：浮动广告，购物广告，恶意弹窗，跟踪代码。让你浏览网页更快更清爽扩展管理器(Extension Manager): 一键管理所有扩展，快速开启/禁用、批量闪电管理，智能排序，右键卸载、锁定、选项配置，角标提醒，大小布局随心配。快捷、简单、安全Simple Allow Copy: 简易复制Reader View: 剥离按钮、背景图像等杂物，改变页面的文字大小、对比度和布局，以提高可读性Diigo Web Collector - Capture and Annotate: 用来在网页上做笔记，高亮显示选中的文本，就像在本子上勾画重点一样(需要登录使用，免费版支持100个网页)在线合并PDF: 快速免费合并PDF文件！Diigo Web Collector: 像在本子上一样为网页做笔记Docs Online Viewer: 直接在谷歌浏览器中在线查看任何文件Instapaper: 是一款运行在chromr内核的能够即时保存网页的chrome网页离线阅读插件Mate Translate: 是一款词典翻译类插件，可轻松翻译 103 种语言的单词、短语或文件。功能包含 [全页翻译]: 只要单击鼠标，Mate 就会为您翻译整个网页！在任何页面上，只要使用上下文菜单中的“翻译此页面”功能，就可立即翻译为您想要的语言Video Downloader professional: 是一款网页视频下载工具，下载前请先播放视频FireShot: 不仅支持区域截图，还支持完整的长网页截图Grammarly for Chrome: 国外最火的文字纠错工具，英语写作助手，帮助你在输出过程中找出英语单词拼写、语法、语调等问题，它为你提供具体的建议，帮助你提高英语写作水平程序员类Octotree - GitHub code treePostman InterceptorSourcegraphGitZip for githubJSON Viewer Pro浏览器美化类MONKNOW 新标签页 - 个性化面板Stylish-为任意网站自定义主题: 利用用户样式管理器Stylish来重新编辑网站的样式，可利用Stylish为许多网站安装主题和皮肤，也可创建自己的主题和皮肤文件共享局域网ftp共享centos7.6下搭建NFS共享并挂载到windows客户端-网络错误53解决方法FTP(File Transfer Protocol)是TCP/IP网络上两台计算机传送文件的协议，使得主机间可以共享文件目前有很多软件都能实现这一功能，然而windows 8系统自带的IIS就可以帮助你搭建FTP服务器打开控制面板 -&gt; 程序 -&gt; 启用或关闭windows功能找到Internet信息服务，并选中FTP服务、FTP扩展性和IIS管理控制台前的复选框，点击确定打开IIS管理器，在左栏的网站上点击右键，打开添加FTP站点win+r后输入inetmgr，即可打开IIS管理器开始配置，选择无SSL在电脑浏览器地址栏中输入ftp://本机ip测试一下在启动你的FTP服务器时建议关闭防火墙常见问题本机可以访问，其他机子无法访问，可能是防火墙问题或ftp服务权限问题方式一: 关闭防火墙  方式二(推荐): 允许FTP服务通过防火墙  关闭被动FTP 将文件复制到ftp服务器出错，请检查是否有权限将文件放到该服务器上win+r后输入inetmgr，打开IIS管理器，进入服务器管理器，单击右侧的[高级设置]选项，进入FTP高级设置对话框，在高级设置对话框中找到允许 UTF-8选项，单击下拉列，然后选择False选项，重启服务即可nfs共享服务部署centos部署win10部署准备工作概述：NFS是一种基于TCP/IP传输的网络文件系统协议，最初由SUN公司开发。通过NFS协议，客户机可以像访问本地目录一样访问远程服务器中的共享资源。NFS得到了如NAS等网络存储的设备极好支持。也是LVS共享存储的首选。这里用centos7环境演示服务端安装nfs-utils、rcpbind软件包1yum -y install nfs-utils rpcbind配置nfs协议，设置各种*port=…参数1vi /etc/sysconfig/nfs共享目录创建，并修改权限：12mkdir /home/hycchmod 777 /home/hyc配置nas，打开配置文件：/etc/exports12/home/hyc 10.xx.xx.xx/22(rw,sync,no_root_squash,no_all_squash)/home/hyc 192.xx.xx.xx/22(rw,sync,no_root_squash,no_all_squash)重新加载NFS配置1exportfs -r/etc/exports参数说明rw：可读写的权限ro：只读的权限anonuid：可以自行设定这个UID的值，这个UID必需要存在于你的/etc/passwd当中anongid：同anonuid，但是变成groupID就是了sync：资料同步写入到内存与硬盘当中async：资料会先暂存于内存当中，而非直接写入硬盘insecure：允许从这台机器过来的非授权访问no_root_squash：登入NFS主机，使用该共享目录时相当于该目录的拥有者如果是root的话，那么对于这个共享的目录来说，他就具有root的权限，这个参数『极不安全』，不建议使用root_squash：登入NFS主机，使用该共享目录时相当于该目录的拥有者但是如果是以root身份使用这个共享目录的时候，那么这个使用者（root）的权限将被压缩成为匿名使用者，即通常他的UID与GID都会变成nobody那个身份all_squash：不论登入NFS的使用者身份为何，他的身份都会被压缩成为匿名使用者，通常也就是nobody启动服务123456# 启动服务systemctl start rpcbindsystemctl start nfs# 设置开机启动systemctl enable rpcbindsystemctl enable nfs查看端口占用，以便开着防火墙的时候设置1rpcinfo -p查看本机共享目录123[root@uslave02 hyc]# showmount -e 192.xx.xx.23Export list for 192.xx.xx.23:/home/hyc 10.xx.xx.xx/22,192.xx.xx.xx/22以下操作需要打开启用或关闭windows功能，勾选NFS客户端和管理工具，然后确定安装nfs服务首先需要到官网下载hanewin，我这里用的是NFS Server 1.2.64安装完毕后来到安装目录，以管理员身份执行firewall.bat添加防火墙规则打开防火墙，检查入站规则中有没有hanewin 开头的这两项，若没有请检查上一步或者自己添加端口号，参考上文`局域网ftp共享，自己的话就是添加111、1058和2049这三个端口右键以管理员模式运行桌面的nfs server快捷方式点击左上角edit -&gt; preferences 如图设置再打开Edit -&gt; Preferences -&gt; Exports -&gt; Edit exports file设置共享磁盘路径，根据自己的情况来，比如共享/H执行挂载linux挂载win10挂载界面方式win10挂载命令行开始挂载123mkdir /mnt/datachmod 777 /mnt/datamount -t nfs 192.xx.xx.23:/home/hyc /mnt/data取消挂载1unmount /mnt/data用这种方式挂载centos的nfs服务，好像会卡死，一直在刷新，导致资源管理器无响应启用或关闭windows功能下，勾选NFS客户端和管理工具，然后确定在我的电脑下，选择映射网路驱动器，添加\\192.xx.xx.23\home\hyc即可1234567# 挂载C:\Users\Administrator&gt;mount 192.xx.xx.xx:/home/hyc Z:Z: 现已成功连接到 192.xx.xx.xx:/home/hyc命令已成功完成。# 取消挂载C:\Users\Administrator&gt;umount Z:路由桥接在面积较大的环境(如别墅、写字间等)，路由器无线覆盖范围有限，部分区域信号较弱到或存在信号盲点无线WDS桥接功能可以扩展无线覆盖范围，实现信号增强、移动漫游的需求主路由器与副路由器通过无线WDS桥接，无线终端可连接副路由器上网，移动过程中自动切换，实现漫游水星路由器去登陆http://melogin.cn/，然后选择无线设置下的WDS无线桥接，再选择源路由器的wifi，输入密码连接即可如果桥接后无法上网，尝试关闭DHCP选择</content>
	</entry>
	<entry>
		<title>Kafka基本安装与使用</title>
		<url>/article/26285.html</url>
		<content>Kafka简介kafka官网下载基本概念Kafka安装和测试下载kafka_2.12-2.8.0下载配置zookeeper.properties1234# the directory where the snapshot is stored.dataDir=/home/huangyc/kafka_2.12-2.8.0/zk_logs# the port at which the clients will connectclientPort=2183server.properties1234567891011121314151617# The address the socket server listens on. It will get the value returned from # java.net.InetAddress.getCanonicalHostName() if not configured.#   FORMAT:#     listeners = listener_name://host_name:port#   EXAMPLE:#     listeners = PLAINTEXT://your.host.name:9092listeners=PLAINTEXT://:9092# Zookeeper connection string (see zookeeper docs for details).# This is a comma separated host:port pairs, each corresponding to a zk# server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;.# You can also append an optional chroot string to the urls to specify the# root directory for all kafka znodes.zookeeper.connect=localhost:2183# A comma separated list of directories under which to store log fileslog.dirs=/home/huangyc/kafka_2.12-2.8.0/kafka_logs启动启动zookeeper1./bin/zookeeper-server-start.sh ./config/zookeeper.properties启动kafka1./bin/kafka-server-start.sh ./config/server.properties查看Kafka进程12[root@Hmaster kafka_2.12-2.8.0]# jps | grep Kafka23425 Kafka测试查看主题列表1bin/kafka-topics.sh --zookeeper localhost:2183 --list消息生产者1bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test 消息消费者123bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning0.9以前版本: kafka-console-consumer.bat --zookeeper localhost:2183 --topic test --from-beginning另一套方案server.properties12345678910111213141516171819202122# The address the socket server listens on. It will get the value returned from # java.net.InetAddress.getCanonicalHostName() if not configured.#   FORMAT:#     listeners = listener_name://host_name:port#   EXAMPLE:#     listeners = PLAINTEXT://your.host.name:9092listeners=PLAINTEXT://:9092# Hostname and port the broker will advertise to producers and consumers. If not set, # it uses the value for &quot;listeners&quot; if configured.  Otherwise, it will use the value# returned from java.net.InetAddress.getCanonicalHostName().advertised.listeners=PLAINTEXT://192.168.123.21:9092# A comma separated list of directories under which to store log fileslog.dirs=/home/huangyc/kafka_2.12-2.8.0/kafka_logs# Zookeeper connection string (see zookeeper docs for details).# This is a comma separated host:port pairs, each corresponding to a zk# server. e.g. &quot;127.0.0.1:3000,127.0.0.1:3001,127.0.0.1:3002&quot;.# You can also append an optional chroot string to the urls to specify the# root directory for all kafka znodes.zookeeper.connect=localhost:2183消息生产者1bin/kafka-console-producer.sh --broker-list localhost:9092 --topiest消息消费者1bin/kafka-console-consumer.sh --bootstrap-server 192.168.123.21:9092其他问题win10访问需要配置hostC:\Windows\System32\drivers\etc\hosts192.168.123.21 Hmaster1234567891011# 查看 hostname[root@Hmaster ~]# hostnamectl   Static hostname: Hmaster         Icon name: computer-desktop           Chassis: desktop        Machine ID: 87af5eaaaf7c48f6b1cd9f42bfb51c76           Boot ID: 9fe58388f3274f0e838b7cd1b720eacd  Operating System: CentOS Linux 7 (Core)       CPE OS Name: cpe:/o:centos:centos:7            Kernel: Linux 3.10.0-1160.el7.x86_64      Architecture: x86-64防火墙开启systemctl start firewalld端口开启firewall-cmd —zone=public —add-port=9092/tcp —permanent生效firewall-cmd —reload</content>
	</entry>
	<entry>
		<title>语法_简单句</title>
		<url>/article/2509a8a1.html</url>
		<content>英语句子基本结构主谓由主语和谓语构成一句话，就叫主谓结构例子The elephant die?  大象死了一句话只有主谓，主语是谓语的发出这还不够，谓语必须有时态正确写法：The elephant died.主谓宾同主谓表一起讲主谓表主谓宾和主谓表的关键差别：主谓宾的谓语一定是实义动词，凡是能够表达动作的词都叫实义动词，也叫行为动词主谓表的谓语一定是系动词，也称连系动词有些动词非常狡猾，它们不仅仅可以做系动词，也可以做实义动词要想知道以上动词在句子中是实义动词还是系动词，我们可以根据句子的意思来区分I grew up in this small town.  我在这个小城镇中长大。 (grew为实义动词)The sweater grew dirty. 这件毛衣变脏了。 (grew为系动词)主谓双宾主谓双宾就是主语+谓语+间接宾语+直接宾语例子He teachers us English.He gave me five sugars.He bought me a birthday gift.第一句中的us作间接宾语，English作直接宾语直接宾语为主要宾语，表示动作是对谁做的或者是为谁做的，在句中不可或缺，常常有表示物的名词有充当间接宾语被称为第二宾语，去掉之后对整个句子的影响不大，多由指人的名词或代词承担主谓宾宾补宾语+补语统称为复合宾语，宾语补足语的主要作用为补充说明宾语的特点、身份，或让宾语完成某个动作担任补语的常常是名词、形容词、副词、介词短语、分词，动词不定式等，具体用哪一个需要根据具体的意思来决定句子的成分句子的主要成分可以分为主干成分和修饰成分主干成分：主语、谓语、宾语、表语修饰成分：同位语、定语、状语、补语能够充当以上句子的成分，总共有名词(短语)、代词、动词(词组)、形容词(短语)、副词(短语)、介词短语、非谓语结构、从句上述这些词和短语能够在句中扮演什么角色呢主语：名词(短语)、代词、非谓语结构、从句谓语：动词(词组)宾语：名词(短语)、代词、非谓语结构、从句表语：名词(短语)、代词、非谓语结构、从句、形容词(短语)、介词短语同位语：名词(短语)、代词、非谓语结构、从句定语：名词(短语)、代词、非谓语结构、从句、形容词(短语)、介词短语状语：副词(短语)、介词短语、非谓语结构、从句例子他一句话都没说就离开了。He left saying nothing. (分词作状语)He left without a word. (介词短语作状语)When he left, he said nothing. (从句作状语)He left decisively. (副词作状语)嘲笑别人是不对的。Laughing at others is not right. (非谓语结构作主语)That you laugh at others is not right. (从句作主语)You are not right if you laugh at others. (代词作主语)一盏美丽、可爱的小油灯正照亮黑暗。A pretty and lovely lamp is brightening the darkness. (形容词短语作定语)A lamp which looks pretty and lovely is brightening the darkness. (从句作定语）A lamp looking pretty and lovely is brightening the darkness. (非谓语结构作定语)分词作定语，虽然意思表达是被动，但是系动词没有被动形式，所以只能加-ing长总结除了谓语只能接动词(词组)，其他各个成分的词性都很灵活，不要把语法学得那么死板，写作中尽量避免重复出现同样的语法点比如你前一个名词(短语)已经补充了定语从句，后面的这个名词(短语)就可以使用其他词作定语谓语谓语成分谓语一定是系动词或实义动词(词组)，并且具有时态和语态情态动词不能单独作谓语多个动词一句话中只能有一个谓语动词，不作谓语的动词需要变为非动词结构把一个动词(词组)变为非动词结构，方法有以下三种：现在分词：多在其后加-ing，变为V.ing，表示主动或进行(主动居多)过去分词：多在其后加-ed，变为V.ed，表示被动或完成(被动居多)不定式：前面加to，变为to do，表示目的或将来(目的居多)非谓语成分非谓语在句中能作除谓语以外的任意成分主语To finish the work by yourself is very hard.  独自完成这项工作是很困难的。To lose your heart means failure.  灰心意味着失败。动词不定式短语做主语时，常用it做形式主语，而把真正的主语不定式后置It is very hard to finish the work by yourself.It is means failure to lose your heart.表语Her job is to look after the babies.  她的工作是照顾婴儿。My dream is becoming a teacher.  我的梦想是成为一名老师。宾语He enjoys staying with his family.  他很喜欢跟家人在一起。I hope to see you soon.  希望不久能见到你。定语He found a good house to live in.  他找到了一个适合居住的好房子。In the following years he worked even harder.  在后来的几年中，他学习更努力了。状语He worked day and night to get the money.  为了赚钱，他夜以继日地工作。Seeing the thief walk into the room, he called the police.  看到小偷走进房间，他报了警。同位语His habit, eating snack before sleeping, has not been changed.  他睡前吃零食的习惯还没改。一句话中如何确定哪个作谓语，哪个作非谓语？句子主要想表达哪个动作，就留哪个动作作谓语。一般跟在主语后面的就是主要动作，剩下的都变成非谓语小结谓语必须是实义动词(词组)或系动词，并且必须有时态和语态一句话中动词(词组)有且只能有一个，多余的变成非谓语结构，少了的就加be动词主语主语成分I am a teacher.My job is to teach English.Helping you learn English well is my task.能作主语的成分，从上面例子中能够看出有代词、名词(短语)、非谓语结构和从句英语中不能没有主语，中文句子很多时候没有主语，翻译成英文是应该怎么办？it作主语外面很热。此时的重庆很冷。雨一直在下。这三句话的共同点是它们都和天气、温度有关系。所以此时就用it作主语。对应英文翻译成：It is hot outside.It is cold in Chongqing now.It is raining.there be句型二师兄的头上有朵花。有很多人喜欢我。有很多原因能解释我的观点。共同点是：都有“有”字都没有主语注意不要认为第一句话有主语。第一句话on the head of the second brother 不能作主语，因为它是介词短语。使用There be句型的前提不是某处有某物，只要满足以上两个条件都可以使用该句型，上面三句话的正确翻译为：There is a flower on the head of the second brother.There are loads of individuals having affection for me.There are numerous factors accounting for my perspective.主动变被动解决无主语的第三个方法：变主动为被动必须指出语法在考研中还是相当重要的。该句翻译成：Grammar must be pointed out to be quite crucial in the examination of postgraduates.作文中不但无主语的句子可以改为被动，一般人称代词作主语的句子也可以写成被动宾语宾语的成分：I love you.I love Liu Xiang.I love singing songs.如果此处先暂时忽略从句，能作宾语的成分和主语完全一样：代词、名词(短语)和非谓语结构表语表语的成分：It’s me.I am a teacher.I am in Chongqing.I am beautiful.My dream is to become a poet.My dream is becoming a poet.从上面的例子中可以看出，能作表语的成分有：代词、名词(短语)、非谓语结构、形容词(短语)和介词短语比宾语多了形容词(短语)和介词短语时态和语态p15</content>
	</entry>
	<entry>
		<title>语法_并列句</title>
		<url>/article/d4db6f86.html</url>
		<content>并列句定义首先举个例子，引出并列句：I love you,you love that dog.显然不能直接用逗号连接两句话，也就是说，逗号前后只能是一个句子，不能是两个句子那如果非得让逗号前后是两个句子怎么办呢？方法有二：加入并列连词连接两个分句，那就是：I love you but you love that dog.将其中的一个句子变成从句，那就是：Although I love you,you love that dog.英语中不能直接用逗号连接两个句子，如果我们想在句子之间用逗号，就需要一个东西的存在，那就是并列连词所以说：并列句就是由并列连词连接的两个或两个以上的句子，这个句子可以是简单句，也可以是从句例子He always takes everything seriously,so he can achieve success effortlessly.他做任何事情一直都很认真，因此成功对他来说是轻而易举的【解析】so是并列连词，表示因果关系，连接的句子是两个简单句As you are trapped in a dilemma,those who don’t understand you and who don’t accompany you will not deserve your love.在你处境困难时，那些不理解你的人和抛弃你的人不配拥有你的爱【解析】主句中的并列连词and连接了两个由who引导的定语从句，表示平行关系并列连词和逻辑关系词常见连词和副词常见的并列连词平行关系(并且、而且)：and, both … and, either … or, neither … nor等转折关系(然而、但是)：but, while, yet, whereas等选择关系(或者、要么)：or, whether … or, either … or …等因果关系(因为、所以)：for，so等递进关系(然后、再者)：not only … but also, then等其实看到这儿，大家会有些疑惑，那些熟悉的词怎么没有出现？比如说however, therefore, thus等，这些单词在实际运用中，经常充当副词，而不是连词那么，常见的表示逻辑关系的副词或介词短语有哪些？与并列连词同义的逻辑关系词平行关系：* 副词：similarly, equally, likewise等介词短语：at the same time, in the meanwhile等转折关系：副词：however, nevertheless, nonetheless, conversely, contrarily, unexpectedly, unfortunately等介词短语：on the contrary, in/by contrast等选择关系：* 副词：alternatively等因果关系：副词：therefore, thus, consequently等。介词短语：as a result, as a consequence等递进关系：副词：besides, additionally, subsequently等介词短语：in addition等连词和副词的区别使用连词时，可以加逗号，也可以不加I love you, but you love my money.或 I love you but you love my money.使用副词或介词短语时，必须把逗号变句号，或者是在副词或介词短语前加连词 andI love you. However you love my money.或 I love you, and however you love my money.例子天空没有痕迹，但是鸟儿已经飞过。There were no trails in the sky, but the birds had flown away.There were no trails in the sky, and conversely the birds had flown away.很多人享受着电脑和网络所带来的快乐和便利，然而它们已经改变了我们的生活方式 并引起了很多问题。Many people enjoy the pleasure and convenience of using the computer and Internet, yet they have changed our lifestyle and caused a lot of problems.Many people enjoy the pleasure and convenience of using the computer and Internet.On the contrary they have changed our lifestyle and caused a lot of problems.这幅画引人深思，并且它描绘和想要表达的是当今社会的一种普遍现象。The picture is thought-provoking, and what it illustrates and tries to convey is a common phenomenon in contemporary society.The picture is thought-provoking. Also what it illustrates and tries to convey is a common phenomenon in contemporary society.很多人都想不劳而获，但也许他们忽略了凡事都有代价。A host of people desire to achieve what they want without any work, whereas they may just ignore that no act comes without a sacrifice.A host of people desire to achieve what they want without any work. Nonetheless they may just ignore that no act comes without a sacrifice.并列句原则对称原则词性对称并列连词前后的词性、单复数、时态和语态等必须一致，通俗来说，连词前是什么，连词后也要是什么连词连接三个或者三个以上的词时，前面的词之间用逗号隔开，只需要在最后一个词前加并列连词He went shopping, bought some gifts and visited his old friend.他去购物了，买了些礼物并且拜访了他的老朋友【解析】这是由连词and连接的三个动作，前两个动作用逗号隔开，在最后一个动作前用and即可并且根据对称原则，连词前后的时态必须一致，第一个动作是went，是动词的过去式，那么连词后也必须是动词的过去式，即bought，visitedHe is an ambitious and positive student.他是一个积极向上、志向远大的学生【解析】并列连词and 前是形容词，后面也是形容词Humans can learn languages by making mistakes and correcting them.人类可以通过犯错和修正错误来学习语言【解析】并列连词and的前面是动词的-ing形式，后面也必须是-ing形式结构对称连词所连接的句子结构要保持一致省略现象省略的成分英语讲究结构美，强调语法和句法的完整性和合理性，因此英语句子中有一个句法原则，叫作避免重复原则在并列句中，语法上的要求是句子前后对称，在这种情况下，有时候会在结构和内容上出现重复因此，为了使得句式优美，一般并列句后面的句子要省略掉重复的内容，这样就出现了并列句的省略虽然省略了重复的内容，但是连词两边的结构依然是对称和平衡的基于以上所说，我们先来看一下，并列句的省略一般会省略什么成分省略主谓：并列连词后的主谓和前面的主谓一致，可以将后面的省略He is not only a nice teacher in class but a nice friend to his students in private.他在课堂上是一个好老师，在私底下是学生们的好朋友。【解析】并列连词是but，并列句要求对称原则，但是在but的后面只有表语 a nice friend，连词后的主谓应和前面一致，很明显这句话后面省略了主谓 he is省略be动词：并列句前后主语不同，但是几个句子有相同的be动词，那么此时可以省略be动词Failing hips can be replaced,clinical depression controlled,and cataracts removed in a 30-minute surgical procedure.髋骨坏了可以更换，临床忧郁症可以得到控制，白内障仅用30分钟手术便可以切除。【解析】由并列连词and连接了三句话，这三句话的主语分别是：failing hips，clinical depression，cataracts并列句前后主语不同，但谓语动词都是be动词，此时可以将be动词省略省略主语：连词后的主语与前面的主语一样时可以省略He is rich,but leads a modest life.他虽然很富有，但依然过着节俭的生活。【解析】由并列连词but连接的两句话，主语都是he。连词后的主语与前面的主语一样时，可以将连词后的主语省略省略重复的成分：如果并列句中前后两个句子出现重复的部分，那么后一个句子可以省略重复的部分，但是没重复的部分要保留。(这个成分可以是短语，也可以是句子。)Do you still remember all those years when scientists argued that smoking would kill us but the doubters insisted that we did not know for sure?你还记得吗？那些年科学家说吸烟会致命，但怀疑者坚持说我们并不确定吸烟是否真的会致命。【解析】由并列连词but连接的两个由that引导的宾语从句，前面的主谓是scientists argued，宾语是that从句。后面句子中know的宾语其实是smoking would kill us，与前面的句子的宾语一样，因此省略了宾语。补充完整就是：Do you still remember all those years when scientists argued that smoking would kill us but the doubters insisted that we did not know ( smoking would kill us) for sure?查找省略省略是英语中常见的语法现象，可以使句子表达简洁，但在分析句子的语法结构时往往会给我们造成理解的障碍唯一的解决办法就是识破其真面目，找到句子省略的内容例子I was beaten and you too.这句话如果直接翻译，那么结果是：我挨打了，你也。是不是觉得句子不完整？这就是找并列连词的原因所在一—有并列连词出现的地方，通常会有省略翻译的时候，我们需要先把省略的部分补充上然后再翻译本句话补充完整后就是：I was beaten and you were beaten too.  (我挨打了，你也挨打了。）下面我们的任务就是学会如何查找省略的内容。以下是三点总结：一句话只要有省略就一定会省略到并列连词的后面所以并列连词后面有的成分并列连词前面通常都要有并列连词前面有，而并列连词后面没有的成分便是省略的内容</content>
	</entry>
	<entry>
		<title>语法_名词性从句</title>
		<url>/article/be621570.html</url>
		<content>名词能做的成分用我们大家熟知的电影Forrest Gump来举几个例子：The movie is so moving and encouraging.I appreciate his mother.Gump is a real man.I enjoy the part,the warm end.这四句话中，名词短语分别作主语、宾语、表语、同位语和名词(短语)这里需要解释一下同位语，使用同位语则说明前后两部分指的是同一个东西比如：我爱我的女朋友——王大侠。再如：我们——新时代的大学生，应该忧国忧民。也就是说两个名词(短语)指的是同一个东西，事实上，在作文中只要出现名词(短语)，都可以试图加另外一个名词(短语)作它的同位语，但是前后要用逗号或破折号隔开例子English is playing a key role in our communicating with others.加上同位语的效果是：English, a worldwide language, is playing a key role in our communicating with others.A boy writes the name of Beckham on his face. (2006年考研写作真题)加上同位语的效果是：A boy-a crazy football fan-writes the name of Beckham, quite a well-known superstar, on his face.名词性从句从句定义这在简单句中就已经有所提示，从句就是引导词+分句，也就是说引导词+主语+谓语它和一个简单句的区别是，前面加了一个叫作引导词的东西(比如that, who, where, when均为引导词)名词性从句我们先细看下，下面的四句话和前文的四句话有什么区别。What I saw repeatedly is so moving and encouraging.I appreciate what his mother said.Gump is who we should learn from.I enjoy the part that a feather is flying in the sky.显而易见，在前文的四句话中，名词短语分别作主语、宾语、表语和同位语；而这四句话中，则用从句(引导词+主语+谓语)充当了名词短语本来充当的成分从句作主语，就是主语从句；同样，从句作宾语，就是宾语从句；从句作表语，就是表语从句；从句作同位语，就是同位语从句所以，名词性从句就是名词(短语)能够充当的成分，从句也能充当。它一共包括四类：主语从句宾语从句表语从句同位语从句引导词我爱你是我的错。I love you is my fault.I love you这个完整的句子能在另外一个句子里作主语吗？当然不能，只有从句才能在另外一个句子里面充当成分，所以我们需要在I love you前加个引导词，把它变成从句一个完整的句子在另外一个句子中是不能作任何成分的，如果想让一个句子在另外一个句子中作成分，就必须在它前面加上引导词，变成从句你爱我吗是显而易见的。Do you love me is obvious.这句话和上句是同样的错误，一个句子在另外一个句子中不能作主语，同样的解决办法：加引导词你爱谁真是一个秘密。Who do you love is really a secret.凭你那么多年所谓的语感，Who应该是个引导词，实际上并不是名词性从句的引导词就是按照从句的类型来划分，一共分成三类陈述句: 我爱你。一般疑问句: 你爱我吗？特殊疑问句: 你爱谁？1）that: 当从句是陈述句时。需要注意的是that在从句中不充当成分，也没有实义。在不影响句子意思的情况下，宾语从句的引导词that可以省略2）whether/if当从句是一般疑问句时。whether/if在从句中不充当成分，但意思为“是否”需要注意的是，这两个引导词中只有whether可以引导所有的名词性从句，而if只能引导宾语从句3）特殊疑问词：当从句是特殊疑问句时。常见的连接副词有：when，where，why，how；常见的连接代词有：who，whom，what,which，whose，并且这些特殊疑问词一般都有固定的意思，不能省略所有的从句，包括后面定语从句、状语从句，都应该是陈述句的语序，也就是”引导词+主语+谓语”的顺序从句详解主语从句她成功地找到了朋友，这使我很高兴。That she succeeded in finding a friend makes me happy.读书的目的是涤荡我们的灵魂，这是毋庸置疑的。That reading is to purify our minds is beyond dispute.上述句子是不是给人头重脚轻的感觉?于是英语中通常把主语从句放到句末， 用it作形式主语，结果就是：It makes me happy that she succeeded in finding a friend.It is beyond dispute that reading is to purify our minds.主语从句的满分表达就是it作形式主语It is common knowledge that …  …是一个常识It is my belief that … = I think that …  我认为…It is self-evident that …  …是显而易见的It has been found that … 人们已经发现…It is beyond dispute that … …是毋庸置疑的It is universally acknowledged that … 人们普遍认为/众所周知…It has been widely accepted that …   人们普遍认为/众所周知…It is generally agreed that …  人们普遍认为/众所周知…不仅仅是在从句作主语时，可以用it作形式主语，当不定式（短语）、动名词（短语）在句子中作主语时，为了保持句子结构平衡，避免句式结构混乱，也常用it作形式主语，而将真正的主语放在句尾此时t仍只起先行引导作用，本身无词义。概括起来常见的情况如下：It+is+adj./n.+to do sth.It is a good idea to think in this way.  用这种方式思考是个好办法。It is indispensable to train ourselves before any examination.  在考试前好好训练自己是很重要的。It is so kind of him to help his classmates in need.  他很善良，总是帮助需要帮助的同学。It+v.+to do sth.It took us 3 hours to finish the homework.  我们花了三个小时来完成这份作业。It happens to me to come across her on the way home.  我碰巧在回家的路上遇见了她。It’s+adj./n.+doing sth.   在it作形式主语的句型中，动名词作主语一般用在no good,waste，useless，no use， dangerous 等词后It is useless doing that.  那样做是没用的。It is no good playing games in bed on weekends.  周末躺在床上打游戏是没有什么好处的。It is dangerous staying with wild animals including tigers and lions.  和老虎、狮子这样的野生 动物待在一起是很危险的。同位语从句没有人会无缘无故就变得优秀，这个事实总是被很多人忽略。这句话中，显然”没有人会无缘无故就变得优秀”和”这个事实”是同位语，它们指向一致但是英文和汉语不同，在英文中，当一个名词(短语)或从句是同位语时，应该写在它所要解释说明的名词(短语)之后其顺序也就是”名词(短语)+引导词+从句”，(同位语从句放在句末)【翻译】The truth that nobody can become brilliant without endeavors is ignored by a sea of men.或 The truth is ignored by a sea of men that nobody can become brilliant without endeavors.没有什么能够掩盖她正在变老这个事实。同样，”她正在变老”和”这个事实”是同位语，应该先写名词(短语)再写从句，其他部分该是什么顺序就是什么顺序。【翻译】Nothing can hide the truth that she is growing old.很多女生都穿上了漂亮的裙子这个事实表明夏天已经来了。这个句子中有两个从句，第一个是同位语从句，第二个是宾语从句。【翻译】The evidence that many girls have worn attractive skirts manifests that summer is coming.科学家们已经得出结论：地球上的温度将会变得越来越高。主句是主谓宾结构，宾语是”结论”，结论后跟了一个从句来解释这个结论的内容，即同位语从句。【翻译】Scientists have reached the conclusion that the temperature on earth will get higher and higher.同位语从句的满分表达就是:The evidence/truth that..manifests/indicates/suggests that.. ，翻译成汉语就是：……这个事实表明…… 比如：她长着一双大眼睛这个事实表明她是一个美女。The evidence that she has a pair of big eyes manifests that she is really a beauty.</content>
	</entry>
	<entry>
		<title>语法_特殊结构</title>
		<url>/article/b80bb25bce.html</url>
		<content>测试</content>
	</entry>
	<entry>
		<title>IDE快捷键</title>
		<url>/article/7978bafd.html</url>
		<content>参考: https://mp.weixin.qq.com/s/r70Q0eqy5-R1CrwihREeQA（IntelliJ IDEA 快捷键终极大全，速度收藏！）自动代码常用的有fori/sout/psvm+Tab即可生成循环、System.out、main方法等boilerplate样板代码 。例如要输入for(User user : users)只需输入user.for+Tab ；再比如，要输入Date birthday = user.getBirthday()只需输入user.getBirthday().var+Tab即可。代码标签输入完成后，按Tab，生成代码。123456789101112131415161718192021222324&gt;Ctrl+Alt+O 优化导入的类和包 &gt;Alt+Insert 生成代码(如get,set方法,构造函数等)   或者右键（Generate） &gt;fori/sout/psvm + Tab  &gt;Ctrl+Alt+T  生成try catch  或者 Alt+enter &gt;CTRL+ALT+T  把选中的代码放在 TRY{} IF{} ELSE{} 里 &gt;Ctrl + O 重写方法  &gt;Ctrl + I 实现方法 &gt;Ctr+shift+U 大小写转化  &gt;ALT+回车    导入包,自动修正  &gt;ALT+/       代码提示 &gt;CTRL+J      自动代码  &gt;Ctrl+Shift+J，整合两行为一行 &gt;CTRL+空格   代码提示  &gt;CTRL+SHIFT+SPACE 自动补全代码  &gt;CTRL+ALT+L  格式化代码  &gt;CTRL+ALT+I  自动缩进  &gt;CTRL+ALT+O  优化导入的类和包  &gt;ALT+INSERT  生成代码(如GET,SET方法,构造函数等)  &gt;CTRL+E      最近更改的代码  &gt;CTRL+ALT+SPACE  类名或接口名提示  &gt;CTRL+P   方法参数提示  &gt;CTRL+Q，可以看到当前方法的声明 &gt;Shift+F6  重构-重命名 (包、类、方法、变量、甚至注释等) &gt;Ctrl+Alt+V 提取变量 查询快捷键123456789101112131415161718192021222324252627282930313233343536373839404142Ctrl＋Shift＋Backspace可以跳转到上次编辑的地 CTRL+ALT+ left/right 前后导航编辑过的地方 ALT+7  靠左窗口显示当前文件的结构 Ctrl+F12 浮动显示当前文件的结构 ALT+F7 找到你的函数或者变量或者类的所有引用到的地方 CTRL+ALT+F7  找到你的函数或者变量或者类的所有引用到的地方 Ctrl+Shift+Alt+N 查找类中的方法或变量 双击SHIFT 在项目的所有目录查找文件 Ctrl+N   查找类 Ctrl+Shift+N 查找文件 CTRL+G   定位行  CTRL+F   在当前窗口查找文本  CTRL+SHIFT+F  在指定窗口查找文本  CTRL+R   在 当前窗口替换文本  CTRL+SHIFT+R  在指定窗口替换文本  ALT+SHIFT+C  查找修改的文件  CTRL+E   最近打开的文件  F3   向下查找关键字出现位置  SHIFT+F3  向上一个关键字出现位置  选中文本，按Alt+F3 ，高亮相同文本，F3逐个往下查找相同文本 F4   查找变量来源  CTRL+SHIFT+O  弹出显示查找内容 Ctrl+W 选中代码，连续按会有其他效果 F2 或Shift+F2 高亮错误或警告快速定位 Ctrl+Up/Down 光标跳转到第一行或最后一行下 Ctrl+B 快速打开光标处的类或方法  CTRL+ALT+B  找所有的子类  CTRL+SHIFT+B  找变量的类  Ctrl+Shift+上下键  上下移动代码 Ctrl+Alt+ left/right 返回至上次浏览的位置 Ctrl+X 删除行 Ctrl+D 复制行 Ctrl+/ 或 Ctrl+Shift+/  注释（// 或者/*...*/ ） Ctrl+H 显示类结构图 Ctrl+Q 显示注释文档 Alt+F1 查找代码所在位置 Alt+1 快速打开或隐藏工程面板 Alt+ left/right 切换代码视图 ALT+ ↑/↓  在方法间快速移动定位  CTRL+ALT+ left/right 前后导航编辑过的地方 Ctrl＋Shift＋Backspace可以跳转到上次编辑的地 Alt+6    查找TODO 其他快捷键12345678910SHIFT+ENTER 另起一行 CTRL+Z   倒退(撤销) CTRL+SHIFT+Z  向前(取消撤销) CTRL+ALT+F12  资源管理器打开文件夹  ALT+F1   查找文件所在目录位置  SHIFT+ALT+INSERT 竖编辑模式  CTRL+F4  关闭当前窗口 Ctrl+Alt+V，可以引入变量。例如：new String(); 自动导入变量定义 Ctrl+~，快速切换方案（界面外观、代码风格、快捷键映射等菜单） svn快捷键12ctrl+k 提交代码到SVN ctrl+t 更新代码 调试快捷键其实常用的 就是F8 F7 F9 最值得一提的就是Drop Frame 可以让运行过的代码从头再来。推荐：Intellij IDEA Debug 调试技巧。12345678910111213141516alt+F8    debug时选中查看值 Alt+Shift+F9，选择 Debug Alt+Shift+F10，选择 Run Ctrl+Shift+F9，编译 Ctrl+Shift+F8，查看断点 F7，步入 Shift+F7，智能步入 Alt+Shift+F7，强制步入 F8，步过 Shift+F8，步出 Alt+Shift+F8，强制步过 Alt+F9，运行至光标处 Ctrl+Alt+F9，强制运行至光标处 F9，恢复程序 Alt+F10，定位到断点 重构123456Ctrl+Alt+Shift+T，弹出重构菜单 Shift+F6，重命名 F6，移动 F5，复制 Alt+Delete，安全删除 Ctrl+Alt+N，内联 IDEA异常处理idea 源发行版 8 需要目标发行版 1.81，Project Structure里确认两个地方:Project sdk以及project language level2，Project Structure-&gt;Modules里Sources里的Language level3，settings-&gt;java Compiler -&gt;target bytecode Version这些个地方需要一致。十大Intellij IDEA快捷键Intellij IDEA中有很多快捷键让人爱不释手，stackoverflow上也有一些有趣的讨论。每个人都有自己的最爱，想排出个理想的榜单还真是困难。 以前也整理过Intellij的快捷键，这次就按照我日常开发时的使用频率，简单分类列一下我最喜欢的十大快捷-神-键吧。 智能提示Intellij首当其冲的当然就是Intelligence智能！基本的代码提示用Ctrl+Space，还有更智能地按类型信息提示Ctrl+Shift+Space，但因为Intellij总是随着我们敲击而自动提示，所以很多时候都不会手动敲这两个快捷键(除非提示框消失了)。推荐：Intellij Idea非常6的10个姿势！用F2/ Shift+F2移动到有错误的代码，Alt+Enter快速修复(即Eclipse中的Quick Fix功能)。当智能提示为我们自动补全方法名时，我们通常要自己补上行尾的反括号和分号，当括号嵌套很多层时会很麻烦，这时我们只需敲Ctrl+Shift+Enter就能自动补全末尾的字符。而且不只是括号，例如敲完if/for时也可以自动补上{}花括号。 最后要说一点，Intellij能够智能感知Spring、Hibernate等主流框架的配置文件和类，以静制动，在看似“静态”的外表下，智能地扫描理解你的项目是如何构造和配置的。 重构Intellij重构是另一完爆Eclipse的功能，其智能程度令人瞠目结舌，比如提取变量时自动检查到所有匹配同时提取成一个变量等。尤其看过《重构-改善既有代码设计》之后，有了Intellij的配合简直是令人大呼过瘾！也正是强大的智能和重构功能，使Intellij下的TDD开发非常顺畅。推荐： Intellij IDEA 那些隐藏好用的小技巧切入正题，先说一个无敌的重构功能大汇总快捷键Ctrl+Shift+Alt+T，叫做Refactor This。按法有点复杂，但也符合Intellij的风格，很多快捷键都要双手完成，而不像Eclipse不少最有用的快捷键可以潇洒地单手完成(不知道算不算Eclipse的一大优点)，但各位用过Emacs的话就会觉得也没什么了(非Emacs黑)。此外，还有些最常用的重构技巧，因为太常用了，若每次都在Refactor This菜单里选的话效率有些低。比如Shift+F6直接就是改名，Ctrl+Alt+V则是提取变量。关注Java技术栈微信公众号，在后台回复关键字：IDEA，可以获取一份栈长整理的 IDEA 最新技术干货。 代码生成这一点类似Eclipse，虽不是独到之处，但因为日常使用频率极高，所以还是罗列在榜单前面。常用的有fori/sout/psvm+Tab即可生成循环、System.out、main方法等boilerplate样板代码，用Ctrl+J可以查看所有模板。后面“辅助”一节中将会讲到Alt+Insert，在编辑窗口中点击可以生成构造函数、toString、getter/setter、重写父类方法等。这两个技巧实在太常用了，几乎每天都要生成一堆main、System.out和getter/setter。 另外，Intellij IDEA 13中加入了后缀自动补全功能(Postfix Completion)，比模板生成更加灵活和强大。例如要输入for(User user : users)只需输入user.for+Tab。再比如，要输入Date birthday = user.getBirthday();只需输入user.getBirthday().var+Tab即可。 编辑编辑中不得不说的一大神键就是能够自动按语法选中代码的Ctrl+W以及反向的Ctrl+Shift+W了。此外，Ctrl+Left/Right移动光标到前/后单词，Ctrl+[/]移动到前/后代码块，这些类Vim风格的光标移动也是一大亮点。以上Ctrl+Left/Right/[]加上Shift的话就能选中跳跃范围内的代码。Alt+Forward/Backward移动到前/后方法。还有些非常普通的像Ctrl+Y删除行、Ctrl+D复制行、Ctrl+折叠代码就不多说了。 关于光标移动再多扩展一点，除了Intellij本身已提供的功能外，我们还可以安装ideaVim或者emacsIDEAs享受到Vim的快速移动和Emacs的AceJump功能(超爽！)。另外，Intellij的书签功能也是不错的，用Ctrl+Shift+Num定义1-10书签(再次按这组快捷键则是删除书签)，然后通过Ctrl+Num跳转。这避免了多次使用前/下一编辑位置Ctrl+Left/Right来回跳转的麻烦，而且此快捷键默认与Windows热键冲突(默认多了Alt，与Windows改变显示器显示方向冲突，一不小心显示器就变成倒着显式的了，冏啊)。 推荐：Intellij IDEA 撸码最头大的问题。查找打开  类似Eclipse，Intellij的Ctrl+N/Ctrl+Shift+N可以打开类或资源，但Intellij更加智能一些，我们输入的任何字符都将看作模糊匹配，省却了Eclipse中还有输入*的麻烦。最新版本的IDEA还加入了Search Everywhere功能，只需按Shift+Shift即可在一个弹出框中搜索任何东西，包括类、资源、配置项、方法等等。 类的继承关系则可用Ctrl+H打开类层次窗口，在继承层次上跳转则用Ctrl+B/Ctrl+Alt+B分别对应父类或父方法定义和子类或子方法实现，查看当前类的所有方法用Ctrl+F12。 要找类或方法的使用也很简单，Alt+F7。要查找文本的出现位置就用Ctrl+F/Ctrl+Shift+F在当前窗口或全工程中查找，再配合F3/Shift+F3前后移动到下一匹配处。 Intellij更加智能的又一佐证是在任意菜单或显示窗口，都可以直接输入你要找的单词，Intellij就会自动为你过滤。 关注Java技术栈微信公众号，在后台回复关键字：IDEA，可以获取一份栈长整理的 IDEA 最新技术干货。 其他辅助以上这些神键配上一些辅助快捷键，即可让你的双手90%以上的时间摆脱鼠标，专注于键盘仿佛在进行钢琴表演。这些不起眼却是至关重要的最后一块拼图有：命令：Ctrl+Shift+A可以查找所有Intellij的命令，并且每个命令后面还有其快捷键。所以它不仅是一大神键，也是查找学习快捷键的工具。 Ø  新建：Alt+Insert可以新建类、方法等任何东西。 Ø  格式化代码：格式化import列表Ctrl+Alt+O，格式化代码Ctrl+Alt+L。 Ø  切换窗口：Alt+Num，常用的有1-项目结构，3-搜索结果，4/5-运行调试。Ctrl+Tab切换标签页，Ctrl+E/Ctrl+Shift+E打开最近打开过的或编辑过的文件。 Ø  单元测试：Ctrl+Alt+T创建单元测试用例。 Ø  运行：Alt+Shift+F10运行程序，Shift+F9启动调试，Ctrl+F2停止。 Ø  调试：F7/F8/F9分别对应Step into，Step over，Continue。 此外还有些我自定义的，例如水平分屏Ctrl+|等，和一些神奇的小功能Ctrl+Shift+V粘贴很早以前拷贝过的，Alt+Shift+Insert进入到列模式进行按列选中。 Ø  Top #10切来切去：Ctrl+TabØ  Top #9选你所想：Ctrl+WØ  Top #8代码生成：Template/Postfix +TabØ  Top #7发号施令：Ctrl+Shift+AØ  Top #6无处藏身：Shift+ShiftØ  Top #5自动完成：Ctrl+Shift+EnterØ  Top #4创造万物：Alt+Insert太难割舍，前三名并列吧！ Ø  Top #1智能补全：Ctrl+Shift+SpaceØ  Top #1自我修复：Alt+EnterØ  Top #1重构一切：Ctrl+Shift+Alt+TCTRL+ALT+ left/right 前后导航编辑过的地方 Ctrl＋Shift＋Backspace可以跳转到上次编辑的地方</content>
	</entry>
	<entry>
		<title>JDBC</title>
		<url>/article/8fe47ff4.html</url>
		<content></content>
	</entry>
	<entry>
		<title>JDBC_JDBCTemplate</title>
		<url>/article/e73474f0.html</url>
		<content>【黑马程序员-Java语言高级部分11】JDBCSpring JDBC主要功能Spring框架对JDBC的简单封装。提供了一个JDBCTemplate对象简化JDBC的开发步骤：导入jar包创建JdbcTemplate对象。依赖于数据源DataSourceJdbcTemplate template = new JdbcTemplate(ds);调用JdbcTemplate的方法来完成CRUD的操作update(): 执行DML语句。增、删、改语句queryForMap(): 查询结果将结果集封装为map集合，将列名作为key，将值作为value 将这条记录封装为一个map集合注意：这个方法查询的结果集长度只能是1queryForList(): 查询结果将结果集封装为list集合注意：将每一条记录封装为一个Map集合，再将Map集合装载到List集合中query(): 查询结果，将结果封装为JavaBean对象query的参数：RowMapper一般我们使用BeanPropertyRowMapper实现类。可以完成数据到JavaBean的自动封装new BeanPropertyRowMapper&lt;类型&gt;(类型.class)queryForObject()：查询结果，将结果封装为对象一般用于聚合函数的查询使用示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131import cn.itcast.utils.JDBCUtils;import org.junit.Test;import org.springframework.jdbc.core.BeanPropertyRowMapper;import org.springframework.jdbc.core.JdbcTemplate;import org.springframework.jdbc.core.RowMapper;import java.sql.ResultSet;import java.sql.SQLException;import java.util.List;import java.util.Map;import java.util.Date;public class JdbcTemplateDemo {    //1. 获取JDBCTemplate对象    private JdbcTemplate template = new JdbcTemplate(JDBCUtils.getDataSource());    /**     * 1. 修改1号数据的 salary 为 10000     */    @Test    public void test1(){        //2. 定义sql        String sql = &quot;update emp set salary = 10000 where id = 1001&quot;;        //3. 执行sql        int count = template.update(sql);        System.out.println(count);    }    /**     * 2. 添加一条记录     */    @Test    public void test2(){        String sql = &quot;insert into emp(id,ename,dept_id) values(?,?,?)&quot;;        int count = template.update(sql, 1015, &quot;郭靖&quot;, 10);        System.out.println(count);    }    /**     * 3.删除刚才添加的记录     */    @Test    public void test3(){        String sql = &quot;delete from emp where id = ?&quot;;        int count = template.update(sql, 1015);        System.out.println(count);    }    /**     * 4.查询id为1001的记录，将其封装为Map集合     * 注意：这个方法查询的结果集长度只能是1     */    @Test    public void test4(){        String sql = &quot;select * from emp where id = ? or id = ?&quot;;        Map&lt;String, Object&gt; map = template.queryForMap(sql, 1001,1002);        System.out.println(map);        //{id=1001, ename=孙悟空, job_id=4, mgr=1004, joindate=2000-12-17, salary=10000.00, bonus=null, dept_id=20}    }    /**     * 5. 查询所有记录，将其封装为List     */    @Test    public void test5(){        String sql = &quot;select * from emp&quot;;        List&lt;Map&lt;String, Object&gt;&gt; list = template.queryForList(sql);        for (Map&lt;String, Object&gt; stringObjectMap : list) {            System.out.println(stringObjectMap);        }    }    /**     * 6. 查询所有记录，将其封装为Emp对象的List集合     */    @Test    public void test6(){        String sql = &quot;select * from emp&quot;;        List&lt;Emp&gt; list = template.query(sql, new RowMapper&lt;Emp&gt;() {            @Override            public Emp mapRow(ResultSet rs, int i) throws SQLException {                Emp emp = new Emp();                int id = rs.getInt(&quot;id&quot;);                String ename = rs.getString(&quot;ename&quot;);                int job_id = rs.getInt(&quot;job_id&quot;);                int mgr = rs.getInt(&quot;mgr&quot;);                Date joindate = rs.getDate(&quot;joindate&quot;);                double salary = rs.getDouble(&quot;salary&quot;);                double bonus = rs.getDouble(&quot;bonus&quot;);                int dept_id = rs.getInt(&quot;dept_id&quot;);                emp.setId(id);                emp.setEname(ename);                emp.setJob_id(job_id);                emp.setMgr(mgr);                emp.setJoindate(joindate);                emp.setSalary(salary);                emp.setBonus(bonus);                emp.setDept_id(dept_id);                return emp;            }        });        for (Emp emp : list) {            System.out.println(emp);        }    }    /**     * 6. 查询所有记录，将其封装为Emp对象的List集合     */    @Test    public void test6_2(){        String sql = &quot;select * from emp&quot;;        List&lt;Emp&gt; list = template.query(sql, new BeanPropertyRowMapper&lt;Emp&gt;(Emp.class));        for (Emp emp : list) {            System.out.println(emp);        }    }    /**     * 7. 查询总记录数     */    @Test    public void test7(){        String sql = &quot;select count(id) from emp&quot;;        Long total = template.queryForObject(sql, Long.class);        System.out.println(total);    }}1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889import java.sql.Date;public class Emp {    private Integer id;    private String ename;    private Integer job_id;    private Integer mgr;    private Date joindate;    private Double salary;    private Double bonus;    private Integer dept_id;    public Integer getId() {        return id;    }    public void setId(Integer id) {        this.id = id;    }    public String getEname() {        return ename;    }    public void setEname(String ename) {        this.ename = ename;    }    public Integer getJob_id() {        return job_id;    }    public void setJob_id(Integer job_id) {        this.job_id = job_id;    }    public Integer getMgr() {        return mgr;    }    public void setMgr(Integer mgr) {        this.mgr = mgr;    }    public Date getJoindate() {        return joindate;    }    public void setJoindate(Date joindate) {        this.joindate = joindate;    }    public Double getSalary() {        return salary;    }    public void setSalary(Double salary) {        this.salary = salary;    }    public Double getBonus() {        return bonus;    }    public void setBonus(Double bonus) {        this.bonus = bonus;    }    public Integer getDept_id() {        return dept_id;    }    public void setDept_id(Integer dept_id) {        this.dept_id = dept_id;    }    @Override    public String toString() {        return &quot;Emp{&quot; +                &quot;id=&quot; + id +                &quot;, ename='&quot; + ename + '\'' +                &quot;, job_id=&quot; + job_id +                &quot;, mgr=&quot; + mgr +                &quot;, joindate=&quot; + joindate +                &quot;, salary=&quot; + salary +                &quot;, bonus=&quot; + bonus +                &quot;, dept_id=&quot; + dept_id +                '}';    }}</content>
	</entry>
	<entry>
		<title>JDBC_JDBC连接池</title>
		<url>/article/ae528721.html</url>
		<content>【黑马程序员-Java语言高级部分11】JDBC数据库连接池概念：其实就是一个容器(集合)，存放数据库连接的容器。   当系统初始化好后，容器被创建，容器中会申请一些连接对象，当用户来访问数据库时，从容器中获取连接对象，用户访问完之后，会将连接对象归还给容器。好处：节约资源+用户访问高效标准接口：DataSource   javax.sql包下的方法：获取连接：getConnection()归还连接：Connection.close()。如果连接对象Connection是从连接池中获取的，那么调用Connection.close()方法，则不会再关闭连接了。而是归还连接一般我们不去实现它，有数据库厂商来实现C3P0：数据库连接池技术Druid：数据库连接池实现技术，由阿里巴巴提供的C3P0数据库连接池技术步骤：导入jar包 (两个) c3p0-0.9.5.2.jar mchange-commons-java-0.2.12.jar ，不要忘记导入数据库驱动jar包定义配置文件：名称： c3p0.properties 或者 c3p0-config.xml路径：直接将文件放在src目录下即可。```xml    &lt;!--  连接参数 --&gt;&lt;property name=&quot;driverClass&quot;&gt;com.mysql.jdbc.Driver&lt;/property&gt;&lt;property name=&quot;jdbcUrl&quot;&gt;jdbc:mysql://localhost:3306/db4&lt;/property&gt;&lt;property name=&quot;user&quot;&gt;root&lt;/property&gt;&lt;property name=&quot;password&quot;&gt;root&lt;/property&gt;&lt;!-- 连接池参数 --&gt;&lt;!--初始化申请的连接数量--&gt;&lt;property name=&quot;initialPoolSize&quot;&gt;5&lt;/property&gt;&lt;!--最大的连接数量--&gt;&lt;property name=&quot;maxPoolSize&quot;&gt;10&lt;/property&gt;&lt;!--超时时间--&gt;&lt;property name=&quot;checkoutTimeout&quot;&gt;3000&lt;/property&gt;  &lt;/default-config&gt;  &lt;!--  连接参数 --&gt;&lt;property name=&quot;driverClass&quot;&gt;com.mysql.jdbc.Driver&lt;/property&gt;&lt;property name=&quot;jdbcUrl&quot;&gt;jdbc:mysql://localhost:3306/db3&lt;/property&gt;&lt;property name=&quot;user&quot;&gt;root&lt;/property&gt;&lt;property name=&quot;password&quot;&gt;root&lt;/property&gt;&lt;!-- 连接池参数 --&gt;&lt;property name=&quot;initialPoolSize&quot;&gt;5&lt;/property&gt;&lt;property name=&quot;maxPoolSize&quot;&gt;8&lt;/property&gt;&lt;property name=&quot;checkoutTimeout&quot;&gt;1000&lt;/property&gt;  &lt;/named-config&gt;&lt;/c3p0-config&gt;123456789&gt;3. 创建核心对象 数据库连接池对象 ComboPooledDataSource&gt;4. 获取连接： getConnection&gt;```java&gt;// 1. 创建数据库连接池对象DataSource ds  = new ComboPooledDataSource();// 2. 获取连接对象Connection conn = ds.getConnection();Druid数据库连接池实现技术，由阿里巴巴提供的步骤：导入jar包 druid-1.0.9.jar定义配置文件：druid.properties形式可以叫任意名称，可以放在任意目录下```propertiesdriverClassName=com.mysql.jdbc.Driverurl=jdbc:mysql:///db3username=rootpassword=root初始化连接数量initialSize=5最大连接数maxActive=10最大等待时间maxWait=3000123456789101112131415163. 加载properties配置文件4. 获取数据库连接池对象：通过工厂来来获取  DruidDataSourceFactory5. 获取连接：getConnection```java// 3. 加载配置文件Properties pro = new Properties();InputStream is = DruidDemo.class.getClassLoader().getResourceAsStream(&quot;druid.properties&quot;);pro.load(is);// 4. 获取连接池对象DataSource ds = DruidDataSourceFactory.createDataSource(pro);// 5. 获取连接Connection conn = ds.getConnection();JDBCUtils.java1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677import com.alibaba.druid.pool.DruidDataSourceFactory;import javax.sql.DataSource;import java.io.IOException;import java.sql.Connection;import java.sql.ResultSet;import java.sql.SQLException;import java.sql.Statement;import java.util.Properties;/** * Druid连接池的工具类 */public class JDBCUtils {    //1.定义成员变量 DataSource    private static DataSource ds ;    static{        try {            //1.加载配置文件            Properties pro = new Properties();            pro.load(JDBCUtils.class.getClassLoader().getResourceAsStream(&quot;druid.properties&quot;));            //2.获取DataSource            ds = DruidDataSourceFactory.createDataSource(pro);        } catch (IOException e) {            e.printStackTrace();        } catch (Exception e) {            e.printStackTrace();        }    }    /**     * 获取连接     */    public static Connection getConnection() throws SQLException {        return ds.getConnection();    }    /**     * 释放资源     */    public static void close(Statement stmt,Connection conn){       close(null,stmt,conn);    }    public static void close(ResultSet rs , Statement stmt, Connection conn){        if(rs != null){            try {                rs.close();            } catch (SQLException e) {                e.printStackTrace();            }        }        if(stmt != null){            try {                stmt.close();            } catch (SQLException e) {                e.printStackTrace();            }        }        if(conn != null){            try {                conn.close();//归还连接            } catch (SQLException e) {                e.printStackTrace();            }        }    }    /**     * 获取连接池方法     */    public static DataSource getDataSource(){        return  ds;    }}</content>
	</entry>
	<entry>
		<title>Java core programming</title>
		<url>/article/d13ef431.html</url>
		<content>Java 核心编程技术干货参考: https://mp.weixin.qq.com/s/-KbOyp4gXYY4Ok8S-rPa6Q（Java 核心编程技术干货）自动代码</content>
	</entry>
	<entry>
		<title>Java基础</title>
		<url>/article/f7ede91d.html</url>
		<content></content>
	</entry>
	<entry>
		<title>Java基础_反射机制</title>
		<url>/article/3c274368.html</url>
		<content>反射机制反射——Java高级开发必须懂的Java高级特性——反射Class类的使用Class类在面向对象的世界里，万事万物皆对象。java语言中，静态的成员、普通数据类型类是不是对象呢？类是谁的对象呢？类是对象，类是java.lang.Class类的实例对象这个对象到底如何表示？There is a class named Class12345678910111213141516171819202122232425262728293031323334353637383940414243class Foo {    void print() {        System.out.println(&quot;foo&quot;);    }}public class ClassDemo1 {    public static void main(String[] args) {        // 第一种表示方式---&gt;实际在告诉我们任何一个类都有一个隐含的静态成员变量class        Class c1 = Foo.class;        // 第二种表达方式  已经知道该类的对象通过getClass方法        // 类也是对象，是Class类的实例对象        // 这个对象我们称为该类的类类型        // 不管c1 or c2都代表了Foo类的类类型，一个类只可能是Class类的一个实例对象        Class c2 = new Foo().getClass();        System.out.println(c1 == c2);        //第三种表达方式        Class c3 = null;        try {            c3 = Class.forName(&quot;com.imooc.reflect.Foo&quot;);        } catch (ClassNotFoundException e) {            e.printStackTrace();        }        System.out.println(c2 == c3);        // 我们完全可以通过类的类类型创建该类的对象实例----&gt;通过c1 or c2 or c3创建Foo的实例对象        try {            Foo foo = (Foo) c1.newInstance();//需要有无参数的构造方法            foo.print();        } catch (InstantiationException e) {            e.printStackTrace();        } catch (IllegalAccessException e) {            e.printStackTrace();        }    }}// 输出truetruefoo动态加载类Class类Class.forName(“类的全称”）不仅表示了类的类类型，还代表了动态加载类 ·请大家区分编译、运行编译时刻加载类是静态加载类、运行时刻加载类是动态加载类 12345678910111213141516171819202122232425262728293031323334&gt;interface OfficeAble {   void start();&gt;}&gt;class Word implements OfficeAble {   @Override   public void start() {       System.out.println(&quot;word..starts.…&quot;);   }&gt;}&gt;class Excel implements OfficeAble {   @Override   public void start() {       System.out.println(&quot;Excel..starts.…&quot;);   }&gt;}&gt;public class OfficeBetter {   public static void main(String[] args) {       try {           // 动态加载类，在运行时刻加载           Class c = Class.forName(&quot;com.imooc.reflect.Word&quot;);           // 通过类类型，创建该类对象           OfficeAble oa = (OfficeAble) c.newInstance();           oa.start();       } catch (Exception e) {           e.printStackTrace();       }   }&gt;}&gt;// 输出&gt;word..starts.…获取方法信息123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import java.lang.reflect.Constructor;import java.lang.reflect.Field;import java.lang.reflect.Method;public class ClassUtil {/** * 打印类的信息，包括类的成员函数、成员变量(只获取成员函数) * @param obj 该对象所属类的信息 */static void printClassMethodMessage(Object obj){// 要获取类的信息  首先要获取类的类类型Class c = obj.getClass();//传递的是哪个子类的对象  c就是该子类的类类型//获取类的名称System.out.println(&quot;类的名称是:&quot;+c.getName());/* * Method类，方法对象 * 一个成员方法就是一个Method对象 * getMethods()方法获取的是所有的public的函数，包括父类继承而来的 * getDeclaredMethods()获取的是所有该类自己声明的方法，不问访问权限 */Method[] ms = c.getMethods(); //c.getDeclaredMethods()for(int i = 0; i &lt; ms.length;i++){// 得到方法的返回值类型的类类型Class returnType = ms[i].getReturnType();System.out.print(returnType.getName()+&quot; &quot;);// 得到方法的名称System.out.print(ms[i].getName()+&quot;(&quot;);// 获取参数类型---&gt;得到的是参数列表的类型的类类型Class[] paramTypes = ms[i].getParameterTypes();for (Class class1 : paramTypes) {System.out.print(class1.getName()+&quot;,&quot;);}System.out.println(&quot;)&quot;);}}}public class ClassDemo3 {public static void main(String[] args) {String s = &quot;hello&quot;;ClassUtil.printClassMethodMessage(s);    Integer n1 = 1;    ClassUtil.printClassMethodMessage(n1);}}// 输出类的名称是:java.lang.Stringboolean equals(java.lang.Object,)java.lang.String toString()int hashCode()int compareTo(java.lang.String,)...获取成员变量构造函数信息12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.lang.reflect.Constructor;import java.lang.reflect.Field;import java.lang.reflect.Method;public class ClassUtil {    /**     * 获取成员变量的信息     */public static void printFieldMessage(Object obj) {Class c = obj.getClass();/* * 成员变量也是对象 * java.lang.reflect.Field * Field类封装了关于成员变量的操作 * getFields()方法获取的是所有的public的成员变量的信息 * getDeclaredFields获取的是该类自己声明的成员变量的信息 *///Field[] fs = c.getFields();Field[] fs = c.getDeclaredFields();for (Field field : fs) {//得到成员变量的类型的类类型Class fieldType = field.getType();String typeName = fieldType.getName();//得到成员变量的名称String fieldName = field.getName();System.out.println(typeName+&quot; &quot;+fieldName);}}}public class ClassDemo4 {public static void main(String[] args) {ClassUtil.printFieldMessage(&quot;hello&quot;);System.out.println(&quot;=============&quot;);ClassUtil.printFieldMessage(new Integer(1));}}// 输出[C valueint hashlong serialVersionUID[Ljava.io.ObjectStreamField; serialPersistentFieldsjava.util.Comparator CASE_INSENSITIVE_ORDER=============int MIN_VALUEint MAX_VALUEjava.lang.Class TYPE...12345678910111213141516171819202122232425262728293031323334353637383940414243import java.lang.reflect.Constructor;import java.lang.reflect.Field;import java.lang.reflect.Method;public class ClassUtil {/** * 打印对象的构造函数的信息 * @param obj */public static void printConMessage(Object obj){Class c = obj.getClass();/* * 构造函数也是对象 * java.lang. Constructor中封装了构造函数的信息 * getConstructors获取所有的public的构造函数 * getDeclaredConstructors得到所有的构造函数 *///Constructor[] cs = c.getConstructors();Constructor[] cs = c.getDeclaredConstructors();for (Constructor constructor : cs) {System.out.print(constructor.getName()+&quot;(&quot;);//获取构造函数的参数列表---&gt;得到的是参数列表的类类型Class[] paramTypes = constructor.getParameterTypes();for (Class class1 : paramTypes) {System.out.print(class1.getName()+&quot;,&quot;);}System.out.println(&quot;)&quot;);}}}public class ClassDemo5 {public static void main(String[] args) {ClassUtil.printConMessage(&quot;hello&quot;);ClassUtil.printConMessage(new Integer(1));}}// 输出java.lang.String([B,int,int,)java.lang.String([B,java.nio.charset.Charset,)java.lang.String([B,java.lang.String,)...方法反射的基本操作方法的反射如何获取某个方法 方法的名称和方法的参数列表才能唯一决定某个方法方法反射的操作method.invoke(对象，参数列表）12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.lang.reflect.Method;class A {     public void print() {           System.out.println(&quot;helloworld&quot;);       }       public void print(int a, int b) {           System.out.println(a + b);       }       public void print(String a, String b) {           System.out.println(a.toUpperCase() + &quot;,&quot; + b.toLowerCase());       }}public class MethodDemo1 {     public static void main(String[] args) {          // 获取方法名称和参数列表来决定           // getMethod获取的是public的方法           // getDelcaredMethod自己声明的方法           A a1 = new A();           Class c = a1.getClass();           try {               // 1.获取方法print(int,int)               // Method m =  c.getMethod(&quot;print&quot;, new Class[]{int.class,int.class});               Method m = c.getMethod(&quot;print&quot;, int.class, int.class);               // 方法的反射操作是用m对象来进行方法调用 和a1.print调用的效果完全相同               // Object o = m.invoke(a1,new Object[]{10,20});               Object o = m.invoke(a1, 10, 20);               System.out.println(&quot;==================&quot;);              // 2.获取方法print(String,String)               Method m1 = c.getMethod(&quot;print&quot;, String.class, String.class);               o = m1.invoke(a1, &quot;hello&quot;, &quot;WORLD&quot;);               System.out.println(&quot;===================&quot;);              // 3.获取方法print()               // Method m2 = c.getMethod(&quot;print&quot;, new Class[]{});               Method m2 = c.getMethod(&quot;print&quot;);               // m2.invoke(a1, new Object[]{});               m2.invoke(a1);           } catch (Exception e) {               e.printStackTrace();           }       }}// 输出30==================HELLO,world===================helloworld通过反射了解集合泛型的本质可以用反射绕过泛型123456789101112131415161718192021222324252627282930313233343536373839import java.lang.reflect.Method;import java.util.ArrayList;public class MethodDemo4 {public static void main(String[] args) {ArrayList list = new ArrayList();ArrayList&lt;String&gt; list1 = new ArrayList&lt;String&gt;();list1.add(&quot;hello&quot;);//list1.add(20);错误的Class c1 = list.getClass();Class c2 = list1.getClass();System.out.println(c1 == c2);//反射的操作都是编译之后的操作/* * c1==c2结果返回true说明编译之后集合的泛型是去泛型化的 * Java中集合的泛型，是防止错误输入的，只在编译阶段有效， * 绕过编译就无效了 * 验证：我们可以通过方法的反射来操作，绕过编译 */try {Method m = c2.getMethod(&quot;add&quot;, Object.class);m.invoke(list1, 20);//绕过编译操作就绕过了泛型System.out.println(list1.size());System.out.println(list1);/*for (String string : list1) {System.out.println(string);}*///现在不能这样遍历} catch (Exception e) {  e.printStackTrace();}}}// 输出true2[hello, 20]jOOR反射apijOOR反射api123456&lt;!-- https://mvnrepository.com/artifact/org.jooq/joor --&gt;&lt;dependency&gt;    &lt;groupId&gt;org.jooq&lt;/groupId&gt;    &lt;artifactId&gt;joor&lt;/artifactId&gt;    &lt;version&gt;0.9.5&lt;/version&gt;&lt;/dependency&gt;12345678910111213141516171819import org.joor.Reflect;/** * 基于Joor的反射学习*/private static void joorReflex() {    Properties pro = GetProperties.getPro(pathname);    String fruitClass = pro.getProperty(&quot;apple&quot;);    Reflect reflect = Reflect.on(fruitClass).create();    reflect.call(&quot;setPrice&quot;,25.4);    // 为包装类建立一个代理    Fruit fruitProxy = Reflect.on(fruitClass).create().as(Fruit.class);    Reflect.on(fruitProxy).call(&quot;setPrice&quot;,13.2);    Fruit fruit = FruitFactory.getInstance(fruitClass);    Reflect.on(fruit).call(&quot;setPrice&quot;,17.6);    System.out.println();}</content>
	</entry>
	<entry>
		<title>Java基础_后台服务器开发</title>
		<url>/article/3b72c11.html</url>
		<content>后台服务器开发远程通信方式WebService平台技术XML+XSD,SOAP和WSDL就是构成WebService平台的三大技术XML+XSDWebService采用HTTP协议传输数据，采用XML格式封装数据（即XML中说明调用远程服务对象的哪个方法，传递的参数是什么，以及服务对象的返回结果是什么）。XML是WebService平台中表示数据的格式。除了易于建立和易于分析外，XML主要的优点在于它既是平台无关的，又是厂商无关的。无关性是比技术优越性更重要的：软件厂商是不会选择一个由竞争对手所发明的技术的。XML解决了数据表示的问题，但它没有定义一套标准的数据类型，更没有说怎么去扩展这套数据类型。例如，整形数到底代表什么？16位，32位，64位？这些细节对实现互操作性很重要。XML Schema(XSD)就是专门解决这个问题的一套标准。它定义了一套标准的数据类型，并给出了一种语言来扩展这套数据类型。WebService平台就是用XSD来作为其数据类型系统的。当你用某种语言(如VB.NET或C#)来构造一个Web service时，为了符合WebService标准，所有你使用的数据类型都必须被转换为XSD类型。你用的工具可能已经自动帮你完成了这个转换，但你很可能会根据你的需要修改一下转换过程。SOAPWebService通过HTTP协议发送请求和接收结果时，发送的请求内容和结果内容都采用XML格式封装，并增加了一些特定的HTTP消息头，以说明HTTP消息的内容格式，这些特定的HTTP消息头和XML内容格式就是SOAP协议。SOAP提供了标准的RPC方法来调用Web Service。SOAP协议 = HTTP协议 + XML数据格式SOAP协议定义了SOAP消息的格式，SOAP协议是基于HTTP协议的，SOAP也是基于XML和XSD的，XML是SOAP的数据编码方式。打个比喻：HTTP就是普通公路，XML就是中间的绿色隔离带和两边的防护栏，SOAP就是普通公路经过加隔离带和防护栏改造过的高速公路。WSDL好比我们去商店买东西，首先要知道商店里有什么东西可买，然后再来购买，商家的做法就是张贴广告海报。 WebService也一样，WebService客户端要调用一个WebService服务，首先要有知道这个服务的地址在哪，以及这个服务里有什么方法可以调用，所以，WebService务器端首先要通过一个WSDL文件来说明自己家里有啥服务可以对外调用，服务是什么（服务中有哪些方法，方法接受的参数是什么，返回值是什么），服务的网络地址用哪个url地址表示，服务通过什么方式来调用。WSDL(Web Services Description Language)就是这样一个基于XML的语言，用于描述Web Service及其函数、参数和返回值。它是WebService客户端和服务器端都能理解的标准格式。因为是基于XML的，所以WSDL既是机器可阅读的，又是人可阅读的，这将是一个很大的好处。一些最新的开发工具既能根据你的Web service生成WSDL文档，又能导入WSDL文档，生成调用相应WebService的代理类代码。WSDL文件保存在Web服务器上，通过一个url地址就可以访问到它。客户端要调用一个WebService服务之前，要知道该服务的WSDL文件的地址。WebService服务提供商可以通过两种方式来暴露它的WSDL文件地址：1.注册到UDDI服务器，以便被人查找；2.直接告诉给客户端调用者。常见通信协议浅谈几种通信方式(dubbo,webservice,restful)TCP/IP1TCP/IP 协议是一个协议族。里面包括很多协议。UDP 只是其中的一个。因为TCP/IP比较重要所以用他两命名。TCP和UDP12tcp和udp都是传输协议，主要区别是tcp协议连接需要3次握手，断开需要四次握手，是通过流来传输的，就是确定连接后，一直发送信息，传完后断开。udp不需要进行连接，直接把信息封装成多个报文，直接发送。所以udp的速度更快写，但是不保证数据的完整性和数据顺序(这个是可以在实现时通过验证手段来手动确定完整性)。HTTP12http协议是建立在TCP协议之上的一种应用，是Web联网的基础，最显著的特点是客户端发送的每次请求都需要服务器回送响应，在请求结束后，会主动释放连接。从建立连接到关闭连接的过程称为“一次连接”。SOCKET12socket并不是一种协议，是在程序员层面上对TCP/IP协议的封装和应用。其实是一个调用接口，方便程序员使用TCP/IP协议栈而已。程序员通过socket来使用tcp/ip协议。但是socket并不是一定要使用tcp/ip协议，Socket编程接口在设计的时候，就希望也能适应其他的网络协议。RPC12RPC(Remote Procedure Call)—远程过程调用，它是一种通过网络从远程计算机程序上请求服务，而不需要了解底层网络技术的协议。RPC协议假定某些传输协议的存在，如TCP或UDP，为通信程序之间携带信息数据。在OSI网络通信模型中，RPC跨越了传输层和应用层。RPC使得开发包括网络分布式多程序在内的应用程序更加容易。SOAP1简单对象访问协议是交换数据的一种协议规范，是一种轻量的、简单的、基于XML（标准通用标记语言下的一个子集）的协议，它被设计成在WEB上交换结构化的和固化的信息。常见通信方式Webservice1&gt;效率不高基于soap协议，其主要的特点是跨语言、跨平台的。项目中不推荐使用，可用于不同公司间接口的调用。restful1&gt;http+json。很多项目中应用。如果服务太多，服务之间调用关系混乱，需要治疗服务。dubbo1&gt;使用rpc协议进行远程调用，直接使用socket通信。传输效率高，并且可以统计出系统之间的调用关系、调用次数。使用Java语言开发，只能用于Java语言开发的项目间的通信，不具备跨语言，跨平台的特点！webservice之Restful风格webservice之Restful风格12&gt;使用restful风格的webservice 也就是cxf发布的webservice的地址是restful风格的&gt;使用restful风格发布的webservice不采用soap协议 ，轻量级 速度快，采用http传输协议。SOAP webserivce 和 RESTful webservice 对比及区别 RESTful跟WebService的区别webserver/servlet容器关于tomcat和jetty对比(不喜欢jetty的勿看)tomcat与jetty的区别Jetty篇一之初识Jetty相同点Tomcat和Jetty都是一种Servlet引擎，他们都支持标准的servlet规范和JavaEE的规范。 架构比较Jetty的架构比Tomcat的更为简单。Jetty的架构是基于Handler来实现的，主要的扩展功能都可以用Handler来实现，扩展简单。Tomcat的架构是基于容器设计的，进行扩展是需要了解Tomcat的整体设计结构，不易扩展。性能比较Jetty和Tomcat性能方面差异不大。Jetty可以同时处理大量连接而且可以长时间保持连接，适合于web聊天应用等等。Jetty的架构简单，因此作为服务器，Jetty可以按需加载组件，减少不需要的组件，减少了服务器内存开销，从而提高服务器性能。Jetty默认采用NIO结束在处理I/O请求上更占优势，在处理静态资源时，性能较高。Tomcat适合处理少数非常繁忙的链接，也就是说链接生命周期短的话，Tomcat的总体性能更高。Tomcat默认采用BIO处理I/O请求，在处理静态资源时，性能较差。其它比较Jetty的应用更加快速，修改简单，对新的Servlet规范的支持较好。Tomcat目前应用比较广泛，对JavaEE和Servlet的支持更加全面，很多特性会直接集成进来。Google 选择 Jetty, 放弃 TomcatJetty的基本架构&gt;Tomcat的基本架构不同开发实现&gt;&gt;&gt;花了近十年的时间，整理出史上最全面Java面试题</content>
	</entry>
	<entry>
		<title>Java基础_四舍五入</title>
		<url>/article/48f373f3.html</url>
		<content>四舍五入1234567891011121314151617181920212223242526272829303132333435363738394041424344454647ROUND_UP：远离零方向舍入。向绝对值最大的方向舍入，只要舍弃位非0即进位。ROUND_DOWN：趋向零方向舍入。向绝对值最小的方向输入，所有的位都要舍弃，不存在进位情况。ROUND_CEILING：向正无穷方向舍入。向正最大方向靠拢。若是正数，舍入行为类似于ROUND_UP，若为负数，舍入行为类似于ROUND_DOWN。Math.round()方法就是使用的此模式。ROUND_FLOOR：向负无穷方向舍入。向负无穷方向靠拢。若是正数，舍入行为类似于ROUND_DOWN；若为负数，舍入行为类似于ROUND_UP。HALF_UP：最近数字舍入(5进)。这是我们最经典的四舍五入。HALF_DOWN：最近数字舍入(5舍)。在这里5是要舍弃的。HAIL_EVEN：银行家舍入法。public class Test {    public static void main(String[] args) {        double a = 1.66728D;        double b = 1.33333D;        double c = 1.00000D;        BigDecimal aa = new BigDecimal(a);        BigDecimal bb = new BigDecimal(b);        BigDecimal cc = new BigDecimal(c);        System.out.println(aa.setScale(2, BigDecimal.ROUND_UP));        System.out.println(aa.setScale(2, BigDecimal.ROUND_DOWN));        System.out.println(bb.setScale(2, BigDecimal.ROUND_UP));        System.out.println(bb.setScale(2, BigDecimal.ROUND_DOWN));        System.out.println(cc.setScale(2, BigDecimal.ROUND_UP));        System.out.println(cc.setScale(2, BigDecimal.ROUND_DOWN));        System.out.println(&quot;-------------------------------------&quot;);        System.out.println(aa.setScale(2, RoundingMode.UP));        System.out.println(aa.setScale(2, RoundingMode.DOWN));        System.out.println(bb.setScale(2, RoundingMode.UP));        System.out.println(bb.setScale(2, RoundingMode.DOWN));        System.out.println(cc.setScale(2, RoundingMode.UP));        System.out.println(cc.setScale(2, RoundingMode.DOWN));    }}1.671.661.341.331.001.00-------------------------------------1.671.661.341.331.001.00</content>
	</entry>
	<entry>
		<title>Java基础_对象正反序列化</title>
		<url>/article/336a2a82.html</url>
		<content>对象正反序列化123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687import java.io.*;import java.util.Base64;/** * 对象编码工具类 */public class ObjEncodeUtil {    public static String encodeBase64(Object object) {        if (object == null) {            return null;        }        ObjectOutputStream oos = null;        String result;        try {            ByteArrayOutputStream bos = new ByteArrayOutputStream();            oos = new ObjectOutputStream(bos);            oos.writeObject(object);            result = Base64.getEncoder().encodeToString(bos.toByteArray());        } catch (IOException e) {            throw new RuntimeException(e);        } finally {            try {                if (oos != null) {                    oos.close();                }            } catch (IOException e) {                e.printStackTrace();            }        }        return result;    }    public static &lt;T&gt; T decodeBase64(String str) {        ObjectInputStream ois = null;        T result;        try {            ByteArrayInputStream bis = new ByteArrayInputStream(Base64.getDecoder().decode(str));            ois = new ObjectInputStream(bis);            result = (T) ois.readObject();        } catch (IOException | ClassNotFoundException e) {            throw new RuntimeException(e);        } finally {            if (ois != null) {                try {                    ois.close();                } catch (IOException e) {                    e.printStackTrace();                }            }        }        return result;    }}class Bean{        int length;        double width;        public Bean() {        }        public Bean(int length, double width) {            this.length = length;            this.width = width;        }        public int getLength() {            return length;        }        public void setLength(int length) {            this.length = length;        }        public double getWidth() {            return width;        }        public void setWidth(double width) {            this.width = width;        }    }Bean bean = new Bean(5,3.2);String modelStr = ObjEncodeUtil.encodeBase64(bean);Bean bean2 = ObjEncodeUtil.decodeBase64(modelStr);123456import com.alibaba.fastjson.JSON;ModelBean bean = new ModelBean();String jsonResult = JSON.toJSONString(bean);ModelBean bean2 = JSON.parseObject(jsonResult, ModelBean.class);</content>
	</entry>
	<entry>
		<title>Java基础_打jar包</title>
		<url>/article/e4e21ef8.html</url>
		<content>详细步骤assembly.xml12345678910111213141516171819202122232425&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;assembly        xmlns=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.3&quot;        xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;        xsi:schemaLocation=&quot;http://maven.apache.org/plugins/maven-assembly-plugin/assembly/1.1.3 http://maven.apache.org/xsd/assembly-1.1.3.xsd&quot;&gt;    &lt;id&gt;test&lt;/id&gt;    &lt;formats&gt;        &lt;format&gt;jar&lt;/format&gt;    &lt;/formats&gt;    &lt;includeBaseDirectory&gt;false&lt;/includeBaseDirectory&gt;    &lt;fileSets&gt;        &lt;!--将测试类和应用都打包进jar包--&gt;        &lt;fileSet&gt;            &lt;directory&gt;${project.build.directory}/test-classes&lt;/directory&gt;            &lt;outputDirectory&gt;/&lt;/outputDirectory&gt;            &lt;includes&gt;                &lt;include&gt;**/*.class&lt;/include&gt;                &lt;include&gt;**/*.properties&lt;/include&gt;                &lt;include&gt;**/*.xml&lt;/include&gt;            &lt;/includes&gt;            &lt;useDefaultExcludes&gt;false&lt;/useDefaultExcludes&gt;        &lt;/fileSet&gt;    &lt;/fileSets&gt;&lt;/assembly&gt;pom.xml1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot;         xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;         xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0         http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt;    &lt;parent&gt;        &lt;groupId&gt;com.code.hyc&lt;/groupId&gt;        &lt;artifactId&gt;test&lt;/artifactId&gt;        &lt;version&gt;1.0&lt;/version&gt;    &lt;/parent&gt;    &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;    &lt;artifactId&gt;hycCall&lt;/artifactId&gt;    &lt;packaging&gt;jar&lt;/packaging&gt;    &lt;!--构建配置，不要删，需要的话打开--&gt;    &lt;build&gt;        &lt;plugins&gt;            &lt;!--打成可执行jar包--&gt;            &lt;plugin&gt;                &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt;                &lt;version&gt;2.3&lt;/version&gt;                &lt;configuration&gt;                    &lt;descriptor&gt;src/main/resources/assembly.xml&lt;/descriptor&gt;                &lt;/configuration&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;make-assembly&lt;/id&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;single&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;archive&gt;                                &lt;manifest&gt;                                    &lt;!--该处设置为要执行的主类--&gt;                                    &lt;mainClass&gt;                                        com.code.hyc.HycTest                                    &lt;/mainClass&gt;                                &lt;/manifest&gt;                            &lt;/archive&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;            &lt;!--依赖导出插件--&gt;            &lt;plugin&gt;                &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;                &lt;artifactId&gt;maven-dependency-plugin&lt;/artifactId&gt;                &lt;executions&gt;                    &lt;execution&gt;                        &lt;id&gt;copy-dependencies&lt;/id&gt;                        &lt;phase&gt;package&lt;/phase&gt;                        &lt;goals&gt;                            &lt;goal&gt;copy-dependencies&lt;/goal&gt;                        &lt;/goals&gt;                        &lt;configuration&gt;                            &lt;outputDirectory&gt;${project.build.directory}/lib&lt;/outputDirectory&gt;                            &lt;excludeTransitive&gt;false&lt;/excludeTransitive&gt;                            &lt;stripVersion&gt;true&lt;/stripVersion&gt;                        &lt;/configuration&gt;                    &lt;/execution&gt;                &lt;/executions&gt;            &lt;/plugin&gt;        &lt;/plugins&gt;    &lt;/build&gt;&lt;/project&gt;HycTest.java12345public class HycTest{    public static void main(String[] args) throws Throwable {        System.out.println(&quot;打包成功！&quot;);    }}执行maven-&gt;clean-&gt;package在target将hycCall.jar移动到lib文件夹下，此时lib文件夹和hycCal-testl.jar就是可以执行的jar包了命令行1234567891011121314// 命令行debug模式(-Xms4G -Xmx8G -XX:PermSize=4G -XX:MaxPermSize=8G为指定内存大小)java -Xdebug -Xrunjdwp:transport=dt_socket,address=5005,server=y,suspend=y -jar -Xms4G -Xmx8G -XX:PermSize=4G -XX:MaxPermSize=8G -Djava.ext.dirs=./lib mist-calculate-1.0-test.jar// 后台执行模式(nohup xxx &amp;)nohup java -jar -Xms4G -Xmx8G -XX:PermSize=4G -XX:MaxPermSize=8G -Djava.ext.dirs=./lib mist-calculate-1.0-test.jar &amp;// 日志查看tail -10f nohup.out// 查看java命令的进程ps -a | grep java// 杀死指定线程号kill -9  线程号</content>
	</entry>
	<entry>
		<title>Java基础_正则匹配</title>
		<url>/article/6d8205e4.html</url>
		<content>正则匹配</content>
	</entry>
	<entry>
		<title>Java基础_泛型</title>
		<url>/article/73fcdf9d.html</url>
		<content>泛型</content>
	</entry>
	<entry>
		<title>Java基础_解析与生成XML</title>
		<url>/article/7543a0fc.html</url>
		<content>解析与生成XMLJava生成xml文件的四种方式四种生成和解析XML文档的方法详解（介绍+优缺点比较+示例）Java 自定义生成与解析（Dom4J）XML 文件12345四种方式总结:    1.dom基于tree，sax基于事件，JDOM和DOM4J基于底层API    2.dom生成的内容会保存到内存中，方便删除和修改    3.sax不能修改已经生成的标签     4.添加JUnit在Libraries选择Add Library中的JUnit即可【DOM】DOM方式生成xml是基于DOM树的结构，整个DOM树会存在内存中，所以使用DOM方式可以频繁的修改xml的内容，但是因为DOM树是存在内存中的，所以对内存消耗较大。DOM方式比较适用于需要频繁删改的情况。【SAX】SAX方式生成xml是逐步写入的，也就是说，在SAX写入时，已经写入的部分是无法再回头修改的，因为SAX是基于事件驱动的，在写完一个标签之后是不能回头的，也因此，SAX的效率比较快，但是不能进行删改。【JDOM】JDOM方式不是Java提供的基本的生成xml方式，使用时需要导入额外的jar包，但是它是基于基础的API实现的。【DOM4J】DOM4J方式也不是Java提供的基本生成xml的方式，使用时也需要导入额外的jar包，它也是基于基础的API实现的，它功能强大，性能优异，在实际开发中经常使用。DOM方式是最慢的，SAX方式反而是最快的，DOM4J方式仅次于SAX方式。Dom方式&gt;&gt;1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798&gt;import org.junit.Test;&gt;import org.w3c.dom.DOMConfiguration;&gt;import org.w3c.dom.Document;&gt;import org.w3c.dom.Node;&gt;import org.w3c.dom.bootstrap.DOMImplementationRegistry;&gt;import org.w3c.dom.ls.DOMImplementationLS;&gt;import org.w3c.dom.ls.LSOutput;&gt;import org.w3c.dom.ls.LSSerializer;&gt;import org.xml.sax.InputSource;&gt;import javax.xml.parsers.DocumentBuilder;&gt;import javax.xml.parsers.DocumentBuilderFactory;&gt;import javax.xml.transform.Transformer;&gt;import javax.xml.transform.TransformerFactory;&gt;import javax.xml.transform.dom.DOMSource;&gt;import javax.xml.transform.stream.StreamResult;&gt;import java.io.ByteArrayOutputStream;&gt;import java.io.IOException;&gt;import java.io.StringReader;&gt;import java.util.ArrayList;&gt;import java.util.List;&gt;import java.io.File;&gt;import javax.xml.parsers.DocumentBuilder;&gt;import javax.xml.parsers.DocumentBuilderFactory;&gt;import javax.xml.transform.OutputKeys;&gt;import javax.xml.transform.Transformer;&gt;import javax.xml.transform.TransformerFactory;&gt;import javax.xml.transform.dom.DOMSource;&gt;import javax.xml.transform.stream.StreamResult;&gt;import org.junit.Test;&gt;import org.w3c.dom.Document;&gt;import org.w3c.dom.Element;&gt;public class DomXml {@Testpublic void test(){Long start = System.currentTimeMillis();createXml();System.out.println(&quot;运行时间：&quot;+ (System.currentTimeMillis() - start));}/**    * 生成xml方法   */   public static void createXml() {       try {           // 创建解析器工厂           DocumentBuilderFactory factory = DocumentBuilderFactory.newInstance();           DocumentBuilder db = factory.newDocumentBuilder();           Document document = db.newDocument();           // 不显示standalone=&quot;no&quot;           document.setXmlStandalone(true);           Element bookstore = document.createElement(&quot;bookstore&quot;);           // 向bookstore根节点中添加子节点book           Element book = document.createElement(&quot;book&quot;);           Element name = document.createElement(&quot;name&quot;);           // 不显示内容 name.setNodeValue(&quot;不好使&quot;);           name.setTextContent(&quot;雷神&quot;);           book.appendChild(name);           // 为book节点添加属性           book.setAttribute(&quot;id&quot;, &quot;1&quot;);           // 将book节点添加到bookstore根节点中           bookstore.appendChild(book);           // 将bookstore节点（已包含book）添加到dom树中           document.appendChild(bookstore);           // 方式一: 保存xml文件           // 创建TransformerFactory对象&gt;//            TransformerFactory tff = TransformerFactory.newInstance();           // 创建 Transformer对象&gt;//            Transformer tf = tff.newTransformer();           // 输出内容是否使用换行&gt;//            tf.setOutputProperty(OutputKeys.INDENT, &quot;yes&quot;);           // 创建xml文件并写入内容&gt;//            tf.transform(new DOMSource(document), new StreamResult(new File(&quot;book1.xml&quot;)));&gt;//            System.out.println(&quot;生成book1.xml成功&quot;);           // 方式二: 转字符串           // 创建TransformerFactory对象           TransformerFactory transformerFactory = TransformerFactory.newInstance();           // 创建Transformer对象           Transformer transformer = transformerFactory.newTransformer();           // 设置输出数据时换行           // tf.setOutputProperty(OutputKeys.INDENT, &quot;yes&quot;);           // xml格式化           ByteArrayOutputStream bos = new ByteArrayOutputStream();           transformer.transform(new DOMSource(document), new StreamResult(bos));           String xmlString = bos.toString();           System.out.println(new String(prettyXml(xmlString)));       } catch (Exception e) {           e.printStackTrace();           System.out.println(&quot;生成book1.xml失败&quot;);       }   }&gt;}123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/**    * 格式化输出    *    * @param xmlStr: xml文档字符串    * @return 格式化xml    */   private static byte[] prettyXml(String xmlStr) {       byte[] data = null;       StringReader stringReader = null;       try {           DocumentBuilderFactory dbf = DocumentBuilderFactory.newInstance();           DocumentBuilder db = dbf.newDocumentBuilder();           stringReader = new StringReader(xmlStr);           InputSource is = new InputSource(stringReader);           Document doc = db.parse(is);           data = prettyXml(doc);       } catch (Exception e) {           throw new RuntimeException(e);       } finally {           if (stringReader != null) {               stringReader.close();           }       }       return data;   }   /**    * 格式化输出    *    * @param node: 文档树节点    * @return 格式化完的节点    */   private static byte[] prettyXml(Node node) {       ByteArrayOutputStream byteArrayOutputStream = null;       byte[] data = null;       try {           DOMImplementationRegistry registry = DOMImplementationRegistry.newInstance();           DOMImplementationLS impl = (DOMImplementationLS) registry.getDOMImplementation(&quot;XML 3.0&quot;);           LSSerializer serializer = impl.createLSSerializer();           DOMConfiguration domConfiguration = serializer.getDomConfig();           boolean isSupport = domConfiguration.canSetParameter(&quot;format-pretty-print&quot;, true);           if (isSupport) {               domConfiguration.setParameter(&quot;format-pretty-print&quot;, true);           }           LSOutput output = impl.createLSOutput();           output.setEncoding(&quot;UTF-8&quot;);           byteArrayOutputStream = new ByteArrayOutputStream();           output.setByteStream(byteArrayOutputStream);           serializer.write(node, output);           data = byteArrayOutputStream.toByteArray();       } catch (Exception e) {           throw new RuntimeException(e);       } finally {           if (byteArrayOutputStream != null) {               try {                   byteArrayOutputStream.close();               } catch (IOException e) {                   e.printStackTrace();               }           }       }       return data;   }123456&gt;&lt;?xml version=&quot;1.0&quot;encoding=&quot;UTF-8&quot;?&gt;&gt;&lt;bookstore&gt;   &lt;book id=&quot;1&quot;&gt;       &lt;name&gt;雷神&lt;/name&gt;   &lt;/book&gt;&gt;&lt;/bookstore&gt; Dom4j方式&gt;&gt;123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&gt;import java.io.File;&gt;import java.io.FileOutputStream;&gt;import org.dom4j.Document;&gt;import org.dom4j.DocumentHelper;&gt;import org.dom4j.Element;&gt;import org.dom4j.io.OutputFormat;&gt;import org.dom4j.io.XMLWriter;&gt;import org.junit.Test;&gt;public class Dom4jXml {@Testpublic void test(){Long start = System.currentTimeMillis();createXml();System.out.println(&quot;运行时间：&quot;+ (System.currentTimeMillis() - start));}/** * 生成xml方法 */public static void createXml(){try {// 1、创建document对象Document document = DocumentHelper.createDocument();// 2、创建根节点rssElement rss = document.addElement(&quot;rss&quot;);// 3、向rss节点添加version属性rss.addAttribute(&quot;version&quot;, &quot;2.0&quot;);// 4、生成子节点及子节点内容Element channel = rss.addElement(&quot;channel&quot;);Element title = channel.addElement(&quot;title&quot;);title.setText(&quot;国内最新新闻&quot;);// 5、设置生成xml的格式OutputFormat format = OutputFormat.createPrettyPrint();// 设置编码格式format.setEncoding(&quot;UTF-8&quot;);// 6、生成xml文件File file = new File(&quot;rss.xml&quot;);XMLWriter writer = new XMLWriter(new FileOutputStream(file), format);// 设置是否转义，默认使用转义字符writer.setEscapeText(false);writer.write(document);writer.close();System.out.println(&quot;生成rss.xml成功&quot;);} catch (Exception e) {e.printStackTrace();System.out.println(&quot;生成rss.xml失败&quot;);}}&gt;}JDom方式&gt;&gt;1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253&gt;import java.io.File;&gt;import java.io.FileOutputStream;&gt;import org.jdom.Document;&gt;import org.jdom.Element;&gt;import org.jdom.output.Format;&gt;import org.jdom.output.XMLOutputter;&gt;import org.junit.Test;&gt;public class JDomXml {@Testpublic void test(){Long start = System.currentTimeMillis();createXml();System.out.println(&quot;运行时间：&quot;+ (System.currentTimeMillis() - start));}/** * 生成xml方法 */public static void createXml(){try {// 1、生成一个根节点Element rss = new Element(&quot;rss&quot;);// 2、为节点添加属性rss.setAttribute(&quot;version&quot;, &quot;2.0&quot;);// 3、生成一个document对象Document document = new Document(rss);Element channel = new Element(&quot;channel&quot;);rss.addContent(channel);Element title = new Element(&quot;title&quot;);title.setText(&quot;国内最新新闻&quot;);channel.addContent(title);Format format = Format.getCompactFormat();// 设置换行Tab或空格format.setIndent(&quot;&quot;);format.setEncoding(&quot;UTF-8&quot;);// 4、创建XMLOutputter的对象XMLOutputter outputer = new XMLOutputter(format);// 5、利用outputer将document转换成xml文档File file = new File(&quot;rssNew.xml&quot;);outputer.output(document, new FileOutputStream(file));System.out.println(&quot;生成rssNew.xml成功&quot;);} catch (Exception e) {System.out.println(&quot;生成rssNew.xml失败&quot;);}}&gt;}Sax方式&gt;&gt;&gt;xml解析&gt;&gt;&gt;123import org.springframework.util.Assert;Assert.notNull(featureDataRow, &quot;数据不能为空&quot;);   Assert.noNullElements(featureDataRow.getColumns().values().toArray(), &quot;该样本存在null特征&quot;);</content>
	</entry>
	<entry>
		<title>Java基础_读取配置文件</title>
		<url>/article/f3dd51e8.html</url>
		<content>读取配置文件读取properties配置文件properties类中的主要方法1）getProperty(String key)用指定的键在此属性列表中搜索属性。也就是通过参数key，得到key所对应的value。2）load(InputStream inStream)从输入流中读取属性列表（键和元素对）。以供getProperty( String key)来搜索。3）setProperty(String key, String value)调用Hashtable的方法put 。他通过调用基类的put方法来设置键-值对。4）store(OutputStream out, String comments)以适合使用load方法加载到Properties表中的格式，将此Properties表中的属性列表（键和元素对）写入输出流。与load方法相反，该方法将键-值对写入到指定的文件中去。5）clear()清除所有装载的键-值对。该方法在基类中提供12apple=src.main.java.per.hyc.ReflexAndInvokeTest.Appleorange=src.main.java.per.hyc.ReflexAndInvokeTest.Orange12345678910111213141516171819202122232425262728import java.io.File;import java.io.FileInputStream;import java.io.FileOutputStream;import java.util.Properties;/** * 获取properties文件的配置 */public class GetProperties {    public static Properties getPro(String pathname) {        Properties pro = new Properties();        File f = new File(pathname);        try {            if (f.exists()) {                pro.load(new FileInputStream(f));            }            //建立一个新的属性文件，同时设置好默认内容            else {                pro.setProperty(&quot;apple&quot;, &quot;per.hyc.ReflexAndInvokeTest.Apple&quot;);                pro.setProperty(&quot;orange&quot;, &quot;per.hyc.ReflexAndInvokeTest.Orange&quot;);                pro.store(new FileOutputStream(f), &quot;FRUIT CLASS&quot;);            }        } catch (Exception e) {            e.printStackTrace();        }        return pro;    }}读取xml配置文件12345&lt;?xml version=&quot;1.0&quot;?&gt;&lt;config&gt;    &lt;className&gt;per.hyc.designPattern.Bridge.Blue&lt;/className&gt;    &lt;className&gt;per.hyc.designPattern.Bridge.SmallPen&lt;/className&gt;&lt;/config&gt;123456789101112131415161718192021222324252627282930313233343536import javax.xml.parsers.*;import org.w3c.dom.*;import java.io.*;/** * 使用java反射创建具体的颜色和画笔 */public class XMLUtilPen {    // 该方法用于从XML配置文件中提取具体类类名，并返回一个实例对象    public static Object getBean(String args) {        try {            //创建文档对象            DocumentBuilderFactory dFactory = DocumentBuilderFactory.newInstance();            DocumentBuilder builder = dFactory.newDocumentBuilder();            Document doc;            doc = builder.parse(new File(&quot;.\\src\\main\\resources\\configPen.xml&quot;));            Node classNode = null;            NodeList nl = doc.getElementsByTagName(&quot;className&quot;);            if (args.equals(&quot;color&quot;)) {                //获取包含类名的文本节点                classNode = nl.item(0).getFirstChild();            } else if (args.equals(&quot;pen&quot;)) {                //获取包含类名的文本节点                classNode = nl.item(1).getFirstChild();            }            String cName = classNode.getNodeValue();            //通过类名生成实例对象并将其返回            return Class.forName(cName).newInstance();        } catch (Exception e) {            e.printStackTrace();            return null;        }    }}Spring 读取properties文件Java中读取Properties配置文件的几种方式@ConfigurationProperties(prefix = “student”)@ValueEnvironment详见项目springbootdemo利用第三方配置工具owner读取配置文件</content>
	</entry>
	<entry>
		<title>Java基础_集合</title>
		<url>/article/d6e2fafb.html</url>
		<content>集合</content>
	</entry>
	<entry>
		<title>Java进阶</title>
		<url>/article/599c28b5.html</url>
		<content></content>
	</entry>
	<entry>
		<title>Java进阶_注解</title>
		<url>/article/657dc990.html</url>
		<content>【黑马程序员-Java语言高级部分9.3】Java 注解注解基础概念注释：用文字描述程序的，给程序员看的百度上的解释：Java 注解（Annotation）又称 Java 标注，是 JDK5.0 引入的一种注释机制，是一种代码级别的说明。Java 语言中的类、方法、变量、参数和包等都可以被标注。和 Javadoc 不同，Java 标注可以通过反射获取标注内容。在编译器生成类文件时，标注可以被嵌入到字节码中。Java 虚拟机可以保留标注内容，在运行时可以获取到标注内容 。 当然它也支持自定义 Java 标注。概念描述：JDK1.5 之后的新特性用来说明程序的使用注解：@注解名称作用分类编译检查：通过代码里标识的注解让编译器能够实现基本的编译检查【Override】编写文档：通过代码里标识的注解生成文档，API文档是通过抽取代码中的文档注释生成的。代码分析：通过代码里标识的注解对代码进行分析【使用反射】大多数时候，我们会使用注解而不是自定义注解注解给编译器和解析程序用注解不是程序的一部分，可以理解为标签预定义注解@Override：检测被该注解标注的方法是否搜集继承自父类(接口)的，定义在java.lang.Override中，此注释只适用于修辞方法，表示一个方法声明打算重写超类中的另一个方法声明@Deprecated：将该注解标注的内容，表示已过时，定义在java.lang.Deprecated中，此注释可以用于修辞方法，属性，类，表示不鼓励程序员使用这样的元素，通常是因为它很危险或者存在更好的选择@SuppressWarnings：压制警告，一般传递参数all，定义在java.lang.SuppressWarnings中，用来抑制编译时的警告信息。与前两个注释有所不同，你需要添加一个参数才能正确使用，这些参数都是已经定义好了的，我们选择性的使用就好了如@SuppressWarnings(“all”)、@SuppressWarnings(“all”)、@SuppressWarnings(“unchecked”)、@SuppressWarnings(value={“unchecked”,”deprecation”})自定义注解基本格式12&gt;@元注解&gt;public @interface 注解名称{}注解的本质: 注解本质上就是一个接口，该接口默认继承java.lang.annotation.Annotation接口123456789101112131415&gt;// 将以下注解编译过后进行反编译，得到结果：&gt;// 1. 编译前&gt;public @interface MyAnno{String name default &quot;hyc&quot;;    // 每一个方法实际上是声明了一个配置参数，本质上是抽象方法   // 方法的名称就是参数的名称   // 返回值类型就是参数的类型（返回值只能是基本类型，注解，String，Enum，以上类型的数组)   // 可以通过default来声明参数的默认值   // 如果只有一个参数成员，一般参数名为value，那么value可以省略，直接赋值即可   // 注解元素必须要有值，我们定义注解元素时，经常使用空字符串，0作为默认值   // 数组赋值时，值使用大括号包裹，如果数组中只有一个值，那么{}可以省略&gt;}&gt;// 2. 反编译后&gt;public interface MyAnno extends java.lang.annotation.Annotation{}元注解概念：用于描述注解的注解。元注解的作用就是负责注解其他注解，Java定义了4个标准的meta-annotation类型，他们被用来提供对其他annotation类型作说明。这些类型和它们所支持的类在java.lang.annotation包中可以找到。@Target：描述能够作用的位置（即：被描述的注解可以用在什么地方）123@Target(value = {ElementType.TYPE,ElementType.METHOD,ElementType.FIELD}) //表示该MyAnno注解可以同时作用于类上，方法上和成员变量上public @interface MyAnno {}其中value中ElementType取值可以有以下几种情况：TYPE:可以作用在类上METHOD:可以作用在方法上FIELD：可以作用于成员变量上@Retention：描述注解被保留的阶段，用于描述注解的生命周期@Retention(RetentionPolicy.RUNTIME)：当前被描述的注解，会保留到字节码文件中，并被JVM读取到，一般自己定义的注解都加RUNTIME@Documented：描述该注解是否会被抽取到api文档中@Inherited：描述注解是否被子类继承使用注解注解在程序中经常和反射一起使用，注解大多数来说都是用来替换配置文件的1234567891011&gt;import java.lang.annotation.ElementType;&gt;import java.lang.annotation.Retention;&gt;import java.lang.annotation.RetentionPolicy;&gt;import java.lang.annotation.Target;&gt;@Target(ElementType.TYPE) //可以被作用在类上&gt;@Retention(RetentionPolicy.RUNTIME)&gt;public @interface AnnoReflect {  String className();  String methodName();&gt;}1234567891011121314151617181920212223242526272829303132333435363738394041&gt;// 使用注解的方式来淘汰配置文件(注释很重要)：&gt;import java.io.InputStream;&gt;import java.lang.reflect.Method;&gt;import java.util.Properties;&gt;@AnnoReflect(className = &quot;cn.other.annotation.AnnoTest&quot;,methodName = &quot;play&quot;)&gt;public class ReflectAnnotationTest {  public static void main(String[] args) throws Exception {      /**       * 前提：不能改变该类的任何代码。可以创建任意类的对象，可以执行任意方法       * 即：拒绝硬编码       */      //1. 解析注解      //1.1 获取该类的字节码文件对象      Class&lt;ReflectAnnotationTest&gt; rac = ReflectAnnotationTest.class;      //1.2 获取上面的注解对象,其实就是在内存中生成了一个该注解接口的子类实现对象      AnnoReflect an = rac.getAnnotation(AnnoReflect.class);      /* 相当于          public class AnnotationReflect implements AnnoReflect{              public String className(){                  return &quot;cn.other.annotation.AnnoTest1&quot;;              }              public String methodName(){                  return &quot;play&quot;;              }          } */      //2. 调用注解对象中定义的抽象方法，获取返回值      String className = an.className();      String methodName = an.methodName();      //3.加载该类进内存      Class cls = Class.forName(className);      //4.创建对象      Object obj = cls.newInstance();      //5.获取方法对象      Method method = cls.getMethod(methodName);      //6.执行方法      method.invoke(obj);  }&gt;}&gt;在程序中使用注解：获取注解中定义的属性值获取注解定义的位置的对象 (Class, Method, Field)获取指定的注解：getAnnotation(Class)调用注解中的抽象方法获取配置的属性值案例分析测试框架需求：设计一个框架，检测一个类中的方法使用有异常，并进行统计。123456789101112131415161718192021&gt;public class calculator {   @Check   public void add(){       System.out.println(&quot;1+0=&quot;+(1+0));  }   @Check   public void sub(){       System.out.println(&quot;1-0=&quot;+(1-0));   }   @Check   public void mul(){       System.out.println(&quot;1*0=&quot;+(1*0));   }   @Check   public void div(){       System.out.println(&quot;1/0=&quot;+(1/0));   }   public void show(){       System.out.println(&quot;今天天气真不错！&quot;);   }}1234// 自定义一个注解@Retention(RetentionPolicy.RUNTIME) //运行时@Target(ElementType.METHOD) //加在方法前面public @interface Check {}编写一个类专门用于检查(注意注释)：12345678910111213141516171819202122232425262728293031323334353637383940414243444546&gt;import java.io.BufferedWriter;&gt;import java.io.FileWriter;&gt;import java.io.IOException;&gt;import java.lang.reflect.InvocationTargetException;&gt;import java.lang.reflect.Method;&gt;// 简单的测试框架&gt;// 当主方法执行后，会自动自行检测所有方法(加了check注解的方法)，判断方法是否有异常并记录&gt;public class TestCheck {  public static void main(String[] args) throws IOException {      //1. 创建计算机对象      calculator c = new calculator();      //2. 获取字节码文件对象       Class cls = c.getClass();      //3. 获取所有方法       Method[] methods = cls.getMethods();       int num = 0; //记录出现异常的次数       BufferedWriter bw = new BufferedWriter(new FileWriter(&quot;bug.txt&quot;));       for(Method method:methods){           //4. 判断方法上是否有Check注解           if(method.isAnnotationPresent(Check.class)){               //5. 有注解就执行,捕获异常               try {                   method.invoke(c);               } catch (Exception e) {                   e.printStackTrace();                   //6.将异常记录在文件中                   num++;                   bw.write(method.getName()+&quot;方法出异常了&quot;);                   bw.newLine();                   bw.write(&quot;异常的名称是：&quot;+e.getCause().getClass().getSimpleName());                   bw.newLine();                   bw.write(&quot;异常原因：&quot;+e.getCause().getMessage());                   bw.newLine();                   bw.write(&quot;=====================&quot;);                   bw.newLine();               }           }       }       bw.write(&quot;本次测试一共出现&quot;+num+&quot;次异常&quot;);       bw.flush();       bw.close();   }}运行TestCheck类中的主方法，就会自动检查所有注解@Check的方法是否异常：&gt;ORM实例&gt;12345678910111213&gt;// 类名的注解 &gt;@Target(ElementType.TYPE) &gt;@Retention(RetentionPolicy.RUNTIME) &gt;@interface Tablekuang{ string value(); }&gt;// 属性的注解 &gt;@Target(ElementType.FIELD) &gt;@Retention(RetentionPolicy.RUNTIME) &gt;@interface Fieldkuang{ String columnName(); string type(); int length();&gt;}123456789101112131415&gt;@Tablekuang(&quot;db_student&quot;) &gt;class student{ @Fieldkuang(columnName=&quot;db_id&quot;,type=&quot;int&quot;,length =10) private int id;@Fieldkuang(columnName=&quot;db_age&quot;,type=&quot;int&quot;,length=10) private int age; @Fieldkuang(columnName =&quot;db_name&quot;,type =&quot;&quot;varchar&quot;,length=3) private String name; public student(){}public student(int id,int age,String name){ this.id=id;this.age=age;this.name=name;}&gt;}1234567891011121314151617&gt;public static void main(string[] args) throws ClassNotFoundException { Class c1=Class.forName(&quot;com.huangy.reflection.Student&quot;); //通过反射获得注解 Annotation[] annotations=c1.getAnnotations();   annotations.foreach(System.out::println);      //获得注解的alue的值。   Tablekuang tablekuang =(Tablekuang)c1.getAnnotation(Tablekuang.class);    System.out.printn(ablekuang.value());    //获得类指定的注解    Field f=c1.getDeclaredField(&quot;name&quot;);    Fieldkuang annotation=f.getAnnotation(Fieldkuang.class);    System.out.println(annotation.columnName());   System.out.println(annotation.type());   System.out.println(annotation.length());&gt;}</content>
	</entry>
	<entry>
		<title>MySQL</title>
		<url>/article/c24675b4.html</url>
		<content></content>
	</entry>
	<entry>
		<title>MySQL_事务</title>
		<url>/article/eebdb36a.html</url>
		<content>MySQL多表查询与事务的操作基本介绍概念：一个包含多个步骤的业务操作，被事务管理，那么这些操作要么同时成功，要么同时失败。操作：事务四大特征原子性：是不可分割的最小操作单位，要么同时成功，要么同时失败。持久性：当事务提交或回滚后，数据库会持久化的保存数据。隔离性：多个事务之间。相互独立。一致性：事务操作前后，数据总量不变事务隔离级别概念：多个事务之间隔离的，相互独立的。但是如果多个事务操作同一批数据，则会引发一些问题，设置不同的隔离级别就可以解决这些问题存在问题脏读：一个事务，读取到另一个事务中没有提交的数据不可重复读(虚读)：在同一个事务中，两次读取到的数据不一样。幻读：一个事务操作(DML)数据表中所有记录，另一个事务添加了一条数据，则第一个事务查询不到自己的修改。隔离级别read uncommitted：读未提交(产生的问题：脏读、不可重复读、幻读)A事务还没执行提交，B事务已经可以查到数据了read committed：读已提交 (Oracle)(产生的问题：不可重复读、幻读)A事务执行提交，B事务在同一次查询中查到已被修改的数据repeatable read：可重复读 (MySQL默认)(产生的问题：幻读)忽略A事务执行状态，B事务在同一次查询中数据不会变化serializable：串行化(可以解决所有的问题)注意：隔离级别从小到大安全性越来越高，但是效率越来越低数据库查询隔离级别1select @@tx_isolation;数据库设置隔离级别1set global transaction isolation level  级别字符串;演示12345set global transaction isolation level read uncommitted;start transaction;-- 转账操作update account set balance = balance - 500 where id = 1;update account set balance = balance + 500 where id = 2;例子12345678910111213141516171819202122&gt;CREATE TABLE account (id INT PRIMARY KEY AUTO_INCREMENT,NAME VARCHAR(10),balance DOUBLE&gt;);&gt;-- 添加数据&gt;INSERT INTO account (NAME, balance) VALUES ('zhangsan', 1000), ('lisi', 1000);&gt;SELECT * FROM account;&gt;UPDATE account SET balance = 1000;&gt;-- 张三给李四转账 500 元&gt;-- 0. 开启事务&gt;START TRANSACTION; -- 失败的话会回滚到这里&gt;-- 1. 张三账户 -500&gt;UPDATE account SET balance = balance - 500 WHERE NAME = 'zhangsan';&gt;-- 2. 李四账户 +500&gt;-- 出错了...&gt;UPDATE account SET balance = balance + 500 WHERE NAME = 'lisi';&gt;-- 发现执行没有问题，提交事务&gt;COMMIT;&gt;-- 发现出问题了，回滚事务&gt;ROLLBACK;事务提交自动提交：mysql就是自动提交的一条DML(增删改)语句会自动提交一次事务手动提交：Oracle 数据库默认是手动提交事务需要先开启事务，再提交修改事务的默认提交方式：查看事务的默认提交方式1SELECT @@autocommit; -- 1 代表自动提交  0 代表手动提交修改默认提交方式1set @@autocommit = 0;</content>
	</entry>
	<entry>
		<title>MySQL_基础</title>
		<url>/article/5dca6626.html</url>
		<content>【黑马程序员-Java语言高级部分10】MySQL入门学习基础概念数据库的英文单词： DataBase(简称DB)数据库: 用于存储和管理数据的仓库数据库的特点：持久化存储数据的。其实数据库就是一个文件系统方便存储和管理数据使用了统一的方式操作数据库 — SQL常见的数据库软件数据库软件安装卸载去mysql的安装目录找到my.ini文件复制 datadir=”C:/ProgramData/MySQL/MySQL Server 5.5/Data/“卸载MySQL删除C:/ProgramData目录下的MySQL文件夹。配置MySQL服务启动手动cmd—&gt; services.msc 打开服务的窗口使用管理员打开cmdnet start mysql : 启动mysql的服务net stop mysql:关闭mysql服务MySQL登录mysql -uroot -p密码mysql -hip -uroot -p连接目标的密码mysql —host=ip —user=root —password=连接目标的密码MySQL退出(exit or quit)MySQL目录结构MySQL安装目录：basedir=”D:/develop/MySQL/“配置文件 my.iniMySQL数据目录：datadir=”C:/ProgramData/MySQL/MySQL Server 5.5/Data/“几个概念数据库：文件夹表：文件数据：数据SQL基础什么是SQLStructured Query Language：结构化查询语言其实就是定义了操作所有关系型数据库的规则。每一种数据库操作的方式存在不一样的地方，称为“方言”。SQL通用语法SQL 语句可以单行或多行书写，以分号结尾。可使用空格和缩进来增强语句的可读性。MySQL 数据库的 SQL 语句不区分大小写，关键字建议使用大写。3种注释单行注释: — 注释内容 或 # 注释内容(mysql 特有) 多行注释: / 注释 /SQL分类DDL(Data Definition Language)数据定义语言    用来定义数据库对象：数据库，表，列等。关键字：create, drop,alter 等DML(Data Manipulation Language)数据操作语言    用来对数据库中表的数据进行增删改。关键字：insert, delete, update 等DQL(Data Query Language)数据查询语言    用来查询数据库中表的记录(数据)。关键字：select, where 等DCL(Data Control Language)数据控制语言(了解)    用来定义数据库的访问权限和安全级别，及创建用户。关键字：GRANT， REVOKE 等DDL: 操作数据库、表操作数据库：CRUDC(Create): 创建创建数据库：1create database 数据库名称;创建数据库，判断不存在，再创建：1create database if not exists 数据库名称;创建数据库，并指定字符集1create database 数据库名称 character set 字符集名;练习： 创建db4数据库，判断是否存在，并制定字符集为gbk1create database if not exists db4 character set gbk;R(Retrieve)：查询查询所有数据库的名称:1show databases;查询某个数据库的字符集:查询某个数据库的创建语句1show create database 数据库名称;U(Update): 修改修改数据库的字符集1alter database 数据库名称 character set 字符集名称;D(Delete): 删除删除数据库1drop database 数据库名称;判断数据库存在，存在再删除1drop database if exists 数据库名称;使用数据库查询当前正在使用的数据库名称1select database();使用数据库1use 数据库名称;操作表C(Create): 创建 123456create table 表名(列名1 数据类型1,列名2 数据类型2,....列名n 数据类型n);注意：最后一列，不需要加逗号（,）数据库类型：int：整数类型age int,double:小数类型score double(5,2)date:日期，只包含年月日，yyyy-MM-dddatetime:日期，包含年月日时分秒     yyyy-MM-dd HH:mm:sstimestamp:时间错类型    包含年月日时分秒     yyyy-MM-dd HH:mm:ss    如果将来不给这个字段赋值，或赋值为null，则默认使用当前的系统时间，来自动赋值varchar：字符串name varchar(20): 姓名最大20个字符zhangsan 8个字符  张三 2个字符创建表:12345678create table student(id int,name varchar(32),age int ,score double(4,1),birthday date,insert_time timestamp);复制表：1create table 表名 like 被复制的表名;R(Retrieve): 查询查询某个数据库中所有的表名称1show tables;查询表结构1desc 表名;U(Update): 修改修改表名1alter table 表名 rename to 新的表名;修改表的字符集1alter table 表名 character set 字符集名称;添加一列1alter table 表名 add 列名 数据类型;修改列名称 类型12alter table 表名 change 列名 新列别 新数据类型;alter table 表名 modify 列名 新数据类型;删除列1alter table 表名 drop 列名;D(Delete):删除12drop table 表名;drop table  if exists 表名 ;DML: 增删改表中数据添加数据：1insert into 表名(列名1,列名2,...列名n) values(值1,值2,...值n);注意:1234列名和值要一一对应。如果表名后，不定义列名，则默认给所有列添加值insert into 表名 values(值1,值2,...值n);除了数字类型，其他类型需要使用引号(单双都可以)引起来删除数据：1delete from 表名 [where 条件]注意:1234如果不加条件，则删除表中所有记录。如果要删除所有记录delete from 表名; -- 不推荐使用。有多少条记录就会执行多少次删除操作TRUNCATE TABLE 表名; -- 推荐使用，效率更高 先删除表，然后再创建一张一样的表。修改数据：1update 表名 set 列名1 = 值1, 列名2 = 值2,... [where 条件];注意: 如果不加任何条件，则会将表中所有记录全部修改。备用DQL: 查询表中的记录语法1234567891011121314select字段列表from表名列表where条件列表group by分组字段having分组之后的条件order by排序limit分页限定;基础查询多个字段的查询1select 字段名1，字段名2... from 表名；注意：如果查询所有字段，则可以使用*来替代字段列表。去除重复: distinct计算列一般可以使用四则运算计算一些列的值。（一般只会进行数值型的计算）ifnull(表达式1,表达式2)：null参与的运算，计算结果都为null表达式1：哪个字段需要判断是否为null如果该字段为null后的替换值。起别名  as：as也可以省略备用条件查询where子句后跟条件运算符1234567891011&gt; 、&lt; 、&lt;= 、&gt;= 、= 、&lt;&gt;BETWEEN...AND  IN( 集合) LIKE：模糊查询占位符：_: 单个任意字符%：多个任意字符IS NULL  and  或 &amp;&amp;or  或 || not  或 !示例1234567891011121314151617181920212223242526272829303132333435363738-- 查询年龄大于20岁SELECT * FROM student WHERE age &gt; 20;SELECT * FROM student WHERE age &gt;= 20;  -- 查询年龄等于20岁SELECT * FROM student WHERE age = 20;  -- 查询年龄不等于20岁SELECT * FROM student WHERE age != 20;SELECT * FROM student WHERE age &lt;&gt; 20;  -- 查询年龄大于等于20 小于等于30SELECT * FROM student WHERE age &gt;= 20 &amp;&amp;  age &lt;=30;SELECT * FROM student WHERE age &gt;= 20 AND  age &lt;=30;SELECT * FROM student WHERE age BETWEEN 20 AND 30;  -- 查询年龄22岁，18岁，25岁的信息SELECT * FROM student WHERE age = 22 OR age = 18 OR age = 25SELECT * FROM student WHERE age IN (22,18,25);  -- 查询英语成绩为nullSELECT * FROM student WHERE english = NULL; -- 不对的。null值不能使用 = （!=） 判断SELECT * FROM student WHERE english IS NULL;  -- 查询英语成绩不为nullSELECT * FROM student WHERE english  IS NOT NULL;  -- 查询姓马的有哪些？ likeSELECT * FROM student WHERE NAME LIKE '马%';  -- 查询姓名第二个字是化的人SELECT * FROM student WHERE NAME LIKE &quot;_化%&quot;;  -- 查询姓名是3个字的人SELECT * FROM student WHERE NAME LIKE '___';  -- 查询姓名中包含德的人SELECT * FROM student WHERE NAME LIKE '%德%';备用排序查询语法：order by 子句order by 排序字段1 排序方式1 ，  排序字段2 排序方式2…排序方式：ASC：升序，默认的。DESC：降序。注意：如果有多个排序条件，则当前边的条件值一样时，才会判断第二条件。聚合函数将一列数据作为一个整体，进行纵向的计算count：计算个数一般选择非空的列：主键count(*)max：计算最大值min：计算最小值sum：计算和avg：计算平均值注意：聚合函数的计算，排除null值。解决方案：选择不包含非空的列进行计算IFNULL函数分组查询语法：group by 分组字段；注意：分组之后查询的字段：分组字段、聚合函数where 和 having 的区别？where 在分组之前进行限定，如果不满足条件，则不参与分组。having在分组之后进行限定，如果不满足结果，则不会被查询出来where 后不可以跟聚合函数，having可以进行聚合函数的判断。123456789101112-- 按照性别分组。分别查询男、女同学的平均分SELECT sex , AVG(math) FROM student GROUP BY sex;-- 按照性别分组。分别查询男、女同学的平均分,人数SELECT sex , AVG(math),COUNT(id) FROM student GROUP BY sex;--  按照性别分组。分别查询男、女同学的平均分,人数 要求：分数低于70分的人，不参与分组SELECT sex , AVG(math),COUNT(id) FROM student WHERE math &gt; 70 GROUP BY sex;--  按照性别分组。分别查询男、女同学的平均分,人数 要求：分数低于70分的人，不参与分组,分组之后。人数要大于2个人SELECT sex , AVG(math),COUNT(id) FROM student WHERE math &gt; 70 GROUP BY sex HAVING COUNT(id) &gt; 2;SELECT sex , AVG(math),COUNT(id) 人数 FROM student WHERE math &gt; 70 GROUP BY sex HAVING 人数 &gt; 2;分页查询语法：limit 开始的索引,每页查询的条数;公式：开始的索引 = （当前的页码 - 1） * 每页显示的条数limit 是一个MySQL”方言”1234-- 每页显示3条记录 SELECT * FROM student LIMIT 0,3; -- 第1页SELECT * FROM student LIMIT 3,3; -- 第2页SELECT * FROM student LIMIT 6,3; -- 第3页DCL：管理用户和授权用户管理添加用户1CREATE USER '用户名'@'主机名' IDENTIFIED BY '密码';删除用户1DROP USER '用户名'@'主机名';修改用户密码12345UPDATE USER SET PASSWORD = PASSWORD('新密码') WHERE USER = '用户名';UPDATE USER SET PASSWORD = PASSWORD('abc') WHERE USER = 'lisi';SET PASSWORD FOR '用户名'@'主机名' = PASSWORD('新密码');SET PASSWORD FOR 'root'@'localhost' = PASSWORD('123');忘记密码管理员运行cmd1net stop mysql --停止mysql服务使用无验证方式启动mysql服务1mysqld --skip-grant-tables使用无验证方式启动mysql服务打开新的cmd窗口,直接输入mysql命令，敲回车就可以登录成功12use mysql;update user set password = password('你的新密码') where user = 'root';关闭两个窗口打开任务管理器，手动结束mysqld.exe 的进程启动mysql服务，使用新密码登录查询用户12345-- 1. 切换到mysql数据库USE myql;-- 2. 查询user表SELECT * FROM USER;-- * 通配符： % 表示可以在任意主机使用用户登录数据库权限管理查询权限12SHOW GRANTS FOR '用户名'@'主机名';SHOW GRANTS FOR 'lisi'@'%';授予权限1234-- 授予权限grant 权限列表 on 数据库名.表名 to '用户名'@'主机名';-- 给张三用户授予所有权限，在任意数据库任意表上GRANT ALL ON *.* TO 'zhangsan'@'localhost';撤销权限123-- 撤销权限revoke 权限列表 on 数据库名.表名 from '用户名'@'主机名';REVOKE UPDATE ON db3.`account` FROM 'lisi'@'%';</content>
	</entry>
	<entry>
		<title>MySQL_多表查询</title>
		<url>/article/336443d5.html</url>
		<content>MySQL多表查询与事务的操作sql准备123456789101112131415161718192021# 创建部门表CREATE TABLE dept(id INT PRIMARY KEY AUTO_INCREMENT,NAME VARCHAR(20));INSERT INTO dept (NAME) VALUES ('开发部'),('市场部'),('财务部');# 创建员工表CREATE TABLE emp (id INT PRIMARY KEY AUTO_INCREMENT,NAME VARCHAR(10),gender CHAR(1), -- 性别salary DOUBLE, -- 工资join_date DATE, -- 入职日期dept_id INT,FOREIGN KEY (dept_id) REFERENCES dept(id) -- 外键，关联部门表(部门表的主键));INSERT INTO emp(NAME,gender,salary,join_date,dept_id) VALUES('孙悟空','男',7200,'2013-02-24',1);INSERT INTO emp(NAME,gender,salary,join_date,dept_id) VALUES('猪八戒','男',3600,'2010-12-02',2);INSERT INTO emp(NAME,gender,salary,join_date,dept_id) VALUES('唐僧','男',9000,'2008-08-08',2);INSERT INTO emp(NAME,gender,salary,join_date,dept_id) VALUES('白骨精','女',5000,'2015-10-07',3);INSERT INTO emp(NAME,gender,salary,join_date,dept_id) VALUES('蜘蛛精','女',4500,'2011-03-14',1);内连接查询从哪些表中查询数据、条件是什么、查询哪些字段隐式内连接使用where条件消除无用数据123456789101112131415-- 查询所有员工信息和对应的部门信息SELECT * FROM emp,dept WHERE emp.`dept_id` = dept.`id`;-- 查询员工表的名称，性别。部门表的名称SELECT emp.name,emp.gender,dept.name FROM emp,dept WHERE emp.`dept_id` = dept.`id`;SELECT t1.name, -- 员工表的姓名t1.gender,-- 员工表的性别t2.name -- 部门表的名称FROMemp t1,dept t2WHERE t1.`dept_id` = t2.`id`;显式内连接语法： select 字段列表 from 表名1 [inner] join 表名2 on 条件12SELECT * FROM emp INNER JOIN dept ON emp.`dept_id` = dept.`id`;SELECT * FROM emp JOIN dept ON emp.`dept_id` = dept.`id`;外连接查询左外连接语法：select 字段列表 from 表1 left [outer] join 表2 on 条件；123-- 查询的是左表所有数据以及其交集部分。-- 查询所有员工信息，如果员工有部门，则查询部门名称，没有部门，则不显示部门名称SELECT t1.*,t2.`name` FROM emp t1 LEFT JOIN dept t2 ON t1.`dept_id` = t2.`id`;右外连接语法：select 字段列表 from 表1 right [outer] join 表2 on 条件；12-- 查询的是右表所有数据以及其交集部分SELECT * FROM dept t2 RIGHT JOIN emp t1 ON t1.`dept_id` = t2.`id`;子查询概念：查询中嵌套查询，称嵌套查询为子查询12345678&gt;-- 查询工资最高的员工信息&gt;-- 1 查询最高的工资是多少 9000&gt;SELECT MAX(salary) FROM emp;&gt;-- 2 查询员工信息，并且工资等于9000的&gt;SELECT * FROM emp WHERE emp.`salary` = 9000;&gt;-- 一条sql就完成这个操作。子查询&gt;SELECT * FROM emp WHERE emp.`salary` = (SELECT MAX(salary) FROM emp);子查结果是单行单列子查询可以作为条件，使用运算符去判断。 运算符： &gt; &gt;= &lt; &lt;= =12&gt;-- 查询员工工资小于平均工资的人&gt;SELECT * FROM emp WHERE emp.salary &lt; (SELECT AVG(salary) FROM emp);子查询结果是多行单列子查询可以作为条件，使用运算符in来判断12345&gt;-- 查询'财务部'和'市场部'所有的员工信息&gt;SELECT id FROM dept WHERE NAME = '财务部' OR NAME = '市场部';&gt;SELECT * FROM emp WHERE dept_id = 3 OR dept_id = 2;&gt;-- 子查询&gt;SELECT * FROM emp WHERE dept_id IN (SELECT id FROM dept WHERE NAME = '财务部' OR NAME = '市场部');子查询结果是多行多列子查询可以作为一张虚拟表参与查询123456&gt;-- 查询员工入职日期是2011-11-11日之后的员工信息和部门信息&gt;-- 子查询&gt;SELECT * FROM dept t1 ,(SELECT * FROM emp WHERE emp.`join_date` &gt; '2011-11-11') t2 WHERE t1.id = t2.dept_id;&gt;-- 普通内连接&gt;SELECT * FROM emp t1,dept t2 WHERE t1.`dept_id` = t2.`id` AND t1.`join_date` &gt;  '2011-11-11'多表查询练习见表语句详见附录12345678910111213141516&gt;-- 1.查询所有员工信息。查询员工编号，员工姓名，工资，职务名称，职务描述&gt;/*分析：1.员工编号，员工姓名，工资，需要查询emp表  职务名称，职务描述 需要查询job表2.查询条件 emp.job_id = job.id&gt;*/&gt;SELECT    t1.`id`, -- 员工编号   t1.`ename`, -- 员工姓名   t1.`salary`,-- 工资   t2.`jname`, -- 职务名称   t2.`description` -- 职务描述&gt;FROM emp t1, job t2&gt;WHERE t1.`job_id` = t2.`id`;12345678910111213141516171819&gt;-- 2.查询员工编号，员工姓名，工资，职务名称，职务描述，部门名称，部门位置&gt;/*分析：1. 员工编号，员工姓名，工资 emp  职务名称，职务描述 job  部门名称，部门位置 dept2. 条件： emp.job_id = job.id and emp.dept_id = dept.id&gt;*/&gt;SELECT    t1.`id`, -- 员工编号   t1.`ename`, -- 员工姓名   t1.`salary`,-- 工资   t2.`jname`, -- 职务名称   t2.`description`, -- 职务描述   t3.`dname`, -- 部门名称   t3.`loc` -- 部门位置&gt;FROM    emp t1, job t2,dept t3&gt;WHERE t1.`job_id` = t2.`id` AND t1.`dept_id` = t3.`id`;   12345678910111213141516&gt;-- 3.查询员工姓名，工资，工资等级&gt;/*分析：1.员工姓名，工资 emp  工资等级 salarygrade2.条件 emp.salary &gt;= salarygrade.losalary and emp.salary &lt;= salarygrade.hisalaryemp.salary BETWEEN salarygrade.losalary and salarygrade.hisalary&gt;*/&gt;SELECT    t1.ename ,   t1.`salary`,   t2.*&gt;FROM emp t1, salarygrade t2&gt;WHERE t1.`salary` BETWEEN t2.`losalary` AND t2.`hisalary`;1234567891011121314151617181920&gt;-- 4.查询员工姓名，工资，职务名称，职务描述，部门名称，部门位置，工资等级&gt;/*分析：1. 员工姓名，工资 emp ， 职务名称，职务描述 job 部门名称，部门位置，dept  工资等级 salarygrade2. 条件： emp.job_id = job.id and emp.dept_id = dept.id and emp.salary BETWEEN salarygrade.losalary and salarygrade.hisalary&gt;*/&gt;SELECT t1.`ename`,t1.`salary`,t2.`jname`,t2.`description`,t3.`dname`,t3.`loc`,t4.`grade`&gt;FROM emp t1,job t2,dept t3,salarygrade t4&gt;WHERE t1.`job_id` = t2.`id` AND t1.`dept_id` = t3.`id`AND t1.`salary` BETWEEN t4.`losalary` AND t4.`hisalary`;1234567891011121314151617&gt;-- 5.查询出部门编号、部门名称、部门位置、部门人数&gt;/*分析：1.部门编号、部门名称、部门位置 dept 表。 部门人数 emp表2.使用分组查询。按照emp.dept_id完成分组，查询count(id)3.使用子查询将第2步的查询结果和dept表进行关联查询&gt;*/&gt;SELECT t1.`id`,t1.`dname`,t1.`loc` , t2.total&gt;FROM dept t1,(SELECTdept_id,COUNT(id) totalFROM empGROUP BY dept_id) t2&gt;WHERE t1.`id` = t2.dept_id;1234567891011121314151617&gt;-- 6.查询所有员工的姓名及其直接上级的姓名,没有领导的员工也需要查询&gt;/*分析：1.姓名 emp， 直接上级的姓名 emp emp表的id 和 mgr 是自关联2.条件 emp.id = emp.mgr3.查询左表的所有数据，和 交集数据使用左外连接查询&gt;*/&gt;SELECT t1.ename,t1.mgr,t2.`id`,t2.`ename`&gt;FROM emp t1LEFT JOIN emp t2ON t1.`mgr` = t2.`id`;附录12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394-- 部门表CREATE TABLE dept (  id INT PRIMARY KEY PRIMARY KEY, -- 部门id  dname VARCHAR(50), -- 部门名称  loc VARCHAR(50) -- 部门所在地);-- 添加4个部门INSERT INTO dept(id,dname,loc) VALUES (10,'教研部','北京'),(20,'学工部','上海'),(30,'销售部','广州'),(40,'财务部','深圳');-- 职务表，职务名称，职务描述CREATE TABLE job (  id INT PRIMARY KEY,  jname VARCHAR(20),  description VARCHAR(50));-- 添加4个职务INSERT INTO job (id, jname, description) VALUES(1, '董事长', '管理整个公司，接单'),(2, '经理', '管理部门员工'),(3, '销售员', '向客人推销产品'),(4, '文员', '使用办公软件');-- 员工表CREATE TABLE emp (  id INT PRIMARY KEY, -- 员工id  ename VARCHAR(50), -- 员工姓名  job_id INT, -- 职务id  mgr INT , -- 上级领导  joindate DATE, -- 入职日期  salary DECIMAL(7,2), -- 工资  bonus DECIMAL(7,2), -- 奖金  dept_id INT, -- 所在部门编号  CONSTRAINT emp_jobid_ref_job_id_fk FOREIGN KEY (job_id) REFERENCES job (id),  CONSTRAINT emp_deptid_ref_dept_id_fk FOREIGN KEY (dept_id) REFERENCES dept (id));-- 添加员工INSERT INTO emp(id,ename,job_id,mgr,joindate,salary,bonus,dept_id) VALUES (1001,'孙悟空',4,1004,'2000-12-17','8000.00',NULL,20),(1002,'卢俊义',3,1006,'2001-02-20','16000.00','3000.00',30),(1003,'林冲',3,1006,'2001-02-22','12500.00','5000.00',30),(1004,'唐僧',2,1009,'2001-04-02','29750.00',NULL,20),(1005,'李逵',4,1006,'2001-09-28','12500.00','14000.00',30),(1006,'宋江',2,1009,'2001-05-01','28500.00',NULL,30),(1007,'刘备',2,1009,'2001-09-01','24500.00',NULL,10),(1008,'猪八戒',4,1004,'2007-04-19','30000.00',NULL,20),(1009,'罗贯中',1,NULL,'2001-11-17','50000.00',NULL,10),(1010,'吴用',3,1006,'2001-09-08','15000.00','0.00',30),(1011,'沙僧',4,1004,'2007-05-23','11000.00',NULL,20),(1012,'李逵',4,1006,'2001-12-03','9500.00',NULL,30),(1013,'小白龙',4,1004,'2001-12-03','30000.00',NULL,20),(1014,'关羽',4,1007,'2002-01-23','13000.00',NULL,10);-- 工资等级表CREATE TABLE salarygrade (  grade INT PRIMARY KEY,   -- 级别  losalary INT,  -- 最低工资  hisalary INT -- 最高工资);-- 添加5个工资等级INSERT INTO salarygrade(grade,losalary,hisalary) VALUES (1,7000,12000),(2,12010,14000),(3,14010,20000),(4,20010,30000),(5,30010,99990);-- 需求：-- 1.查询所有员工信息。查询员工编号，员工姓名，工资，职务名称，职务描述-- 2.查询员工编号，员工姓名，工资，职务名称，职务描述，部门名称，部门位置   -- 3.查询员工姓名，工资，工资等级-- 4.查询员工姓名，工资，职务名称，职务描述，部门名称，部门位置，工资等级-- 5.查询出部门编号、部门名称、部门位置、部门人数 -- 6.查询所有员工的姓名及其直接上级的姓名,没有领导的员工也需要查询</content>
	</entry>
	<entry>
		<title>MySQL_数据库设计</title>
		<url>/article/71074955.html</url>
		<content>表间关系MySQL约束与设计关系分类一对一(了解)：如：人和身份证分析：一个人只有一个身份证，一个身份证只能对应一个人实现方式：一对一关系实现，可以在任意一方添加唯一外键指向另一方的主键一对多(多对一)：如：部门和员工分析：一个部门有多个员工，一个员工只能对应一个部门实现方式：在多的一方建立外键，指向一的一方的主键多对多：如：学生和课程分析：一个学生可以选择很多门课程，一个课程也可以被很多学生选择实现方式：多对多关系实现需要借助第三张中间表。中间表至少包含两个字段，这两个字段作为第三张表的外键，分别指向两张表的主键案例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758&gt;-- 创建旅游线路分类表 tab_category&gt;-- cid 旅游线路分类主键，自动增长&gt;-- cname 旅游线路分类名称非空，唯一，字符串 100&gt;CREATE TABLE tab_category (cid INT PRIMARY KEY AUTO_INCREMENT,cname VARCHAR(100) NOT NULL UNIQUE&gt;);&gt;-- 创建旅游线路表 tab_route&gt;/*   rid 旅游线路主键，自动增长   rname 旅游线路名称非空，唯一，字符串 100   price 价格   rdate 上架时间，日期类型   cid 外键，所属分类 */&gt;CREATE TABLE tab_route(rid INT PRIMARY KEY AUTO_INCREMENT,rname VARCHAR(100) NOT NULL UNIQUE,price DOUBLE,rdate DATE,cid INT,FOREIGN KEY (cid) REFERENCES tab_category(cid)&gt;);&gt;/*创建用户表 tab_user   uid 用户主键，自增长   username 用户名长度 100，唯一，非空   password 密码长度 30，非空   name 真实姓名长度 100   birthday 生日   sex 性别，定长字符串 1   telephone 手机号，字符串 11   email 邮箱，字符串长度 100 */&gt;CREATE TABLE tab_user (uid INT PRIMARY KEY AUTO_INCREMENT,username VARCHAR(100) UNIQUE NOT NULL,PASSWORD VARCHAR(30) NOT NULL,NAME VARCHAR(100),birthday DATE,sex CHAR(1) DEFAULT '男',telephone VARCHAR(11),email VARCHAR(100)&gt;);&gt;/*创建收藏表 tab_favorite   rid 旅游线路 id，外键   date 收藏时间   uid 用户 id，外键   rid 和 uid 不能重复，设置复合主键，同一个用户不能收藏同一个线路两次 */&gt;CREATE TABLE tab_favorite (rid INT, -- 线路idDATE DATETIME,uid INT, -- 用户id-- 创建复合主键PRIMARY KEY(rid,uid), -- 联合主键FOREIGN KEY (rid) REFERENCES tab_route(rid),FOREIGN KEY(uid) REFERENCES tab_user(uid)&gt;);设计范式基本概念概念：设计数据库时，需要遵循的一些规范。要遵循后边的范式要求，必须先遵循前边的所有范式要求设计关系数据库时，遵从不同的规范要求，设计出合理的关系型数据库，这些不同的规范要求被称为不同的范式，各种范式呈递次规范，越高的范式数据库冗余越小。目前关系数据库有六种范式：第一范式（1NF）、第二范式（2NF）、第三范式（3NF）、巴斯-科德范式（BCNF）、第四范式(4NF）和第五范式（5NF，又称完美范式）。分类: 第一范式(1NF)：每一列都是不可分割的原子数据项第二范式(2NF)：在1NF的基础上，非码属性必须完全依赖于码（在1NF基础上消除非主属性对主码的部分函数依赖）函数依赖：A—&gt;B,如果通过A属性(属性组)的值，可以确定唯一B属性的值。则称B依赖于A例如：学号—&gt;姓名完全函数依赖：A—&gt;B， 如果A是一个属性组，则B属性值得确定需要依赖于A属性组中所有的属性值。例如：(学号, 课程名称) —&gt; 分数部分函数依赖：A—&gt;B， 如果A是一个属性组，则B属性值得确定只需要依赖于A属性组中某一些值即可。例如：(学号, 课程名称) — &gt; 姓名传递函数依赖：A—&gt;B, B — &gt;C . 如果通过A属性(属性组)的值，可以确定唯一B属性的值，在通过B属性（属性组）的值可以确定唯一C属性的值，则称 C 传递函数依赖于A 例如：学号—&gt;系名，系名—&gt;系主任码：如果在一张表中，一个属性或属性组，被其他所有属性所完全依赖，则称这个属性(属性组)为该表的码 例如：该表中码为：(学号, 课程名称)主属性：码属性组中的所有属性非主属性：除了码属性组的属性第三范式(3NF)：在2NF基础上，任何非主属性不依赖于其它非主属性（在2NF基础上消除传递依赖）示例普通表学号姓名系名系主任课程名称分数10010张无忌经济系张三丰高等数学9510010张无忌经济系张三丰大学英语8710010张无忌经济系张三丰计算机基础6510011令狐冲法律系任我行法理学7710011令狐冲法律系任我行大学英语8710011令狐冲法律系任我行法律社会学6510012杨过法律系任我行法律社会学9510012杨过法律系任我行法理学9710012杨过法律系任我行大学英语991NF学号姓名系名系主任课程名称分数10010张无忌经济系张三丰高等数学9510010张无忌经济系张三丰大学英语8710010张无忌经济系张三丰计算机基础6510011令狐冲法律系任我行法理学7710011令狐冲法律系任我行大学英语8710011令狐冲法律系任我行法律社会学6510012杨过法律系任我行法律社会学9510012杨过法律系任我行法理学9710012杨过法律系任我行大学英语99计算机系殷天正存在的问题：存在非常严重的数据冗余(重复)：姓名、系名、系主任数据添加存在问题：添加新开设的系和系主任时，数据不合法数据删除存在问题：张无忌同学毕业了，删除数据，会将系的数据一起删除。2NF选课表学号课程名称分数10010高等数学9510010大学英语8710010计算机基础6510011法理学7710011大学英语8710011法律社会学6510012法律社会学9510012法理学9710012大学英语99学生表学号姓名系名系主任10010张无忌经济系张三丰10011令狐冲法律系任我行10012杨过法律系任我行计算机系殷天正存在的问题：数据添加存在问题：添加新开设的系和系主任时，数据不合法数据删除存在问题：张无忌同学毕业了，删除数据，会将系的数据一起删除。3NF选课表学号课程名称分数10010高等数学9510010大学英语8710010计算机基础6510011法理学7710011大学英语8710011法律社会学6510012法律社会学9510012法理学9710012大学英语99学生表学号姓名系名10010张无忌经济系10011令狐冲法律系10012杨过法律系系表系名系主任经济系张三丰法律系任我行计算机系殷天正刚刚所有的三个问题全部被解决掉！！！数据库备份和还原命令行：备份： 1mysqldump -u用户名 -p密码 数据库名称 &gt; 保存的路径还原：登录数据库创建数据库使用数据库执行文件 (source 文件路径)图形化工具：</content>
	</entry>
	<entry>
		<title>MySQL_约束</title>
		<url>/article/fcd59143.html</url>
		<content>MySQL约束与设计 约束概念： 对表中的数据进行限定，保证数据的正确性、有效性和完整性。    分类：主键约束：primary key非空约束：not null唯一约束：unique外键约束：foreign key非空约束：not null，值不能为null12345678910-- 创建表时添加约束CREATE TABLE stu(   id INT,   NAME VARCHAR(20) NOT NULL -- name为非空);-- 创建表完后，添加非空约束ALTER TABLE stu MODIFY NAME VARCHAR(20) NOT NULL;-- 删除name的非空约束ALTER TABLE stu MODIFY NAME VARCHAR(20);唯一约束：unique值不能重复123456789101112&gt;-- 创建表时，添加唯一约束&gt;CREATE TABLE stu(id INT,phone_number VARCHAR(20) UNIQUE -- 添加了唯一约束&gt;);&gt;-- 注意mysql中，唯一约束限定的列的值可以有多个null&gt;-- 删除唯一约束&gt;ALTER TABLE stu DROP INDEX phone_number;&gt;-- 在创建表后，添加唯一约束&gt;ALTER TABLE stu MODIFY phone_number VARCHAR(20) UNIQUE;主键约束：primary key注意：含义：非空且唯一一张表只能有一个字段为主键主键就是表中记录的唯一标识1234567891011121314151617181920212223242526-- 在创建表时，添加主键约束create table stu(id int primary key,-- 给id添加主键约束name varchar(20));-- 删除主键-- 错误 alter table stu modify id int ;ALTER TABLE stu DROP PRIMARY KEY;-- 创建完表后，添加主键ALTER TABLE stu MODIFY id INT PRIMARY KEY;-- 自动增长：-- 概念：如果某一列是数值类型的，使用 auto_increment 可以来完成值得自动增长-- 在创建表时，添加主键约束，并且完成主键自增长create table stu(id int primary key auto_increment,-- 给id添加主键约束name varchar(20));-- 删除自动增长ALTER TABLE stu MODIFY id INT;-- 添加自动增长ALTER TABLE stu MODIFY id INT AUTO_INCREMENT;外键约束：foreign key让表于表产生关系，从而保证数据的正确性。123456789101112-- 在创建表时，可以添加外键create table 表名(....外键列constraint 外键名称 foreign key (外键列名称) references 主表名称(主表列名称));-- 删除外键ALTER TABLE 表名 DROP FOREIGN KEY 外键名称;-- 创建表之后，添加外键ALTER TABLE 表名 ADD CONSTRAINT 外键名称 FOREIGN KEY (外键字段名称) REFERENCES 主表名称(主表列名称);级联操作123456-- 添加级联操作ALTER TABLE 表名 ADD CONSTRAINT 外键名称 FOREIGN KEY (外键字段名称) REFERENCES 主表名称(主表列名称) ON UPDATE CASCADE ON DELETE CASCADE;-- 分类：1. 级联更新：ON UPDATE CASCADE 2. 级联删除：ON DELETE CASCADE </content>
	</entry>
	<entry>
		<title>Spring注解</title>
		<url>/article/51d35c0d.html</url>
		<content></content>
	</entry>
	<entry>
		<title>多线程</title>
		<url>/article/5729df21.html</url>
		<content></content>
	</entry>
	<entry>
		<title>多线程_JUC原子类</title>
		<url>/article/304974ef.html</url>
		<content>JUC原子类根据修改的数据类型，可以将JUC包中的原子操作类可以分为4类。基本类型: AtomicInteger, AtomicLong, AtomicBoolean ;数组类型: AtomicIntegerArray, AtomicLongArray, AtomicReferenceArray ;引用类型: AtomicReference, AtomicStampedRerence, AtomicMarkableReference ;对象的属性修改类型: AtomicIntegerFieldUpdater, AtomicLongFieldUpdater, AtomicReferenceFieldUpdater 。这些类存在的目的是对相应的数据进行原子操作。所谓原子操作，是指操作过程不会被中断，保证数据操作是以原子方式进行的。AtomicLong介绍和函数列表AtomicLong是作用是对长整形进行原子操作。在32位操作系统中，64位的long 和 double 变量由于会被JVM当作两个分离的32位来进行操作，所以不具有原子性。而使用AtomicLong能让long的操作保持原子型。12345678910111213141516171819202122232425262728293031323334353637// AtomicLong函数列表// 构造函数AtomicLong()// 创建值为initialValue的AtomicLong对象AtomicLong(long initialValue)// 以原子方式设置当前值为newValue。final void set(long newValue) // 获取当前值final long get() // 以原子方式将当前值减 1，并返回减1后的值。等价于“--num”final long decrementAndGet() // 以原子方式将当前值减 1，并返回减1前的值。等价于“num--”final long getAndDecrement() // 以原子方式将当前值加 1，并返回加1后的值。等价于“++num”final long incrementAndGet() // 以原子方式将当前值加 1，并返回加1前的值。等价于“num++”final long getAndIncrement()    // 以原子方式将delta与当前值相加，并返回相加后的值。final long addAndGet(long delta) // 以原子方式将delta添加到当前值，并返回相加前的值。final long getAndAdd(long delta) // 如果当前值 == expect，则以原子方式将该值设置为update。成功返回true，否则返回false，并且不修改原值。final boolean compareAndSet(long expect, long update)// 以原子方式设置当前值为newValue，并返回旧值。final long getAndSet(long newValue)// 返回当前值对应的int值int intValue() // 获取当前值对应的long值long longValue()    // 以 float 形式返回当前值float floatValue()    // 以 double 形式返回当前值double doubleValue()    // 最后设置为给定值。延时设置变量值，这个等价于set()方法，但是由于字段是volatile类型的，因此次字段的修改会比普通字段（非volatile字段）有稍微的性能延时（尽管可以忽略），所以如果不是想立即读取设置的新值，允许在“后台”修改值，那么此方法就很有用。如果还是难以理解，这里就类似于启动一个后台线程如执行修改新值的任务，原线程就不等待修改结果立即返回（这种解释其实是不正确的，但是可以这么理解）。final void lazySet(long newValue)// 如果当前值 == 预期值，则以原子方式将该设置为给定的更新值。JSR规范中说：以原子方式读取和有条件地写入变量但不 创建任何 happen-before 排序，因此不提供与除 weakCompareAndSet 目标外任何变量以前或后续读取或写入操作有关的任何保证。大意就是说调用weakCompareAndSet时并不能保证不存在happen-before的发生（也就是可能存在指令重排序导致此操作失败）。但是从Java源码来看，其实此方法并没有实现JSR规范的要求，最后效果和compareAndSet是等效的，都调用了unsafe.compareAndSwapInt()完成操作。final boolean weakCompareAndSet(long expect, long update)123456789101112131415// 以incrementAndGet()为例，对AtomicLong的原理进行说明。public final long incrementAndGet() {    for (;;) {        // 获取AtomicLong当前对应的long值        long current = get();        // 将current加1        long next = current + 1;        // 通过CAS函数，更新current的值        if (compareAndSet(current, next))            return next;    }}说明：(01) incrementAndGet()首先会根据get()获取AtomicLong对应的long值。该值是volatile类型的变量，get()的源码如下：(02) incrementAndGet()接着将current加1,然后通过CAS函数，将新的值赋值给value。123456789101112131415161718192021222324252627282930313233343536373839// AtomicLong示例// LongTest.java的源码import java.util.concurrent.atomic.AtomicLong;public class LongTest {    public static void main(String[] args){        // 新建AtomicLong对象        AtomicLong mAtoLong = new AtomicLong();        mAtoLong.set(0x0123456789ABCDEFL);        System.out.printf(&quot; s : 0x6X\n&quot;, &quot;get()&quot;, mAtoLong.get());        System.out.printf(&quot; s : 0x6X\n&quot;, &quot;intValue()&quot;, mAtoLong.intValue());        System.out.printf(&quot; s : 0x6X\n&quot;, &quot;longValue()&quot;, mAtoLong.longValue());        System.out.printf(&quot; s : %s\n&quot;, &quot;doubleValue()&quot;, mAtoLong.doubleValue());        System.out.printf(&quot; s : %s\n&quot;, &quot;floatValue()&quot;, mAtoLong.floatValue());        System.out.printf(&quot; s : 0x6X\n&quot;, &quot;getAndDecrement()&quot;, mAtoLong.getAndDecrement());        System.out.printf(&quot; s : 0x6X\n&quot;, &quot;decrementAndGet()&quot;, mAtoLong.decrementAndGet());        System.out.printf(&quot; s : 0x6X\n&quot;, &quot;getAndIncrement()&quot;, mAtoLong.getAndIncrement());        System.out.printf(&quot; s : 0x6X\n&quot;, &quot;incrementAndGet()&quot;, mAtoLong.incrementAndGet());        System.out.printf(&quot; s : 0x6X\n&quot;, &quot;addAndGet(0x10)&quot;, mAtoLong.addAndGet(0x10));        System.out.printf(&quot; s : 0x6X\n&quot;, &quot;getAndAdd(0x10)&quot;, mAtoLong.getAndAdd(0x10));        System.out.printf(&quot;\n s : 0x6X\n&quot;, &quot;get()&quot;, mAtoLong.get());        System.out.printf(&quot; s : %s\n&quot;, &quot;compareAndSet()&quot;, mAtoLong.compareAndSet(0x12345679L, 0xFEDCBA9876543210L));        System.out.printf(&quot; s : 0x6X\n&quot;, &quot;get()&quot;, mAtoLong.get());    }}   get() : 0x0123456789ABCDEF          intValue() : 0x0000000089ABCDEF         longValue() : 0x0123456789ABCDEF       doubleValue() : 8.1985529216486896E16        floatValue() : 8.1985531E16   getAndDecrement() : 0x0123456789ABCDEF   decrementAndGet() : 0x0123456789ABCDED   getAndIncrement() : 0x0123456789ABCDED   incrementAndGet() : 0x0123456789ABCDEF     addAndGet(0x10) : 0x0123456789ABCDFF     getAndAdd(0x10) : 0x0123456789ABCDFF               get() : 0x0123456789ABCE0F     compareAndSet() : false               get() : 0x0123456789ABCE0FAtomicLongArray原子类AtomicLongArray的作用则是对”长整形数组”进行原子操作。123456789101112131415161718192021222324252627282930313233// AtomicLongArray函数列表// 创建给定长度的新 AtomicLongArray。AtomicLongArray(int length)// 创建与给定数组具有相同长度的新 AtomicLongArray，并从给定数组复制其所有元素。AtomicLongArray(long[] array)// 以原子方式将给定值添加到索引 i 的元素。long addAndGet(int i, long delta)// 如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。boolean compareAndSet(int i, long expect, long update)// 以原子方式将索引 i 的元素减1。long decrementAndGet(int i)// 获取位置 i 的当前值。long get(int i)// 以原子方式将给定值与索引 i 的元素相加。long getAndAdd(int i, long delta)// 以原子方式将索引 i 的元素减 1。long getAndDecrement(int i)// 以原子方式将索引 i 的元素加 1。long getAndIncrement(int i)// 以原子方式将位置 i 的元素设置为给定值，并返回旧值。long getAndSet(int i, long newValue)// 以原子方式将索引 i 的元素加1。long incrementAndGet(int i)// 最终将位置 i 的元素设置为给定值。void lazySet(int i, long newValue)// 返回该数组的长度。int length()// 将位置 i 的元素设置为给定值。void set(int i, long newValue)// 返回数组当前值的字符串表示形式。String toString()// 如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。boolean    weakCompareAndSet(int i, long expect, long update)AtomicLongArray示例12345678910111213141516171819202122232425262728293031323334// LongArrayTest.java的源码import java.util.concurrent.atomic.AtomicLongArray;public class LongArrayTest {    public static void main(String[] args){        // 新建AtomicLongArray对象        long[] arrLong = new long[] {10, 20, 30, 40, 50};        AtomicLongArray ala = new AtomicLongArray(arrLong);        ala.set(0, 100);        for (int i=0, len=ala.length(); i&lt;len; i++)             System.out.printf(&quot;get(%d) : %s\n&quot;, i, ala.get(i));        System.out.printf(&quot; s : %s\n&quot;, &quot;getAndDecrement(0)&quot;, ala.getAndDecrement(0));        System.out.printf(&quot; s : %s\n&quot;, &quot;decrementAndGet(1)&quot;, ala.decrementAndGet(1));        System.out.printf(&quot; s : %s\n&quot;, &quot;getAndIncrement(2)&quot;, ala.getAndIncrement(2));        System.out.printf(&quot; s : %s\n&quot;, &quot;incrementAndGet(3)&quot;, ala.incrementAndGet(3));        System.out.printf(&quot; s : %s\n&quot;, &quot;addAndGet(100)&quot;, ala.addAndGet(0, 100));        System.out.printf(&quot; s : %s\n&quot;, &quot;getAndAdd(100)&quot;, ala.getAndAdd(1, 100));        System.out.printf(&quot; s : %s\n&quot;, &quot;compareAndSet()&quot;, ala.compareAndSet(2, 31, 1000));        System.out.printf(&quot; s : %s\n&quot;, &quot;get(2)&quot;, ala.get(2));    }}              get(0) : 100              get(1) : 20              get(2) : 30              get(3) : 40              get(4) : 50  getAndDecrement(0) : 100  decrementAndGet(1) : 19  getAndIncrement(2) : 30  incrementAndGet(3) : 41      addAndGet(100) : 199      getAndAdd(100) : 19     compareAndSet() : true              get(2) : 1000AtomicReference介绍和函数列表AtomicReference是作用是对”对象”进行原子操作。12345678910111213141516171819// AtomicReference函数列表// 使用 null 初始值创建新的 AtomicReference。AtomicReference()// 使用给定的初始值创建新的 AtomicReference。AtomicReference(V initialValue)// 如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。boolean compareAndSet(V expect, V update)// 获取当前值。V get()// 以原子方式设置为给定值，并返回旧值。V getAndSet(V newValue)// 最终设置为给定值。void lazySet(V newValue)// 设置为给定值。void set(V newValue)// 返回当前值的字符串表示形式。String toString()// 如果当前值 == 预期值，则以原子方式将该值设置为给定的更新值。boolean weakCompareAndSet(V expect, V update)123AtomicReference的源码比较简单。它是通过&quot;volatile&quot;和&quot;Unsafe提供的CAS函数实现&quot;原子操作。(01) value是volatile类型。这保证了：当某线程修改value的值时，其他线程看到的value值都是最新的value值，即修改之后的volatile的值。(02) 通过CAS设置value。这保证了：当某线程池通过CAS函数(如compareAndSet函数)设置value时，它的操作是原子的，即线程在操作value时不会被中断。AtomicReference示例123456789101112131415161718192021222324252627282930313233// AtomicReferenceTest.java的源码import java.util.concurrent.atomic.AtomicReference;public class AtomicReferenceTest {    public static void main(String[] args){        // 创建两个Person对象，它们的id分别是101和102。        Person p1 = new Person(101);        Person p2 = new Person(102);        // 新建AtomicReference对象，初始化它的值为p1对象        AtomicReference ar = new AtomicReference(p1);        // 通过CAS设置ar。如果ar的值为p1的话，则将其设置为p2。        ar.compareAndSet(p1, p2);        Person p3 = (Person)ar.get();        System.out.println(&quot;p3 is &quot;+p3);        System.out.println(&quot;p3.equals(p1)=&quot;+p3.equals(p1));    }}class Person {    volatile long id;    public Person(long id) {        this.id = id;    }    public String toString() {        return &quot;id:&quot;+id;    }}// 结果说明：新建AtomicReference对象ar时，将它初始化为p1。紧接着，通过CAS函数对它进行设置。如果ar的值为p1的话，则将其设置为p2。最后，获取ar对应的对象，并打印结果。p3.equals(p1)的结果为false，这是因为Person并没有覆盖equals()方法，而是采用继承自Object.java的equals()方法；而Object.java中的equals()实际上是调用&quot;==&quot;去比较两个对象，即比较两个对象的地址是否相等。p3 is id:102p3.equals(p1)=falseAtomicLongFieldUpdater介绍和函数列表AtomicLongFieldUpdater可以对指定”类的 ‘volatile long’类型的成员”进行原子更新。它是基于反射原理实现的。1234567891011121314151617181920212223242526272829// AtomicLongFieldUpdater函数列表// 受保护的无操作构造方法，供子类使用。protected AtomicLongFieldUpdater()// 以原子方式将给定值添加到此更新器管理的给定对象的字段的当前值。long addAndGet(T obj, long delta)// 如果当前值 == 预期值，则以原子方式将此更新器所管理的给定对象的字段设置为给定的更新值。abstract boolean compareAndSet(T obj, long expect, long update)// 以原子方式将此更新器管理的给定对象字段当前值减 1。long decrementAndGet(T obj)// 获取此更新器管理的在给定对象的字段中保持的当前值。abstract long get(T obj)// 以原子方式将给定值添加到此更新器管理的给定对象的字段的当前值。long getAndAdd(T obj, long delta)// 以原子方式将此更新器管理的给定对象字段当前值减 1。long getAndDecrement(T obj)// 以原子方式将此更新器管理的给定对象字段的当前值加 1。long getAndIncrement(T obj)// 将此更新器管理的给定对象的字段以原子方式设置为给定值，并返回旧值。long getAndSet(T obj, long newValue)// 以原子方式将此更新器管理的给定对象字段当前值加 1。long incrementAndGet(T obj)// 最后将此更新器管理的给定对象的字段设置为给定更新值。abstract void lazySet(T obj, long newValue)// 为对象创建并返回一个具有给定字段的更新器。static &lt;U&gt; AtomicLongFieldUpdater&lt;U&gt; newUpdater(Class&lt;U&gt; tclass, String fieldName)// 将此更新器管理的给定对象的字段设置为给定更新值。abstract void set(T obj, long newValue)// 如果当前值 == 预期值，则以原子方式将此更新器所管理的给定对象的字段设置为给定的更新值。abstract boolean weakCompareAndSet(T obj, long expect, long update)1234567891011121314151617181920212223242526272829// AtomicLongFieldUpdater示例// LongTest.java的源码import java.util.concurrent.atomic.AtomicLongFieldUpdater;public class LongFieldTest {    public static void main(String[] args) {        // 获取Person的class对象        Class cls = Person.class;         // 新建AtomicLongFieldUpdater对象，传递参数是“class对象”和“long类型在类中对应的名称”        AtomicLongFieldUpdater mAtoLong = AtomicLongFieldUpdater.newUpdater(cls, &quot;id&quot;);        Person person = new Person(12345678L);        // 比较person的&quot;id&quot;属性，如果id的值为12345678L，则设置为1000。        mAtoLong.compareAndSet(person, 12345678L, 1000);        System.out.println(&quot;id=&quot;+person.getId());    }}class Person {    volatile long id;    public Person(long id) {        this.id = id;    }    public void setId(long id) {        this.id = id;    }    public long getId() {        return id;    }}id=1000</content>
	</entry>
	<entry>
		<title>多线程_锁</title>
		<url>/article/b4993685.html</url>
		<content>锁概念锁的分类如何理解互斥锁、条件锁、读写锁以及自旋锁？5000字 | 24张图带你彻底理解21种并发锁互斥锁和自旋锁互斥锁和自旋锁是最底层的两种锁，其他的很多锁都是基于他们的实现。当线程A获取到锁后，线程B再去获取锁，有两种处理方式:线程B循环的去尝试获取锁，直到获取成功为止即自旋锁线程B放弃获取锁，在锁空闲时，等待被唤醒，即互斥锁互斥锁互斥锁会释放当前线程的cpu，导致加锁的代码阻塞，直到线程再次被唤醒。互斥锁加锁失败时，会从用户态陷入到内核态，让内核帮我们切换线程，存在一定的性能开销1while (抢锁(lock) == 没抢到) {睡眠直到锁状态改变再唤醒(lock)}当线程加锁失败，内核会把线程的状态由“运行”设置为“睡眠”，让出cpu当锁空闲时，内核唤醒线程，状态设置为“就绪”，获取cpu执行自旋锁自旋锁会自用户态由应用程序完成，不涉及用户态到内核态的转化，没有线程上下文切换，性能相对较好，自旋锁加锁过程:1while (抢锁(lock) == 没抢到) {}查看锁的状态锁空闲，获取锁，否则执行(1)自旋锁会利用cpu一直工作直到获取到锁，中间不会释放cpu，但如果被锁住的代码执行时间较长，导致cpu空转，浪费资源读写锁读写锁由读锁和写锁组成：读锁又称为共享锁，S锁写锁又称为排它锁、X锁，在mysql的事务中大量使用写锁是独占锁，一旦线程获取写锁，其他线程不能获取写锁和读锁。读锁是共享锁，当线程获取读锁，其他线程可以获取读锁不能获取写锁。因为并发数据读取并不会改变共享数据导致数据不一致。读写锁把对共享资源的读操作和写操作分别加锁控制，能够提高读线程的并发性，适用于读多写少的场景。读优先锁、写优先锁、公平读写锁读优先锁读优先锁希望的是读锁能够被更多的线程获取，可以提高读线程的并发性。线程A获取了读锁，线程B想获取写锁，此时会被阻塞，线程c可以继续获取读锁，直到A和c释放锁，线程B才可以获取写锁。如果有很多线程获取读锁，且加锁的代码执行时间很长，就到导致线程B永远获取不到写锁。写优先锁写优先锁希望的是写锁能够被优先获取。线程A获取了读锁，线程B想获取写锁，此时会被阻塞，后面获取读锁都会失败，线程A释放锁，线程B可以获取写锁，其他获取读锁的线程阻塞。如果有很多写线程获取写锁，且加锁的代码执行时间很长，就到导致读线程永远获取不到读锁。公平读写锁上面两种锁都会造成【饥饿】现象，为解决这种问题，可以增加一个队列，把获取锁的线程（不管是写线程还是读线程）按照先进先出的方式排队，每次从队列中取出一个线程获取锁，这种获取锁的方式是公平的。乐观锁和悲观锁乐观锁乐观锁是先修改共享资源，再用历史数据和当前数据比对验证这段时间共享数据有没有被修改，如果没有被修改，那么更新数据，如果有其他线程更新了共享资源，需要重新获取数据，再更新，验证，循环往复的重试，直到更新成功。所以当数据更新操作比较频繁，数据冲突的概率就会比较大，重试的次数就会多，浪费CPU资源。乐观锁其实全程没有加锁，也叫无锁编程，所以针对读多写少的场景，并发性能较高，典型的实现MVCC，mysql中会使用MVCC构建一致性读来保证可重复读。悲观锁悲观锁是在访问共享资源之前统统加锁。当并发冲突概率较高时，乐观锁不再适用，悲观锁就派上用场。互斥锁、自旋锁都是悲观锁的实现。惊群问题[框架]高并发中的惊群效应当你往一群鸽子中间扔一块食物，虽然最终只有一个鸽子抢到食物，但所有鸽子都会被惊动来争夺，没有抢到食物的鸽子只好回去继续睡觉， 等待下一块食物到来。这样，每扔一块食物，都会惊动所有的鸽子，即为惊群。 简单地说：就是扔一块食物，所有鸽子来抢，但最终只一个鸽子抢到了食物。语义分析：食物只有一块，最终只有一个鸽子抢到，但是惊动了所有鸽子，每个鸽子都跑过来，消耗了每个鸽子的能量。（这个很符合达尔文的进化论，物种之间的竞争，适者生存。）操作系统的惊群在多进程/多线程等待同一资源时，也会出现惊群。即当某一资源可用时，多个进程/线程会惊醒，竞争资源。这就是操作系统中的惊群。惊醒所有进程/线程，导致n-1个进程/线程做了无效的调度，上下文切换，cpu瞬时增高多个进程/线程争抢资源，所以涉及到同步问题，需对资源进行加锁保护，加解锁加大系统CPU开销惊群的几种情况在高并发（多线程/多进程/多连接）中，会产生惊群的情况有：accept惊群epoll惊群nginx惊群线程池惊群线程池惊群在多线程设计中，经常会用到互斥和条件变量的问题。当一个线程解锁并通知其他线程的时候，就会出现惊群的现象。pthread_mutex_lock/pthread_mutex_unlock：线程互斥锁的加锁及解锁函数。pthread_cond_wait：线程池中的消费者线程等待线程条件变量被通知；pthread_cond_signal/pthread_cond_broadcast：生产者线程通知线程池中的某个或一些消费者线程池，接收处理任务；这里的惊群现象出现在3里，pthread_cond_signal，语义上看，是通知一个线程。调用此函数后，系统会唤醒在相同条件变量上等待的一个或多个线程（可参看手册）。如果通知了多个线程，则发生了惊群。正常的用法：所有线程共用一个锁，共用一个条件变量当pthread_cond_signal通知时，就可能会出现惊群解决惊群的方法：所有线程共用一个锁，每个线程有自已的条件变量pthread_cond_signal通知时，定向通知某个线程的条件变量，不会出现惊群</content>
	</entry>
	<entry>
		<title>枚举类Enum</title>
		<url>/article/4071.html</url>
		<content>枚举类Enum枚举类有哪些好处枚举类可以方便地表示星期，月份等常数类型，如果你不用枚举类，那么你只能用数字或者字符串。如果你使用数字，用1-7来表示星期数，但一个数字在程序中不仅可以表示星期数，可能还有其他许多含义，这样你在写程序时就必须时刻记住这些数字的含义，这降低了程序的可读性，也导致容易出错。而当你使用字符串时，虽然没有明显的缺点，但在内存中字符串所占内存要比数字多，这就降低了程序的效率。枚举类正好弥补了这两方面的缺点，你可以在代码中使用枚举类，但在内存中存放时使用的是数字，既提高了可读性，又提高了程序效率。更重要的是，Python中的枚举类型是不可变类型，又可以进行迭代，这就意味着你可以随时使用枚举类型而不用担心改变了枚举类型的值。两种枚举类定义方式程序有两种方式来定义枚举类：直接使用 Enum 列出多个枚举值来创建枚举类。通过继承 Enum 基类来派生枚举类。案例简单枚举类定义12345678# 由于枚举类的“new”方法，将会保证内存中只会存在一个枚举类的实例# -*- coding: utf-8 -*from enum import Enum# 这样我们就获得了Month类型的枚举类# value属性则是自动赋给成员的int常量，默认从1开始计数。seasonEnum = Enum('SeasonEnum', ('SPRING', 'SUMMER', 'FALL', 'WINTER'))print(f'seasonEnum: {seasonEnum}')高级枚举类定义代码12345678910111213141516171819202122from enum import Enum, unique# @unique装饰器可以帮助我们检查保证没有重复值。# 冬天 = 3这里就会报错，否则SeasonEnum.__members__.items，会把# WINTER =&gt; SeasonEnum.WINTER# 冬天 =&gt; SeasonEnum.WINTER# 装饰器“@unique”，它会遍历枚举成员，如果发现有重复就会立即抛出“ValueError”@uniqueclass SeasonEnum(Enum):    # SPRING = auto() # 也可以全部使用自动值    SPRING = 0  # Sun的value被设定为0    SUMMER = 1    FALL = 2    WINTER = 3    # 冬天 = 3# 访问这些枚举类型可以有若干种方法：season_1 = SeasonEnum.SPRINGprint(f'season_1: {season_1}')print(f&quot;SeasonEnum['SPRING']: {SeasonEnum['SPRING']}&quot;)print(f'SeasonEnum.SPRING.value: {SeasonEnum.SPRING.value}')print(f'SeasonEnum(1): {SeasonEnum(1)}')print(f'season_1 == SeasonEnum(1): {season_1 == SeasonEnum(1)}')    输出123456seasonEnum: &lt;enum 'SeasonEnum'&gt;season_1: SeasonEnum.SPRINGSeasonEnum['SPRING']: SeasonEnum.SPRINGSeasonEnum.SPRING.value: 0SeasonEnum(1): SeasonEnum.SUMMERseason_1 == SeasonEnum(1): False枚举类比较代码1234567# 枚举类型不能做大小比较，但是可以做身份比较和等值比较。print(f'SeasonEnum.SPRING is SeasonEnum.SPRING: {SeasonEnum.SPRING is SeasonEnum.SPRING}')print(f'SeasonEnum.SPRING is SeasonEnum.WINTER: {SeasonEnum.SPRING is SeasonEnum.WINTER}')# 这是身份比较，每个对象都可以进行身份比较，枚举类也不例外。print(f'SeasonEnum.SPRING == SeasonEnum.SPRING: {SeasonEnum.SPRING == SeasonEnum.SPRING}')print(f'SeasonEnum.SPRING != SeasonEnum.WINTER: {SeasonEnum.SPRING != SeasonEnum.WINTER}')输出1234SeasonEnum.SPRING is SeasonEnum.SPRING: TrueSeasonEnum.SPRING is SeasonEnum.WINTER: FalseSeasonEnum.SPRING == SeasonEnum.SPRING: TrueSeasonEnum.SPRING != SeasonEnum.WINTER: True遍历枚举类代码1234567# 如果尝试遍历枚举类型，则后面重复的不会被打印出来。但是，如果想要获取别名，# 我们可以使用属性“members”，它是一个OrderedDict，包括所有定义的枚举名称，包括别名。for name, member in SeasonEnum.__members__.items():    print(name, '=&gt;', member)for name, member in enumerate(SeasonEnum):    print(name, '=&gt;', member) 输出12345678910SPRING =&gt; SeasonEnum.SPRINGSUMMER =&gt; SeasonEnum.SUMMERFALL =&gt; SeasonEnum.FALLWINTER =&gt; SeasonEnum.WINTER0 =&gt; SeasonEnum.SPRING1 =&gt; SeasonEnum.SUMMER2 =&gt; SeasonEnum.FALL3 =&gt; SeasonEnum.WINTER    </content>
	</entry>
	<entry>
		<title>pandas小记</title>
		<url>/article/62278.html</url>
		<content>pandas学习</content>
	</entry>
	<entry>
		<title>numpy小记</title>
		<url>/article/30791.html</url>
		<content>numpy小记70个NumPy分级练习题：用Python一举搞定机器学习矩阵运算1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556import numpy as np# 1. 替换满足条件的元素而不影响原始数组arr = np.arange(10)out = np.where(arr%2==1, -1, arr)arrOut[4]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])outOut[5]: array([ 0, -1,  2, -1,  4, -1,  6, -1,  8, -1])# 7. 获取两个数组元素匹配的索引号a = np.array([1,2,3,2,3,4,3,4,5,6])b = np.array([7,2,10,2,7,4,9,4,9,8])np.where(a==b)Out[33]: (array([1, 3, 5, 7], dtype=int64),) # 9. 将处理标量的python函数在numpy数组上运行maxx = lambda x,y: x if x&gt;=y else ypair_max = np.vectorize(maxx, otypes=[float])a = np.array([5,7,9,8,6,4,5]) b = np.array([6,3,4,8,9,7,1])pair_max(a,b)Out[45]: array([6., 7., 9., 8., 9., 7., 5.]) # 18. 导入含有数字和文本的数据集，并保持的文本完整性 url = &quot;https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data&quot; irirs_ld = np.genfromtxt(url, delimiter=',', dtype=None)# 21. 计算softmax值def softmax(x): ex = np.exp(x-np.max(x)) return ex/ex.sum(axis=0)     # 29. 概率抽样# 暂定 # 33. 两个数组之间的欧氏距离 arr = np.arange(5) brr = np.arange(4,9)dist = np.linalg.norm(arr-brr)distOut[176]: 8.94427190999916# 38. 不连续的日期数组。通过填补缺失的日期，使其成为连续的日期序列 # 39. 一维数组arr，使用步长生成一个二维数组，窗口长度为4，步长为2def gen_strides(arr, stride_len=5, windows_len=5):    n_strides = ((arr.size-windows_len)//stride_len)+1    return np.array([arr[s:(s+windows_len)] for s in np.arange(0, n_strides*stride_len,stride_len)]) gen_strides(np.arange(15), stride_len=2, windows_len=4)Out[207]: array([[ 0,  1,  2,  3],    [ 2,  3,  4,  5],     [ 4,  5,  6,  7],    [ 6,  7,  8,  9],    [ 8,  9, 10, 11],    [10, 11, 12, 13]])移动平均值cumsum ：计算轴向元素累加和，返回由中间结果组成的数组cumprod ：计算轴向元素累乘积，返回由中间结果组成的数组12345678910data = np.array([1] * 10)acum = data.cumsum()cump = acum.cumprod()print(acum)[ 1  2  3  4  5  6  7  8  9 10]print(cump)array([      1,       2,       6,      24,     120,     720,    5040,         40320,  362880, 3628800])1234567891011# 37. 窗口大小为3的移动平均值def moving_average(a, n=3):    ret = np.cumsum(a, dtype=float)    ret[n:] = ret[n:]-ret[:-n]    return ret[n-1:]/nnp.random.seed(100)Z = np.random.randint(10, size=10)ZOut[191]: array([8, 8, 3, 7, 7, 0, 4, 2, 5, 2])moving_average(Z, n=3).round(2)Out[192]: array([6.33, 6.  , 5.67, 4.67, 3.67, 2.  , 3.67, 3.  ])第n个重复项的索引12345678# 36. 第n个重复项的索引# 找出x中第1个重复n次的索引arr = np.array([1,2,1,1,3,4,3,1,1,2,1,1,2])n = 5[ii for ii,kk in enumerate(arr) if kk==1][n-1]Out[186]: 8np.where(arr==1)[0][n-1]Out[187]: 8按列排序二维数组123456789101112# 31. 按列排序二维数组arr = np.random.uniform(size=(3,4))arrOut[146]: array([[0.54040458, 0.29679375, 0.1107879 , 0.3126403 ], [0.45697913, 0.65894007, 0.25425752, 0.64110126], [0.20012361, 0.65762481, 0.77828922, 0.7795984 ]])arr[arr[:,1].argsort()]Out[147]: array([[0.54040458, 0.29679375, 0.1107879 , 0.3126403 ], [0.20012361, 0.65762481, 0.77828922, 0.7795984 ], [0.45697913, 0.65894007, 0.25425752, 0.64110126]])pearsonr相关系数1234567891011# 25. 计算某两列的pearsonr相关系数arr = np.random.uniform(size=(3,4))arrOut[114]: array([[0.59884338, 0.60380454, 0.10514769, 0.38194344], [0.03647606, 0.89041156, 0.98092086, 0.05994199], [0.89054594, 0.5769015 , 0.74247969, 0.63018394]])from scipy.stats.stats import pearsonrcorr,p_value = pearsonr(arr[:,0], arr[:,2])corrOut[117]: -0.432641494562719唯一值的数量123456789101112131415161718192021# 27. 查找numpy数组中的唯一值的数量arr = np.random.uniform(size=(3,4))np.unique(arr, return_counts=True)Out[121]: (array([0.04486228, 0.14260031, 0.17808099, 0.23769421, 0.34019022,  0.35479561, 0.37625245, 0.50543143, 0.5928054 , 0.62994188,  0.9338413 , 0.94637988]),array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64))# 28. 将数值转换为分类（文本）数组# 将iris_2d的花瓣长度（第3列）组成一个文本数组# &lt;3  -&gt; '小'# 3-5 -&gt; '中'# &gt;=5 -&gt; '大from sklearn import datasetsiris = datasets.load_iris().datapetal_length_bin = np.digitize(iris[:,2].astype('float'), [0,3,5,10])label_map = {1:'small', 2:'medium', 3:'large', 4:np.nan}petal_length_cat = [label_map[x] for x in petal_length_bin]petal_length_cat[:4]Out[130]: ['small', 'small', 'small', 'small']多个条件过滤numpy数组1234567891011# 24. 根据两个或多个条件过滤一个numpy数组# 过滤具有arr（第3列）&gt; 0.5和arr（第1列）&lt;0.8的arr的行。arr = np.random.uniform(size=(3,4))arrOut[110]: array([[0.81622475, 0.27407375, 0.43170418, 0.94002982], [0.81764938, 0.33611195, 0.17541045, 0.37283205], [0.00568851, 0.25242635, 0.79566251, 0.01525497]])condition = (arr[:,2]&gt;0.5) &amp; (arr[:,0]&lt;0.8)arr[condition]Out[112]: array([[0.00568851, 0.25242635, 0.79566251, 0.01525497]])堆叠数组操作12345678910111213141516171819202122232425262728293031323334353637383940414243# 2. 垂直堆叠两个数组a = np.arange(10).reshape(2,-1)b = np.repeat(1,10).reshape(2,-1)aOut[10]: array([[0, 1, 2, 3, 4],[5, 6, 7, 8, 9]])bOut[11]: array([[1, 1, 1, 1, 1],[1, 1, 1, 1, 1]])np.concatenate([a,b], axis=0)Out[12]: array([[0, 1, 2, 3, 4],[5, 6, 7, 8, 9],[1, 1, 1, 1, 1],[1, 1, 1, 1, 1]])np.vstack([a,b])Out[13]: array([[0, 1, 2, 3, 4],[5, 6, 7, 8, 9],[1, 1, 1, 1, 1],[1, 1, 1, 1, 1]])np.r_[a,b]Out[14]: array([[0, 1, 2, 3, 4],[5, 6, 7, 8, 9],[1, 1, 1, 1, 1],[1, 1, 1, 1, 1]])# 3. 水平堆叠两个数组np.concatenate([a,b], axis=1)Out[15]: array([[0, 1, 2, 3, 4, 1, 1, 1, 1, 1],[5, 6, 7, 8, 9, 1, 1, 1, 1, 1]])np.hstack([a,b])Out[16]: array([[0, 1, 2, 3, 4, 1, 1, 1, 1, 1],[5, 6, 7, 8, 9, 1, 1, 1, 1, 1]])np.c_[a,b]Out[17]: array([[0, 1, 2, 3, 4, 1, 1, 1, 1, 1],[5, 6, 7, 8, 9, 1, 1, 1, 1, 1]])提取范围内的所有数字12345678# 8. 提取给定范围内的所有数字a = np.arange(15)index = np.where((a&gt;=5) &amp; (a&lt;=10))a[index]Out[36]: array([ 5,  6,  7,  8,  9, 10])index = np.where(np.logical_and(a&gt;=5, a&lt;=10))a[index]Out[38]: array([ 5,  6,  7,  8,  9, 10])交换数组行列12345678910111213# 10. 交换2维numpy数组中的两个列np.arange(9).reshape(3,3)[:,[1,0,2]]Out[46]: array([[1, 0, 2],[4, 3, 5],[7, 6, 8]])# 11. 交换2维numpy数组中的两个行np.arange(9).reshape(3,3)[[1,0,2], :]Out[47]: array([[3, 4, 5],[0, 1, 2],[6, 7, 8]])反转2维数组行列12345678910111213# 12. 反转2维数组的行np.arange(9).reshape(3,3)[[1,0,2], :]Out[51]: array([[3, 4, 5],[0, 1, 2],[6, 7, 8]])# 13. 反转2维数组的列np.arange(9).reshape(3,3)[:,::-1]Out[48]: array([[2, 1, 0],[5, 4, 3],[8, 7, 6]])数组打印12345678910111213141516171819202122232425# 15. 打印三位小数的numpy数组np.set_printoptions(3)np.random.uniform(5,10,(5,3))Out[59]: array([[5.427, 6.768, 6.71 ],[7.243, 6.434, 7.314],[6.692, 8.417, 7.965],[8.718, 5.061, 5.936],[5.918, 8.064, 7.431]])# 16. 使用科学记数法（如1e10）漂亮的打印数组rand_arrnp.random.seed(100)np.set_printoptions(suppress=True)rand_arr = np.random.random([3,3])/1e3rand_arrOut[68]: array([[0.000543, 0.000278, 0.000425],[0.000845, 0.000005, 0.000122],[0.000671, 0.000826, 0.000137]])# 17. 限制数组输出中打印元素的数量np.set_printoptions(threshold=6)arr = np.arange(15)arrOut[73]: array([ 0,  1,  2, ..., 12, 13, 14])标准化一个数组至0到1之间12345678910111213# 20. 标准化一个数组至0到1之间arr = np.random.uniform(size=(3,4))arrOut[89]: array([[0.88959857, 0.64348368, 0.66692863, 0.41213651], [0.78437761, 0.75333385, 0.10293507, 0.7942196 ], [0.82432451, 0.11447181, 0.30328525, 0.41767367]])s_max,s_min = arr.max(axis=0),arr.min(axis=0)(arr-s_min)/(s_max-s_min)Out[91]: array([[1. , 0.82805338, 1.  , 0.   ],[0.  , 1.  , 0.  , 1.     ],[0.37964781, 0. , 0.35523487, 0.01449204]])缺失值相关12345678910111213141516171819202122# 26. 找出数组是否有缺失的值arr = np.random.uniform(size=(3,4))np.isnan(arr).any()Out[119]: False      # 34. 删除所有nan值arr = np.array([1,2,3,np.nan,5,6,7,np.nan])arr[~np.isnan(arr)]Out[167]: array([1., 2., 3., 5., 6., 7.])    # 23. 数据集的5个随机位插入np.nan值arr = np.random.uniform(size=(3,4))i,j = np.where(arr)np.random.seed(100)arr[np.random.choice((i),5), np.random.choice((j),5)] = np.nanarrOut[103]: array([[  nan, 0.57138137, 0.27694632, 0.06905881],[0.25886229, nan,  nan, 0.0125819 ],[ nan, 0.06872554,  nan, 0.28179248]])np.where(np.isnan(arr))Out[105]: (array([0, 1, 1, 2, 2], dtype=int64), array([0, 1, 2, 0, 2], dtype=int64)) 数组交差集1234567891011# 5. 数组之间的共同元素a = np.array([1,2,3,2,3,4,3,4,5,6])b = np.array([7,2,10,2,7,4,9,4,9,8])np.intersect1d(a,b)Out[27]: array([2, 4])# 6. 删除存在于另一个数组中的元素a = np.array([1,2,3,4,5])b = np.array([5,6,7,8,9])np.setdiff1d(a,b)Out[30]: array([1, 2, 3, 4])数组产生123456789101112131415161718192021222324# 4. 生成自定义序列a = np.array([1,2,3])np.r_[np.repeat(a,3), np.tile(a,3)].reshape(2,-1)Out[24]: array([[1, 1, 1, 2, 2, 2, 3, 3, 3],[1, 2, 3, 1, 2, 3, 1, 2, 3]])# 14. 二维数组，以包含5到10之间的随机浮点数rand_arr = np.random.randint(low=5, high=10, size=(5,3)) + np.random.random((5, 3))rand_arrOut[56]: array([[5.37441948, 8.61709329, 8.24544657],[6.79517054, 6.43953972, 5.01129723],[9.662239  , 6.24575781, 6.51341089],[8.01997704, 6.66118956, 8.18283188],[8.02228762, 6.9609364 , 7.4676691 ]])rand_arr = np.random.uniform(5,10, size=(5,3))rand_arrOut[58]: array([[7.13046251, 7.4977751 , 8.55210656],[7.9222628 , 5.7828763 , 9.57743131],[9.13648012, 5.71533963, 9.18622945],[5.94241285, 8.08918632, 9.32716147],[5.75027771, 8.83210586, 8.3991677 ]])元素查找123456789101112131415161718192021222324252627282930313233343536# 19. 从一维元组数组中提取特定的列iris_2d = np.array([row.tolist()[:4] for row in irirs_ld])iris_2d[:4]Out[78]: array([[5.1, 3.5, 1.4, 0.2], [4.9, 3. , 1.4, 0.2], [4.7, 3.2, 1.3, 0.2], [4.6, 3.1, 1.5, 0.2]])# 22. 找到数组的百分位的值# 第5位和第95百分位的值arr = np.random.uniform(size=(3,4))np.percentile(arr, q=[5,95])Out[94]: array([0.31394255, 0.87443495])# 30. 获得数组中第二大的元素值arr = np.random.uniform(size=(3,4))np.unique(np.sort(arr))[-2]Out[132]: 0.9570126003527981# 32. 首次出现的值大于给定值的位置arr = np.random.uniform(size=(3,4))arrOut[150]: array([[0.61032815, 0.30900035, 0.69773491, 0.8596183 ], [0.62532376, 0.98240783, 0.97650013, 0.16669413], [0.02317814, 0.16074455, 0.92349683, 0.95354985]])np.argmax(arr[:,2].astype('float') &gt; 0.7)Out[151]: 1    # 35. 一维数组所有局部最大值（或峰值）arr = np.array([1,3,7,1,2,6,0,1])doublediff = np.diff(np.sign(np.diff(arr)))peak_locations = np.where(doublediff==-2)[0]+1peak_locationsOut[180]: array([2, 5], dtype=int64)</content>
	</entry>
	<entry>
		<title>python_pipe包管道包学习</title>
		<url>/article/27292.html</url>
		<content>python pipe包管道包学习参考: https://github.com/JulienPalard/Pipe1234567891011121314151617181920212223242526from pipe import *# 此处不是太理解[1, 2, 3, 4, 5] | tee | as_list12345Out[32]: [1, 2, 3, 4, 5]# 将生成器转换成list列表[1, 2, 3, 4, 5, 6] | as_listOut[34]: [1, 2, 3, 4, 5, 6]    # Like Python's built-in &quot;reversed&quot; primitive.[1, 2, 3] | reverse | concatOut[52]: '3, 2, 1'    # Returns index of value in iterable 检索值# 输入value start stop[1,2,3,2,1] | index(value=2,start=2,stop=4)Out[53]: 3[1,2,3,2,1] | index(3)Out[54]: 2构造你自己的pipe管道函数1234567# 构造你自己的pipe管道函数stdout = Pipe(lambda x: sys.stdout.write(str(x)))select = Pipe(lambda iterable, pred: (pred(x) for x in iterable))# 或者@Pipedef stdout(x):    sys.stdout.write(str(x))删除重复值1234567# dedup() Deduplicate values删除重复的值[1,1,2,2,3,3,1,2,3] | dedup |as_listOut[50]: [1, 2, 3]# 删除连续的且重复的值[1,1,2,2,3,3,1,2,3] | uniq | as_listOut[51]: [1, 2, 3, 1, 2, 3]groupby() 对列表实现分组计算123456# groupby() 对列表实现分组计算(1, 2, 3, 4, 5, 6, 7, 8, 9) | groupby(lambda x: x % 2 and &quot;Even&quot; or &quot;Odd&quot;) | select(lambda x: &quot;%s : %s&quot; % (x[0], (x[1] | concat(', ')))) | concat(' / ')Out[46]: 'Even : 1, 3, 5, 7, 9 / Odd : 2, 4, 6, 8'    [5, -4, 3, -2, 1] | sort(key=abs) | concatOut[49]: '1, -2, 3, -4, 5'求和和平均数1234567891011# 加法[1,2,3] | addOut[17]: 6    # 平均数，使用自定义函数，必须在函数前面加上@pipe装饰[1, 2, 3] | select(lambda x: x * x) |averageOut[30]: 4.666666666666667    # 平均数[1, 2, 3, 4, 5, 6] | averageOut[35]: 3.5select函数，相当于map映射操作123# select函数，相当于map映射操作[1,3,5] | select(lambda x:float(x +1)) |as_listOut[20]: [2.0, 4.0, 6.0]聚合函数123# 聚合函数，可以使用lambda函数自定义(1, 2, 3, 4, 5, 6, 7, 8, 9) | aggregate(lambda x, y: x + y)Out[21]: 45筛选和过滤1234567# 筛选和过滤[1, 2, 3] | where(lambda x: x % 2 == 0) | as_listOut[24]: [2][1, 2, 3, 4] | take_while(lambda x: x &lt; 3) | concatOut[25]: '1, 2'[1, 2, 3, 4] | where(lambda x: x &lt; 3) | concatOut[26]: '1, 2'take前几个元素和count生成器的长度12345# take前几个元素和count生成器的长度[1,2,3]|take(2) |as_list |where(lambda x : x ==2) |as_list |countOut[27]: 1[1,2,3]|countOut[28]: 3flatmap操作123# 相当于flatmap[[1, 2], [3, 4], [5]] | chain |as_listOut[31]: [1, 2, 3, 4, 5]两个列表实现zip函数123# 两个列表实现zip函数(1, 2, 3, 4, 5, 6, 7, 8, 9) | izip([9, 8, 7, 6, 5, 4, 3, 2, 1]) | concatOut[38]: '(1, 9), (2, 8), (3, 7), (4, 6), (5, 5), (6, 4), (7, 3), (8, 2), (9, 1)'指定函数排序并选出max1234567# max()按照key中的指定的函数来排序，然后筛选出max的函数('aa', 'b', 'fosdfdfo', 'qwerty', 'bar', 'zoog') | max(key=len)Out[43]: 'fosdfdfo'('aa', 'b', 'foo', 'qwerty', 'bar', 'zoog') | max()Out[44]: 'zoog'('aa', 'b', 'foo', 'qwerty', 'bar', 'zoog') | maxOut[45]: 'zoog'索引截取数组元素12345# 使用islice截取从下表2到7,按照索引截取数组中的元素(1, 2, 3, 4, 5, 6, 7, 8, 9) | islice(2, 8) | concatOut[36]: '3, 4, 5, 6, 7, 8'(1, 2, 3, 4, 5, 6, 7, 8, 9) | islice(0,1) |as_listOut[37]: [1]any()和all()实现1234567891011# any()，只要存在一个就返回true(1, 3, 5, 6, 7) | any(lambda x: x &gt;= 7)Out[39]: True(1, 3, 5, 6, 7) | any(lambda x: x &gt; 7)Out[40]: False# all(),必须全部满足才会返回true(1, 3, 5, 6, 7) | all(lambda x: x &lt; 7)Out[41]: False(1, 3, 5, 6, 7) | all(lambda x: x &lt;= 7)Out[42]: True</content>
	</entry>
	<entry>
		<title>Python增强提案PEP</title>
		<url>/article/65108.html</url>
		<content>Python 增强提案PEPPython中10个必读的PEP提案理解Python数据类：Dataclass 的特征概述 （上）理解Python数据类：Dataclass fields 的概述（下）Python3.7 dataclass使用指南小结​        PEP 是 Python 增强提案(Python Enhancement Proposal)的缩写。社区通过PEP来给 Python 语言建言献策，每个版本你所看到的新特性和一些变化都是通过PEP提案经过社区决策层讨论、投票决议的。PEP 557  数据类（data class）dataclass的定义位于PEP-557，根据定义一个dataclass是指“一个带有默认值的可变的namedtuple”，广义的定义就是有一个类，它的属性均可公开访问，可以带有默认值并能被修改，而且类中含有与这些属性相关的类方法，那么这个类就可以称为dataclass，再通俗点讲，dataclass就是一个含有数据及操作数据方法的容器。Dataclasses 是一些适合于存储数据对象(data object)的 Python 类。下面是一个并不详尽的用于定义数据对象的特征列表：• 他们存储并表示特定的数据类型。例如：一个数字。对于那些熟悉对象关系映射(Object Relational Mapping，简称 ORM)的人来说，一个模型实例就是一个数据对象。它表示了一种特定类型的实体。它存储了用于定义或表示那种实体的属性。• 他们能够被用于和同类型的其他对象进行比较。例如，一个数字可能大于，小于或等于另一个数字。• 当然数据对象还有更多的特征，但上述内容足以帮助你理解关键部分。• 为了理解 Dataclases，我们将实现一个简单的类。它能够存储一个数字，并允许我们执行上面提到的各种运算。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960611.相比普通class，dataclass通常不包含私有属性，数据可以直接访问2.dataclass的repr方法通常有固定格式，会打印出类型名以及属性名和它的值3.dataclass拥有__eq__和__hash__魔法方法4.dataclass有着模式单一固定的构造方式，或是需要重载运算符，而普通class通常无需这些工作# -*- coding: utf-8 -*import randomfrom typing import Listfrom dataclasses import dataclassfrom dataclasses import field# dataclasses.field 接受了一个名为 default_factory 的参数，它的作用是：如果在创建对象时没有赋值，则使用该方法初始化该字段。# default_factory 必须是一个可以调用的无参数方法(通常为一个函数)def get_random_marks():    return [random.randint(1, 10) for _ in range(5)]@dataclass(order=True)class Student:    marks: List[int] = field(default_factory=get_random_marks)    name: str = field(default='noname', compare=False)    age: int = field(default=18, repr=False)    # 在追踪一个对象的状态，并且希望它在初始化时一直被设为False    # 更一般地，这个值在初始化时不能够被传递,init决定是否生成init    verified: bool = field(repr=False, init=False, default=False)    # 不好的方式：自定义比较方法    # 正确方式：    # dataclass能够自动生成 &lt;, =, &gt;, &lt;= 和 &gt;= 这些比较方法。但是这些比较方法的一个缺陷是，    # 它们使用类中的所有字段进行比较, 并且是按定义顺序    # 这里使用(order=True)，把不需要的字段定义为 filed(compare=False)    # def __eq__(self, other):    #     return (self.marks, self.name) == (other.marks, other.name)    # 不好的方法：初始化一个变量为列表, 使用__post_init__方法, 或参数传递    # 正确方式是使用field    # def __post_init__(self):    #     self.marks = get_random_marks()    # 不要自己定义，会自动生成的__repr__方法使用所有的字段用于表示    # 不需要的元素使用field(repr=False)过滤    # def __repr__(self):    #     return self.name+' '+' '.join(self.marks)student_1 = Student([random.randint(1, 10) for _ in range(5)], 'tom', 17)student_2 = Student(age=16, name='Rick')# 这里会报异常# student_3 = Student(verified=True)print(student_1)print(f'student_1 == student_2: {student_1 == student_2}')# 使用dataclasses.asdict和dataclasses.astuple我们可以把数据类实例中的数据转换成字典或者元组：from dataclasses import asdict, astupleprint(asdict(student_2))# 使用dataclasses.is_dataclass可以判断一个类或实例对象是否是数据类：print(f'isinstance(student_1, Student): {isinstance(student_1, Student)}')&gt;&gt;&gt;    Student(marks=[7, 6, 7, 2, 3], name='tom')    student_1 == student_2: False    {'marks': [6, 4, 3, 5, 1], 'name': 'Rick', 'age': 16, 'verified': False}    isinstance(student_1, Student): True123装饰器的原型如下：dataclasses.dataclass(*, init=True, repr=True, eq=True, order=False, unsafe_hash=False, frozen=False)dataclass装饰器将根据类属性生成数据类和数据类需要的方法key含义init是否自动生成init，如果已经有定义同名方法则忽略这个值，也就是指定为True也不会自动生成repr是否自动生成repr；自动生成的打印格式为class_name(arrt1:value1, attr2:value2, …)eq是否生成eq；按属性在类内定义时的顺序逐个比较，全部的值相同才会返回Trueorder自动生成lt，le，gt，ge，比较方式与eq相同；如果order指定为True而eq指定为False，将引发ValueError；如果已经定义同名函数，将引发TypeErrorunsafehash如果是False，将根据eq和frozen参数来生成hash:1. eq和frozen都为True，hash将会生成2. eq为True而frozen为False，hash被设为None3. eq为False，frozen为True，hash将使用超类（object）的同名属性（通常就是基于对象id的hash）当设置为True时将会根据类属性自动生成hash，然而这是不安全的，因为这些属性是默认可变的，这会导致hash的不一致，所以除非能保证对象属性不可随意改变，否则应该谨慎地设置该参数为Truefrozen设为True时对field赋值将会引发错误，对象将是不可变的，如果已经定义了setattr和delattr将会引发TypeError1234567891011121314151617181920field的原型：dataclasses.field(*, default=MISSING, default_factory=MISSING, repr=True, hash=None, init=True, compare=True, metadata=None)通常我们无需直接使用，装饰器会根据我们给出的类型注解自动生成field，但有时候我们也需要定制这一过程，这时dataclasses.field就显得格外有用了。default和default_factory参数将会影响默认值的产生，它们的默认值都是None，意思是调用时如果为指定则产生一个为None的值。其中default是field的默认值，而default_factory控制如何产生值，它接收一个无参数或者全是默认参数的callable对象，然后用调用这个对象获得field的初始值，之后再将default（如果值不是MISSING）复制给callable返回的这个对象。init参数如果设置为False，表示不为这个field生成初始化操作，dataclass提供了hook—— __post_init__供我们利用这一特性：repr参数表示该field是否被包含进repr的输出，compare和hash参数表示field是否参与比较和计算hash值。metadata不被dataclass自身使用，通常让第三方组件从中获取某些元信息时才使用，所以我们不需要使用这一参数。如果指定一个field的类型注解为dataclasses.InitVar，那么这个field将只会在初始化过程中（__init__和__post_init__）可以被使用，当初始化完成后访问该field会返回一个dataclasses.Field对象而不是field原本的值，也就是该field不再是一个可访问的数据对象。举个例子，比如一个由数据库对象，它只需要在初始化的过程中被访问：@dataclassclass C:  i: int  j: int = None  database: InitVar[DatabaseType] = None  def __post_init__(self, database):    if self.j is None and database is not None:      self.j = database.lookup('j')c = C(10, database=my_database)dataclass继承python3.7引入dataclass的一大原因就在于相比namedtuple，dataclass可以享受继承带来的便利。dataclass装饰器会检查当前class的所有基类，如果发现一个dataclass，就会把它的字段按顺序添加进当前的class，随后再处理当前class的field。所有生成的方法也将按照这一过程处理，因此如果子类中的field与基类同名，那么子类将会无条件覆盖基类。子类将会根据所有的field重新生成一个构造函数，并在其中初始化基类。几点注意事项：dataclass通常情况下是unhashable的，因为默认生成的hash是None，所以不能用来做字典的key，如果有这种需求，那么应该指定你的数据类为frozen dataclass小心当你定义了和dataclass生成的同名方法时会引发的问题当使用可变类型（如list）时，应该考虑使用field的default_factory数据类的属性都是公开的，如果你有属性只需要初始化时使用而不需要在其他时候被访问，请使用dataclasses.InitVar</content>
	</entry>
	<entry>
		<title>python多线程</title>
		<url>/article/30423.html</url>
		<content>python多线程Python多线程学习多线程用比较少</content>
	</entry>
	<entry>
		<title>python进阶问题</title>
		<url>/article/4794.html</url>
		<content>高级概念与包函数式编程包operatorform operator import mul可以替代1reduce(lambda a,b:a*b, range(1,1+n))为1reduce(mul, range(1,1+n))itemgetter(1) 可以替代 lambda fields: fields[1]：创建一个接受集合的函数，返回索引位1上的元素attrgetter与itemgetter类似，它创建的函数根据名称提取对象的属性。如果把多个属性名传给attrgetter，它也会返回提取的值构成的元组。methodcaller会自行创建函数，创建的函数会在对象上调用参数指定的方法。123456from operator import methodcallers = 'The time is come'upcase = methodcaller('upper')upcase(s)Out[11]: 'THE TIME IS COME'模块查找策略目标：自动查找其他可用的*_promo函数方式一：1promos = [globals()[name] for name in globals() if name.endswith('_promo') and name != 'best_promo']方式二：将所有促销放在promotions模块12import inspectpromos = [func for name,func in inspect.getmembers(promotions, inspect.isfunction)]inspect.getmembers用于捕获对象(这里是promotions模块)的属性，第二个参数是可选的判断条件(一个布尔值函数)，这里只提取模块中的函数闭包closure和偏函数Partial内层函数+所引用的外层变量，称为闭包12345678910111213# 闭包就是函数里面嵌套一个函数# 因为函数的出现，导致变量出现了分化：全局变量、局部变量# 全局变量：当前模块中任意地方都能访问！# 局部变量：当前函数中可以操作# 全局变量-局部变量：变量-在函数外部调用访问：def outer():    msg = &quot;这是一个局部变量&quot;    print(&quot;这是一个函数&quot;, msg)    def inner():        print(&quot;可以访问外部变量msg&quot;, msg)        return msg    return inner在函数嵌套的前提下内层函数引用了外层函数的变(包括参数)外层函数又把内层函数当做返回值进行返回闭包中，如果要修改引用的外层变量需要使用 nonlocal 变量声明，表示非局部的否则会被当做是闭包内，新定义的变量12345678910111213# PYTHON中的偏函数，就是一个语法糖# 为原来没有默认值的函数，创建带常用值的偏函数from functools import partialdef show2(name, msg):    print(name, &quot;: &quot; , msg)s = partial(show2, msg=&quot;HI&quot;)s(&quot;tom&quot;)show2(&quot;jerry&quot;, &quot;hello&quot;)tom :  HIjerry :  hellopython模块:profile,pstats怪异问题避坑1234567891011121314151617181920212223class Teas:    def __init__(self, lst=[]):        self.lst = lst    def hyc_print(self):        logger.info(self.lst)    def __del__(self):        passif __name__ == '__main__':    # multiprocessing_queue_test()    te1 = Teas()    te1.lst.append(1)    te1.hyc_print()    te2 = Teas()    te2.lst.append(2)    te2.hyc_print()    te1.hyc_print()    logger.info(te1.lst is te2.lst)12342020-05-29 17:14:40 multiprocessing_queue_iterator INFO:  [1]2020-05-29 17:14:40 multiprocessing_queue_iterator INFO:  [1, 2]2020-05-29 17:14:40 multiprocessing_queue_iterator INFO:  [1, 2]2020-05-29 17:14:40 multiprocessing_queue_iterator INFO:  True123456789class Teas:    def __init__(self, lst=None):        self.lst = lst if lst is not None else []    def hyc_print(self):        logger.info(self.lst)    def __del__(self):        pass12342020-05-29 17:15:43 multiprocessing_queue_iterator INFO:  [1]2020-05-29 17:15:43 multiprocessing_queue_iterator INFO:  [2]2020-05-29 17:15:43 multiprocessing_queue_iterator INFO:  [1]2020-05-29 17:15:43 multiprocessing_queue_iterator INFO:  False扩展阅读机器学习8大算法比较1https://mp.weixin.qq.com/s/0dT4BN01g0anVwyfjS-RVA今年GitHub排名前20的Python机器学习开源项目1https://mp.weixin.qq.com/s/-WJ_S6CV7Cc14f4YzthPAQPython基础网站列表(有空再摘录)12345678910111213141516171819202122232425262728293031323334353637381. 元组https://www.jianshu.com/p/b728648501a82. 字典https://www.jianshu.com/p/8b51c9bf6d123. 集合https://www.jianshu.com/p/75eb228b638e4. 列表https://www.jianshu.com/p/636314cf01266.作用域https://www.jianshu.com/p/d8271c03a0f38. 对象(属性限制-公有私有)https://www.jianshu.com/p/c7f6ecf07fbc9. 对象(属性限制-只读)https://www.jianshu.com/p/dd0e1487a4d610. 对象(属性限制-只读优化)https://www.jianshu.com/p/f54e0a5af63511. 对象(系统内置方法-遍历操作)https://www.jianshu.com/p/bd87cca40d8b12. 内存管理机制-引用计数/垃圾回收/循环引用/弱引用https://www.jianshu.com/p/ef8a218c6b8913. 对象思想https://www.jianshu.com/p/0347ba66766714. 综合案例：封装/继承/多态https://www.jianshu.com/p/b35043c76f5015. 异常处理(错误和异常)https://www.jianshu.com/p/507d677e74a416. 包/模块(概念及导入语法)https://www.jianshu.com/p/1d9100f8292a17. 包/模块(导入及其底层逻辑)https://www.jianshu.com/p/6a99e5e4c1b518. 包/模块(三方包安装)https://www.jianshu.com/p/68477d5625fc19. 包/模块(创建和发布)https://www.jianshu.com/p/ee48fde9afd620.其他​    比如：你要去做一个电商后台，存储着每件产品的 ID、名称和价格。现在需要根据商品 ID 找出价格，如何使用最合适的数据结构呢？在 Python 中字典、集合都是经过高度性能优化的数据结构，如果采用列表来存储数据并进行查找，时间复杂度是多少？换成字典呢？哪个更高效？事实上，采用不同数据结构存储十万数据，查找速度差异就有可能差出几千倍。再比如：Python 中的协程和线程有什么区别？生成器如何进化成协程？并发编程中的 future 和 asyncio 有什么关系？如何写出线程安全的高性能代码呢？Python 基础入门必学知识：【Python 基础数据结构】【Python 基础语法】【文件操作】【错误与异常处理】【Python 面向对象】【模块化】第一步，你需要掌握 Python 的核心基础知识。当然，不同于其他基础教材，我不仅仅只讲基础概念、操作，同时也为你整理了很多进阶难度的知识，或是一些重难点、易错点等需要注意的地方。不仅可以让入门级的程序员查漏补缺，打捞基础，也能让有经验的程序员，重新从工程角度认识基础，升华理解。Python 进阶核心知识必学知识：【Python 协议】【Python 高级语法】【Python 正则表达式】【Python 并发编程】【垃圾回收机制】【项目实战】第二步，进阶 Python 核心知识点，比如装饰器、并发编程等等。如果你的工作只是写 100 行以下的脚本程序，可能不怎么会用得到。但如果你做的是大型程序的开发，则非常有必要。规范：编写高质量的 Python 程序这部分着重于教你把程序写得更加规范、更加稳定。我在实际工作中见过不少程序员，会写程序，但写得实在有点“惨不忍睹”，导致最后调试起来错误不断，修改非常费劲儿。因此，我觉得用单独一个版块讲解这个问题非常有必要。当然，我不会用一些似是而非的规范来说教，而是会用具体的编程操作和技巧，教你提高代码质量。比如，如何合理地分解代码、运用 assert，如何写单元测试等等。Python 实战，串联整个知识体系：带你搭建量化交易系统必学知识点：【RESTful】【Socket】【Pandas】【Numpy】【Kafka】【RabbitMQ】【MySQL】【Django】</content>
	</entry>
	<entry>
		<title>python性能优化模块</title>
		<url>/article/26598.html</url>
		<content>系统监控模块psutil模块psutil模块文档psutil是一个跨平台库(http://pythonhosted.org/psutil/)能够轻松实现获取系统运行的进程和系统利用率（包括CPU、内存、磁盘、网络等）信息。它主要用来做系统监控，性能分析，进程管理。它实现了同等命令行工具提供的功能，如ps、top、lsof、netstat、ifconfig、who、df、kill、free、nice、ionice、iostat、iotop、uptime、pidof、tty、taskset、pmap等。一、 安装psutil　　pip install psutil 二、 监控cpu信息import psutilpsutil.cpu_times()   #获取cpu（逻辑cpu的平均）占用时间的详细信息 psutil.cpu_times(percpu=True)   #获取每个cpu占用时间的详细信息psutil.cpt_times().user   #获取用户进程占用cpu的时间（user+sys+idle+wait=total）三、 监控内存信息import psutilpsutil.virtual_memory()   #获取内存信息psutil.virtual_memory().total   #获取内存总量psutil.swap_memory()   #获取swap信息psutil.swqp_memory()   #获取swap总量 四、 监控磁盘信息import psutilpsutil.disk_partitions()   #获取各分区的信息psutil.disk_usage()   #获取各分区的使用情况psutil.disk_io_counters(perdisk=True)   #获取各个分区的io情况psutil.disk_io_counters(perdisk=True)[‘sda1’].read_count   #获取sda1的io读取情况五、 监控网络信息import psutilpsutil.net_io_counters()   #获取所有网络接口io信息psutil.net_io_counters(pernic=True)   #获取每个网络接口的io信息六、进程信息import psutilpsutil.Process(pid)   #查看对应pid的进程信息psutil.Process(pid).username()   #查看是哪个用户创建的该进程psutil.Process(pid).cmdline()   #查看进程所在的路径七、 登录用户信息import psutilpsutil.users()   #查看目前登录用户信息例子12345678910111213141516171819202122232425262728293031323334353637383940414243import psutilimport timedef get_sys_rc():    # 获得cpu信息：核心数量、使用率    cpu_count = psutil.cpu_count()    print(cpu_count)    cpu_usage = psutil.cpu_percent(1)  # 间隔1秒钟统计一次使用率    print(cpu_usage)    # 获得内存大小和使用率    print(&quot;#&quot; * 50)    mem = psutil.virtual_memory()    mem_total = mem.total / 1024 / 1024    mem_usage = mem.percent    print(mem_total, mem_usage)    # 获得磁盘信息和分区信息    disk_info = psutil.disk_partitions()  # 得到所有的分区信息    print(disk_info)    for i in disk_info:  # 遍历所有的分区，得到分区的名字        print(i.device)  # 输出设备名字        part_info = psutil.disk_usage(i.device)  # 得到每个分区的使用率        print(part_info)  # 输出使用率        # round() 四舍五入的方法        print(f&quot;{i.device}总大小为{round(part_info.total / 1024 / 1024 / 1024)}G,使用率{part_info.percent}&quot;)    # 获得网卡信息    net_info = psutil.net_io_counters()    step1 = net_info.bytes_sent    time.sleep(3)    step2 = psutil.net_io_counters().bytes_sent    avg = (step2 - step1) / 3 / 1000    print(f&quot;当前平均的网络流量是{round(avg)}KB&quot;)    # 获得所有网卡的ip地址    # psutil.net_if_stats()    return {&quot;cpu_count&quot;: cpu_count, &quot;cpu_usage&quot;: cpu_usage}    # return  {&quot;cpu_count&quot;:cpu_count,&quot;cpu_usage&quot;:cpu_usage,mem_total,mem_usage,avg}if __name__ == &quot;__main__&quot;:    print(get_sys_rc())output12345678910111213141516171819202122232425262728842.8##################################################32685.93359375 23.9[sdiskpart(device='C:\\', mountpoint='C:\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='D:\\', mountpoint='D:\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='E:\\', mountpoint='E:\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='H:\\', mountpoint='H:\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='Q:\\', mountpoint='Q:\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='S:\\', mountpoint='S:\\', fstype='NTFS', opts='rw,fixed'), sdiskpart(device='U:\\', mountpoint='U:\\', fstype='NTFS', opts='rw,fixed')]C:\sdiskusage(total=116955537408, used=76401147904, free=40554389504, percent=65.3)C:\总大小为109G,使用率65.3D:\sdiskusage(total=174902472704, used=128004796416, free=46897676288, percent=73.2)D:\总大小为163G,使用率73.2E:\sdiskusage(total=107373129728, used=24370171904, free=83002957824, percent=22.7)E:\总大小为100G,使用率22.7H:\sdiskusage(total=214747312128, used=140843151360, free=73904160768, percent=65.6)H:\总大小为200G,使用率65.6Q:\sdiskusage(total=214748360704, used=67760369664, free=146987991040, percent=31.6)Q:\总大小为200G,使用率31.6S:\sdiskusage(total=214748360704, used=79423479808, free=135324880896, percent=37.0)S:\总大小为200G,使用率37.0U:\sdiskusage(total=1073741819904, used=507053940736, free=566687879168, percent=47.2)U:\总大小为1000G,使用率47.2当前平均的网络流量是1KB{'cpu_count': 8, 'cpu_usage': 42.8}时间监控cProfile介绍cProfile官方文档cProfile自python2.5以来就是标准版Python解释器默认的性能分析器。其他版本的python，比如PyPy里没有cProfile的。cProfile是一种确定性分析器，只测量CPU时间，并不关心内存消耗和其他与内存相关联的信息。cProfile模块时间监控装饰器1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v1.0@author: huangyc@file: profiler_helper.py@Description: 时间性能监控类，主要监控执行状态、频率和时长@time: 2021/7/17 15:51&quot;&quot;&quot;from cProfile import Profilefrom functools import wrapsfrom pstats import Statsclass ProfilerHelper:    @staticmethod    def profiler_wrap(sort_by: str = 'cumulative', print_stats: bool = True, print_callers: bool = True):        &quot;&quot;&quot;        行状态、频率和时长装饰器        :param sort_by: 排序规则                        可选参数：                              准则                       含义                       升序/降序排列                              calls                    调用次数                        降序                              cumulative               累计时间                        降序                              cumtime                  累计时间                        降序                              file                     文件                           升序                              filename                 文件名                          升序                              module                   模块名                          升序                              ncalls                   调用总次数                       降序                              pcalls                   原始调用书                       降序                              line                     行号                            升序                              name                     函数名                          升序                              nfl                      函数名/文件名/行号组合             降序                              stdname                  标准名称                         升序                              time                     函数内部运行时间                   降序                              tottime                  函数内部运行总时间                 降序        :param print_stats: Create a Stats object based on the current profile and print the results to stdout.        :param print_callers: 打印受测函数和调用函数的关系        :return:        &quot;&quot;&quot;        def wrapper(func):            @wraps(func)            def inner_func(*args, **kwargs):                profiler = Profile()                res = profiler.runcall(func, *args, **kwargs)                stats = Stats(profiler)                # strip_dirs()：删除报告中所有函数文件名的路径信息，这个方法改变stats实例内部的顺序，任何运行该方法的实例都将随机排列项目的顺序。                # 如果两个项目是相同的，那么这两个项目就可以合并。                stats.strip_dirs()                # sort_stats(*keys)：通过一系列条件依次对所有项目进行排序，从而调整stats对象                stats.sort_stats(sort_by)                if print_stats:                    stats.print_stats()                if print_callers:                    stats.print_callers()                return res            return inner_func        return wrapper@ProfilerHelper.profiler_wrap()def fn(h, a=5, b=8):    import re    print(h, a, b)    return re.compile(&quot;aaa|bbb&quot;)if __name__ == '__main__':    print(fn(464646, 8, b=8))内存监控tracemalloc介绍tracemalloc模块官方文档tracemalloc模块是跟踪python分配的内存块的调试工具。它提供以下信息：回溯对象的分配位置每个文件名和每个行号的已分配内存块的统计信息：已分配内存块的总大小、数量和平均大小计算两个快照之间的差异以检测内存泄漏tracemalloc模块内存监控装饰器12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v1.0@author: huangyc@file: tracemalloc_helper.py@Description: 内存监控类@time: 2021/7/17 15:53&quot;&quot;&quot;import tracemallocfrom functools import wrapsclass TracemallocHelper:    @staticmethod    def tracemalloc_wrap(key_type: str = 'lineno', monitor_type: str = 'statistics', top_k: int = 10,                         filter_self: bool = True):        &quot;&quot;&quot;        内存监控装饰器        :param key_type: 比较的字段，'traceback', 'filename', 'lineno'        :param monitor_type: 监控类型，'statistics', 'change'        :param top_k: 关注top_k的内存占用,默认关注前10个        :param filter_self: 是否过滤掉监控程序本身的消耗        :return:        &quot;&quot;&quot;        def wrapper(func):            @wraps(func)            def inner_func(*args, **kwargs):                tracemalloc.start()                # ... start your application ...                res = func(*args, **kwargs)                snapshot1 = tracemalloc.take_snapshot()                # ... call the function leaking memory ...                snapshot2 = tracemalloc.take_snapshot()                if monitor_type == 'change':                    top_stats = snapshot2.compare_to(snapshot1, key_type)                elif monitor_type == 'statistics':                    top_stats = snapshot2.statistics(key_type)                else:                    top_stats = snapshot2.statistics(key_type)                print(f&quot;[ Top {top_k} differences ]&quot;)                for idx, stat in enumerate(top_stats[:top_k]):                    if filter_self and 'tracemalloc.py' in str(stat):                        continue                    print('d' % idx, stat)                return res            return inner_func        return wrapper@TracemallocHelper.tracemalloc_wrap()def fn(h, a=5, b=8):    print(h, a, b)    d = [dict(zip('xy', (5, 6))) for i in range(100000)]    t = [tuple(zip('xy', (5, 6))) for i in range(100000)]    return d, tif __name__ == '__main__':    fn(68, 7, b=78)</content>
	</entry>
	<entry>
		<title>Anaconda开发环境</title>
		<url>/article/39324.html</url>
		<content>Anaconda介绍Anaconda（官方网站）就是可以便捷获取包且对包能够进行管理，同时对环境可以统一管理的发行版本。Anaconda包含了conda、Python在内的超过180个科学包及其依赖项。套件管理显示已安装的套件12conda listpip list套件安装1234conda install packages # pip可以安装一些conda安装不了的包pip install packages [-i https://pypi.douban.com/simple] # -i部分临时指定pip源easy_install 参数 packagespython setup.py install # 下载源码，进入到源码路径下国内的pip源阿里云 https://mirrors.aliyun.com/pypi/simple/中国科技大学 https://pypi.mirrors.ustc.edu.cn/simple/豆瓣(douban) https://pypi.douban.com/simple/清华大学 https://pypi.tuna.tsinghua.edu.cn/simple/1pip install xx -i http://pypi.douban.com/simple/ --trusted-host=pypi.douban.com/simple从Anaconda.org安装套件123# 如果一个包不能使用conda安装，可在Anaconda.org查找# 在左上角的叫“Search Anaconda Cloud”搜索框中输入“包名”并点击search按钮conda install --channel https://conda.anaconda.ort/pandas bottleneck # 安装bottleneck包删除套件12conda/pip uninstall packageseasy_install -m packages查找套件1conda search packagespip管理升级12345# 试用linux, win下可能会因为权限升级失败pip install --upgrade pip# win或linuxpython -m pip install --upgrade pip如果不小心删掉了pip，可以用以下命令安装1python -m ensurepip --default-pip食用教程安装基础准备(centos7环境)安装wget命令：yum -y install wget安装git相关1234567# 安装gityum install git -y# 安装git lfs（大文件下载）curl -s https://packagecloud.io/install/repositories/github/git-lfs/script.rpm.sh | sudo bashyum install git-lfsgit lfs install安装OpenSSL和libssl-dev12yum install opensslyum install openssl-devel安装gcc：yum install gcc安装步骤，附清华大学开源软件镜像站1234567# 安装anaconda环境wget https://repo.anaconda.com/archive/Anaconda3-2023.07-2-Linux-x86_64.shchmod +x Anaconda3-2023.07-2-Linux-x86_64.sh./Anaconda3-2023.07-2-Linux-x86_64.sh# 创建自己的python环境conda create -n gpt310 python=3.10 anaconda虚拟环境管理使用Anaconda Prompt 管理虚拟环境新建虚拟环境添加后缀 “anaconda”或“biopython”可以创建一个基本科学计算功能完备的环境1234conda create -n 环境名 python=3.10 anaconda# 如果需要指定虚拟环境的路径, 可以使用--prefix参数conda create -n 环境名 --prefix='D:\Program Files\Anaconda4\envs\gpt310' python=3.10 anaconda切换虚拟环境1234# Linux，OS Xsource activate snowflakes# Windowsconda activate snowflakes 关闭虚拟环境1234# Linux，OS Xsource deactivate# Windowsdeactivate查看所有的虚拟环境名称12conda info -econda env list移除虚拟环境1conda remove -n 虚拟环境名称 --all复制虚拟环境1conda create -n 虚拟环境名称 --clone rootIPython 交谈式命令窗口cmd中python模式的加强版观看先前输入的程序代码 history查询使用说明 ?加在命令后面简易智能输入执行python档案 %run 档案路径更新conda本身1conda update conda更新anaconda 应用1conda update anaconda更新python假设当前python环境是3.6.1，而最新版本是3.6.2，那么就会升级到3.6.21conda update pythonconda源管理查看当前源信息1conda info关注 channel URLs 字段内容添加一个镜像12345# 添加镜像(base) C:\Users\hyc&gt;conda config --add channels 'https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/'# 设置搜索时显示channel地址(base) C:\Users\hyc&gt;conda config --set show_channel_urls yes删除一个镜像12# 注意链接要有引号，英文输入法(base) C:\Users\hyc&gt;conda config --remove channels 'https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/'查看conda的镜像12conda config --show-sourcesconda config --show channels删除所有镜像1conda config --remove-key channels以上操作可以直接编辑.condarc文件完成12345678ssl_verify: Falseproxy_servers:  http: http://192.168.4.80:3128  https: http://192.168.4.80:3128channels:  - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/  - defaultsshow_channel_urls: True其他管理运行环境分享执行如下命令可以将当前环境下的 package 信息存入名为 environment 的 YAML 文件中1conda env export &gt; environment.yaml使用yaml文件创建环境1conda env create -f environment.yaml国内常见源anaconda conda 切换为国内源Anaconda 镜像使用帮助国内可用Anaconda 源的镜像站及换国内源方法linux可行12345conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main/conda config --set show_channel_urls yes</content>
	</entry>
	<entry>
		<title>Jupyter_Notebook介绍、安装及使用教程</title>
		<url>/article/1290.html</url>
		<content>Jupyter Notebook使用教程简介Jupyter Notebook是一个开源的Web应用程序，允许用户创建和共享包含代码、方程式、可视化和文本的文档。它的用途包括：数据清理和转换、数值模拟、统计建模、数据可视化、机器学习等等。它具有以下优势：可选择语言：支持超过40种编程语言，包括Python、R、Julia、Scala等。分享笔记本：可以使用电子邮件、Dropbox、GitHub和Jupyter Notebook Viewer与他人共享。交互式输出：代码可以生成丰富的交互式输出，包括HTML、图像、视频、LaTeX等等。大数据整合：通过Python、R、Scala编程语言使用Apache Spark等大数据框架工具。支持使用pandas、scikit-learn、ggplot2、TensorFlow来探索同一份数据。Jupyter Notebook是基于网页的用于交互计算的应用程序。其可被应用于全过程计算：开发、文档编写、运行代码和展示结果。安装与运行12conda install jupyter notebookpython -m pip install jupyter运行Jupyter Notebook，首先cmd进入到自己的工作路径下，执行以下的命令1jupyter notebook [--port &lt;port_number&gt;]默认端口8888编辑界面一个notebook的编辑界面主要由四部分组成：名称、菜单栏、工具条以及单元(Cell)，如下图所示：FileFile中的按钮选项，具体功能如下表：EditEdit中的按钮选项如下图所示：ViewView中的按钮选项如下图所示View中的功能可以让用户更好的展示自己的notebook，但对编写代码、实现功能没有影响。Insert在当前单元上方/下方插入新的单元CellCell按ESC进入无编辑状态，ESC+Y和ESC+M切换cell typeKernelKernelHelpHelp快捷键命令模式下的快捷键编辑模式下的快捷键使用技巧魔法函数使用魔法函数可以简单的实现一些单纯python要很麻烦才能实现的功能。%：行魔法函数，只对本行代码生效。%%：Cell魔法函数，在整个Cell中生效，必须放于Cell首行。%lsmagic：列出所有的魔法函数%magic查看各个魔法函数的说明?后面加上魔法函数名称，可以查看该函数的说明一些常用魔法函数的示例：注意这些命令是在Python kernel中适用的，其他 kernel 不一定适用。md生成目录不同于有道云笔记的Markdown编译器，Jupyter Notebook无法为Markdown文档通过特定语法添加目录，因此需要通过安装扩展来实现目录的添加。1conda install -c conda-forge jupyter_contrib_nbextensions执行上述命令后，启动Jupyter Notebook，你会发现导航栏多了“Nbextensions”的类目，点击“Nbextensions”，勾选“Table of Contents”Nbextensions页面之后再在Jupyter Notebook中使用Markdown，点击下图的图标即可使用啦。md链接并定位语法格式如下：12[添加链接的正文](#自定义索引词)&lt;a id=自定义索引词&gt;跳转提示&lt;/a&gt;加载指定网页源代码1%load URL加载本地Python文件1%load Python文件的绝对路径运行本地Python文件12%run Python文件的绝对路径!python Python文件的绝对路径获取当前位置1%pwd使用shell命令1!shell命令在Jupyter Notebook中的笔记本单元格中用英文感叹号“!”后接shell命令即可执行shell命令。由于目前暂时用不到过多的魔术命令，因此暂时先参考官网的文档。参考Jupyter Notebook介绍、安装及使用教程最详尽使用指南：超快上手Jupyter Notebook</content>
	</entry>
	<entry>
		<title>elementary_os记录</title>
		<url>/article/53484.html</url>
		<content>Elementary os介绍硬盘格式转换在MBR硬盘中，分区信息直接存储于主引导记录（MBR）中（主引导记录中还存储着系统的引导程序）。但在GPT硬盘中，分区表的位置信息储存在GPT头中。但出于兼容性考虑，硬盘的第一个扇区仍然用作MBR，之后才是GPT头。跟现代的MBR一样，GPT也使用逻辑区块地址（LBA）取代了早期的CHS寻址方式。传统MBR信息存储于LBA 0，GPT头存储于LBA 1，接下来才是分区表本身。64位Windows操作系统使用16,384字节（或32扇区）作为GPT分区表，接下来的LBA 34是硬盘上第一个分区的开始。GPT是为了支持2TB以上硬盘而发展的。而且GPT的分区信息是在分区中，而不象MBR一样在主引导扇区，为保护GPT 不受MBR类磁盘管理软件的危害，GPT在主引导扇区建立了一个保护分区 (Protective MBR)的MBR分区表，这种分区的类型标识为0xEE，这个保护分区的大小在Windows下为128MB，Mac OS X下为200MB，在Window磁盘管理器里名为GPT保护分区，可让MBR类磁盘管理软件把GPT看成一个未知格式的分区，而不是错误地当成一个未分区的磁盘。转换使用Diskpart命令即可完成，方法如下：按Shift + F10打开命令提示符输入”Diskpart”(不用输入引号，下同)，并按回车，进入操作界面输入：”list disk”，查看磁盘信息。注意看磁盘容量来选择输入：”select disk 0”，选择disk 0为当前操作的磁盘，这里视你需要的转换盘符，一般为安装系统的那块物理硬盘输入：”Clean”，清空当前磁盘分区输入：”convert mbr”，转换为MBR分区，还有gpt格式操作完成，用exit命令关闭提示符窗口，继续按照正常的方法安装系统即可 双系统安装制作启动盘空白分区tf按F2进入BIOS，设置U盘启动重启安装类别，这里选something else，表示自己选分区（如果是刚装完win10，立马装linux，可以选第一个）这里是因为，原先就是win10+linux，现在想重装linux选择事先分好的分区正式开始安装软件安装安装typora12345678910# or run:# sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys BA300B7755AFCFAEwget -qO - https://typora.io/linux/public-key.asc | sudo apt-key add -# add Typora's repositorysudo add-apt-repository 'deb https://typora.io/linux ./'sudo apt-get update# install typorasudo apt-get install typora显卡驱动Ubuntu 18.04 安装 NVIDIA 显卡驱动我们今天的目标是在 Ubuntu 18.04 上安装 NVIDIA 显卡驱动，请注意，你的显卡一定要是 NVIDIA 的显卡才能按照这篇文章的方法安装。我将给大家介绍三种安装方法，建议使用第一种方法安装。先来说说带有 NVIDIA 独立显卡的电脑为什么要安装 NVIDIA 显卡驱动吧。下面是我从维基百科摘抄的显卡工作原理的介绍：显卡是插在主板上的扩展槽里的（现在一般是 PCI-E 插槽，此前还有 AGP、PCI、ISA 等插槽）。它主要负责把主机向显示器发出的显示信号转化为一般电器信号，使得显示器能明白个人计算机在让它做什么。显卡的主要芯片叫“显示芯片”（Video chipset，也叫 GPU 或 VPU，图形处理器或视觉处理器），是显卡的主要处理单元。显卡上也有和计算机存储器相似的存储器，称为“显示存储器”，简称显存。早期的显卡只是单纯意义的显卡，只起到信号转换的作用；当前我们一般使用的显卡都带有 3D 画面运算和图形加速功能，所以也叫做“图形加速卡”或“3D 加速卡”。PC 上最早的显卡是 IBM 在 1981 年推出的 5150 个人计算机上所搭载的 MDA 和 CGA 两款 2D 加速卡。显卡通常由总线接口、PCB 板、显示芯片、显存、RAMDAC、VGA BIOS、VGA 功能插针、D-sub 插座及其他外围组件构成，现在的显卡大多还具有 VGA、DVI 显示器接口或者 HDMI 接口及 S-Video 端子和 Display Port 接口。电脑硬件有 NVIDIA 的显卡，但是没有驱动，那它是不能工作的，所以为了让显卡能够工作，发挥它的作用，我们需要安装显卡驱动。好的，下面来看看三种不同的安装方法吧。稳定版本安装显卡型号在终端输入：ubuntu-drivers devices，可以看到如下界面：从上图可以看出，我的显卡是：GM107M [GeForce GTX 860M]，推荐安装的版本号是：nvidia-driver-390 - distro non-free recommended。开始安装如果同意安装推荐版本，那我们只需要终端输入：sudo ubuntu-drivers autoinstall 就可以自动安装了。当然我们也可以使用 apt 命令安装自己想要安装的版本，比如我想安装 340 这个版本号的版本，终端输入：sudo apt install nvidia-340 就自动安装了。安装过程中按照提示操作，除非你知道每个提示的真实含义，否则所有的提示都选择默认就可以了，安装完成后重启系统，NVIDIA 显卡就可以正常工作了。安装完成后你可以参照 https://linuxconfig.org/benchmark-your-graphics-card-on-linux 上的介绍测试你的显卡。ppa第三方使用 PPA 第三方软件仓库安装最新版本添加 PPA 软件仓库：sudo add-apt-repository ppa:graphics-drivers/ppa，需要输入用户密码，按照提示还需要按下 Enter 键。更新软件索引：sudo apt update接下来的步骤同方法一，只是这样我们就可以选择安装最新版本的驱动程序了。NVIDIA官网安装打开终端，输入：lshw -numeric -C display，确认自己电脑显卡的型号，如下图：从上图可以看到，我的电脑的显卡是 product: GM107M [GeForce GTX 860M] [10DE:1392]，然后我们到 NVIDIA 的官网下载相应型号的驱动，官网地址是：https://www.nvidia.com/Download/index.aspx，如下图，我选择 GTX 860 显卡的驱动下载。注意每个选项，特别是操作系统那里，一定选择 Linux 64-bit选好后点击 SEARCH 按钮搜索相应的驱动，如下图：看到了吧，官方最新版本号已经是 418.43 了。点击 DOWNLOAD 按钮下载吧。接着需要先安装一些 NVIDIA 显卡依赖的软件，在终端依次执行如下命令：sudo dpkg --add-architecture i386sudo apt updatesudo apt install build-essential libc6:i386Ubuntu 系统默认安装好是使用的一个开源的驱动：nouveau，我们要安装官方的驱动需要先禁用这个开源驱动，方法如下，依次执行：sudo bash -c &quot;echo blacklist nouveau &gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf&quot;sudo bash -c &quot;echo options nouveau modeset=0 &gt;&gt; /etc/modprobe.d/blacklist-nvidia-nouveau.conf&quot;执行完上面两条指令后，我们使用如下命令看看是否成功禁用了开源驱动：cat /etc/modprobe.d/blacklist-nvidia-nouveau.conf。如果和下面一样，表示成功了。123$ cat /etc/modprobe.d/blacklist-nvidia-nouveau.confblacklist nouveauoptions nouveau modeset=0这个时候我们需要先重启一下系统，重启吧。重启成功后打开终端，输入如下命令：sudo telinit 3。然后按快捷键：CTRL+ALT+F1 进入字符界面，输入用户名和密码，然后登录系统，进入我们保存下载 NVIDIA 驱动的目录，默认是：Downloads/ 目录，cd Downloads/，然后执行：bash NVIDIA-Linux-x86_64-418.43.bin。注意：NVIDIA-Linux-x86_64-418.43.bin 要根据你下载的驱动文件的名字相应改动。安装过程中都点同意即可，如果你遇到下面的提示，安装下面我的输入输入后回车继续安装：The distribution-provided pre-install script failed!Are you sure you want to continue? -&gt; CONTINUE INSTALLATIONWould you like to run the nvidia-xconfig utility? -&gt; YES安装完成后重启系统就可以点击软件列表中的 NVIDIA 的配置软件配置显卡驱动了，如果你遇到如下报错，请依次在终端输入如下命令解决：报错：WARNING: Unable to find suitable destination to install 32-bit compatibility libraries解决办法：sudo dpkg —add-architecture i386sudo apt updatesudo apt install libc6:i386好了，今天分享了如何安装 NVIDIA 显卡驱动，再次提醒，如果你的显卡不是 NVIDIA 显卡，千万不要这么干，也不要在虚拟机中这么干。elasticsearch6es和kibana版本ElasticsearchKibanaX-PackBeats^*Logstash^*ES-Hadoop (jar)*APM ServerApp Search5.0.x5.0.x5.0.x1.3.x-5.6.x2.4.x-5.6.x5.0.x-5.6.x5.1.x5.1.x5.1.x1.3.x-5.6.x2.4.x-5.6.x5.0.x-5.6.x5.2.x5.2.x5.2.x1.3.x-5.6.x2.4.x-5.6.x5.0.x-5.6.x5.3.x5.3.x5.3.x1.3.x-5.6.x2.4.x-5.6.x5.0.x-5.6.x5.4.x5.4.x5.4.x1.3.x-5.6.x2.4.x-5.6.x5.0.x-5.6.x5.5.x5.5.x5.5.x1.3.x-5.6.x2.4.x-5.6.x5.0.x-5.6.x5.6.x5.6.x5.6.x1.3.x-6.0.x2.4.x-6.0.x5.0.x-6.0.x6.0.x6.0.x6.0.x5.6.x-6.8.x5.6.x-6.8.x6.0.x-6.8.x6.1.x6.1.x6.1.x5.6.x-6.8.x5.6.x-6.8.x6.0.x-6.8.x6.2.x6.2.x6.2.x5.6.x-6.8.x5.6.x-6.8.x6.0.x-6.8.x6.2.x-6.8.x6.3.x6.3.xN/A**5.6.x-6.8.x5.6.x-6.8.x6.0.x-6.8.x6.2.x-6.8.x6.4.x6.4.xN/A**5.6.x-6.8.x5.6.x-6.8.x6.0.x-6.8.x6.2.x-6.8.x6.5.x6.5.xN/A**5.6.x-6.8.x5.6.x-6.8.x6.0.x-6.8.x6.2.x-6.8.x6.6.x6.6.xN/A**5.6.x-6.8.x5.6.x-6.8.x6.0.x-6.8.x6.2.x-6.8.x6.7.x6.7.xN/A**5.6.x-6.8.x5.6.x-6.8.x6.0.x-6.8.x6.2.x-6.8.x6.8.x6.8.xN/A**5.6.x-6.8.x5.6.x-6.8.x6.0.x-6.8.x6.2.x-6.8.x7.0.x7.0.xN/A**6.8.x-7.4.x6.8.x-7.4.x6.8.x-7.4.x7.0.x-7.4.x*7.1.x7.1.xN/A**6.8.x-7.4.x6.8.x-7.4.x6.8.x-7.4.x7.0.x-7.4.x*7.2.x7.2.xN/A**6.8.x-7.4.x6.8.x-7.4.x6.8.x-7.4.x7.0.x-7.4.x*7.2.x7.3.x7.3.xN/A**6.8.x-7.4.x6.8.x-7.4.x6.8.x-7.4.x7.0.x-7.4.x*7.3.x7.4.x7.4.xN/A**6.8.x-7.4.x6.8.x-7.4.x6.8.x-7.4.x7.0.x-7.4.x*7.4.xkibana国内下载地址jdk环境安装在 /etc/profile 文件下添加以下配置12345# jdk configJAVA_HOME=/usr/java/jdk1.8.0_241-amd64JRE_HOME=/usr/java/jdk1.8.0_241-amd64/jreexport CLASSPATH=.:$JAVA_HOME/lib:$JRE_HOME/lib:$CLASSPATH  export PATH=$JAVA_HOME/bin:$JRE_HOME/bin:$PATH新建用户123456sudo mkdir /home/user_group/sudo groupadd user_groupsudo useradd elasticsearch -g user_group -p elasticsearchpasswd elasticsearcheses不能使用TAB键、上下键，命令行不显示当前路径的解决因默认ubuntu创建的普通帐号，默认shell为/bin/sh，而这不支持tab等键的，所以将「指定用户」帐号的shell改为/bin/bash就可以了。123&gt;&gt;&gt; echo $SHELL/bin/sh&gt;&gt;&gt; usermod -s /bin/bash 用户名es配置解压后进入目录，配置elasticsearch内外网可访问以及端口绑定12cd elasticsearch-6.8.7vi ./config/elasticsearch.yml在最后面加上12network.host: 0.0.0.0http.port: 9200保存退出，这个时候直接运行 ./bin/elasticsearch会报错1bound or publishing to a non-loopback address, enforcing bootstrap checks重新切回root用户1su root编辑 /etc/security/limits.conf，追加以下内容12* soft nofile 65536* hard nofile 65536此文件修改后需要重新登录用户，才会生效编辑 /etc/sysctl.conf，追加以下内容：1vm.max_map_count=655360保存后，执行：1sysctl -p启动切回新建的用户回到elasticsearch目录下12345678910111213141516elasticsearch@hyc:/media/hyc/linux/elementary os/software/elasticsearch-6.7.1$ source /etc/profileelasticsearch@hyc:/media/hyc/linux/elementary os/software/elasticsearch-6.7.1$ echo $JAVA_HOME/usr/java/jdk1.8.0_241-amd64elasticsearch@hyc:/media/hyc/linux/elementary os/software/elasticsearch-6.7.1$ ./bin/elasticsearchwarning: Falling back to java on path. This behavior is deprecated. Specify JAVA_HOME[2020-08-08T13:14:40,834][INFO ][o.e.e.NodeEnvironment    ] [EsTp_cx] using [1] data paths, mounts [[/media/hyc/linux (/dev/sdb6)]], net usable_space [545.8gb], net total_space [999.9gb], types [fuseblk][2020-08-08T13:14:40,836][INFO ][o.e.e.NodeEnvironment    ] [EsTp_cx] heap size [989.8mb], compressed ordinary object pointers [true][2020-08-08T13:14:40,839][INFO ][o.e.n.Node               ] [EsTp_cx] node name derived from node ID [EsTp_cxdSjehAblsJRe8tA]; set [node.name] to override[2020-08-08T13:14:40,840][INFO ][o.e.n.Node               ] [EsTp_cx] version[6.7.1], pid[934], build[default/tar/2f32220/2019-04-02T15:59:27.961366Z], OS[Linux/5.4.0-42-generic/amd64], JVM[Oracle Corporation/Java HotSpot(TM) 64-Bit Server VM/1.8.0_241/25.241-b07][2020-08-08T13:14:40,840][INFO ][o.e.n.Node               ] [EsTp_cx] JVM arguments [-Xms1g, -Xmx1g, -XX:+UseConcMarkSweepGC, -XX:CMSInitiatingOccupancyFraction=75, -XX:+UseCMSInitiatingOccupancyOnly, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Djava.io.tmpdir=/tmp/elasticsearch-5133240268811839837, -XX:+HeapDumpOnOutOfMemoryError, -XX:HeapDumpPath=data, -XX:ErrorFile=logs/hs_err_pid%p.log, -XX:+PrintGCDetails, -XX:+PrintGCDateStamps, -XX:+PrintTenuringDistribution, -XX:+PrintGCApplicationStoppedTime, -Xloggc:logs/gc.log, -XX:+UseGCLogFileRotation, -XX:NumberOfGCLogFiles=32, -XX:GCLogFileSize=64m, -Des.path.home=/media/hyc/linux/elementary os/software/elasticsearch-6.7.1, -Des.path.conf=/media/hyc/linux/elementary os/software/elasticsearch-6.7.1/config, -Des.distribution.flavor=default, -Des.distribution.type=tar][2020-08-08T13:14:42,612][INFO ][o.e.p.PluginsService     ] [EsTp_cx] loaded module [aggs-matrix-stats][2020-08-08T13:14:42,613][INFO ][o.e.p.PluginsService     ] [EsTp_cx] loaded module [analysis-common]．．．．．．．[2020-08-08T13:14:51,868][INFO ][o.e.c.m.MetaDataIndexTemplateService] [EsTp_cx] adding template [.monitoring-alerts] for index patterns [.monitoring-alerts-6][2020-08-08T13:14:51,942][INFO ][o.e.c.m.MetaDataIndexTemplateService] [EsTp_cx] adding template [.monitoring-kibana] for index patterns [.monitoring-kibana-6-*][2020-08-08T13:14:52,103][INFO ][o.e.l.LicenseService     ] [EsTp_cx] license [08975435-c155-421d-ae6f-290e44abae0b] mode [basic] - valid后台启动1./bin/elasticsearch -d访问http://192.168.0.102:9200/1234567891011121314151617{  &quot;name&quot; : &quot;EsTp_cx&quot;,  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,  &quot;cluster_uuid&quot; : &quot;TKaUQKssQlODq4xfwdaQ1Q&quot;,  &quot;version&quot; : {    &quot;number&quot; : &quot;6.7.1&quot;,    &quot;build_flavor&quot; : &quot;default&quot;,    &quot;build_type&quot; : &quot;tar&quot;,    &quot;build_hash&quot; : &quot;2f32220&quot;,    &quot;build_date&quot; : &quot;2019-04-02T15:59:27.961366Z&quot;,    &quot;build_snapshot&quot; : false,    &quot;lucene_version&quot; : &quot;7.7.0&quot;,    &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;,    &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot;  },  &quot;tagline&quot; : &quot;You Know, for Search&quot;}kibana安装设置kibana环境变量12345sudo gedit /etc/profile 添加KIBANA_HOME=/home/tq/kibana-5.6.5-linux-x86_64PATH=$KIBANA_HOME:$PATHexport KIBANA_HOME运行kibana1./bin/kibana访问kibanahttp://127.0.0.1:5601安装head修改elasticsearch.yml，增加跨域的配置(需要重启es才能生效)12345vi /etc/elasticsearch/elasticsearch.yml# 加入配置:http.cors.enabled: truehttp.cors.allow-origin: &quot;*&quot;修改Gruntfile.js文件，修改服务监听地址（增加hostname属性，将其值设置为’0.0.0.0’）92行添加：hostname:’0.0.0.0’,12345678910111213141516171819202122232425watch: {&quot;scripts&quot;: {files: ['src/**/*', 'test/spec/*' ],tasks: ['default'],options: {spawn: false}},&quot;grunt&quot;: {files: [ 'Gruntfile.js' ]}},connect: {server: {options: {hostname:'0.0.0.0',port: 9100,base: '.',keepalive: true}}}});grunt.loadNpmTasks('grunt-contrib-clean');远程桌面rdesktop首先安装rdesktop工具1kevin@bobo:~$ sudo apt-get install rdesktop默认端口是3389注意：远程windows机器的服务中的 Terminal Servies 需要开启。即：我的电脑-右键-属性-远程中，勾选 允许远程用户链接到此计算机。针对rdesktop命令的参数解释：12345678910-f      全屏-a      16位色,可选，不过最高就是16位-u xxxxxx      登录用户，可选-p xxxxxx      登录密码，可选-r clipboard:PRIMARYCLIPBOARD     重要，剪贴板可以与远程桌面交互-z      压缩，可选-g 1024x768     分辨率，可选，缺省是一种比当前本地桌面低的分辨率-P     缓冲，可选-r disk:wj=/home/magicgod      映射虚拟盘，可选，会在远程机器的网上邻居里虚拟出一个映射盘，功能很强，甚至可以是软盘或光盘-r sound:off     关闭声音，当然也可以把远程发的声音映射到本地来。特别注意：rdesktop退出全屏模式 ：使用组合键ctrl+alt+enter进行切换；上面脚本执行后，发现全屏不了，明明已经加了-f参数，这是因为加了”-g 1024x768”的分辨率参数，去掉这个分辨率参数就可以实现全屏了！此外，Ubuntu除了上面的rdesktop命令进行远程连接操作外，还可以使用自带的Remmina进行ssh、VNC、RDP(连接windows机器)远程连接。迅雷123456需要软件：Ktorent, Amule安装软件：sudo apt-get install ktorrentsudo apt-get install amulechrome插件破解VIP会员视频集合Tampermonkeygitzip for githuboctotreeCNKI PDF Download其他技巧自动挂载磁盘查看磁盘属性12345678910111213hyc@hyc:~/Downloads$ blkid/dev/sda1: LABEL=&quot;M-fM-^AM-\&quot;M-eM-$M-^M&quot; UUID=&quot;C43C21073C20F5D8&quot; TYPE=&quot;ntfs&quot; PARTLABEL=&quot;Basic data partition&quot; PARTUUID=&quot;28719635-d995-4e4f-8842-d4dadf1de022&quot;/dev/sda2: UUID=&quot;D621-7B3A&quot; TYPE=&quot;vfat&quot; PARTLABEL=&quot;EFI system partition&quot; PARTUUID=&quot;2aa389ac-d040-4070-92bc-b6840310b686&quot;/dev/sda4: LABEL=&quot;local&quot; UUID=&quot;286C27576C271F56&quot; TYPE=&quot;ntfs&quot; PARTLABEL=&quot;Basic data partition&quot; PARTUUID=&quot;4ad76de5-bf50-4f27-8abf-8f880eb4b4c7&quot;/dev/sda5: UUID=&quot;A64A3E1F4A3DED25&quot; TYPE=&quot;ntfs&quot; PARTUUID=&quot;7e64d242-e149-4910-b732-ecb02266b48d&quot;/dev/sda6: UUID=&quot;2c1a9fc5-6017-4802-bb17-1b697eb4ba39&quot; TYPE=&quot;swap&quot; PARTUUID=&quot;a028e7db-32f0-4298-b259-cb0245a9151e&quot;/dev/sda7: UUID=&quot;03fe0c70-4d06-4023-94fb-d3af41cefee6&quot; TYPE=&quot;ext4&quot; PARTUUID=&quot;754efc8f-c0f9-40c6-8eae-bfe61b029ba6&quot;/dev/sdb2: LABEL=&quot;download&quot; UUID=&quot;5A26CFCB26CFA5FB&quot; TYPE=&quot;ntfs&quot; PARTLABEL=&quot;Basic data partition&quot; PARTUUID=&quot;ee7174a3-6cc8-407b-9a9f-ead9f3ce85f0&quot;/dev/sdb3: LABEL=&quot;software&quot; UUID=&quot;22BACF07BACED707&quot; TYPE=&quot;ntfs&quot; PARTLABEL=&quot;Basic data partition&quot; PARTUUID=&quot;8b742348-2ef4-429e-8a02-76a6e0f49f44&quot;/dev/sdb4: LABEL=&quot;qoding&quot; UUID=&quot;428AE1638AE153C7&quot; TYPE=&quot;ntfs&quot; PARTLABEL=&quot;Basic data partition&quot; PARTUUID=&quot;511c889e-d4ba-46b0-8df6-c9e360268abb&quot;/dev/sdb5: LABEL=&quot;student&quot; UUID=&quot;420EF5340EF52199&quot; TYPE=&quot;ntfs&quot; PARTLABEL=&quot;Basic data partition&quot; PARTUUID=&quot;3349a79c-d864-41ab-8bde-5f65172d9e5a&quot;/dev/sdb6: LABEL=&quot;linux&quot; UUID=&quot;702AA8C82AA88D22&quot; TYPE=&quot;ntfs&quot; PARTLABEL=&quot;Basic data partition&quot; PARTUUID=&quot;696e0d44-726d-455b-ae87-f55db96bd6d3&quot;/dev/sdb7: LABEL=&quot;entertainment&quot; UUID=&quot;009CAE0B9CADFB74&quot; TYPE=&quot;ntfs&quot; PARTLABEL=&quot;Basic data partition&quot; PARTUUID=&quot;78cf7f18-0257-45f8-8fb4-aa49567f54c1&quot;添加挂载项1234hyc@hyc:~/Downloads$ sudo gedit /etc/fstab在末尾添加，/media/hyc路径需要提前创建　qoding是挂载后的名字，自动会创建/dev/sdb4 /media/hyc/qoding ntfs defaults 0 0/dev/sdb6 /media/hyc/linux ntfs defaults 0 0ssh到服务器1ssh root@http://112.126.102.142:22退出远程用logout</content>
	</entry>
	<entry>
		<title>语法_基础</title>
		<url>/article/a3944886.html</url>
		<content>主要是对考研刘晓艳老师的书籍《不就是语法和长难句吗？》，进行笔记整理和学习语法基础英汉句法结构差异英语：讲究结构美，强调语法和句法的完整性和合理性汉语：讲究意境美，无拘无束，挥洒自如何谓主谓宾主语：动作的发出者谓语：动词宾语：动作的承受者例子我们 总是会 忽略 努力的 重要性。主语   (状语)   谓语  (定语)     宾语语法总览名词性从句，按照从句的类型，可以分为四种类型主语从句宾语从句表语从句同位语从句定语从句，按照先行词可以分为以下五种类型人物时间地点原因状语从句，根据引导词本身的意思，可以分为以下九种类型时间状语地点状语因果类原因状语目的状语结果状语条件状语方式状语比较状语让步状语其他句型可以归纳为以下四种强调句倒装句虚拟语气插入语词性_实词名词含义名词就是人或物的名称，如：Alice, dog, book, the Changjiang River分类名词可以分为普通名词和专有名词普通名词可数个体名词: 指单个人或物，book, doctor, dog集体名词: 指某类人或物的总称，people, committee, police, group不可数物质名词: 指物质和材料的名词，air, fire, water抽象名词: 指看不见摸不着的名词，confidence, evolution, intelligence, association专有名词指代人名、国家、地名、机构、组织的名词，首字母大写，Tony, China, the Changjiang River, WTO用法复数形式名词是否要使用复数，是根据具体的意思来确定的只有可数名词本身才有复数形式，而不可数名词本身没有复数形式，需要添加量词才能变复数可数名词复数的变化规律 规则变化1直接在词尾加-slamp-lamps、book-books规则变化2以s、x、sh、ch结尾，词尾加-es(巧记：上学吃少会饿死)bus-buses、box-boxes、brush-brushes、watch-watches特例: stomach-stomachs （ch发/k/的音）以f或fe结尾，把f或fe变成-vesleaf-leaves、wife-wives、life-lives、thief-thieves、shelf-shelves、half-halves、knife-knives、wolf-wolves、scarf-scarves特例: roof-roofs、belief-beliefs、chief-chiefs、cliff-cliffs、gulf-gulfs以辅音+y结尾， 把y变成-iesfactory-factories、city-cites、country-countries特例: 以“元音+y”结尾，词尾加-s以o结尾：1.有生命的加-es 2.无生命的加-spotato-potatoes、Negro-Negroes、tomato-tomatoes、hero-heroes、mosquito-mosquitoes不规则变化把a变成eman—men、woman-women特例: 1.man和woman修饰名词时，两个都变复数：man teacher一men teachers​         2.其他词修饰名词，最后一个变复数：boy student-boy students词尾加-enchild-children、ox-oxen以man/woman/child结尾的名词，把man/woman/child变复数fireman-firemen、chairwoman-chairwomen、grandchild-grandchildren内部oo变eefoot-feet、tooth-teeth、goose-geese特例: room-rooms、boot-boots单复数同形sheep, deer, means, Chinese, Japanese国籍的复数变化：中日不变，英法变(把a变e)，其他国籍按规则加-s/esChinese-Chinese、Japanese-Japanese、Englishman-Englishmen、Frenchman-Frenchmen、American-Americans特例: German-Germans(German不是由“Ger+man”构成的合成词，而是它一出生就长这样)集体名词的复数(谓语用复数)police, people, cattle, army, board, class, club, generation, crew, group特例: people翻译成”人们”是单复数同形，翻译成“民族”有复数形式，即peoples只以复数形式出现noodles, glasses, trousers, thanks, clothes, chopsticks, socks, shorts特例: chopsticks、shoes、trousers，这类词不能被具体数字修饰，不能说two shoes，但可以用表示复数的形容词及量词修饰：many shoes, two pairs of shoes不可数名词及其复数不可数名词就是无法用数量来计量的名词，这一类名词是物质名词和抽象名词物质名词液体: orange, milk, water, wine, juice, tea, coffee, soup食物: bread, meat, grain, sugar, chocolate, wheat自然: soil, sand, wood, weather, snow, wind抽象名词情感: joy, happiness, pleasantness, sadness学科: Chinese, Maths, Geography, Physics, Chemistry, English概念: advice, information, news, time, money, knowledge, music不可数名词的复数形式不可数名词不能加-s/es变复数，如果要表示复数形式，只能借助量词来表示，量词就是表示个体数量单位的词，量词的选择是根据语意来决定的不可数名词的复数结构是：数词+量词+of+不可数名词不可数名词后不能加-s/es如果要表示复数，只能加量词，把量词变复数(重要的事情再说一遍：不可数名词后不能加-s/es)a piece of paper - two pieces of paper、a piece of bread - two pieces of bread、a cup of tea - two cups of tea、a glass of water - two glasses of water区分可数和不可数I need some paper to write the information down. (paper翻译成“纸张”，不可数I have an academic paper to accomplish on this weekend. (paper翻译成“论文”，可数)The paper said that the spaceship has been sent to the space. (paper翻译成“报纸”，可数)Please notice the introduction of the food product on the box, because it will tell you how nutritious the foods are.以前初、高中老师经常说food是不可数名词，在这句话中第一个food，翻译成”食品”，是一个总称概念，属于不可数名词第二个foods翻译成”各种食物”，已经被具体化了，指的是”盒子里的食物”，所以它是一个可数名词这句话就翻译成：请注意盒子上食品的介绍，它会告诉你这些食物有多营养。一个名词到底是可数还是不可数，不是由这个名词的形式或者通过其孤立的意思决定的，而是根据这个名词在不同语境中的意义决定的简单来说，意义和语境的不同，就会导致名词的可数和不可数We do not have much money in our purse. We’ve just got a few dollars.这里的money是一个总称词，因为他没有说明是人民币，还是美元，还是欧元。money是不可数的，所以要用much来修饰。要说具体有多少钱，则用相应的可数名词来表达，此时修饰词也随之改变那么翻译就是：我们没剩下多少钱了，只有几美元词典中标有[C]的表示可数，标有[U]的表示不可数事实上，如果仅仅为了应付考试，即使分不清可数不可数，也不会有太大影响，因为可数名词原形不能单独使用，要么名词前加冠词，要么名词变复数。如果无法确定名词可数不可数，在名词前加定冠词”the”，也是可以的名词所有格什么是名词所有格？咱们还是先看看自己熟悉的汉语：我妈妈的房间如果你没有学过名词所有格，你可能会翻译成：my mother room，不好意思，你写错了，这句话翻译回去成了：我妈妈房间。相比原文少了一个”的”字，就不能够说明my mother和room的所属关系所以名词所有格就是用来表示名词之间的所属关系，在汉语中我们在名词后加”的”来表示所属关系，英语中我们用’s和of来表示‘s所有格基本构成方式可以分为以下四种直接在名词后加’smy son’s watch以s结尾，直接在名词后加’Teachers’ Day, parents’ meeting表示几人共同拥有某物，在最后的名词后加’s，此时“某物”这个名词用单数Henry and Tony’s room 亨利和托尼共有的房间(此时room用单数)表示几人各自拥有某物，分别在名词后加’s，此时”某物”这个名词用复数Henry’s and Tony’s rooms 亨利的房间和托尼的房间(此时room用复数)基本用法可以分为以下四种表示某人的某物many students’ eyesight表示时间、距离、国家、城市、团体、机构、学校、组织等的所属today’s news 今天的新闻the majority’s view多数人的意见the travel agency’s visit 旅行社的访问three days’ walk 三天的路程用于构成各种节日Valentine’s Day 情人节Children’s Day 儿童节Women’s Day 妇女节Mother’s Day 母亲节相当于名词性物主代词的用法Her English is just as good as Lily’s.of所有格汉语：我妈妈的生日英语：the birthday of my mother基本结构是N1+of+N2，与汉语的语序相反教堂的屋顶 —&gt; the roof of the church换句话说：汉语的语序是N1+的+N2，写成英语就是N2+of+N1中国地图 the map of China.歌曲的名字 the name of the song.比赛的结果 the consequence of the contest.动词实义动词实义动词是表示具体动作的词可以分为及物动词和不及物动词及物动词：后面直接加宾语，并且必须加宾语。比如：accept the idea，buy a book。不及物动词：后面不能直接加宾语，必须要加了介词以后才能加宾语。比如：I agree with you.那如何区分及物动词和不及物动词呢根据意思：eat、kiss动词+介词”的动词为不及物动词：arrive in、abound in看单词表死记硬背：V.：既可以作及物动词，也可以作不及物动词Vt.：及物动词Vi.：不及物动词连系动词状态：be动词(连接主语和表语，无意义，用于构成句子)感官：feel，sound，smell，taste，look变化：get,become，turn，grow，fall保持：keep，stand，remain，stay表象：seem，appear终止：prove情态动词含义情态动词是本身有一定的词义、表示说话人主观态度的词用法情态动词不能单独出现，后面必须和实义动词/系动词一起构成谓语分类…情态动词的完成时表示推测通过大雁忘带钥匙的故事，讲完以下四个表示推测的情态动词完成时must have done：一定做过某事needn’t have done：本没必要做某事，但是做了could have done：本能够做某事，但没有做，表示遗憾should have done：本应该做某事例子：大雁把家里的钥匙落在教室了。她到家门口后打不开门，所以只能一直蹲在门口，等到晚上10点多，加班的爸爸妈妈回来，她才进了家门。大雁发现钥匙落在教室后，她很懊悔。此时她可能会抱怨自己：I should have brought the key with me. （我本应该带钥匙的）那时的大雁除了在门外等待爸爸妈妈回来，还有别的解决方案却没做：He could have come back to school to get the key but he didn’t. (其实她本可以回学校拿钥匙的，可是她没有这样做）爸爸妈妈当晚回来发现大雁在门外，了解原因后，爸爸妈妈问她为什么当时不返回学校拿钥匙。爸爸妈妈认为若大雁当时返回学校拿了钥匙的话可能早就进去了。爸爸妈妈可能会说：You needn’t have stayed in front of the door for such a long time.   (其实你本不需要在门口等那么久的)关于为什么当时没有返回学校拿钥匙，大雁解释道：I thought the gate must have already been closed by that time.  (我想学校那时候一定关上大门了)助动词含义助动词就是帮助谓语动词一起构成否定、疑问、时态、语态等的词分类及用法be用法：帮助构成进行时被动语态举例：As the cartoon describes,a hen is making a promise.The story is claimed to be very instructive.do、did和does用法：一般现在时、过去时的否定和疑问强调谓语动词(强调谓语只能用do/did/does)倒装举例：The picture did not show us profound meaning.The picture did show us profound meaning.Only yesterday did he find out that his watch was doeshave和has用法： 帮助构成完成时举例：Republicans left in the recent Euro-elections have  forced him to eat his words and stand down.will用法：帮助构成将来时举例：Imitation of behaviors will mislead behavioral studies.所有的助动词和情态动词都能够帮助句子构成倒装，具体使用哪个助动词或者情态动词，要根据句子的时态和意思来决定Under the table was lying a half-conscious young man.  桌子底下躺着一个神志不清的年轻人。Only after things disappear will we cherish them.  人们只有到失去了才懂得珍惜。Nor did he let the disease stop him from living the kind of life he has always dreamt about.疾病没有使他放弃过上梦想中的生活。Under no circumstances should you lend Paul any money.  在任何情况下你都不应该借钱给保罗。Only by studying hard can students pass the exams.  只有努力学习，学生们才能通过考试。代词代词就是替代名词而出现的词例如：我爱这头熊，我爱它。它就是这头熊，这头熊就是它代词又分成人称代词、物主代词、反身代词、不定代词和指示代词人称代词含义人称代词指代替人和事物的名称，分为主格和宾格两种形式单数单数单数复数复数复数人称代词第一人称第二人称第三人称第一人称第二人称第三人称主格Iyouhe / she / itweyouthey宾格meyouhim / her / itusyouthem词义我你他 / 她 / 它我们你们他们用法主格用来作句子的主语、表语I often go shopping on Sundays.Are they from Japan?It’s he&lt;/u&gt;!宾格用来作及物动词或者介词的宾语Who teaches you English this year?We often write letters to her.当几个人称合在一起作主语时，排列顺序如下：单数：二三一，如：You, he and I are working at that computer company.复数：一二三，如：We, you and they are all from China.物主代词含义物主代词说明人和事物的所属关系，分为形容词性和名词性两种单数单数单数复数复数复数物主代词第一人称第二人称第三人称第一人称第二人称第三人称形容词性myyourhis / her / itsouryourtheir名词性mineyourshis / hers / itsoursyourstheirs词义我的你的他的 / 她的 / 它的我们的你们的他们的用法形容词性物主代词后面要跟名词，相当于形容词Is that your umbrella?I often go to see my aunt on Sundays.名词性物主代词相当于名词，既代替事物又表明所属关系。在句子中往往独立作主语、宾语或者表语，后面千万不可以跟名词This is your cup. Mine is on the desk.Your classroom is very big. What about theirs?反身代词含义反身代词表示谓语的动作与主语有关或者宾语补足语的动作与宾语有关单数单数单数复数复数复数反身代词第一人称第二人称第三人称第一人称第二人称第三人称词形myselfyourselfhimself / herself / itselfourselvesyourselvesthemselves词义我自己你自己他自己 / 她自己 / 它自己我们自己你们自己他们自己用法反身代词在句子中作宾语表示反射(指一个动作回到该动作执行者本身)Don’t play with the knife, you might hurt yourself. 不要玩刀子，那会割伤你的。反身代词在句子中作同位语表示强调(即用来强调名词或代词的语气)The story itself is good. Only he didn’t tell it well. 故事本身是好的，只是他没有讲好。不定代词不定代词指代不明确的人或物something(某事)，someone(某人)，somebody(某人)anything(任何事)，anyone(任何人)，anybody(任何人)nothing(没事)，none(没有人)，nobody(没有人)everything(一切)，everyone(每个人)，everybody(每个人)指示代词指示代词近指远指用法单数thisthat谓语用单数复数thesethose谓语用复数不需要查找代词的指代的情况It is conspicuous that he works so hard.It is accepted that ability is of great importance.It is important to learn English well.They found it very hard to learn English well.It was him that I met in the street.Because of more ways to keep fit, it points out,human’s life will be prolonged.在句①、②、③、④中，it作形式主语和形式宾语在句⑤中，it引导强调句型在句⑥中，it在插入语中形容词含义形容词就是用来修饰名词的词，等于给名词穿了一件衣服用法形容词作定语embarrassing scandals 尴尬的丑闻regular exercise日常锻炼I have something urgent to tell you. (注意：形容词修饰不定代词，需要放到形容词之后)形容词作表语，说明主语的特点、状态The fact is that most Chinese people are hospitable, courteous and helpful to the visitors.V.-ing 和V.-ed形容词如果说”书很有趣”，我们要用interesting: The book is interesting.而如果要说”他感兴趣”，我们则要用interested: He is interested.或者，我们把二者作定语，来修饰一个名词the interesting bookthe interested person短语①中，interesting表示”能够引起人们兴趣的书”，即”一本有趣的书”短语②中，interested表示”这个人对某个事物感兴趣”，即”一个感兴趣的人”从以上两个简单的例子我们看到interesting往往是与事物联系在一起的，而interested往往是与人联系在一起的由此推而广之，我们可以得出下列一般规律V-ing形容词主要是”某事物令人感到……”V.-ed形容词主要是”人(主观上)感到……”请比较：the exciting news和the excited boy短语①翻译成”令人激动的新闻”，短语②翻译成”激动的男孩”比较等级比较等级形容词的三个级别：原级、比较级、最高级原级：没有比较，即”和……一样”比较级：两者之间的比较，”比…….更……”最高级：三者及三者以上的比较，”最……”用法原级：与……一样/不一样肯定：由”as+形容词原级+as”或”as+形容词+名词+as”构成My mother is as energetic as a young man.否定：由”not as/so+形容词原级+as”构成Luckily, the weather is not so wet as it was yesterday.比较级和最高级规则变化变化规律原级比较级最高级变化规律一般在词尾加-er/estyoungyoungeryoungest以字母e结尾的词，加-r/stbravebraverbravest以辅元辅结尾，双写尾字母，加-er/esthothotterhottest以”辅音+y”结尾，把y变成-ier/iestshyshiershiest以y结尾的双音节词，把y变成-ier/iesthappyhappierhappiest其他双音节和多音节词，在词前加more/mostcarefulmore carefulmost careful不规则变化，须特殊记忆原级比较级最高级good / wellbetterbestbad / badly / illworseworstfarfarther / furtherfarthest / furthestoldolder / elderoldest / eldestlittlelessleastmany / muchmoremost基本用法 比较级：I am more handsome than you.You are uglier than I.最高级：I am the tallest in our class.You are the oldest among these people.形容词的比较级前可以用表示程度的词或短语来进行修饰：a bit, a little bit, a little, a lot, a great deal, any, even, far, by far(还可用在最高级的前面)，much, very much, no, rather, still和slightly等具体用哪一个根据汉语意思来选择，请看例句：Things are no better than before. 情况并没有比以前有所改善。He makes far fewer mistakes in spelling than before. 他犯的拼写错误比以前少多了。注意，下面这些词一般不用来修饰比较级：very, many, quite和fairly等比如不说: My girlfriend is very older than me.比较级+and+比较级 双重比较用来表示事物持续不断的变化，相当于汉语中的”越来越”The city is becoming more and more beautiful. 这座城市变得越来越美丽了。The weather is becoming colder and colder. 天气变得越来越冷了。the+比较级+主谓，the+比较级+主谓The more difficult the question is, the more careful we should be.The harder the youngsters work, the more prosperous the country will be.常考的than的比较结构a. more than  不仅仅是b. more..…than…  比……多；与其……不如说c. nothing more than  只不过是d. anything less than  毫不，绝不e. nothing less than  简直，就是f. better than  比.……更好g. rather than  而不是h. other than  除了i. less than  少于j. no less than  多于，不少于、简直是、与…一样，不比…差词性_虚词冠词含义冠词是限定名词，对名词起到泛指和特指作用的词This is one small step for man,one giant leap for mankind.该句话是当年阿姆斯特朗登月后说的，翻译是：这是对人类的一小步，是人类的一大步很明显语义不通，看出哪里不对了吗？问题在于man，前面不加冠词翻译成“人类”，加冠词翻译成“男人”所以这句名言其实少了冠词，正确写法应该是：This is one small step for a man,one giant leap for mankind.分类冠词分为不定冠词和定冠词不定冠词：a/an表示泛指，即不确定的一个定冠词：the表示特指，即特定的一个不定冠词a/an的区别不定冠词在元音音素前用an，通常是以a、e、i、o、u等元音字母开头的单词，如：an egg，an apple特例：元音字母开头不发元音的情况: a uniform、a university、a useful book for Europeans、a one-way street辅音字母开头发元音的情况: an hour、an honest boy(拼读时h不发音，o发元音)用法不定冠词a/an的用法表示“一个”I have a cat.表示“某一个”A man is waiting for you.定冠词the的用法表示特指The air in the room is stuffy. Please open the window. (特指“屋里的空气”)用于上文中提到的事物My father bought a dog just now,and the dog looks cute.用在世界上独一无二的事物或方位名词前面the universe 宇宙，the East东方用在序数词、形容词最高级及only前面It is the second country they will visit in Asia. 这是他们在亚洲第二个参观的国家。Autumn is the best season in Beijing. 秋天是北京最好的季节。He is the only student who didn’t pass the exam.他是唯一一个没有通过考试的学生。用在江河、湖海、山脉、岛屿等名词前the River Seine塞纳河用在乐器名词前play the piano 弹钢琴和某些形容词连用，表示一类人或事物，是复数概念the rich 富人，the young年轻人不用冠词的情况表示下棋、打球、语言、学科等play chess, play basketball, speak English前面有物主代词、指示代词、’s所有格等限定词时my cat，these advantages，Bill’s camera专有名词通常使用零冠词Lu Xi is a great Chinese writer.季节、月份、星期以及节假日(中国传统节日除外)等名词前Summer begins in June in this part of the country. 这个地区夏天从六月份开始。三餐饭菜的名词前面have supper 吃晚饭在有形容词修饰的物质名词、抽象名词前面This is the problem of great importance.特殊名词nature, society, space, man, history小结一月、季、星期、节假日、三餐、球类、学科、棋类等名词前不用冠词这幅画生动地描述了一只母鸡正在做出承诺。(1998)his picture vividly depicts a hen making a promise.这幅画生动地描述了一个美国女孩正在穿着中国的传统服饰，面带甜美的微笑。(2002)This picture vividly depicts an American girl wearing traditional Chinese clothes with a sweet smile.这幅画揭示了人生就像慢跑的过程，需要我们不断付出努力，永不止步。(2004)This painting reveals that life is like the process of jogging, in which one should make constant efforts and never stop.随着全球化进展，追星在青少年中非常流行。(2006)With the development of globalization,star-pursuing is very popular among teenagers.这幅漫画的目的是要告诉我们，行动很重要。(2017) The purpose of the cartoon is to tell us that action is important.副词分类时间: just, recently, before, already, finally, today地点: here, there, nearby方式: fast, hard, carefully, slowly程度: much, little, very, enough, particularly频率: usually, often, sometimes, never, always, hardly, constantly, frequently, seldom用法时间、地点副词可以放句首，也可以放句末。放句末时，先写地点，再写时间The boy read quietly over there all the afternoon.They encountered their teachers in the street yesterday.修饰形容词，放在形容词前He is an extremely excellent teacher.修饰动词，可以放动词前后，也可以放句末He works very hard.He speaks English very well.方式副词修饰动词，放句末Please read this book carefully.频率副词放在实义动词前，be动词后I sometimes have lunch at school.He is always nice to me.修饰整个句子，放句首Luckily, he survived in the accident.Actually, he is just telling the truth.比较等级副词的比较等级和形容词完全一样，只是形容词是修饰名词，副词的比较等级是倾向动作的比较主语+动词+副词比较级+than+比较对象He started two hours earlier than we did.He speaks English more fluently than he did before.as…as…和not so/as.as..He can work as well as me.the+比较级，the+比较级The more constantly we do exercise, the healthier we will be.介词含义用法</content>
	</entry>
	<entry>
		<title>语法_定语和定语从句</title>
		<url>/article/20c5ebba.html</url>
		<content>定语从句定语的成分定语就是修饰名词(短语)的成分，这与中文一致。她是一只善良的夜莺。【解析】”善良的”修饰”夜莺”。住在年轻人隔壁的那个女孩应该很漂亮。【解析】”住在年轻人隔壁的”修饰”那个女孩”。家境贫困的年轻人爱上了教授的女儿。【解析】”家境贫困的”修饰”年轻人”；”教授的”修饰”女儿”。其实只要在中文中听到”…的+名词(短语)”，”…的”就是修饰这个名词(短语)的定语成分能做定语的成分主要包括了以下几种：形容词(短语)作定语那只善良的夜莺最后死了。That kind nightingale died at last.这朵非凡的玫瑰花变成了深红色。The extraordinary rose turns dark red.名词(短语)作定语，此时的名词(短语)已经形容词化了夜莺的歌声能使这朵玫瑰花开放。The nightingale’s singing can make the rose bloom.或 The singing of the nightingale can make the rose bloom.切记写作时几乎每个同学都会忽略的知识点：可数名词通常不能单独使用，必须有修饰成分；如果单独使用，需要在名词前加the或a，或者用复数形式。学生爱上了教授的女儿。The student falls in love with the professor’s daughter.或 The student falls in love with the daughter of the professor.总结: 虽然名词(短语)修饰名词(短语)有两种写法，但是英文还是更倾向于写成of结构。介词短语作定语窗外的夜莺听到了年轻人的感叹。The nightingale out of the window heard the sighs of the young man.我只有和大海的浪花沫一样白的玫瑰花。I only have roses as white as the foam in the sea.分词、不定式(非惯语)作定语唱歌的夜莺最终因爱而死。In the end,the sing nightingale died because of love.这句话表述正确吗？显然，这句话有两个动词，记得简单句中重复过无数遍的东西吗?一句话只能有一个动词作谓语，那sing怎么处理呢？需要将其变成不是动词的形式。因为此处动作表示主动，所以sing后加-ing：In the end,the singing nightingale died because of love.单纯的夜莺一定是爱上了那个被女孩欺骗的学生。The innocent nightingale must have loved the student deceived by the girl.因为表示被动，所以deceive 后加-ed。从句作定语夜莺用生命换来的玫瑰花并没有帮助到那个学生。The rose which the nightingale exchanged with her life failed to help the student.定语位置毫无疑问，中文总是把定语放到名词前，但是英文不是这样，那英文中定语到底是放在名词(短语)前还是名词(短语)后呢?请回看前面定语的成分，你会发现一个规律，那就是”前小后大“，也就是说定语的位置取决于它的长短当一个单词修饰名词(短语)时通常放在名词(短语)前面当两个以上的单词修饰名词(短语)时通常放在名词(短语)后面但是，注意过去分词作定语，即使是一个单词通常也放在名词(短语)后。他是一个被抛弃的男人。 He is a man abandoned.构成与引导词构成成分：先行词[名词(短语)] + 引导词 + 分句定语从句的引导词按照先行词的种类，一共分成五类：当先行词是人时，引导词有：who, whom, whose当先行词是物时，引导词有：that, which, whose当先行词是时间时，引导词有：that, which, when当先行词是地点时，引导词有：that, which, where当先行词是原因时，引导词有：that, which, why在一般情况下，that都等同于which，这种分类的方法缩小了引导词可选的范围，比如说当先行词是人时，可以首先排除where例子He is the man __ loves me.He is the man __ I love.He is the man __ father is wealthy.以上三道题先行词都一样，相信大部分同学都能填正确。三道题的引导词分别是：who、who/whom和whose句1中先行词是人，只能从三个单词who，whom，whose中选择，那到底选哪个单词呢?让我们看看从句吧，显然从句中缺少主语。再看以上三个引导词who,whom，whose。who用作关系代词时，意思是…的人，那个…”他；她；他们”，一般只用来指人，在从句中可以充当主语或宾语；whom用作关系代词时与who同义，一般只用来指人，在从句中只能充当宾语whose用作关系代词时，意思是“那个人的，那些人的；那一个的，那一些的”，既可指人，也可指物，但只能在从句中充当定语句1中从句缺主语，所以填who；句2中从句缺宾语，所以填who和whom均可；句3中从句主系表都不缺，显然不可能填who和whom，那whose刚好作定语，修饰father例子I will never forget the day __ I met you.I will never forget the day __ we spent.先行词是the day，表示时间，所以引导词就是which/that/when，一般情况下that=which,以句4、5就有两个引导词可选，要么是which/that，要么是whenthat/which显然是代词，可以在从句中充当主语或宾语，when是副词，不能充当主谓宾句4中从句不缺主谓宾，不正好填 when吗，句5则不同，从句缺宾语，只能填 which/that例子You had better have some reason __ you are late.You had better have some reason __ sounds perfect.同上，句6填why，因为why是副词，而从句中不缺主谓宾。句7填which/that，因为从句中缺主语的成分所以，根据我们前面的论述，定语从句的引导词也可以按照引导词本身的词性或者按照引导词在从句中充当的成分分为三类：代词(在从句中充当主语或宾语的成分)：who, whom(在从句中仅能充当宾语的成分)，that，which副词(在从句中不能充当主干成分)：where, when, why形容词[在从句中只能充当定语成分，修饰名词(短语)]：whose特殊用法that引导的从句that 引导定语从句时，如果在从句中充当宾语，that 可以省略比如I enjoy the movie Zhao Wei directed，在名词movie和Zhao Wei之间就省略了that所以，当分析长难句时如果见到两个名词(短语)直接放在一起，很可能是省略了that的定语从句例子The potential evolution of today’s technology,and its social consequences,is dazzlingly complicated,and it’s perhaps best left to science-fiction writers and futurologists to explore the many possibilities we can envisage.该句子由and连接的两个并列分句构成。第二个分句中又包含一个定语从句we can envisage，省略了引导词that，用来修饰 possibilities。【参考翻译】现代技术的潜在发展和它产生的社会影响是极其复杂的，所以最好是把它 们留给科幻小说家和未来学家去发掘我们所能想象到的许许多多的可能性。A young man can decide on a likely spouse on his own and then ask his parents to arrange the marriage negotiations,or the young man’s parents may make the choice of a spouse,giving the child little to say in the selection,but a girl may veto the spouse her parents have chosen.该句子由or和but连接的三个并列分句构成在第一个并列分句A young man can decide on a likely spouse on his own and then ask his parents to arrange the marriage negotiations 中，又包含两个小的并列结构，由and连接在第二个并列分句the young man’s parents may make the choice of a spouse,giving the child little to say in the selection 中,the young man’s parents may make the choice of a spouse 是主干，giving the child little to say in the selection是现在分词短语 作结果状语，修饰主干第三个并列分句a girl may veto the spouse her parents have chosen包含一 个省略了引导词that 的定语从句her parents have chosen，引导词that在从句中作宾语，因而可以省略，用来修饰spouse【参考翻译】年轻男士可以自己决定合适的配偶，然后请父母安排婚姻协商事宜；或者父母也可能为儿子择好配偶，在选择中不给儿子留有说话的余地；但是女方却可以拒绝她父母所选择的对象。限制性和非限制性提起它们的区别，大家能想到的就是”,”；或者是限制用that，非限制用which，其实它们最大的区别不在于此I love teacher Liu who is beautiful.I love teacher Liu, who is beautiful.句1翻译成：我爱漂亮的刘老师。句2翻译成：我爱刘老师，她挺漂亮的。【解析】能看出什么区别？句1暗含的意思：如果有一天我老了或被毁容了，你就不再爱我了。句2则不同：即使我不美了，你仍然爱着我，因为你爱的不是我的美，而是我。再比如下面这个例子A nurse is a person who looks after patients.Tom’s father, who is over sixty, still works very hard.句1翻译成：护士是照顾病人的人。句2翻译成：汤姆的父亲依然很努力工作，他已经年过六旬了。句1是一个限制性定语从句，这个定语从句如果去掉了就翻译成：护士是一个人，那很明显没有意义句2是一个非限制性定语从句，定语从句可以去掉，因为它仅仅起到一个补充说明的作用，相当于插入语。去掉定语从句后依然有意义，可翻译成：汤姆的父亲依然很努力工作。小结限制性定语从句和非限制性定语从句的区别限制性定语从句：定语从句紧跟在先行词后，对先行词进行修饰，不能省略，省略后意思不完整。非限制性定语从句：对前面的先行词或者整个句子进行补充说明，即使省略也不影响总体意思，有逗号隔开所以非限制性定语从句的特点有两个：第一，可以省略；第二，起到补充说明作用| 从句及区别点               | 限制性定语从句               | 非限制性定语从句                                   || ————————————— | —————————————— | ————————————————————————— || 与先行词的关系             | 关系紧密，删除后影响句子意思 | 影响关系疏松，起补充说明作用，删除后不影响主句意思 || 逗号的作用                 | 不用逗号                     | 用逗号                                             || that的作用                 | 可以用that                   | 不可用that                                         || which/whom在从句中作可省略 | 可省略                       | 不可省略                                           || 是否可以修饰句子           | 不可以                       | 可以                                               || 翻译时的区别               | 常译为定语                   | 常译为两个独立的句子                               |非限制性定语从句的引导词非限制性定语从句的引导词不能用that，但选择方法和限制性定语从句的引导词是一样的，当先行词是人或物的时候，引导词要用who/which。如：Mr.Smith, whom I have learned a lot from, is a nice person.Mr.Smith是人，且在从句中作宾语，所以引导词用whom。史密斯先生是一个好人，在他身上我学到了很多。He said he had climbed up the top of the Everest, which I suspect very much.本句中的非限制性定语从句是对前面的句子进行补充说明，所以引导词用which。他说他爬上了珠穆朗玛峰之巅，我对此表示十分怀疑。My gardener, who is very pessimistic, says that there will be no apples this year.先行词是人，本该用who/hat,但这是一个非限制性定语从句，只能用who,而不能用that。我家的园丁非常悲观，他说今年将不结苹果。非限制性定语从句的翻译方法如果修饰前面的名词，常常翻译成两个独立的句子。如：Edison,whose inventions have been over 2,000, is a famous scientist.爱迪生是一个伟大的科学家，他的发明超过了两千项。Above the trees are the mountains, whose magnificence faithfully reflects on the surface ofthe river.树林的上方是山，其壮丽的景色完全映照在河面上。These apple trees, which I planted three years ago, have not borne any fruit.这些苹果树是我三年前栽的，还没有结过果实。France,which prides itself as the global innovator of fashion, has decided its fashion industryhas lost an absolute right to define physical beauty for women.法国一直以全球时尚革新者的身份自居，但现在它认为，其时尚行业已失去定义女士形体美的绝对权力。如果引导词是对前面句子进行补充说明并且充当从句主语时，常常翻译成”这；它；那”；充当其他成分时，则要根据汉语的语言习惯来翻译。The picture is fairly thought-provoking, which intends to convey to us this message.which引导的是一个非限制性定语从句，which代替前面整个主句的内容，充当从句主语，起到补充说明的作用，因此应该根据汉语的语言习惯，将whch翻译成：它。这幅图片很令人深思，它想向我们传达这个信息。The two policemen were completely trusted, which in fact they were.which引导的是一个非限制性定语从句，which代替前面整个主句的内容，起到补充说明的作用，表示这件事确实如此。which在从句中不作主语，而是表语。因此这个时候就不能翻译成”这；它；那”，应该根据汉语的语言习惯来翻译，使句子通顺。那两个警察完全受到信任，事实上也确实如此。He was little tense in the presence of so many people, which was understandable.which引导的是一个非限制性定语从句，which代替前面整个主句的内容，充当从句主语，起到补充说明的作用，因此应该根据汉语的语言习惯，将wich翻译成：这。他在那么多人面前有点紧张，这是可以理解的。He helped popularize the idea that some diseases not previously thought to have a bacterial cause were actually infections, which aroused much controversy when it was first suggested.which引导的是一个非限制性定语从句，which代替前面整个主句的内容，充当从句主语，起到补充说明的作用。因此应该根据汉语的语言习惯，将which翻译成：这一观点。他帮助普及了一种观点，即某些以前被认为不是由细菌导致的疾病实际上也是传染病，这一观点在首次提出时就引起了很多争议。  介词+关系代词引导的定语为什么会有”介词+关系代词”引导定语从句在以下情况下的定语从句为了使得句式结构优美整洁，可以将介词提前到先行词前，构成了”介词+关系代词”引导定语从句。关系副词在定语从句中作状语时，可以用“介词+关系代词”代替关系副词，但作宾语时不可以替换This is the city where I have lived for 20 years. = This is the city in which I have lived for 20 years.when：所有介词+关系代词where：所有介词+关系代词why：当先行词是reason时，只能用for which体说到介词根据句子的意思来选择定语从句的谓语是固定搭配的”动词+介词”，并且先行词在从句中作宾语。如：This is a subject about which we might argue for a long while.注意固定搭配argue about，翻译: 这是一个我们可能要长时间争论的问题。用于表达与上文中的先行词或者句子的关系，并避免下文与上文的重复。如：Plying basketball is a beneficial exercise from which we can obtain many advantages.which指代前面的篮球运动，如果再写一遍这项运动，就会造成重复。打篮球是一项有益的运动，我们可以从中得到诸多好处。介词+关系代词中代词的选择修饰人时，只能用whomHe is the student with whom my teacher talked.修饰物时，只能用whichThis is the hotel at which Mr.Jason likes to stay.whose用关系代词替换，是of which/whomMr.Liu criticized the students whose/of whom homework hasn’t been handed in.We live in a house whose/of which windows open to the south.如何选择介词 (先行词 = 定语从句引导词)根据从句中谓语动词与介词的搭配，并且要判断”动词+介词”与先行词的逻辑关系The girl for whom he had fought 2 years seems no longer important to him.从句的谓语动词构成的搭配是ight for sb.（为…而斗争)，所以用for whom.。这个他已经为之奋斗了两年的女孩，似乎对他不那么重要了。I have bought many things on which I spent most of my money that I saved.从句的谓语动词构成的搭配是spend money/time/.energy on sth.（在…上花费金钱/时间/精力)我用节省下来的大部分钱去买了很多东西。根据从句中动词与先行词的逻辑关系This is the farmyard on which my father has worked for 20 years.看动词work和先行词farmyard的逻辑关系，应该表示在农场上面工作，而不是说在农场里或者农场下面工作，所以选择的介词是on这就是我父亲工作了二十年的农场。This is the roof on which people often enjoy the moon.看动词enjoy和roof的逻辑关系，应该是在屋顶上欣赏，所以选择的介词是on这就是人们经常站在上面赏月的那个屋顶。根据”形容词+介词”与先行词的逻辑关系来选择English is the subject in our school in which every student is interested.注意搭配be interested in,将in提到关系代词前。英语是我们学校每个学生都很感兴趣的课程。He referred me to some reference books with which I was not very familiar.注意搭配be familiar with,将with提到关系代词前。他要我去查看一些我不是很熟悉的参考书。根据先行词与句子的意思来确定The colorless gas without which we can not live is called oxygen.这一情况比较难，需要我们有良好的语言基础和逻辑分析能力，首先我们看一下，先行词是colourless gas无色气体；定语从句的意思是：我们不能活下去。那么它们的逻辑关系应该是：没有这种气体，我们就不能活下去。所以选择介词without这种无色气体称为氧气，没有它我们就活不下去。This is the classroom in the front of which there is a desk.先行词是classroom，定语从句的意思是：有一张讲台，逻辑关系很明显，即在教室前面有讲台，这是一种常识，所以选择介词短语in the front of在教室前面有一张讲台。介词+关系代词的翻译方法找出关系代词的指代将介词与指代的内容还原到定语从句的谓语后将主句和定语从句按字面意思翻译调整语序，使主从句符合汉语的表达习惯，保持句子通顺All of us work through problems in ways of which we’re unaware.第一步：找到关系代词which指代的是ways第二步：还原”介词+关系代词”与定语从句：we’re unaware of ways第三步：将定语从句放到先行词前翻译，即：我们不知道的方法第四步：调整语序，使句子通顺：”我们不知道的方法”非常拗口，我们可以这么翻译：无人所知的方法我们所有人都通过这种无人所知的方法来解决问题。固定搭配引导的从句the same … as所引导的定语从句的从句内容与先行词同类而不同物，翻译成“像…样的”the same … that表示所引导的从句内容与先行词同类同物，即同一事物，翻译成”正是…”Mr.Darcy is the same man as I love.Mr.Darcy is the same man that I love.句1翻译成：我喜欢的是像达西先生一样的男人。(表明我喜欢的只是像达西先生这样的男人而已，但不是喜欢达西先生，先行词与定语从句内容都是同类，都表示人，但不是同一个人)句2翻译成：我爱的男人正是达西先生。(表明我爱的男人正是达西先生，先行词与定语从句内容一致，指同一个人)He is using the same camera that I lost a few days ago.他正用着我前几天丢的那台相机。(暗含：他偷了我的相机)He is using the same camera as I lost a few days ago.他正在用的相机和我前几天丢失的那台很像。(暗含：他用的相机不是我丢的那一台，只是长得像而已)as和which引导的从句相同点两者都可以引导非限制性定语从句不同点as引导非限制性定语从句，位置随便放(as引导的从句可位于主句前、中、后)，翻译成”正如”；which只能放在所修饰的句子的后面As we all know, Man struggles upwards, and water flows downwards.Man struggles upwards and water flows downwards, which is known to us all.​    众所周知，人往高处走，水往低处流。紧跟as后的动词是be动词，而which后可以用任何类型的动词As is known to the United States, Mark Twain is a great writer.美国人都知道，马克·吐温是一位伟大的作家Immigrants are quickly fitting into this common culture, which may not be altogether elevating but is hardly poisonous.移民们很快适应了这里的大众文化，它虽然可能不完全引人向上，但至少不会毒害百姓引导词which知that的区别两者都可指物，常可互换。其区别主要在于：用which的情况引导非限制性定语从句The current,.which is very rapid,makes the river dangerous.水流湍急，使这条河很危险。The London team, which played so well last season, has done badly this season.伦敦队上一个 季度打得很好，这个季度却打得很差。放在介词后作宾语时，通常要用whichThe documents for which they were searching have been recovered.他们找寻的文件已找到了。He had only the long nights in which he could study.他只能利用漫长的夜晚来学习。用that的情况当先行词是不定代词much, little, none, all, few, every(thing), any(thing), no(thing)等时I did nothing that might hurt you.我没做任何可能伤害你的事。The sleeping man’s subconscious mind retained everything that was said around him.酣睡的人的潜意识能够记住周围的人说的一切。当主句以here, there开头时Here is the hotel that you’ve been looking for.这就是你一直在找的那个旅馆。There is a room in the building that is still free.大楼里还有一个房间是空的。当先行词被the only, the same, the very等限定词修饰时When people talk about Chongqing, the first that comes to mind is the Hotpot.人们一说到重庆， 第一个想到的就是火锅。This is the first film that has been shown in our school this term.这是我们学校本学期放映的 第一部电影。当先行词被any, few, little, no, all, much, some等词修饰时You can take any seat that is free.任何空着的座位你都可以坐。There is little work that fits for you.这里几乎没有适合你的工作。当先行词被序数词或形容词最高级修饰时This is the most interesting story book that I have ever read.这是我读过的最有趣的一本故事书。I do remember the first time that I had ever heard the sweetest voice in the world.我很清楚地记得我第一次听到世界上最美的声音的那一刻。当先行词是疑问代词who, which, what或主句以这些词开头时Who that has ever worked together with him doesn’t admire him?曾经和他一起工作过的人谁不钦佩他？Which is the star that is nearer to the earth?哪个是离地球比较近的恒星？当先行词既有人又有物的时候She likes taking pictures of those people and things that she is interested in.她喜欢给她所感兴 趣的人和物拍照。They are talking of the heroines and their deeds that interest them.他们正在谈论他们感兴趣 的女英雄及她们的事迹。区别定语从句和同位语从句通过下面的例子来讲解定语从句和同位语从句的区别。I had a dream that was definitely sweet.I had a dream that I would become a rich lady someday.先确定，第一个是定语从句，第二个是同位语从句：然后讨论两个从句的相同点和不同点。相同点：构成相同，都是””先行词+引导词+分句”。不同点：第一，首先看引导词that，定语从句的引导词that在从句中要充当主语或宾语，而同位语从句的that在从句中不作任何成分，也没有任何意思。第二，看从句本身的意思。句1的意思：我做了一个很美的梦。你知道我做的是什么梦吗？只知道是美梦，美梦太多了，你并不知道我具体做了什么梦。句2的意思：我做了一个有一天我会变成有钱人的梦。你知道我的梦了吗？显然知道，梦就是：我会变成有钱人。​    所以，第二个区别是定语从句是对前面名词的修饰，而同位语从句是对前面名词的解释。第三，看先行词，定语从句的先行词可以是任何名词，而同位语从句的先行词只能是可以解释的名词一抽象名词。第四，最后一个区别，虽然不够严谨，但是也可以作为参考。那就是定语从句的引导词有很多，而同位语从句的引导词大部分都是that(因为同位语从句大部分都是陈述句)。</content>
	</entry>
	<entry>
		<title>语法_状语与状语从句</title>
		<url>/article/1fb67a92.html</url>
		<content>引语形容词(短语)放在系动词后面作表语，例如：He becomes tough these days.放在宾语后面作补语(此用法不太常见)，例如：The reality makes him tough these days.放在名词(短语)前面作定语，例如：He has become a tough man these days.总结我们发现了一个规律，不论形容词(短语)作什么成分，都是围着名词(短语)/代词转，也就是说形容词(短语)无论作什么成分都是修饰名词(短语)/代词的。Liu is honest. 此时的honest修饰主语Liu。Liu can make me happy. 此时的happy修饰宾语me。Liu is an honest woman. 此时的honest修饰woman。副词(短语)副词(短语)在本文中还没有提到过，或从未专门提到过，这里就来正式进行讲解。He smiles sweetly. 副词sweetly修饰实义动词smiles。He looks pretty sweet. 副词pretty修饰形容词sweet。He smiles especially sweetly. 副词especially修饰副词sweetly。Luckily,he passed the examination. 副词Luckily修饰整个句子。副词(短语)可以修饰实义动词(词组)、形容词(短语)和整个句子，副词甚至还可以修饰副词。副词在句中作状语，并且副词(短语)只能作状语。状语含义在一个句子中，用来修饰实义动词(词组)、形容词(短语)、副词或整个句子的成分，叫作状语。成分副词：She smiles sweetly.副词短语：I tried again and again.介词短语：He runs fast like a crazy dog.分词、不定式：He leaves, crying.从句：I will return the book as soon as I have read it.位置位置状语的位置极其灵活，可以随意放置在句中，但是放在句首或句末的情况居多，如果放在句首的话，通常用逗号隔开。比如：In Chongqing, I am now working.I am now working in Chongqing.I am now in Chongqing working.状语从句引导词状语从句可以放在主句之前，也可以放在主句之后，所以状语从句的构成是：引导词+状语从句+”,”+主句(状语从句放句首时，必须加逗号)或主句+引导词+状语从句(状语从句放在主句后，可以加逗号，也可以不加逗号)名词性从句是按照从句所充当的成分来进行分类的，可分为四类；定语从句的引导词是按照先行词的种类来进行分类的，可分为五类。那状语从句的引导词应该按照什么标准来分类呢？If引导什么状语从句?——条件When引导什么状语从句？——时间Because引导什么状语从句?——原因由上述示例，同学们大概已经知道了状语从句引导词的分类标准，那就是：按照引导词本身的意思进行分类，一共分为九类：时间状语从句、地点状语从句、原因状语从句、目的状语从句、结果状语从句、条件状语从句、让步状语从句、方式状语从句和比较状语从句。就是因为状语从句的引导词是按照引导词本身的意思来进行分类的，所以状语从句可以说是最简单的从句，每个引导词都有确定的意思。比如翻译这个中文句子；因为我爱你，我要一生都陪着你。”因为”只能翻译成because、since等表示原因的词，而不能翻译成as if(好像)等其他不表示原因的词。时态一般情况下，在状语从句中，当主句表示过去的时候，主句与从句都可以使用相应的过去时态。如：Before I went home,my mother had slept. 在我回家之前，我的妈妈已经睡了。在这个句子中，两个动作都是发生在过去，理应使用相应的过去时，但是主句的动作发生在从句之前，因此主句应使用过去完成时。当主句表示将来的时候，主句使用一般将来时，从句使用一般现在时，用”现在”表示”将来”。例如：I will treat you if I manage to pass the examination of postgraduates.如果我考研成功，我就请你吃饭。这是由if引导的条件状语从句，主句表将来，但从句中的谓语动词应该用一般现在时，绝不可用will manage。当主句表示将来完成的时候，主句使用将来完成时，从句要使用现在完成时。如：As soon as I have finished this work,I will have gone home.如果我早完成工作的话，我现在已经到家了。这种用法很少见到，同学们不需要过分在意。该从句中的谓语动词用现在完成时have finished 来表示将来完成，绝不可用will have finished。九大从句时间状语从句时间状语从句的引导词：普通引导词：when(在…时)，as(正当/随着)，while(在…期间)，once(一旦)，as soon as(…就…)，before(在…之前)，after(在…之后)，since(自从…以来)，not … until(直到…才)，until/till(直到…时)名词短语：the moment(一…就)，every time(每当)，the day(那一天)，the instant(当…的时候)副词：immediately, directly, no sooner … than, hardly … when, scarcely … when(这五个意思相同，都译为”一…就”)when，while和as的区别when引导的从句的谓语动词可以是延续性动词，也可以是短暂性动词，通常表示”当…时”。例如：When the teacher came in,we stop talking.当老师进来时，我们都不说话了。(短暂性动词)When I lived in the countryside, I used to live a tough life.当我住在农村时，我过着艰苦的生活。(延续性动词)while引导的从句的谓语动词必须是延续性的动词，多用于进行时态，表示”在…时”，强调时间段的持续性，也可强调主从句的对比。例如：While my mother was reading the newspaper,I was watching TV.当我妈妈在看报纸时，我正在看电视。(was reading是延续性的动词，was reading和was watching同时发生。)I like playing football while you like playing basketball.我喜欢踢足球，而你喜欢打篮球。(对比)as引导从句的谓语动词是延续性的动词，表示”一边…一边”，一般用于主句和从句动作同时发生；as也可表示””随着…”。例如：We always sing as we walk.我们总是边走边唱。(as表示”一边…一边”)As families moved away from their stable community,the informal flow of information is cut off.随着家庭离开他们稳定的社区，这种非正式的信息流就被切断了。(as表示”随着…”)no sooner … than和hardly … when引导的时间状语从句，含义：表示”一…就…”时态：主句用过去完成时，从句用一般过去时。如：I had no sooner begun to take a shower, the water was unavailable. 我一开始洗澡，就停水了。They hardly had arrived at the bus stop when the bus started to leave. 他们一到车站，车就开走了。高级用法：no sooner … than和hardly … when引导时间状语从句时可以使用倒装句式，上述句子可改成：No sooner had I begun to take a shower, the water was unavailable.Hardly had they arrived at the bus stop when the bus started to leave.when引导时间状语从句的特殊用法：When I was a little girl, I loved Jeff so much.When引导时间状语从句，如果从句和主句主语一致，并且从句的谓语中有be动词，此时从句的主语和be动词可以一起省略省略后是：When a little girl, I loved Jeff so much.但是When I loved Jeff, I could sing all his songs. 这句话就不能写成上面的形式，因为从句的谓语中没有be区分until和not … untilI will wait here until you come.I will not leave until you come.其实not … until并没有翻译成否定的意思它们的区别在于主句的谓语动词，如果主句的谓语动词是短暂性动词，就用not … until；如果是延续性动词，就用until例子：直到我有了自己的女儿，我才意识到母亲的伟大。I did not realize the greatness of mothers until I had my own daughter.realize是短暂性动词，所以用not.…until地点状语从句常用引导词：where不常用引导词：wherever，anywhere，everywhere等where引导的地点状语从句与定语从句的区别和转换：区别地点状语从句和定语从句的引导词都有where，所以非常容易混淆。两种从句的翻译方法是一样的，但两者在语法意义上是天差地别的区分这两种句子最关键的就是看where是否指代前面的先行词，如果指代前面的先行词就是定语从句，否则是地点状语从句当where引导地点状语从句时，where是从属连词，从句修饰主句的谓语动词，where前边没有表示地点的先行词。如：I find my phone where I had lost it. 我在丢手机的地方找到了我的手机。where引导的句子修饰find这个动词，说明在哪里找到的，虽然从句前有名词，但不是表示地点的，只是谓语动词的宾语。当where引导定语从句时，从句修饰先行词，where是关系副词，代替表地点的先行词在从句中作地点状语。如：This is the house where I lived two years ago. 这就是我住了两年的房子。where引导的从句修饰house这个名词，表示是个什么样的房子—我居住了两年的房子。再看刻舟求剑的故事：The man made a mark where he has left the swoop. (地点状语从句)The man made a mark at the place where he has left the swoop. (定语从句)两句话都翻译成：这个人在丢剑的地方做了标记。两个句子的中文翻译是一样的，但是英文的语法结构非常不同虽然两个句子中where前面都有名词，但明显句2定语从句修饰的是表示地点的先行词the place，起修饰限定作用，表示”在丢剑的这个地方”做标记句1中，主句的动作发生在”丢剑的地方”，与”标记”没有关系，反而是”做标记”与从句有关，即在什么位置做标记因此，句1的从句是修饰动词词组make a mark的。转换：一般在状语从句前加上”in/at the+具体地点”，就变成定语从句。如：我们将从上次停下来的地方开始。状语从句：We will start where we left off.改写为定语从句：We will start at the point where we left off.Everywhere I see rural laborers in the city,it will remind me of my old father.每当看到城市里的农民工，我就会想起我的老父亲。Wherever you go,whatever you do,I am just here waiting for you.无论你去哪里，无论你做什么，我都在这里等你。这种花在潮湿的地方生长得很好。The flower grows well where it is moist.其实很多时候，我们不会把这句话写成状语从句，通常都是写成介词短语作状语。所以，上面的句子通常会写成：The flower grows well in moist places.原因状语从句常用引导词：because, since, as, for等。不常用引导词：seeing that, now that, in that, considering that, given that, as much as, so muchas虽然不常用，但是和常用的引导词相比，用法没有太大区别需要注意的是有一些介词短语，也翻译成”因为”，但它们后面只能跟名词(短语)，不能跟从句：because of, due to, owing to, thanks to, for the sake of, as a result of例子：因为他粗心，他昨天出了一场车祸。Because he was careless, he had a car accident yesterday.或Because of his carelessness, he had a car accident yesterday.需要注意的是在这批表示”因为”的介词短语中，due to通常放在句末，不放在句首；而owing to通常放在句首，不放在句末；其他的短语则可前可后。如：Owing to tolerance between the couple,the marriage can last long.The marriage can last long due to tolerance between the couple.目的状语从句常用引导词：sothat，in order that常见的动词不定式也可以表示目的：to，in order to，so as to等，后跟动词原形。为了能有一个更美好的未来，我决定考研。I am determined to take part in the examination of postgraduates so that I could have a brighter future.或 I am determined to take part in the examination of postgraduates in order to have a brighter.为了不让儿女担心，慢慢老去的父母总对他们说着各种各样的谎话。Parents getting old gradually are always telling a variety of lies in order that their daughters or sons will not worry about them.或 In order not to let their children worry about them, parents geting old gradually are always telling a variety of lies.用in order to时一定要保持前后主语一致，主语不一致时只能用in order that，比如：He works hard in order that his family can live well. 就不能用in order to作目的状语结果状语从句常用引导词：so that, so … that, such … that不常用引导词：to the degree that, to the extent that, to such a degree thatso that，so … that和such … that的区别so that既可以引导目的状语从句，也可以引导结果状语从句。引导目的状语从句时， 会和情态动词一起连用；引导结果状语从句时，则不和情态动词连用目的状语从句：I hurried through my work so that I could be on time for the TV.我匆忙完成工作以便准时收看电视节目。结果状语从句：We moved to the country so that we were away from the noisy city.我们搬去郊外，这样便远离了喧闹的城市。so … that的用法so+形容词/副词+thatThe movie was so boring that I gave up watching it halfway through.这部电影太乏味了，以至于我看到一半就不想看了。so+many/much/few/little+名词+thatHe got so little money that his family had to live on welfare.他挣钱很少，以至于他的家人不得不靠救济金生活。so+形容词+a/an+可数名词单数+thatLiu Xiaoyan is so nice a teacher that everyone loves and respects her.刘晓艳是如此好的一个老师，以至于每个人都敬爱她。such … that的用法such+a/an+形容词+可数名词单数+thatIt was such a terrible show that none of us would pay for watching it again.这场演出太糟糕了，以至于我们都不会再花钱去看了。注意：s0.…that 和such..that两个结构可以互换，但要调整冠词的位置。such+形容词+可数名词复数/不可数名词+thatThey are such adorable babies that we all love them.这些婴儿都很可爱，我们都很喜欢他们。This was such wonderful music that it deserved replaying.这种音乐很好听，值得单曲循环。条件状语从句常用引导词：if，unless(表示if not)特殊引导词：as/so long as(只要), only if(只要), providing/provided that(假如), supposing(that)(假如), in case that(以防), on condition that(如果)只要有一丝希望，我就不会放弃。I will never give up so long as there is a ghost of hope.如果爱是一杯毒药，我也愿意一饮而尽。On condition that love is a cup of poison,I will drink it up without hesitating.事实上，条件状语从句经常会与虚拟语气结合使用让步状语从句常用引导词：though, although, even if, even though等特殊引导词：as, while(作“虽然”之意讲，用在句首), no matter … (无论), in spiteof the fact that(虽然，尽管), whatever, whoever, wherever, whenever, however, whichever等让步状语从句绝对是作文中的经典表达，就像我们虽然只穿一件很普通的衣服，但是如果佩戴了一条很华丽的丝巾，瞬间就能衬托出我们的气质当我们的表达过于绝对的时候，用上表示让步的引导词，如although，效果就会变得很好他虽然经历了多次失败，但依然坚持梦想。Although/Though he failed again and again, he still adheres to pursuing his dream.虽然肝炎患者痊愈了，但仍须对其进行定期的健康检查。The periodical physical examination must be given to a patient whoever has recuperatedfrom hepatitis.自动化机器虽然有很多优点，但它们只能干人们吩咐它们要干的事。As Automatic machines have many advantages, it can only do the jobs they have beentold to do.刘是一个美女。若写成”Liu is a beauty.”则显得单调，再美的人也会有缺点Liu is a beauty although she has a pair of little eyes.盲目追星会浪费我们很多时间和精力。我们不能一下子全部否定明星，他们也一定有不少优点。Pursuing stars blindly will waste a lot of time and energy although they may workhard or be quite handsome.污染很严重。Pollution becomes increasingly serious although a sea of adults fail to realize it.总结事实上，在作文中，几乎每句话的后面都可以加一个由although引导的让步状语从句如果分析原因时，写出来的句子比较短，就可以在后面加一个由although引导的让步状语从句来拉长句子如：宠物的确给我奶奶带来了很多快乐，虽然照顾它们需要时间和精力。Pets indeed bring much delight to my grandma although taking care of them needs time andenergy.方式状语从句常用引导词：as, as if, asthough(好像，似乎)特殊引导词：the way(方式)除此以外，有些介词(短语)也表示方式：by, through, in … way, in … manner(以…的方式)她和我说话的样子好像她是我妈一样。Shetalks with me as she were my mother.通过一种冷静的、客观的方式，他破解了这个密码。Thepassword was decoded by him in a dispassionate and objective manner.这一切我都记忆犹新，仿佛就是昨天发生的事一样。I can still remember the whole thing in great detail as though it were yesterday.他们谈话的那股亲热劲就如同是多年的老朋友似的。They are talking as if they had been friends for years你希望人家怎样待你，你就要怎样待人。Always do to the others as you would be done by.当宇宙飞船以惊人的速度离开地球时，宇航员们觉得自己似乎被紧压在宇宙飞船的底壁上。When the spaceship leaves the earth at a tremendous speed, the astronauts feel as ifthey were being crushed against the spaceship floor.比较状语从句常用引导词：as, than特殊引导词：the more … the more … , more than, more … than … , not so much … as …The youth of today are better off than we used to be.今天的年轻人的境况比我们过去要好。She is not so energetic as she used to be.她不像过去那样充满活力了。The result of this experiment is better than that of the last one.这次实验的结果要比上次好。巩俐非常优雅。Gong Li looks as graceful as a red rose, who can take many people’s breath away.英语很重要。English proves as essential as air and water, which plays a key role in our daily life.污染很严重。Pollution becomes as serious as earthquakes, which has threatened our existence or even taken people’s lives away.分词作状语先写两个句子来引出该语法点如果冬天来了，春天就不远了If winter comes, spring will arrive soon.当我还是一个小女孩的时候，我特别渴望长大When I was a little girl, I was especially eager to grow up.虽然这两个句子没有语法错误，但是都没有达到出奇制胜的效果，原因很可能在于同学们没有让分词作状语，而是让从句作了状语把状语从句转化成分词作状语把状语从句最明显的标志—引导词if去掉，那么就会出现一个语法错误的现象—逗号在连接两个句子Winter comes, spring will arrive soon.既然逗号不能连接两个句子，那我们可以考虑把前面的句子变成非句子结构，那怎么把句子变成不是句子(非句子结构)呢？只需要把动词(词组)变成不是动词(词组)(非谓语结构)就可以实现这一步那就是：Winter coming, spring will arrive soon.看非谓语部分的主语和后面句子的主语是否一致。如果一致，分词的主语省略；如果不一致，分词的主语保留就本句话而言，显然主语不一致，所以保留同样三步走，句2可以转化为：Being a little girl, I was especially eager to grow up.小结：分词的主语如果和主句的主语不一致，这种英语语法现象就叫作独立主格；分词的主语如果和主句的主语一致，就叫作非谓语动词作状语例子：工作结束后，我们都各自回家了。      状语从句：Afterthe job was finished, we went home respectively.分词作状语：Thejob finished, we went home respectively.因为我胖，所以我快乐。状语从句：BecauseI am fat, I feel delighted.分词作状语：Being fat, I feel delighted.是不是非得出现状语从句才能使用分词作状语呢当然不是，只要是在能够用逗号来连接两个句子的情况下，都可以把那个次要的句子变成分词作状语那个老妇人坐在角落里，眼里充满了泪水。Theold lady sits in the corner, her eyes are filled with tears.该翻译显然是错误的，因为逗号不能连接两个句子。所以要把第一个句子的谓语动词变成非谓语动词，于是写成：The old lady sitting in the corner, her eyes are filled with tears.自信是一种积极的心态，它能使很多奇迹降临。    Self-confidence is an active mentality, it enables many miracles to happen.同理，逗号不能连接两个句子。要把前面的句子变成分词作状语，于是写成：Being an active mentality, self-confidence enables many miracles to happen.  ​    定语和状语的区别我们可以先来看几个例句：She smiled when she saw me. when后是状语The woman who saw her son smiled immediately. who 后是定语I loved the man when I met him. when后是状语定语从句中，如果先行词是人，引导词不可能是when区分既像定语又像状语的情况比如：I love a boy in the kindergarten.如果是定语就译为：我爱上了一个幼儿园的小男生。如果是状语就译为：在幼儿园，我爱上了一个小男生。分析长难句时如果遇到这种情况，两种译法都没有错误(除非有明显的语境提示)但是一般情况下，定语比状语出现的概率大，所以如果遇到这种情况，本书推荐定语优先原则区分when引导的是定语从句还是状语从句I was singing when she came in.I still remember that day when you promised you would love me forever以上两句话，区分的关键在于看when之前是不是表示时间的词如果是表示时间的词，那通常就是定语从句，when翻译为”当时，这时”或”届时”如果when前面不是表示时间的词，那通常是状语从句，when前置翻译成”当…的时候”于是上面两句话就翻译成：当她进来的时候，我正在唱歌。我仍然记得那天，当时你承诺会爱我一辈子。as专题as+名词(短语)此时的as是介词，as后跟名词(短语)时，一般译为”作为”As a logical consequence of this development, separate journals have now appeared aimedmainly towards either professional or amateur readership.“as+名词短语”在本句中作状语，后面是主句separate指”分开的，不同的”；journal指”杂志，期刊”，journalist翻译成”记者”；aim作动词，指”以…为目标”；professionalor amateur翻译成”专业的或业余的”作为这种发展的合理结果，不同的期刊现在都主要针对专业的或业余的读者。As a result, the support for ambity to be and andin the mind of the young, is probably lower than it has ever been in the United States.“as+名词短语”放在ambition的后面充当定语。主句谓语是is,句子的主干就是thesupport for ambition is probably lower than it has ever been; impulse后面是impulse的同位语，也可以认为是插入语；inthe United States是地点状语impulse作名词，指”冲动”；quality指”质量，品质”，此处指”品质”结果，在美国(状语前置)，作为一种健康的冲动，一种应该被年轻人尊重并且扎根于他们思想的高贵品质，雄心壮志所得到的支持或许比以往任何时期都低。动词(词组)+名词(短语)+as此时的as仍然是介词，它的意思取决于前面动词(词组)的意思。一定注意动词(词组)和as中间应该有名词(短语)。当发现没有名词(短语)的时候，可能是名词(短语)被放到其他地方去了常见的动词词组有：regard … as …, view … as …, define … as …, classify … as …shall define him as an individual who has elected as his pleasure in lifethe activity of thinking in Socratic(苏格拉底的) way about moral problems.句子主干是：Ishall define him as an individual。在who所引导的定语从句中，elected和as中间没有名词(短语)，说明名词(短语)的位置被后移了。介词短语in Socratic way是状语，通常应该放在句末，那么the activity of thinking about moral problems便是elected和as中间的宾语define指”给 … 下定义”，definite指”明确的”，definition指”定义”；elect … as … 指”选择…作为…”我要把他定义成一个选择以苏格拉底的方式思考道德问题作为其生命中主要职责和乐趣的人。Tylor defined culture as “ … that complex whole which includes belief, art, morals, lawcustom, and any other capabilities and habits acquired by man as a member of society.”句子的主干也很清晰：Tylor defined culture as “ … that complex whole”。其中which引导定语从句，acquired by man as a member of society是过去分词短语作后置定语，修饰前面的名词本句中的所有单词都比较基础。如果你还是觉得这些单词很难，那就需要加快背单词的步伐。泰勒把文化定义成”… 一个复杂的整体，它(代词替代先行词，定语后置)包括了作为社会一员的人所获得的信念、艺术、道德、法律、传统和任何其他的能力和习惯”。as引导定语从句as可以引导修饰整个句子的定语从句，which也可以，但which只能放在句末，as则可前可后As is true of any developed society, in America a complex set of cultural signals, assumptions, and conventions underlies all social interrelationships .该句是主谓宾结构。谓语是underlies, acomplex set of后跟了三个并列的成分一起充当主语assumption指”假定，设想”；interrelationship指”相互关系”就如同在任何发达国家一样，在美国，一系列复杂的文化信号、设想和传统组成了所有社会关系的基础。as引导状语从句as应该怎么翻译由前后句子的意思决定Shortlists for interviews, election ballon balliers, lists of confertion and attendees: all tend to be drawn up alphabetically, and their recipients lose interest as they ploughthrough them .该句是由句中第二个and连接的并列句，冒号前是三个并列的名词短语，冒号后表示总结。句中as引导时间状语从句。shortlist指”最终候选人名单”；ballotpaper指”选票”；list指”清单”；draw up 指”起草，制定”；alphabetically指”按照字母顺序排列”；recipient原意为”接受者”，此处翻译成”看这些名单的人”；ploughthrough指”费力阅读”求职面试名单、投票选举名单、会议发言人及与会者名单，往往都是按照字母顺序排列的，当人们费劲地往后翻看这些名单时，兴味就会随之索然。(as翻译成”当…时”)With other audiences you mustn’t attempt to cut in with humor as they will resent an outsider making disparaging remarks about their canteen or their chairman.主句是you mustn’t attempt to cut in with humor。句中as引导原因状语从句，resent指”厌恶，怨恨”；disparaging指”毁谤的，蔑视的”对于其他听众，你千万不要试图贸然去使用这种幽默，因为他们会反感一个外人对他们的餐厅或总裁发表蔑视性的评论。(as翻译成”因为”)New forms of thought as well as new subjects for thought must arise in the future as theyhave in the past, giving rise to new standards of elegance.句子主干是主谓结构：Newforms of thought as well as new subjects for thoughtmust arise。主语是由as well as连接的两个名词短语共同构成句中in the future, as they havein the past 和giving rise to new standards of elegance都是状语，其中as引导的是方式状语从句。subject作名词，指”学科，主题，主观”，作动词构成搭配subject A to B，指”使A服从于B，使A遭受B”；give rise to指”产生，导致”；elegance作名词，指”优雅”就像过去那样，将来新的思维方式和新的思维主题一定会出现，从而出现关于优雅的新的标准。(as翻译成”像…一样”)</content>
	</entry>
	<entry>
		<title>JDBC_JDBC基础</title>
		<url>/article/d6fc26d1.html</url>
		<content>【黑马程序员-Java语言高级部分11】JDBC基础概念概念：Java DataBase Connectivity  Java 数据库连接， Java语言操作数据库JDBC笔记JDBC本质：其实是官方（sun公司）定义的一套操作所有关系型数据库的规则，即接口。各个数据库厂商去实现这套接口，提供数据库驱动jar包。我们可以使用这套接口（JDBC）编程，真正执行的代码是驱动jar包中的实现类。快速入门步骤：导入驱动jar包 mysql-connector-java-5.1.37-bin.jar复制mysql-connector-java-5.1.37-bin.jar到项目的libs目录下右键—&gt;Add As Library注册驱动获取数据库连接对象 Connection定义sql获取执行sql语句的对象 Statement执行sql，接受返回结果处理结果释放资源代码实现12345678910111213141516&gt;// 1. 导入驱动jar包&gt;// 2.注册驱动&gt;Class.forName(&quot;com.mysql.jdbc.Driver&quot;);&gt;// 3.获取数据库连接对象&gt;Connection conn = DriverManager.getConnection(&quot;jdbc:mysql://localhost:3306/db3&quot;, &quot;root&quot;, &quot;root&quot;);&gt;// 4.定义sql语句&gt;String sql = &quot;update account set balance = 500 where id = 1&quot;;&gt;// 5.获取执行sql的对象 Statement&gt;Statement stmt = conn.createStatement();&gt;// 6.执行sql&gt;int count = stmt.executeUpdate(sql);&gt;// 7.处理结果&gt;System.out.println(count);&gt;// 8.释放资源&gt;stmt.close();&gt;conn.close();关键对象DriverManager(驱动管理对象)功能：12345678910111213// 1. 注册驱动：告诉程序该使用哪一个数据库驱动jarstatic void registerDriver(Driver driver) :注册与给定的驱动程序 DriverManager 。 // 写代码使用：  Class.forName(&quot;com.mysql.jdbc.Driver&quot;);// 通过查看源码发现：在com.mysql.jdbc.Driver类中存在静态代码块static {try {java.sql.DriverManager.registerDriver(new Driver());} catch (SQLException E) {throw new RuntimeException(&quot;Can't register driver!&quot;);}}// 注意：mysql5之后的驱动jar包可以省略注册驱动的步骤。123// 2. 获取数据库连接：// 方法：static Connection getConnection(String url, String user, String password) // 细节：如果连接的是本机mysql服务器，并且mysql服务默认端口是3306，则url可以简写为：jdbc:mysql:///数据库名称Connection(数据库连接对象)获取执行sql 的对象 Statement createStatement() PreparedStatement prepareStatement(String sql)  管理事务   开启事务：setAutoCommit(boolean autoCommit) ：调用该方法设置参数为false，即开启事务   提交事务：commit()    回滚事务：rollback() Statement(执行sql的对象)boolean execute(String sql) ：可以执行任意的sql 了解 int executeUpdate(String sql) ：执行DML（insert、update、delete）语句、DDL(create，alter、drop)语句返回值：影响的行数，可以通过这个影响的行数判断DML语句是否执行成功 返回值&gt;0的则执行成功，反之，则失败。ResultSet executeQuery(String sql): 执行DQL(select)语句示例:123456789101112131415161718192021222324252627282930313233343536373839404142&gt;Statement stmt = null;&gt;Connection conn = null;&gt;try {// 1. 注册驱动Class.forName(&quot;com.mysql.jdbc.Driver&quot;);// 2. 定义sqlString sql = &quot;insert into account values(null,'王五',3000)&quot;;// 3. 获取Connection对象conn = DriverManager.getConnection(&quot;jdbc:mysql:///db3&quot;, &quot;root&quot;, &quot;root&quot;);// 4. 获取执行sql的对象 Statementstmt = conn.createStatement();// 5. 执行sqlint count = stmt.executeUpdate(sql);//影响的行数// 6. 处理结果System.out.println(count);if(count &gt; 0){System.out.println(&quot;添加成功！&quot;);}else{System.out.println(&quot;添加失败！&quot;);}&gt;} catch (ClassNotFoundException e) {e.printStackTrace();&gt;} catch (SQLException e) {e.printStackTrace();&gt;}finally {// 7. 释放资源// 避免空指针异常if(stmt != null){       try {           stmt.close();} catch (SQLException e) {e.printStackTrace();}}if(conn != null){try {conn.close();} catch (SQLException e) {e.printStackTrace();}}&gt;}ResultSet(结果集对象)boolean next(): 游标向下移动一行，判断当前行是否是最后一行末尾(是否有数据)，如果是，则返回false，如果不是则返回truegetXxx(参数): 获取数据Xxx：代表数据类型   如： int getInt() ,    String getString()参数：int：代表列的编号,从1开始   如： getString(1)String：代表列名称。 如： getDouble(“balance”)123456789&gt;// 循环判断游标是否是最后一行末尾。&gt;while(rs.next()){// 获取数据// 6.2 获取数据int id = rs.getInt(1);String name = rs.getString(&quot;name&quot;);double balance = rs.getDouble(3);System.out.println(id + &quot;---&quot; + name + &quot;---&quot; + balance);&gt;}PreparedStatement(执行sql的对象)QL注入问题：在拼接sql时，有一些sql的特殊关键字参与字符串的拼接。会造成安全性问题输入用户随便，输入密码：a’ or ‘a’ = ‘asql：select * from user where username = ‘fhdsjkf’ and password = ‘a’ or ‘a’ = ‘a’ 解决sql注入问题：使用PreparedStatement对象来解决预编译的SQL：参数使用?作为占位符驱动jar包 mysql-connector-java-5.1.37-bin.jar注册驱动获取数据库连接对象 Connection定义sql    sql的参数使用?作为占位符。 如：select * from user where username=? and password = ?;获取执行sql语句的对象 PreparedStatement  Connection.prepareStatement(String sql) 给？赋值   setXxx(位置编号, 参数值)执行sql，接受返回结果，不需要传递sql语句处理结果释放资源优点可以防止SQL注入效率更高JDBC工具类目的：简化书写抽取一个方法获取连接对象需求：不想传递参数（麻烦），还得保证工具类的通用性解决：配置文件jdbc.properties1234url=jdbc:mysql:///db3user=rootpassword=rootdriver=com.mysql.jdbc.Driver123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106&gt;import java.io.FileReader;&gt;import java.io.IOException;&gt;import java.net.URL;&gt;import java.sql.*;&gt;import java.util.Properties;&gt;/*** JDBC工具类*/&gt;public class JDBCUtils {   private static String url;   private static String user;   private static String password;   private static String driver;   /**    * 文件的读取，只需要读取一次即可拿到这些值。使用静态代码块    */   static{       //读取资源文件，获取值       try {           //1. 创建Properties集合类。           Properties pro = new Properties();           //获取src路径下的文件的方式---&gt;ClassLoader 类加载器           ClassLoader classLoader = JDBCUtils.class.getClassLoader();           URL res  = classLoader.getResource(&quot;jdbc.properties&quot;);           String path = res.getPath();          //2. 加载文件           pro.load(new FileReader(path));           //3. 获取数据，赋值           url = pro.getProperty(&quot;url&quot;);           user = pro.getProperty(&quot;user&quot;);           password = pro.getProperty(&quot;password&quot;);           driver = pro.getProperty(&quot;driver&quot;);           //4. 注册驱动           Class.forName(driver);       } catch (IOException e) {           e.printStackTrace();       } catch (ClassNotFoundException e) {           e.printStackTrace();       }   }   /**    * 获取连接    * @return 连接对象    */   public static Connection getConnection() throws SQLException {       return DriverManager.getConnection(url, user, password);   }   /**    * 释放资源    * @param stmt    * @param conn    */   public static void close(Statement stmt,Connection conn){       if( stmt != null){           try {               stmt.close();           } catch (SQLException e) {               e.printStackTrace();           }       }       if( conn != null){           try {               conn.close();           } catch (SQLException e) {               e.printStackTrace();           }       }   }   /**    * 释放资源    * @param stmt    * @param conn    */   public static void close(ResultSet rs,Statement stmt, Connection conn){       if( rs != null){           try {               rs.close();           } catch (SQLException e) {               e.printStackTrace();           }       }       if( stmt != null){           try {               stmt.close();           } catch (SQLException e) {               e.printStackTrace();           }       }       if( conn != null){           try {               conn.close();           } catch (SQLException e) {               e.printStackTrace();           }       }   }&gt;}修改记录1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859package cn.itcast.jdbc;import java.sql.Connection;import java.sql.DriverManager;import java.sql.SQLException;import java.sql.Statement;/** * account表 修改记录 */public class JDBCDemo3 {    public static void main(String[] args) {        Connection conn = null;        Statement stmt = null;        try {            //1. 注册驱动            Class.forName(&quot;com.mysql.jdbc.Driver&quot;);            //2.获取连接对象            conn = DriverManager.getConnection(&quot;jdbc:mysql:///db3&quot;, &quot;root&quot;, &quot;root&quot;);            //3.定义sql            String sql  = &quot;update account set balance = 1500 where id = 3&quot;;            //4.获取执行sql对象            stmt = conn.createStatement();            //5.执行sql            int count = stmt.executeUpdate(sql);            //6.处理结果            System.out.println(count);            if(count &gt; 0){                System.out.println(&quot;修改成功！&quot;);            }else{                System.out.println(&quot;修改失败&quot;);            }        } catch (ClassNotFoundException e) {            e.printStackTrace();        } catch (SQLException e) {            e.printStackTrace();        } finally {            //7.释放资源            if(stmt != null){                try {                    stmt.close();                } catch (SQLException e) {                    e.printStackTrace();                }            }            if(conn != null){                try {                    conn.close();                } catch (SQLException e) {                    e.printStackTrace();                }            }        }    }}添加记录123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960package cn.itcast.jdbc;import java.sql.Connection;import java.sql.DriverManager;import java.sql.SQLException;import java.sql.Statement;/** * account表 添加一条记录 insert 语句 */public class JDBCDemo2 {    public static void main(String[] args) {        Statement stmt = null;        Connection conn = null;        try {            //1. 注册驱动            Class.forName(&quot;com.mysql.jdbc.Driver&quot;);            //2. 定义sql            String sql = &quot;insert into account values(null,'王五',3000)&quot;;            //3.获取Connection对象            conn = DriverManager.getConnection(&quot;jdbc:mysql:///db3&quot;, &quot;root&quot;, &quot;root&quot;);            //4.获取执行sql的对象 Statement            stmt = conn.createStatement();            //5.执行sql            int count = stmt.executeUpdate(sql);//影响的行数            //6.处理结果            System.out.println(count);            if(count &gt; 0){                System.out.println(&quot;添加成功！&quot;);            }else{                System.out.println(&quot;添加失败！&quot;);            }        } catch (ClassNotFoundException e) {            e.printStackTrace();        } catch (SQLException e) {            e.printStackTrace();        }finally {            //stmt.close();            //7. 释放资源            //避免空指针异常            if(stmt != null){                try {                    stmt.close();                } catch (SQLException e) {                    e.printStackTrace();                }            }            if(conn != null){                try {                    conn.close();                } catch (SQLException e) {                    e.printStackTrace();                }            }        }    }}删除记录123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263package cn.itcast.jdbc;import cn.itcast.util.JDBCUtils;import java.sql.Connection;import java.sql.DriverManager;import java.sql.SQLException;import java.sql.Statement;/** * account表 删除一条记录 */public class JDBCDemo4 {    public static void main(String[] args) {        Connection conn = null;        Statement stmt = null;        try {            //1. 注册驱动            Class.forName(&quot;com.mysql.jdbc.Driver&quot;);            //2.获取连接对象            conn = DriverManager.getConnection(&quot;jdbc:mysql:///db3&quot;, &quot;root&quot;, &quot;root&quot;);           //conn = JDBCUtils.getConnection(&quot;jdbc:mysql:///db3&quot;, &quot;root&quot;, &quot;root&quot;);            //3.定义sql            String sql  = &quot;delete from account where id = 3&quot;;            //4.获取执行sql对象            stmt = conn.createStatement();            //5.执行sql            int count = stmt.executeUpdate(sql);            //6.处理结果            System.out.println(count);            if(count &gt; 0){                System.out.println(&quot;删除成功！&quot;);            }else{                System.out.println(&quot;删除失败&quot;);            }        } catch (ClassNotFoundException e) {            e.printStackTrace();        } catch (SQLException e) {            e.printStackTrace();        } finally {            //7.释放资源            if(stmt != null){                try {                    stmt.close();                } catch (SQLException e) {                    e.printStackTrace();                }            }            if(conn != null){                try {                    conn.close();                } catch (SQLException e) {                    e.printStackTrace();                }            }        }    }}执行DDL语句1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465package cn.itcast.jdbc;import java.sql.*;/** * 执行DDL语句 */public class JDBCDemo7 {    public static void main(String[] args) {        Connection conn = null;        Statement stmt = null;        ResultSet rs = null;        try {            //1. 注册驱动            Class.forName(&quot;com.mysql.jdbc.Driver&quot;);            //2.获取连接对象            conn = DriverManager.getConnection(&quot;jdbc:mysql:///db3&quot;, &quot;root&quot;, &quot;root&quot;);            //3.定义sql            String sql  = &quot;select * from account&quot;;            //4.获取执行sql对象            stmt = conn.createStatement();            //5.执行sql            rs = stmt.executeQuery(sql);            //6.处理结果            //循环判断游标是否是最后一行末尾。            while(rs.next()){                //获取数据                //6.2 获取数据                int id = rs.getInt(1);                String name = rs.getString(&quot;name&quot;);                double balance = rs.getDouble(3);                System.out.println(id + &quot;---&quot; + name + &quot;---&quot; + balance);            }        } catch (ClassNotFoundException e) {            e.printStackTrace();        } catch (SQLException e) {            e.printStackTrace();        } finally {            //7.释放资源            if(rs != null){                try {                    rs.close();                } catch (SQLException e) {                    e.printStackTrace();                }            }            if(stmt != null){                try {                    stmt.close();                } catch (SQLException e) {                    e.printStackTrace();                }            }            if(conn != null){                try {                    conn.close();                } catch (SQLException e) {                    e.printStackTrace();                }            }        }    }}查询并封装123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147import cn.itcast.domain.Emp;import cn.itcast.util.JDBCUtils;import java.sql.*;import java.util.ArrayList;import java.util.List;/** * * 定义一个方法，查询emp表的数据将其封装为对象，然后装载集合，返回。 */public class JDBCDemo8 {    public static void main(String[] args) {        List&lt;Emp&gt; list = new JDBCDemo8().findAll2();        System.out.println(list);        System.out.println(list.size());    }    /**     * 查询所有emp对象     * @return     */    public List&lt;Emp&gt; findAll(){        Connection conn = null;        Statement stmt = null;        ResultSet rs = null;        List&lt;Emp&gt; list = null;        try {            //1.注册驱动            Class.forName(&quot;com.mysql.jdbc.Driver&quot;);            //2.获取连接            conn = DriverManager.getConnection(&quot;jdbc:mysql:///db3&quot;, &quot;root&quot;, &quot;root&quot;);            //3.定义sql            String sql = &quot;select * from emp&quot;;            //4.获取执行sql的对象            stmt = conn.createStatement();            //5.执行sql            rs = stmt.executeQuery(sql);            //6.遍历结果集，封装对象，装载集合            Emp emp = null;            list = new ArrayList&lt;Emp&gt;();            while(rs.next()){                //获取数据                int id = rs.getInt(&quot;id&quot;);                String ename = rs.getString(&quot;ename&quot;);                int job_id = rs.getInt(&quot;job_id&quot;);                int mgr = rs.getInt(&quot;mgr&quot;);                Date joindate = rs.getDate(&quot;joindate&quot;);                double salary = rs.getDouble(&quot;salary&quot;);                double bonus = rs.getDouble(&quot;bonus&quot;);                int dept_id = rs.getInt(&quot;dept_id&quot;);                // 创建emp对象,并赋值                emp = new Emp();                emp.setId(id);                emp.setEname(ename);                emp.setJob_id(job_id);                emp.setMgr(mgr);                emp.setJoindate(joindate);                emp.setSalary(salary);                emp.setBonus(bonus);                emp.setDept_id(dept_id);                //装载集合                list.add(emp);            }        } catch (ClassNotFoundException e) {            e.printStackTrace();        } catch (SQLException e) {            e.printStackTrace();        }finally {            if(rs != null){                try {                    rs.close();                } catch (SQLException e) {                    e.printStackTrace();                }            }            if(stmt != null){                try {                    stmt.close();                } catch (SQLException e) {                    e.printStackTrace();                }            }            if(conn != null){                try {                    conn.close();                } catch (SQLException e) {                    e.printStackTrace();                }            }        }        return list;    }    /**     * 演示JDBC工具类     * @return     */    public List&lt;Emp&gt; findAll2(){        Connection conn = null;        Statement stmt = null;        ResultSet rs = null;        List&lt;Emp&gt; list = null;        try {           /* //1.注册驱动            Class.forName(&quot;com.mysql.jdbc.Driver&quot;);            //2.获取连接            conn = DriverManager.getConnection(&quot;jdbc:mysql:///db3&quot;, &quot;root&quot;, &quot;root&quot;);*/            conn = JDBCUtils.getConnection();            //3.定义sql            String sql = &quot;select * from emp&quot;;            //4.获取执行sql的对象            stmt = conn.createStatement();            //5.执行sql            rs = stmt.executeQuery(sql);            //6.遍历结果集，封装对象，装载集合            Emp emp = null;            list = new ArrayList&lt;Emp&gt;();            while(rs.next()){                //获取数据                int id = rs.getInt(&quot;id&quot;);                String ename = rs.getString(&quot;ename&quot;);                int job_id = rs.getInt(&quot;job_id&quot;);                int mgr = rs.getInt(&quot;mgr&quot;);                Date joindate = rs.getDate(&quot;joindate&quot;);                double salary = rs.getDouble(&quot;salary&quot;);                double bonus = rs.getDouble(&quot;bonus&quot;);                int dept_id = rs.getInt(&quot;dept_id&quot;);                // 创建emp对象,并赋值                emp = new Emp();                emp.setId(id);                emp.setEname(ename);                emp.setJob_id(job_id);                emp.setMgr(mgr);                emp.setJoindate(joindate);                emp.setSalary(salary);                emp.setBonus(bonus);                emp.setDept_id(dept_id);                //装载集合                list.add(emp);            }        } catch (SQLException e) {            e.printStackTrace();        }finally {            JDBCUtils.close(rs,stmt,conn);        }        return list;    }}JDBC控制事务事务：一个包含多个步骤的业务操作。如果这个业务操作被事务管理，则这多个步骤同时成功或同时失败。操作：开启事务提交事务回滚事务使用Connection对象来管理事务开启事务：setAutoCommit(boolean autoCommit) ：调用该方法设置参数为false，即开启事务在执行sql之前开启事务提交事务：commit() ，当所有sql都执行完提交事务回滚事务：rollback() ，在catch中回滚事务123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657&gt;import cn.itcast.util.JDBCUtils;&gt;import java.sql.Connection;&gt;import java.sql.PreparedStatement;&gt;import java.sql.SQLException;&gt;/*** 事务操作*/&gt;public class JDBCDemo10 {   public static void main(String[] args) {       Connection conn = null;       PreparedStatement pstmt1 = null;       PreparedStatement pstmt2 = null;       try {           //1.获取连接           conn = JDBCUtils.getConnection();           //开启事务           conn.setAutoCommit(false);           //2.定义sql           //2.1 张三 - 500           String sql1 = &quot;update account set balance = balance - ? where id = ?&quot;;           //2.2 李四 + 500           String sql2 = &quot;update account set balance = balance + ? where id = ?&quot;;           //3.获取执行sql对象           pstmt1 = conn.prepareStatement(sql1);           pstmt2 = conn.prepareStatement(sql2);           //4. 设置参数           pstmt1.setDouble(1,500);           pstmt1.setInt(2,1);           pstmt2.setDouble(1,500);           pstmt2.setInt(2,2);           //5.执行sql           pstmt1.executeUpdate();           // 手动制造异常           int i = 3/0;           pstmt2.executeUpdate();           //提交事务           conn.commit();       } catch (Exception e) {           //事务回滚           try {               if(conn != null) {                   conn.rollback();               }           } catch (SQLException e1) {               e1.printStackTrace();           }           e.printStackTrace();       }finally {           JDBCUtils.close(pstmt1,conn);           JDBCUtils.close(pstmt2,null);       }   }&gt;}</content>
	</entry>
	<entry>
		<title>Java进阶_反射</title>
		<url>/article/b885131b.html</url>
		<content>【黑马程序员-Java语言高级部分9.2】Java 反射反射机制笔记【狂神说Java】注解和反射JAVA注解与反射和类的加载机制第十六个专题反射概念基本概念框架：半成品软件，可以在框架的基础上进行软件开发，简化编码。学习框架并不需要了解反射，但是要是想自己写一个框架，那么就要对反射机制有很深入的了解。反射机制：将类的各个组成部分封装为其他对象，这就是反射机制。反射的好处：可以在程序运行过程中，操作这些对象。可以解耦，提高程序的可扩展性。假设我们使用的是new这种形式进行对象的实例化。此时如果在项目的某一个小模块中我们的一个实例类丢失了，那么在编译期间就会报错，以导致整个项目无法启动。对于反射创建对象Class.forName(“全类名”);这种形式，我们在编译期需要的仅仅只是一个字符串（全类名），在编译期不会报错，这样其他的模块就可以正常的运行，而不会因为一个模块的问题导致整个项目崩溃。这就是Spring框架中IOC控制反转的本质。Reflection(反射)是Java被视为动态语言的关键，反射机制允许程序在执行期借助于Reflection API取得任何类的内部信息，并能直接操作任意对象的内部属性及方法。加载完类之后，在堆内存的方法区中就产生了一个Class类型的对象（一个类只有一个Class对象），这个对象就包含了完整的类的结构信息。我们可以通过这个对象看到类的结构。这个对象就像一面镜子，透过这个镜子看到类的结构，所以，我们形象的称之为：反射代码的三个阶段Source源代码阶段：*.java被编译成*.class字节码文件。Class类对象阶段：*.class字节码文件被类加载器加载进内存，并将其封装成Class对象（用于在内存中描述字节码文件），Class对象将原字节码文件中的成员变量抽取出来封装成数组Field[]，将原字节码文件中的构造函数抽取出来封装成数组Construction[]，在将成员方法封装成Method[]。RunTime运行时阶段：创建对象的过程new。Java内存堆: 存放new的对象和数组，被所有的线程共享，不会存放别的对象引用栈: 存放基本变量类型(会包含这个基本类型的具体数值)，引用对象的变量(会存放这个引用在堆里面的具体地址)方法区: 可以被所有的线程共享，包含了所有的class和static变量类加载过程当程序主动使用某个类时，如果该类还未被加载到内存中，则系统会通过如下三个步骤来对该类进行初始化。类加载与ClassLoader 加载：将class文件字节码内容加载到内存中，并将这些静态数据转换成方法区的运行时数据结构，然后生成一个代表这个类的java.lang.Class对象。 链接：将Java类的二进制代码合并到JVM的运行状态之中的过程。验证：确保加载的类信息符合JVM规范，没有安全方面的问题准备：正式为类变量（static）分配内存并设置类变量默认初始值的阶段，这些内存都将在方法区中进行分配。解析：虚拟机常量池内的符号引用（常量名）替换为直接引用（地址）的过程。初始化：执行类构造器()方法的过程。类构造器()方法是由编译期自动收集类中所有类变量的赋值动作和静态代码块中的语句合并产生的。类构造器是构造类信息的，不是构造该类对象的构造器。当初始化一个类的时候，如果发现其父类还没有进行初始化，则需要先触发其父类的初始化。虚拟机会保证一个类的()方法在多线程环境中被正确加锁和同步。类初始化时机类主动引用（一定会发生类的初始化）当虚拟机启动，先初始化main方法所在的类new一个类的对象调用类的静态成员（除了final常量）和静态方法使用java.lang.reflect包的方法对类进行反射调用当初始化一个类，如果其父类没有被初始化，则先会初始化它的父类类被动引用（不会发生类的初始化）当访问一个静态域时，只有真正声明这个域的类才会被初始化。如：当通过子类引用父类的静态变量，不会导致子类初始化通过数组定义类引用，不会触发此类的初始化引用常量不会触发此类的初始化（常量在链接阶段就存入调用类的常量池中了）类加载器的作用 类加载的作用：将class文件字节码内容加载到内存中，并将这些静态数据转换成方法区的运行时   数据结构，然后在堆中生成一个代表这个类的java.lang.Class对象，作为方法区中类数据的访问入口。 类缓存：标准的JavaSE类加载器可以按要求查找类，但一旦某个类被加载到类加载器中，它将维持加载(缓存)一段时间。不过JVM垃圾回收机制可以回收这些Class对象&gt; 类加载器作用是用来把类(class)装载进内存的。   JVM规范定义了如下类型的类的加载器。&gt; 1234567891011121314151617public static void main(string[] args){// 获取系统类的加载器     ClassLoader systemClassloader = ClassLoader.getsystemclassloade();     System.out.println(systemclassLoader);     // 获取系统类加载器的父类加载器-一&gt;扩展类加载器     ClassLoader parent = systemclassLoader.getParent();     System.out.println(parent);     // 获取扩展类加载器的父类加载器--&gt;根加载器（C/c++）     ClassLoader parent1=parent.getParent();    System.out.println(parent1);// 测试当前类是哪个加载器加载的     ClassLoader classLoader = Class.forName(&quot;com.reflection.Test&quot;).getClassLoader();     System.out.println(classLoader);     // 测试JDK内置的类是谁加载的    classLoader = Class.forName(&quot;java.lang.object&quot;).getclassLoader();     system.out.println(classloader);}&gt;自定义网络类加载1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859&gt;import java.io.ByteArrayOutputStream;&gt;import java.io.File;&gt;import java.io.FileInputStream;&gt;import java.io.IOException;&gt;/**&gt;* 自定义网络类加载器&gt;*/&gt;public class NetworkClassLoader extends ClassLoader {   private String filePath;   private String fileExtention = &quot;.class&quot;;   public NetworkClassLoader(String filePath) {       this.filePath = filePath;   }   public Class findClass(String name) {       System.out.println(&quot;findClass name=&quot; + name);       byte[] bArr = null;       try {      bArr = loadClassData(name);       } catch (IOException e) {        e.printStackTrace();       }       return defineClass(name, bArr, 0, bArr.length);   }   private byte[] loadClassData(String name) throws IOException {   // name = &quot;annotatonreflection.NetworkClassLoader&quot;;       name = name.replace(&quot;.&quot;, &quot;/&quot;);       name += fileExtention;       File file = new File(filePath, name);       FileInputStream fileInputStream = new FileInputStream(file);       ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream();       int len = -1;       while ((len = fileInputStream.read()) != -1) {       byteArrayOutputStream.write(len);       }       return byteArrayOutputStream.toByteArray();   }      public static void main(String[] args) {       NetworkClassLoader classLoader = new NetworkClassLoader(&quot;CDesktop&quot;);       try {           Class clazz = classLoader.loadClass(&quot;annotatonreflection.Person&quot;);           Object obj = clazz.newInstance();           System.out.println(obj);           // System.out.println(clazz.getClassLoader());           ClassLoader classLoader2 = clazz.getClassLoader();           while (classLoader2 != null) {               System.out.println(classLoader2);               classLoader2 = classLoader2.getParent();           }           System.out.println(&quot;引导类加载器&quot;+classLoader2); //引导类加载器是null       } catch (ClassNotFoundException e) {       e.printStackTrace();       } catch (InstantiationException e) {       e.printStackTrace();       } catch (IllegalAccessException e) {       e.printStackTrace();       }   }&gt;}Class对象获取方式获取Class对象的三种方式对应着java代码在计算机中的三个阶段Source源代码阶段: Class.forName(“全类名”)：将字节码文件加载进内存，返回Class对象多用于配置文件，将类名定义在配置文件中。读取文件，加载类。 Class类对象阶段: 类名.class：通过类名的属性class获取多用于参数的传递 Runtime运行时阶段: 对象.getClass()：getClass()方法是定义在Objec类中的方法多用于对象的获取字节码的方式  结论: 同一个字节码文件(*.class)在一次程序运行过程中，只会被加载一次，无论通过哪一种方式获取的Class对象都是同一个。若已知具体的类，通过类的class属性获取，该方法最为安全可靠，程序性能最高。Class clazz=Person.class；已知某个类的实例，调用该实例的getClass（）方法获取Class对象Class clazz=person.getClass()；已知一个类的全类名，且该类在类路径下，可通过Class类的静态方法forName()获取，可能抛出ClassNotFoundExceptionClass clazz=Class.forName(“demo01.Student”);内置基本数据类型可以直接用类名.Type还可以利用ClassLoader我们之后讲解哪些类型可以有Class对象？class: 外部类，成员(成员内部类，静态内部类)，局部内部类，匿名内部类。interface: 接口[]: 数组enum: 枚举annotation: 注解@interfaceprimitive type: 基本数据类型void1234567891011121314151617181920212223242526&gt;//所有关型Class &gt;public class Test{    public static void main(string[] args){        Class c1 = Object.class; // 类       Class c2 = Comparable.class; // 接口       class c3 = String[].class; // 一维数组        Class c4 = int[][].class; // 二维数组        Class c5 = Override.class; // 注解        Class c6 = ElementType.class; // 枚举        Class c7 = Integer.class; // 基本数据类型       Class c8 = void.class; // void        Class c9 = Class.class; // class        System.out.println(c1);        System.out.println(c2);        System.out.println(c3);        System.out.println(c4);        System.out.println(c5);        System.out.println(c6);        System.out.println(c7);        System.out.println(c8);        System.out.println(c9);        int[] a = new int[10];        int[] b = new int[100];        system.out.println(a.getclass().hashcode(); }&gt;}1234567891011121314151617181920212223242526@Testpublic void reflect() throws ClassNotFoundException {     // 方式一：Class.forName(&quot;全类名&quot;);     Class cls1 = Class.forName(&quot;com.test.domain.Person&quot;);   //P erson自定义实体类     System.out.println(&quot;cls1 = &quot; + cls1);     // 方式二：类名.class     Class cls2 = Person.class;     System.out.println(&quot;cls2 = &quot; + cls2);     // 方式三：对象.getClass();     Person person = new Person();             Class cls3 = person.getClass();     System.out.println(&quot;cls3 = &quot; + cls3);     // == 比较三个对象     System.out.println(&quot;cls1 == cls2 : &quot; + (cls1 == cls2));    //true     System.out.println(&quot;cls1 == cls3 : &quot; + (cls1 == cls3));    //true           // 方式四：基本内置类型的包装类都有一个Type属性         Class c4=Integer.TYPE;        System.out.printin(c4);         // 获得父类类型         Class c5=c1.getsuperclass();         System.out.printin(c5);}反操作泛型Method和Field、Constructor对象都有setAccessible）方法。setAccessible作用是启动和禁用访问安全检查的开关。参数值为true则指示反射的对象在使用时应该取消Java语言访问检查。提高反射的效率。如果代码中必须用反射，而该句代码需要频繁的被调用，那么请设置为true。使得原本无法访问的私有成员也可以访问参数值为false则指示反射的对象应该实施Java语言访问检查 1234567891011121314151617181920212223242526272829public class Test { public void testel(Map&lt;String,User&gt; map,List&lt;User&gt; list){     System.out.println(&quot;&quot;testo1&quot;); }public Map&lt;string,User&gt;testo2(){     System.out.println(&quot;testo2&quot;);     return null;     }public static void main(String[] args) throws NoSuchMethodException {         Method method=Test11.class.getMethod(&quot;teste1&quot;,Map.class,List.class);         Type[] genericParameterTypes=method.getGenericParameterTypes();        for(Type genericParameterType:genericParameterTypes){            System.out.println(&quot;#&quot;+genericParameterType);             if(genericParameterType instanceof ParameterizedType){                 Type[] actualTypeArguments=((ParameterizedType)genericParameterType).getActualTypeArguments();                 for (Type actualTypeArgument:actualTypeArguments){                     System.out.println(actualTypeArgument);                    method=Test11.class.getMethod(name:&quot;testo2&quot;,.…parameterTypes:null);                     Type genericReturnType=method.getGenericReturnType();                     if(genericReturnType instanceof ParameterizedType){                         Type[]actualTypeArguments=((ParameterizedType) genericReturnType).getActualTypeArguments();                         for(Type actualTypeArgument:actualTypeArguments){                             System.out.println(actualTypeArgument);                                             }                    }                }            }        }    }}Class对象功能获取功能1）获取成员变量们12345&gt;Field[] getFields() ：获取所有public修饰的成员变量&gt;Field getField(String name)   获取指定名称的 public修饰的成员变量&gt;Field[] getDeclaredFields()  获取所有的成员变量，不考虑修饰符&gt;Field getDeclaredField(String name) 2）获取构造方法们12345&gt;Constructor&lt;?&gt;[] getConstructors()  &gt;Constructor&lt;T&gt; getConstructor(类&lt;?&gt;... parameterTypes)  &gt;Constructor&lt;?&gt;[] getDeclaredConstructors()  &gt;Constructor&lt;T&gt; getDeclaredConstructor(类&lt;?&gt;... parameterTypes)  3）获取成员方法们12345&gt;Method[] getMethods()  &gt;Method getMethod(String name, 类&lt;?&gt;... parameterTypes)  &gt;Method[] getDeclaredMethods()  &gt;Method getDeclaredMethod(String name, 类&lt;?&gt;... parameterTypes)4）获取全类名和简单类名1&gt;String getName() Field：成员变量设置值: void set(Object obj, Object value)获取值: get(Object obj)忽略访问权限修饰符的安全检查 setAccessible(true): 暴力反射12345678910111213&gt;import lombok.Getter;&gt;import lombok.Setter;&gt;import lombok.ToString;&gt;@Setter&gt;@Getter&gt;@ToString&gt;public class Person {   public String a;        //最大范围public       protected String b;     //受保护类型       String c;               //默认的访问权限       private String d;       //私有类型   }1234567891011121314151617181920212223242526272829&gt;// 测试getFields和getField(String name)方法&gt;**&gt;* 获取成员变量&gt;*   Field[] getFields()*   Field getField(String name)*/@Testpublic void reflect() throws Exception {  // 0. 获取Person的Class对象  Class personClass = Person.class;   // 1. Field[] getFields()获取所有public修饰的成员变量  Field[] fields = personClass.getFields();   for(Field field : fields){       System.out.println(field);   }   System.out.println(&quot;=============================&quot;);   // 2. Field getField(String name)   Field a = personClass.getField(&quot;a&quot;);   // 3. 获取成员变量a 的值 [也只能获取公有的，获取私有的或者不存在的字符会抛出异常]  Person p = new Person();   Object value = a.get(p);   System.out.println(&quot;value = &quot; + value);   // 4 设置属性a的值  a.set(p,&quot;张三&quot;);   System.out.println(p);}123456789101112131415161718192021222324252627&gt;// 测试getDeclaredFields和getDeclaredField(String name)方法&gt;/***  Field[] getDeclaredFields()*  Field getDeclaredField(String name)*/@Test&gt;public void reflect3() throws Exception {  Class personClass = Person.class;   // Field[] getDeclaredFields()：获取所有的成员变量，不考虑修饰符  Field[] declaredFields = personClass.getDeclaredFields();   for(Field filed : declaredFields){       System.out.println(filed);   }   System.out.println(&quot;===================================&quot;);   // Field getDeclaredField(String name)   Field d = personClass.getDeclaredField(&quot;d&quot;);     //private String d;   Person p = new Person();   // Object value1 = d.get(p);    //会抛出异常  // System.out.println(&quot;value1 = &quot; + value1);       //对于私有变量虽然能会获取到，但不能直接set和get  //忽略访问权限修饰符的安全检查   d.setAccessible(true);//暴力反射   Object value2 = d.get(p);   System.out.println(&quot;value2 = &quot; + value2);}&gt;没有忽略访问修饰符直接访问抛出的异常Constructor:构造方法创建对象：T newInstance(Object… initargs)注意：如果使用空参数构造方法创建对象，操作可以简化：Class对象的newInstance方法12345678910111213141516171819&gt;import lombok.Getter;&gt;import lombok.Setter;&gt;import lombok.ToString;&gt;@Setter&gt;@Getter&gt;@ToString&gt;public class Person {       private String name;       private Integer age;   //无参构造函数       public Person() {}       //单个参数的构造函数，且为私有构造方法       private Person(String name){}       //有参构造函数       public Person(String name, Integer age) {          this.name = name;          this.age = age; }&gt;}12345678910111213141516171819202122232425262728293031323334353637383940414243444546    /**   * 获取构造方法们&gt;*   Constructor&lt;?&gt;[] getConstructors()&gt;*   Constructor&lt;T&gt; getConstructor(类&lt;?&gt;... parameterTypes)&gt;*/&gt;@Test&gt;public void reflect4() throws Exception {   Class personClass = Person.class;   //Constructor&lt;?&gt;[] getConstructors()   Constructor[] constructors = personClass.getConstructors();  for(Constructor constructor : constructors){         // Constructor 对象reflect包下的 import java.lang.reflect.Constructor;       System.out.println(constructor);  }   System.out.println(&quot;==========================================&quot;);   // 获取无参构造函数 注意：Person类中必须要有无参的构造函数，不然抛出异常   Constructor constructor1 = personClass.getConstructor();   System.out.println(&quot;constructor1 = &quot; + constructor1);  // 获取到构造函数后可以用于创建对象   Object person1 = constructor1.newInstance();   // Constructor类内提供了初始化方法newInstance();方法   System.out.println(&quot;person1 = &quot; + person1);   System.out.println(&quot;==========================================&quot;);   // 获取有参的构造函数     // public Person(String name, Integer age)    // 参数类型顺序要与构造函数内一致，且参数类型为字节码类型  Constructor constructor2 = personClass.getConstructor(String.class,Integer.class);  System.out.println(&quot;constructor2 = &quot; + constructor2);      // 创建对象   Object person2 = constructor2.newInstance(&quot;张三&quot;, 23);       // 获取的是有参的构造方法，就必须要给参数   System.out.println(person2);   System.out.println(&quot;=========================================&quot;);   // 对于一般的无参构造函数，我们都不会先获取无参构造器之后在进行初始化。  // 而是直接调用Class类内的newInstance()方法   Object person3 = personClass.newInstance();  System.out.println(&quot;person3 = &quot; + person3);   // 我们之前使用的 Class.forName(&quot;&quot;).newInstance   // 其本质上就是调用了类内的无参构造函数来完成实例化的   // 以后在使用  Class.forName(&quot;&quot;).newInstance; 反射创建对象时，一定要保证类内有无参构造函数}&gt;对于getDeclaredConstructor方法和getDeclaredConstructors方法getDeclaredConstructor方法可以获取到任何访问权限的构造器，而getConstructor方法只能获取public修饰的构造器。此外在构造器的对象内也有setAccessible(true);方法，并设置成true就可以操作了。​    关于为什么要使用private访问权限的构造器，使用这个构造器不就不能外部访问了嘛，不也就无法进行实例化对象了吗？无法在类的外部实例化对象正是私有构造器的意义所在，在单例模式下经常使用，整个项目只有一个对象，外部无法实例化对象，可以在类内的进行实例化并通过静态方法返回（由于实例化的对象是静态的，故只有一个对象，也就是单例的）。网上说这就是单例模式中的饿汉模式，不管是否调用，都创建一个对象。123456789101112131415161718192021&gt;class SingletonDemo{// 私有化构造方法private SingletonDemo(){   }   // 创建一个对象  类内实例化（静态的对象）   private static SingletonDemo singleton = new SingletonDemo();   // 提供public方法供外部访问，返回这个创建的对象   public static SingletonDemo getInstance(){       return singleton;   }&gt;}&gt;public class Singleton {public static void main(String[] args) {SingletonDemo s1 = SingletonDemo.getInstance();//输出对象的地址，如果有地址存在，则说明对象创建成功并获取到System.out.println(s1);  SingletonDemo s2 = SingletonDemo.getInstance();//如果结果为true，则说明是同一个对象System.out.println(s1==s2);    //输出结果为true}&gt;}Method：方法对象执行方法：Object invoke(Object obj, Object… args)获取方法名称：String getName();12345678910111213141516171819202122232425&gt;import lombok.Getter;&gt;import lombok.Setter;&gt;import lombok.ToString;&gt;@Setter&gt;@Getter&gt;@ToString&gt;public class Person {   private String name;       private Integer age;       //无参构造函数   public Person() {}       //有参构造函数       public Person(String name, Integer age) {      this.name = name;          this.age = age;}   //无参方法       public void eat(){          System.out.println(&quot;eat...&quot;);        }        //重载有参方法       public void eat(String food){      System.out.println(&quot;eat...&quot;+food);       }   }invoke方法 123456789101112131415161718192021222324252627282930313233    /**   * 获取成员方法们：&gt;*  Method[] getMethods()&gt;*  Method getMethod(String name, 类&lt;?&gt;... parameterTypes)&gt;*/&gt;@Test&gt;public void reflect5() throws Exception {  Class personClass = Person.class;   // 获取指定名称的方法       Method eat_method1 = personClass.getMethod(&quot;eat&quot;);   // 执行方法   Person person = new Person();  Object rtValue = eat_method1.invoke(person);   //如果方法有返回值类型可以获取到，没有就为null   // 输出返回值 eat方法没有返回值，故输出null  System.out.println(&quot;rtValue = &quot; + rtValue);   System.out.println(&quot;--------------------------------------------&quot;);   // 获取有参的构造函数  有两个参数 第一个方法名 第二个参数列表 ，不同的参数是不同的方法（重载）   Method eat_method2 = personClass.getMethod(&quot;eat&quot;, String.class);   //执行方法   eat_method2.invoke(person,&quot;饭&quot;);   System.out.println(&quot;============================================&quot;);   //获取方法列表  Method[] methods = personClass.getMethods();   for(Method method : methods){            //注意：获取到的方法名称不仅仅是我们在Person类内看到的方法       System.out.println(method);          //继承下来的方法也会被获取到（当然前提是public修饰的）  }}&gt; getName方法getName()方法获取的方法名是仅仅就是方法名（不带全类名），且不带有参数列表。1234567891011&gt;@Test&gt;public void reflect6() throws NoSuchMethodException {       Class personClass = Person.class;       Method[] methods = personClass.getMethods();       for(Method method : methods){           System.out.println(method);           // 获取方法名           String name = method.getName();             System.out.println(name);          }&gt;}获取类名getClass()方法是Object类的方法，需要注意一点获取的类名是全类名（带有路径）123456@Testpublic void reflect7(){     Class personClass = Person.class;     String className = personClass.getName();     System.out.println(className);}关于获取成员方法们的另外两个方法同之前的叙述一样，带有Declared关键字的方法这两个方法，可以获取到任意修饰符的方法。同样的提供了setAccessible(true);方法进行暴力反射。   Method和Field、Constructor对象都有setAccessible）方法。setAccessible作用是启动和禁用访问安全检查的开关。参数值为true则指示反射的对象在使用时应该取消Java语言访问检查。   提高反射的效率。如果代码中必须用反射，而该句代码需要频繁的被调用，那么请设置为true。使得原本无法访问的私有成员也可以访问&gt;参数值为false则指示反射的对象应该实施Java语言访问检查综上说述：对于反射机制来说，在反射面前没有公有私有，都可以通过暴力反射解决。123&gt;Method[] getDeclaredMethods()  &gt;Method getDeclaredMethod(String name, 类&lt;?&gt;... parameterTypes)&gt;method.setAccessible(true);   //暴力反射</content>
	</entry>
	<entry>
		<title>Spark集群搭建</title>
		<url>/article/d192a1af.html</url>
		<content>spark环境搭建版本总览12345678910111213141516171819(ray37) [root@Slave03 huangyc]# java -versionjava version &quot;1.8.0_281&quot;(ray37) [root@Slave03 huangyc]# hadoop versionHadoop 3.2.1Compiled with protoc 2.5.0(ray37) [root@Slave03 huangyc]# scala -versionScala code runner version 2.12.15 -- Copyright 2002-2021, LAMP/EPFL and Lightbend, Inc.(ray37) [root@Slave03 huangyc]# sh /usr/spark-3.0/bin/spark-shellSpark context Web UI available at http://Slave03:4040Welcome to      ____              __     / __/__  ___ _____/ /__    _\ \/ _ \/ _ `/ __/  '_/   /___/ .__/\_,_/_/ /_/\_\   version 3.0.3      /_/  Using Scala version 2.12.10 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_281)百度网盘下载  提取码: 7g8vscala下载并安装scala下载 scala-2.12安装rpm -ivh scala-2.12.15.rpm查看scala版本scala -version配置环境查询包中文件安装位置 -l：rpm -ql 包名编辑/etc/profile，文件末尾添加配置123# 配置scala环境变量export SCALA_HOME=/usr/share/scala/binexport PATH=$PATH:$SCALA_HOME执行source /etc/profile令其生效其他节点同样配置即可免密登录修改主机名(非必须)查看主机名用hostname修改主机名/etc/hostname修改后重启服务器reboot配置host文件vi /etc/hosts123192.168.123.21  Slave00192.168.123.23  Slave02192.168.123.24  Slave03/etc/init.d/network restart生成秘钥对使用 RSA 类型的加密类型来创建密钥对1234# 生成秘钥(-f换名字好像不行)ssh-keygen -t rsa# 配置本机免密cat id_rsa.pub&gt;&gt;authorized_keys-f 参数表示指定密钥对生成位置与名称 密钥对通常放在 ~/.ssh目录下 回车即可创建密钥对，需要输入密码如果不需要为密钥对进行加密，那么可以一路回车有需要清空文件的可以执行truncate -s 0 authorized_keys创建成功之后，可以看到 .ssh 目录下多了两个文件，分别是：id_key：密钥对的私钥，通常放在客户端id_rsa.pub：密钥对中的公钥，通常放在服务端秘钥上传服务器文件权限12sudo chmod -R 700 ~/.sshsudo chmod -R 600 ~/.ssh/authorized_keys将your_key.pub 公钥文件上传至需要连接的服务器123456789# 方式一：追加在其他服务器文件末尾(本机不需要)cat ~/.ssh/id_rsa.pub | ssh root@192.168.123.21 &quot;cat - &gt;&gt; ~/.ssh/authorized_keys&quot;cat ~/.ssh/id_rsa.pub | ssh root@192.168.123.23 &quot;cat - &gt;&gt; ~/.ssh/authorized_keys&quot;cat ~/.ssh/id_rsa.pub | ssh root@192.168.123.24 &quot;cat - &gt;&gt; ~/.ssh/authorized_keys&quot;# 方式二ssh-copy-id -i ~/.ssh/id_key.pub root@192.168.123.21ssh-copy-id -i ~/.ssh/id_key.pub root@192.168.123.23ssh-copy-id -i ~/.ssh/id_key.pub root@192.168.123.24-i 参数表示使用指定的密钥，-p参数表示指定端口，ssh 的默认端口是 22公钥查看本地的公钥文件上传在服务器的.ssh/authorized_keys 文件中1cat ~/.ssh/authorized_keys免密检测123ssh Slaver00ssh Slaver02ssh Slaver03hadoop新建用户(非必须)新建hadoop用户1useradd -m hadoop -s /bin/bash修改hadoop用户密码1passwd hadoop切换用户1su hadoop新建文件夹12mkdir /home/hadoop/appsmkdir /home/hadoop/data关闭防火墙12systemctl stop firewalldsystemctl disable firewalld关闭selinux12vim /etc/sysconfig/selinux修改SELINUX=enforcing为SELINUX=disabled安装下载并解压hadoop到/usr/hadoop-3.2.0下载 hadoop-3.2.0，解压到/usr/hadoop-3.2.0配置环境变量12export HADOOP_HOME=/usr/hadoop-3.2.0export PATH=$PATH:$HADOOP_HOME执行source /etc/profile令其生效查看hadoop版本，hadoop version配置文件Hadoop配置文件的配置，需要配置的文件有几个分别是hadoop-env.shcore-site.xmlhdfs-site.xmlmapred-site.xmlyarn-site.xmlworkers文件这些文件均可以在/usr/hadoop-3.2.0/etc/hadoop下找到workers文件123Slave00Slave02Slave03  # 主节点旧版本以及网上的教程是修改etc/hadoop/slaves文件，但是新版已经移除了这一个文件，取而代之的是workers文件，上述设置代表我的集群有三个datanode结点hadoop_env.sh在hadoop根目录下执行mkdir pids，用于存放pid文件12345export JAVA_HOME=${JAVA_HOME}                    #设置JAVA_HOME的路径，需要再次指明export HADOOP_HOME=${HADOOP_HOME}export HADOOP_PID_DIR=/usr/hadoop-3.2.0/pids     # pid文件根目录，不设置的默认值为/tmp，一段时间后/tmp下的文件会被清除，导致无法关闭hadoop集群# 解决启动时警告 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicableexport JAVA_LIBRARY_PATH=$HADOOP_HOME/lib/native注意的是，如果之前没有设置JAVA_HOME的环境变量，此处直接这样引用会出现错误，改用绝对路径即可消除错误。core-site.xml12345678910111213&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;    &lt;!-- 设置了主节点的ip --&gt;    &lt;property&gt;      &lt;name&gt;fs.defaultFS&lt;/name&gt;      &lt;value&gt;hdfs://Slave03:9000&lt;/value&gt;    &lt;/property&gt;    &lt;!-- 设置了临时目录的地址 --&gt;    &lt;property&gt;        &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;        &lt;value&gt;/home/huangyc/hadoop_data/data/tmp&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;hdfs-site.xml123456789101112131415161718192021222324&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.secondary.http-address&lt;/name&gt;        &lt;value&gt;Slave03:50090&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;!-- 配置网页端口访问 不配置的话 默认端口为9870--&gt;        &lt;name&gt;dfs.namenode.http.address&lt;/name&gt;        &lt;value&gt;Slave03:50070&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.replication&lt;/name&gt;        &lt;value&gt;2&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt;        &lt;value&gt;file:///data/hadoop/hdfs/nn&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt;        &lt;value&gt;file:///data/hadoop/hdfs/dn&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;mapred-site.xml123456789101112131415&lt;!-- Put site-specific property overrides in this file. --&gt;&lt;configuration&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.framework.name&lt;/name&gt;        &lt;value&gt;yarn&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.jobhistory.address&lt;/name&gt;        &lt;value&gt;Slave03:10020&lt;/value&gt;    &lt;/property&gt;    &lt;property&gt;        &lt;name&gt;mapreduce.jobhistory.webapp.address&lt;/name&gt;        &lt;value&gt;Slave03:19888&lt;/value&gt;    &lt;/property&gt;&lt;/configuration&gt;yarn-site.xml1234567891011121314151617&lt;configuration&gt;  &lt;!-- Site specific YARN configuration properties --&gt;  &lt;!-- 指定ResourceManager的地址--&gt;  &lt;property&gt;      &lt;name&gt;yarn.resourcemanager.hostname&lt;/name&gt;      &lt;value&gt;Slave03&lt;/value&gt;  &lt;/property&gt;  &lt;!-- 指定reducer获取数据的方式--&gt;  &lt;property&gt;      &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;      &lt;value&gt;mapreduce_shuffle&lt;/value&gt;  &lt;/property&gt;  &lt;property&gt;      &lt;name&gt;yarn.nodemanager.local-dirs&lt;/name&gt;      &lt;value&gt;file:///data/hadoop/yarn/nm&lt;/value&gt;  &lt;/property&gt;&lt;/configuration&gt;从节点设置将hadoop目录拷贝到从节点的对应目录仿照主节点对应的数据目录设置，设置对应的数据目录（即/data一系列目录）hadoop启动和测试第一次开启时首先要初始化hdfs12cd /usr/hadoop-3.2.0./bin/hdfs namenode -format此处需要注意一个问题，第二次开启不需要再次初始化，遇到问题需要再次初始化，建议删除存放文件上述配置文件中指明了存放数据的文件为dfs，到时候删除即可主节点启动123cd /usr/hadoop-3.2.0/sbin# 启动前./start-all.sh查看进程123456789101112131415# 主节点[root@Slave03 sbin]# jps28260 SecondaryNameNode28810 ResourceManager27706 DataNode30236 Jps29341 NodeManager27326 NameNode# 从节点[root@Slave00 sbin]# jps3570 DataNode4115 NodeManager22413 Worker27503 Jps查看对应的数据结点是否正确12345678910111213141516[root@Slave03 sbin]# hdfs dfsadmin -reportConfigured Capacity: 160982630400 (149.93 GB)Present Capacity: 82032381952 (76.40 GB)DFS Remaining: 82032345088 (76.40 GB)DFS Used: 36864 (36 KB)DFS Used%: 0.00%Replicated Blocks:        Under replicated blocks: 0        Blocks with corrupt replicas: 0        Missing blocks: 0        Missing blocks (with replication factor 1): 0        Low redundancy blocks with highest priority to recover: 0        Pending deletion blocks: 0Erasure Coded Block Groups:         Low redundancy block groups: 0        Block groups with corrupt internal blocks: 0hdfs主页Slave03:50070，如果不行，试试默认端口9870hadoop主页Slave03:8088主要命令启动集群: ./sbin/start-all.sh关闭集群: ./sbin/stop-all.sh常见问题stop-all.sh的时候hadoop的相关进程都无法停止解决方案: 参考spark的常见问题，Hadoop的pid命名规则：1pid=$HADOOP_PID_DIR/hadoop-$HADOOP_IDENT_STRING-$command.pid因此，这里的pid文件名为:hadoop-root-datanode.pidhadoop-root-namenode.pidhadoop-root-nodemanager.pidhadoop-root-resourcemanager.pidhadoop-root-secondarynamenode.pid通过jps查看相关进程的pid，恢复这些pid文件即可使用stop-all.sh停止hadoop，根治方案参考spark常见问题部分spark安装安装配置下载解压，复制到/usr/spark-3.0配置环境变量vi /etc/profile123# 配置spark环境export SPARK_HOME=/usr/spark-3.0export PATH=$PATH:$SPARK_HOME/bin执行source /etc/profile令其生效配置spark-env.sh将conf文件夹下的spark-env.sh.template重命名为spark-env.sh，并添加以下内容：在spark根目录下执行mkdir pids，用于存放pid文件123456789101112# 环境变量export JAVA_HOME=/usr/java/jdk1.8.0_281-amd64export SCALA_HOME=/usr/share/scalaexport HADOOP_HOME=/usr/hadoop-3.2.1# 详细配置export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoopexport SPARK_MASTER_HOST=Slave03export SPARK_LOCAL_DIRS=/usr/spark-3.0export SPARK_DRIVER_MEMORY=16g           # 内存export SPARK_EXECUTOR_MEMORY=8g          # 执行内存export SPARK_WORKER_CORES=4              # cpu核心数export SPARK_PID_DIR=/usr/spark-3.0/pids # pid文件根目录，不设置的默认值为/tmp，一段时间后/tmp下的文件会被清除，导致无法关闭spark集群配置slaves将conf文件夹下的slaves.template重命名为slaves，并添加以下内容：123Slave00Slave02Slave03  # 主节点配置从节点(将spark目录复制到其他节点的同一个目录下)1scp -r root@192.168.123.24:/usr/spark-3.0 /usr/在sbin目录下使用start-all.sh启动集群启动成功后，我们在浏览器输入Slave03:8080看到有三个结点，就代表我们安装成功了。如果发现启动错误，请查看logs目录下的日志，自行检查配置文件！日志配置配置spark的执行日志等级，进入到spark根目录下的conf目录cp log4j.properties.template log4j.properties，修改配置如下1234567891011121314151617181920212223242526# Set everything to be logged to the consolelog4j.rootCategory=WARN, consolelog4j.appender.console=org.apache.log4j.ConsoleAppenderlog4j.appender.console.target=System.errlog4j.appender.console.layout=org.apache.log4j.PatternLayoutlog4j.appender.console.layout.ConversionPattern=%d{yy/MM/dd HH:mm:ss} %p %c{1}: %m%n# Set the default spark-shell log level to WARN. When running the spark-shell, the# log level for this class is used to overwrite the root logger's log level, so that# the user can have different defaults for the shell and regular Spark apps.log4j.logger.org.apache.spark.repl.Main=WARN# Settings to quiet third party logs that are too verboselog4j.logger.org.sparkproject.jetty=WARNlog4j.logger.org.sparkproject.jetty.util.component.AbstractLifeCycle=ERRORlog4j.logger.org.apache.spark.repl.SparkIMain$exprTyper=INFOlog4j.logger.org.apache.spark.repl.SparkILoop$SparkILoopInterpreter=INFOlog4j.logger.org.apache.parquet=ERRORlog4j.logger.parquet=ERROR# SPARK-9183: Settings to avoid annoying messages when looking up nonexistent UDFs in SparkSQL with Hive supportlog4j.logger.org.apache.hadoop.hive.metastore.RetryingHMSHandler=FATALlog4j.logger.org.apache.hadoop.hive.ql.exec.FunctionRegistry=ERROR# 关闭警告# WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicablelog4j.logger.org.apache.hadoop.util.NativeCodeLoader=ERROR主要命令启动集群: ./sbin/start-all.sh关闭集群: ./sbin/stop-all.sh常见问题stop-all.sh的时候spark的相关进程都无法停止$SPARK_PID_DIR中存放的是pid文件，就是要停止进程的pid。其中$SPARK_PID_DIR默认是在系统的/tmp目录系统每隔一段时间就会清除/tmp目录下的内容。到/tmp下查看一下，果然没有相关进程的pid文件了。这才导致了stop-all.sh无法停止集群。解决方案: $SPARK_PID_DIR下新建pid文件，pid文件命名规则如下1$SPARK_PID_DIR/spark-$SPARK_IDENT_STRING-$command-$instance.pid$SPARK_PID_DIR默认是/tmp$SPARK_IDENT_STRING是登录用户$USER，我的集群中用户名是root$command是调用spark-daemon.sh时的参数，有两个：org.apache.spark.deploy.master.Masterorg.apache.spark.deploy.worker.Worker$instance也是调用spark-daemon.sh时的参数，我的集群中是1因此pid文件名如下(名字不对的情况下，可以执行./start-all.sh，重新启动查看后，再执行./stop-all.sh进行本次集群的关闭，注意这里关闭的是本次打开的，之前无法关闭的进程仍然还在)：spark-root-org.apache.spark.deploy.master.Master-1.pidspark-root-org.apache.spark.deploy.worker.Worker-1.pid通过jps查看相关进程的pid，将pid保存到对应的pid文件即可，之后调用spark的stop-all.sh，即可正常停止spark集群要根治这个问题，只需要在集群所有节点都设置$SPARK_PID_DIR，$HADOOP_PID_DIR和$YARN_PID_DIR即可123456# 修改hadoop-env.sh，增加：export HADOOP_PID_DIR=/home/ap/cdahdp/app/pids# 修改yarn-env.sh，增加：export YARN_PID_DIR=/home/ap/cdahdp/app/pids# 修改spark-env.sh，增加：export SPARK_PID_DIR=/home/ap/cdahdp/app/pidspyspark环境配置基础环境spark集群服务器配置环境变量，vi /etc/profile12export PYSPARK_PYTHON=/root/anaconda3/envs/ray37/bin/python3.7export PYSPARK_DRIVER_PYTHON=/root/anaconda3/envs/ray37/bin/python3.7执行source /etc/profile使其生效安装pyspark库1234# 这里需要注意版本需要和spark的版本一致pip install pyspark==3.0.3# 在安装pyspark时默认会装上对应的py4j，如果没有的话，手动安装下pip install py4j==0.10.9对于spark服务器和work间环境不一致的情况方式一: 重新安装python虚拟环境，使得路径完全一致方式二: 配置软链接123ln -s 源文件 目标文件# 需要事先建好路径ln -s /opt/anaconda/install/envs/ray37/bin/python3.7 /root/anaconda3/envs/ray37/bin/python3.7在服务器 liunx 环境上修改查看python的包路径site-package12from distutils.sysconfig import get_python_libprint(get_python_lib())graphx环境安装graphframes1pip install graphframes==0.6去graphframes官网下载对应jar包，这里spark集群是3.0.3，所以可以下载Version: 0.8.2-spark3.0-s_2.12这个版本的复制到spark根目录下的jars目录中，集群中每个节点都需要如果还会出错，可以把该jar包也复制一份到python环境下/opt/anaconda/install/envs/ray37/lib/python3.7/site-packages/pyspark/jars配置远程解释器在PyCharm中，可以使用远程配置解释器和项目功能来进行远程开发。这使您可以在本地使用PyCharm编写代码，并将代码部署和运行在远程服务器上。下面是对远程配置解释器和项目的解释：远程配置解释器：远程配置解释器是指在PyCharm中设置和使用位于远程服务器上的解释器。这样，您可以在本地的PyCharm中编写代码，并使用远程服务器上的Python解释器来运行和调试代码。通过远程配置解释器，您可以利用服务器上的计算资源和环境来开发和测试代码远程配置项目：远程配置项目是指将整个项目与远程服务器相关联，以便在远程服务器上运行和管理项目。通过远程配置项目，您可以在本地的PyCharm中编辑代码，并将更改同步到远程服务器上的项目目录。您可以使用远程配置项目来访问和管理远程服务器上的文件、库和其他资源配置远程连接配置连接的远程服务器本地先新建文件夹，用于存放远程项目，新建完用pycharm打开项目打开Pycharm -&gt; Tools -&gt; Deployment -&gt; Configuration，点击加号，选择SFTP，给服务器取一个名字点击SSH configuration后面的省略号输入服务器IP地址和账号密码，结束后测试一下连接情况Connection Parameters下的心跳可以设置10秒确认后，保存退出当前窗口Connection选项卡点击自动检测，获取远程服务器的工作路径Mappings选项卡的Local path选择一开始新建的文件夹，Deployment path选择远程项目的路径，这里为了避免不必要的麻烦，远程项目最好赋予777权限保存，退出右键项目名称，选择Deployment下的Download from …，此时会将服务器上的项目同步到本地配置解释器选择远程的解释器点击move即可next配置路径，远程的选择自己之前新建的/home/huangyc/hyc_testAutomatically upload ...表示会自动上传项目到服务器中其它功能在Pycharm上显示远程代码：选择Tools --&gt;Deployment--&gt;Browse Remote Host更新代码：将本地代码上传到服务器上Tools --&gt;Deployment--&gt;upload to服务器上代码下载到本地代码上Tools --&gt;Deployment--&gt;Download from，如果无效，就右击项目下的Deployment测试例子简单词统计 + pagerank例子123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v1.0@author: huangyc@file: connenct_test.py@Description: @time: 2022/1/25 14:49&quot;&quot;&quot;import osimport platformimport tracebackfrom typing import Listfrom graphframes import *from pyspark import SparkConffrom pyspark.sql import SparkSessionfrom pyspark.sql import functions as Fpy_root = &quot;/root/anaconda3/envs/ray37/bin/python3.7&quot;os.environ[&quot;PYSPARK_PYTHON&quot;] = py_rootos.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = py_rootis_local_py: bool = Trueif is_local_py and platform.system().lower() == 'windows':    os.environ['JAVA_HOME'] = 'G:\Java\jdk1.8.0_201'    os.environ[&quot;SPARK_HOME&quot;] = r&quot;E:\PycharmWS\remote_spark\spark-3.0&quot;class PySparkClient:    def __init__(self, appname: str, master: str):        conf = SparkConf().setAppName(appname).setMaster(master)  # spark资源配置        conf.set(&quot;spark.driver.maxResultSize&quot;, &quot;4g&quot;)        conf.set(&quot;spark.executor.num&quot;, &quot;4&quot;)        conf.set(&quot;spark.executor.memory&quot;, &quot;2g&quot;)        conf.set(&quot;spark.executor.cores&quot;, &quot;4&quot;)        conf.set(&quot;spark.cores.max&quot;, &quot;16&quot;)        conf.set(&quot;spark.driver.memory&quot;, &quot;2g&quot;)        try:            self.spark = SparkSession.builder.config(conf=conf).getOrCreate()            self.sc = self.spark.sparkContext        except:            traceback.print_exc()  # 返回出错信息    def word_count(self, log_file: str):        if not self.sc:            return        log_data = self.sc.textFile(log_file).cache()        words_rdd = log_data.flatMap(lambda sentence: sentence.split(&quot; &quot;))        res = words_rdd.countByValue()        res_rdd = words_rdd.filter(lambda k: len(k) &gt; 0).map(lambda word: (word, 1)).reduceByKey(lambda a, b: a + b)        # 将rdd转为collection并打印        res_rdd_coll = res_rdd.takeOrdered(5, lambda x: -x[1])        for line in res_rdd_coll:            print(line)    def simple_test(self, words: List[str]):        if not self.sc:            return        counts = self.sc.parallelize(words).count()        print(&quot;Number of elements in RDD is %i&quot; % counts)        test = self.spark.createDataFrame(            [('001', '1', 100, 87, 67, 83, 98), ('002', '2', 87, 81, 90, 83, 83), ('003', '3', 86, 91, 83, 89, 63),             ('004', '2', 65, 87, 94, 73, 88), ('005', '1', 76, 62, 89, 81, 98), ('006', '3', 84, 82, 85, 73, 99),             ('007', '3', 56, 76, 63, 72, 87), ('008', '1', 55, 62, 46, 78, 71), ('009', '2', 63, 72, 87, 98, 64)],            ['number', 'class', 'language', 'math', 'english', 'physic', 'chemical'])        test.show()        test.printSchema()        test.select('number', 'class', 'language', 'math', 'english').describe().show()        print(&quot;============   simple_test over   ============&quot;)    def simple_graph(self):        # Create a Vertex DataFrame with unique ID column &quot;id&quot;        spk = self.spark        v = spk.createDataFrame([            (&quot;a&quot;, &quot;Alice&quot;, 34),            (&quot;b&quot;, &quot;Bob&quot;, 36),            (&quot;c&quot;, &quot;Charlie&quot;, 30),        ], [&quot;id&quot;, &quot;name&quot;, &quot;age&quot;])        # Create an Edge DataFrame with &quot;src&quot; and &quot;dst&quot; columns        e = spk.createDataFrame([            (&quot;a&quot;, &quot;b&quot;, &quot;friend&quot;),            (&quot;b&quot;, &quot;c&quot;, &quot;follow&quot;),            (&quot;c&quot;, &quot;b&quot;, &quot;follow&quot;),        ], [&quot;src&quot;, &quot;dst&quot;, &quot;relationship&quot;])        # Create a GraphFrame        g = GraphFrame(v, e)        # Query: Get in-degree of each vertex.        g.inDegrees.show()        # Query: Count the number of &quot;follow&quot; connections in the graph.        print(g.edges.filter(&quot;relationship = 'follow'&quot;).count())        # Run PageRank algorithm, and show results.        results = g.pageRank(resetProbability=0.1, maxIter=1)        res = results.vertices.select(&quot;id&quot;, F.bround(&quot;pagerank&quot;, scale=4).alias('pagerank'))        res.orderBy(res.pagerank.desc()).show(5)    def stop(self):        try:            self.sc.stop()        except:            passif __name__ == '__main__':    appname = &quot;test_hyc00&quot;  # 任务名称    master = &quot;spark://192.168.xx.xx:7077&quot;  # 单机模式设置    # master = &quot;local&quot;    py_spark_client = PySparkClient(appname=appname, master=master)    # 简单功能测试    all_words = [&quot;scala&quot;, &quot;java&quot;, &quot;hadoop&quot;, &quot;spark&quot;, &quot;akka&quot;, &quot;spark vs hadoop&quot;,                 &quot;akka&quot;, &quot;spark vs hadoop&quot;, &quot;pyspark&quot;, &quot;pyspark and spark&quot;]    py_spark_client.simple_test(words=all_words)    # 文档词频统计    logFile = r&quot;hdfs://192.168.xx.xx:9000/test_data/README.md&quot;    py_spark_client.word_count(log_file=logFile)    py_spark_client.simple_graph()    # 关闭客户端连接    py_spark_client.stop()执行方法可以直接在pycharm中执行，这里pycharm使用的是远程的python环境，这里执行时的内存是默认值 1024.0 MiB在pycharm中执行可能需要加上变量设置，但使用spark-submit则可以不需要12os.environ[&quot;PYSPARK_PYTHON&quot;] = &quot;/root/anaconda3/envs/ray37/bin/python3.7&quot;os.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = &quot;/root/anaconda3/envs/ray37/bin/python3.7&quot;或者执行以下命令执行，这里执行时的内存是spark中配置的 8.0 GiB1./bin/spark-submit  --master spark://192.168.xx.xx:7077 /home/huangyc/hyc_test/tests/pyspark_test/connenct_test.py本地连接除了配置远程服务器的py解释器，还可以配置本地的模式需要在spark集群中配置host映射，通过vi /etc/hosts命令打开hosts文件，添加本地的主机名映射12# hyc10.10.0.xx DSE-20191111ZOU最后执行/etc/init.d/network restart刷新DNS如果没有的话，可能会报错误Caused by: java.io.IOException: Failed to connect to DSE-20191111ZOU:65320Caused by: java.net.UnknownHostException: DSE-20191111ZOUpy代码中需要添加以下的环境参数才能连接上spark集群123456789101112131415161718192021222324252627import os# 配置集群的python环境路径py_root = &quot;/root/anaconda3/envs/ray37/bin/python3.7&quot;os.environ[&quot;PYSPARK_PYTHON&quot;] = py_rootos.environ[&quot;PYSPARK_DRIVER_PYTHON&quot;] = py_root# 配置本地Jdk路径，版本尽量和spark集群的一致os.environ['JAVA_HOME'] = 'G:\Java\jdk1.8.0_201'# 配置本地的spark路径，这里直接从spark集群copy一份下载即可os.environ[&quot;SPARK_HOME&quot;] = r&quot;E:\PycharmWS\remote_spark\spark-3.0&quot;class PySparkClient:    def __init__(self, appname: str, master: str):        conf = SparkConf().setAppName(appname).setMaster(master)  # spark资源配置        # 以下的参数不用全部配置，看自己的需求        conf.set(&quot;spark.driver.maxResultSize&quot;, &quot;4g&quot;)        conf.set(&quot;spark.executor.num&quot;, &quot;4&quot;)        conf.set(&quot;spark.executor.memory&quot;, &quot;2g&quot;)        conf.set(&quot;spark.executor.cores&quot;, &quot;4&quot;)        conf.set(&quot;spark.cores.max&quot;, &quot;16&quot;)        conf.set(&quot;spark.driver.memory&quot;, &quot;2g&quot;)        try:            self.spark = SparkSession.builder.config(conf=conf).getOrCreate()            self.sc = self.spark.sparkContext        except:            traceback.print_exc()  # 返回出错信息⚡: 本地的python版本尽量和spark集群上的一致，避免不必要的错误常见问题</content>
	</entry>
	<entry>
		<title>Spring注解_介绍</title>
		<url>/article/7819d42.html</url>
		<content>Java中的注解到底是如何工作的？Java中的注解到底是如何工作的？Java 必须掌握的 12 种 Spring 常用注解！java中的注解，真的很重要，你理解了嘛自Java5.0版本引入注解之后，它就成为了Java平台中非常重要的一部分。开发过程中，我们也时常在应用代码中会看到诸如@Override，@Deprecated这样的注解。这篇文章中，我将向大家讲述到底什么是注解，为什么要引入注解，注解是如何工作的，如何编写自定义的注解(通过例子)，什么情况下可以使用注解以及最新注解和ADF(应用开发框架)。这会花点儿时间，所以为自己准备一杯咖啡，让我们来进入注解的世界吧。 https://www.zhihu.com/question/64671972 Spring和SpringBoot比较，解惑区别用Spring Boot颠覆Java应用开发什么是注解？用一个词就可以描述注解，那就是元数据，即一种描述数据的数据。所以，可以说注解就是源代码的元数据。比如，下面这段代码：1234@Overridepublic String toString() {    return &quot;This is String Representation of current object.&quot;;}上面的代码中，我重写了toString()方法并使用了@Override注解。但是，即使我不使用@Override注解标记代码，程序也能够正常执行。那么，该注解表示什么？这么写有什么好处吗？事实上，@Override告诉编译器这个方法是一个重写方法(描述方法的元数据)，如果父类中不存在该方法，编译器便会报错，提示该方法没有重写父类中的方法。如果我不小心拼写错误，例如将toString()写成了toStrring(){double r}，而且我也没有使用@Override注解，那程序依然能编译运行。但运行结果会和我期望的大不相同。现在我们了解了什么是注解，并且使用注解有助于阅读程序。推荐阅读：Spring MVC常用注解，你会几个？Annotation是一种应用于类、方法、参数、变量、构造器及包声明中的特殊修饰符。它是一种由JSR-175标准选择用来描述元数据的一种工具。为什么要引入注解？使用Annotation之前(甚至在使用之后)，XML被广泛的应用于描述元数据。不知何时开始一些应用开发人员和架构师发现XML的维护越来越糟糕了。他们希望使用一些和代码紧耦合的东西，而不是像XML那样和代码是松耦合的(在某些情况下甚至是完全分离的)代码描述。如果你在Google中搜索“XML vs. annotations”，会看到许多关于这个问题的辩论。最有趣的是XML配置其实就是为了分离代码和配置而引入的。上述两种观点可能会让你很疑惑，两者观点似乎构成了一种循环，但各有利弊。下面我们通过一个例子来理解这两者的区别。假如你想为应用设置很多的常量或参数，这种情况下，XML是一个很好的选择，因为它不会同特定的代码相连。如果你想把某个方法声明为服务，那么使用Annotation会更好一些，因为这种情况下需要注解和方法紧密耦合起来，开发人员也必须认识到这点。另一个很重要的因素是Annotation定义了一种标准的描述元数据的方式。在这之前，开发人员通常使用他们自己的方式定义元数据。例如，使用标记interfaces，注释，transient关键字等等。每个程序员按照自己的方式定义元数据，而不像Annotation这种标准的方式。目前，许多框架将XML和Annotation两种方式结合使用，平衡两者之间的利弊。编写自定义的Annotation在讲述这部分之前，建议你首先下载Annotation的示例代码AnnotationsSample.zip 。下载之后放在你习惯使用的IDE中，这些代码会帮助你更好的理解Annotation机制。推荐阅读：Java 必须掌握的 12 种 Spring 常用注解！编写Annotation非常简单，可以将Annotation的定义同接口的定义进行比较。我们来看两个例子：一个是标准的注解@Override，另一个是用户自定义注解@Todo。1234@Target(ElementType.METHOD)@Retention(RetentionPolicy.SOURCE)   public @interface Override {}对于@Override注释你可能有些疑问，它什么都没做，那它是如何检查在父类中有一个同名的函数呢。当然，不要惊讶，我是逗你玩的。@Override注解的定义不仅仅只有这么一点代码。这部分内容很重要，我不得不再次重复：Annotations仅仅是元数据，和业务逻辑无关。理解起来有点困难，但就是这样。如果Annotations不包含业务逻辑，那么必须有人来实现这些逻辑。元数据的用户来做这个事情。Annotations仅仅提供它定义的属性(类/方法/包/域)的信息。Annotations的用户(同样是一些代码)来读取这些信息并实现必要的逻辑。当我们使用Java的标注Annotations(例如@Override)时，JVM就是一个用户，它在字节码层面工作。到这里，应用开发人员还不能控制也不能使用自定义的注解。因此，我们讲解一下如何编写自定义的Annotations。我们来逐个讲述编写自定义Annotations的要点。上面的例子中，你看到一些注解应用在注解上。J2SE5.0版本在 java.lang.annotation提供了四种元注解，专门注解其他的注解：1234@Documented  – 注解是否将包含在JavaDoc中@Retention   – 什么时候使用该注解@Target?     – 注解用于什么地方@Inherited   – 是否允许子类继承该注解@Documented–一个简单的Annotations标记注解，表示是否将注解信息添加在java文档中。@Retention– 定义该注解的生命周期。123RetentionPolicy.SOURCE  – 在编译阶段丢弃。这些注解在编译结束之后就不再有任何意义，所以它们不会写入字节码。@Override, @SuppressWarnings都属于这类注解。RetentionPolicy.CLASS   – 在类加载的时候丢弃。在字节码文件的处理中有用。注解默认使用这种方式。RetentionPolicy.RUNTIME – 始终不会丢弃，运行期也保留该注解，因此可以使用反射机制读取该注解的信息。我们自定义的注解通常使用这种方式。@Target – 表示该注解用于什么地方。如果不明确指出，该注解可以放在任何地方。需要说明的是：属性的注解是兼容的，如果你想给7个属性都添加注解，仅仅排除一个属性，那么你需要在定义target包含所有的属性。12345678ElementType.TYPE:用于描述类、接口或enum声明ElementType.FIELD:用于描述实例变量ElementType.METHODElementType.PARAMETERElementType.CONSTRUCTORElementType.LOCAL_VARIABLEElementType.ANNOTATION_TYPE 另一个注释ElementType.PACKAGE 用于记录java文件的package信息@Inherited – 定义该注释和子类的关系那么，注解的内部到底是如何定义的呢？Annotations只支持基本类型、String及枚举类型。注释中所有的属性被定义成方法，并允许提供默认值。指定子类可以继承父类的注解，只能是类上的注解，方法和字段的注解不能继承。即如果父类上的注解是@Inherited修饰的就能被子类继承。123456789@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)@interface Todo {    public enum Priority {LOW, MEDIUM, HIGH}    public enum Status {STARTED, NOT_STARTED}    String author() default &quot;Yash&quot;;    Priority priority() default Priority.LOW;    Status status() default Status.NOT_STARTED;}下面的例子演示了如何使用上面的注解。12345@Todo(priority = Todo.Priority.MEDIUM, author = &quot;Yashwant&quot;, status = Todo.Status.STARTED)public void incompleteMethod1() {       //Some business logic is written       //But it’s not complete yet}如果注解中只有一个属性，可以直接命名为“value”，使用时无需再标明属性名。123456@interface Author{   String value();}@Author(&quot;Yashwant&quot;)public void someMethod() {}但目前为止一切看起来都还不错。我们定义了自己的注解并将其应用在业务逻辑的方法上。现在我们需要写一个用户程序调用我们的注解。这里我们需要使用反射机制。如果你熟悉反射代码，就会知道反射可以提供类名、方法和实例变量对象。所有这些对象都有getAnnotation()这个方法用来返回注解信息。我们需要把这个对象转换为我们自定义的注释(使用 instanceOf()检查之后)，同时也可以调用自定义注释里面的方法。看看以下的实例代码，使用了上面的注解:12345678910Class businessLogicClass = BusinessLogic.class;for(Method method : businessLogicClass.getMethods()) {　　Todo todoAnnotation = (Todo)method.getAnnotation(Todo.class);　　if(todoAnnotation != null) {　　　　System.out.println(&quot; Method Name : &quot; + method.getName());　　　　System.out.println(&quot; Author : &quot; + todoAnnotation.author());　　　　System.out.println(&quot; Priority : &quot; + todoAnnotation.priority());　　　　System.out.println(&quot; Status : &quot; + todoAnnotation.status());　　}}jdk1.8又提供了以下两个元注解@Native指定字段是一个常量，其值引用native code。@Repeatable注解上可以使用重复注解，即可以在一个地方可以重复使用同一个注解，像spring中的包扫描注解就使用了这个。所有元注解定义在java.lang.annotation包下面，其中Annotation是注解的基本接口，所有的注解都继承这个接口自定义注解：　　使用@interface自定义注解时，自动继承了java.lang.annotation.Annotation接口，由编译程序自动完成其他细节。在定义注解时，不能继承其他的注解或接口。@interface用来声明一个注解，其中的每一个方法实际上是声明了一个配置参数。方法的名称就是参数的名称，返回值类型就是参数的类型（返回值类型只能是基本类型、Class、String、enum）。可以通过default来声明参数的默认值。　　定义注解格式：1public @interface 注解名 {定义体}　　注解参数的可支持数据类型：所有基本数据类型（int,float,boolean,byte,double,char,long,short)String类型Class类型enum类型Annotation类型以上所有类型的数组　　Annotation类型里面的参数该怎么设定: 第一, 只能用public或默认(default)这两个访问权修饰.例如,String value();这里把方法设为defaul默认类型；　 　第二, 参数成员只能用基本类型byte,short,char,int,long,float,double,boolean八种基本数据类型和 String,Enum,Class,annotations等数据类型,以及这一些类型的数组.例如,String value();这里的参数成员就为String;　　第三, 如果只有一个参数成员,最好把参数名称设为”value”,后加小括号.例:下面的注解就只有一个参数成员，是一个String数组。123456@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface Entrance{   String[] value() default {};}注解元素的默认值：　　注解元素必须有确定的值，要么在定义注解的默认值中指定，要么在使用注解时指定，非基本类型的注解元素的值不可为null。因此, 使用空字符串或0作为默认值是一种常用的做法。这个约束使得处理器很难表现一个元素的存在或缺失的状态，因为每个注解的声明中，所有元素都存在，并且都具有相应的值，为了绕开这个约束，我们只能定义一些特殊的值，例如空字符串或者负数，一次表示某个元素不存在，在定义注解时，这已经成为一个习惯用法。123456@Target(ElementType.METHOD)@Retention(RetentionPolicy.RUNTIME)public @interface Entrance{public String name() default &quot;&quot;;}注解用例注解的功能很强大，Spring和Hebernate这些框架在日志和有效性中大量使用了注解功能。注解可以应用在使用标记接口的地方。不同的是标记接口用来定义完整的类，但你可以为单个的方法定义注释，例如是否将一个方法暴露为服务。推荐阅读：Spring Boot 最核心的 25 个注解，都是干货！在最新的servlet3.0中引入了很多新的注解，尤其是和servlet安全相关的注解。HandlesTypes –该注解用来表示一组传递给ServletContainerInitializer的应用类。HttpConstraint – 该注解代表所有HTTP方法的应用请求的安全约束，和ServletSecurity注释中定义的HttpMethodConstraint安全约束不同。HttpMethodConstraint – 指明不同类型请求的安全约束，和ServletSecurity 注解中描述HTTP协议方法类型的注释不同。MultipartConfig –该注解标注在Servlet上面，表示该Servlet希望处理的请求的 MIME 类型是 multipart/form-data。ServletSecurity 该注解标注在Servlet继承类上面，强制该HTTP协议请求遵循安全约束。WebFilter – 该注解用来声明一个Server过滤器；WebInitParam – 该注解用来声明Servlet或是过滤器的中的初始化参数，通常配合 @WebServlet 或者 @WebFilter 使用。WebListener –该注解为Web应用程序上下文中不同类型的事件声明监听器。WebServlet –该注解用来声明一个Servlet的配置。ADF(应用程序框架)和注解现在我们开始讨论文章的最后一部分了。应用程序框架，被称为ADF，由Oracle开发用来创建Oracle融合应用。我们已经了解了注解的优缺点，也知道如何编写自定义的注解，但我们应该将注解应用在ADF的哪部分呢？ADF是否提供了一些朴素的注解？很好的问题，确实在ADF中大量使用注解有一些限制。之前提到的应用框架如Spring和Hibernate使用AOP(面向侧面的程序设计)。在AOP中，框架提供了一种机制，在事件的预处理和后续处理中注入代码。例如：你有一个钩子用来在方法执行之前和之后添加代码，所以你可以在这些地方编写你的用户代码。ADF不使用AOP。如果我们有任何注解的用例可用，我们可能需要通过继承的方式实现。</content>
	</entry>
	<entry>
		<title>Spring注解_常用注解</title>
		<url>/article/a0f3b9ff.html</url>
		<content>12种 Spring 常用注解声明bean的注解@Component 组件，没有明确的角色@Service 在业务逻辑层使用（service层）@Repository 在数据访问层使用（dao层）@Controller 在展现层使用，控制器的声明（Controller层）注入bean的注解ANNOTATIONPACKAGESOURCE@Resourcejavax.annotationJava JSR-250@Injectjavax.injectJava JSR-330@Autowiredorg.springframework.bean.factorySpring 2.5+JSR是Java Specification Requests的缩写，意思是Java 规范提案。是指向JCP(Java Community Process)提出新增一个标准化技术规范的正式请求。任何人都可以提交JSR，以向Java平台增添新的API和服务。JSR已成为Java界的一个重要标准。@Resource,@Autowired,@Inject3种注入方式详解@Autowired：由Spring提供默认按类型装配，找不到或者找到多个则报错。如果要按名称装配，需要结合Spring另外一个注解Qualifier(“name”)使用。默认必须装配requred=true，如果可以为空，可以设置为false，在Spring4+结合jdk8+的情况下还可以使用Optional和false同等的效果，如12@Autowiredprivate Optional&lt;UserService&gt; userService;@Inject：由JSR-330提供和@Autowired类似，可以完全代替@Autowired，但这个没有required属性，要求bean必须存在。如果要按名称装配，需要结合javax另外一个注解N(“name”)使用。@Resource：由JSR-250提供它有两个关键的属性：name－名称，type－类型1、如果指定了name,type，则从Spring容器中找一个名称和类型相当应的一个bean，找不到则报错。2、如果只指定了name，则从Spring容器中找一个名称和name一样的bean，找不到则报错。3、如果只指定了type，则从Spring容器中找一个类型和type一样的bean，找不到或者找到多个则报错。4、如果没有指定参数，则默认找字段名称装配，找不到则按类型装配，找不到则报错。都可以注解在set方法和属性上，推荐注解在属性上（一目了然，少写代码）。Spring常用的三种注入方式Spring通过DI（依赖注入）实现IOC（控制反转），常用的注入方式主要有三种：构造方法注入，setter注入，基于注解的注入。1. 构造方法注入在spring的配置文件中注册UserService，将UserDaoJdbc通过constructor-arg标签注入到UserService的某个有参数的构造方法123456&lt;!-- 注册userService --&gt;&lt;bean id=&quot;userService&quot; class=&quot;com.lyu.spring.service.impl.UserService&quot;&gt;   &lt;constructor-arg ref=&quot;userDaoJdbc&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt;&lt;!-- 注册jdbc实现的dao --&gt;&lt;bean id=&quot;userDaoJdbc&quot; class=&quot;com.lyu.spring.dao.impl.UserDaoJdbc&quot;&gt;&lt;/bean&gt;如果只有一个有参数的构造方法并且参数类型与注入的bean的类型匹配，那就会注入到该构造方法中。123456789public class UserService implements IUserService {       private IUserDao userDao;       public UserService(IUserDao userDao) {           this.userDao = userDao;       }       public void loginUser() {           userDao.loginUser();       }}问题一：如果有多个有参数的构造方法并且每个构造方法的参数列表里面都有要注入的属性，那userDaoJdbc会注入到哪里呢？12345678910111213141516public class UserService implements IUserService {       private IUserDao userDao;       private User user;       public UserService(IUserDao userDao) {           System.out.println(&quot;这是有一个参数的构造方法&quot;);           this.userDao = userDao;       }       public UserService(IUserDao userDao, User user) {           System.out.println(&quot;这是有两个参数的构造方法&quot;);           this.userDao = userDao;           this.user = user;       }       public void loginUser() {           userDao.loginUser();       }}结果：会注入到只有一个参数的构造方法中，并且经过测试注入哪一个构造方法与构造方法的顺序无关问题二：如果只有一个构造方法，但是有两个参数，一个是待注入的参数，另一个是其他类型的参数，那么这次注入可以成功吗？1234567891011public class UserService implements IUserService {       private IUserDao userDao;       private User user;       public UserService(IUserDao userDao, User user) {           this.userDao = userDao;           this.user = user;       }       public void loginUser() {           userDao.loginUser();       }}结果：失败了，即使在costract-arg标签里面通过name属性指定要注入的参数名userDao也会失败.问题三：如果我们想向有多个参数的构造方法中注入值该在配置文件中怎么写呢？1234567891011public class UserService implements IUserService {       private IUserDao userDao;       private User user;       public UserService(IUserDao userDao, User user) {           this.userDao = userDao;           this.user = user;       }       public void loginUser() {           userDao.loginUser();       }}参考写法：通过name属性指定要注入的值，与构造方法参数列表参数的顺序无关12345678910&lt;!-- 注册userService --&gt;&lt;bean id=&quot;userService&quot; class=&quot;com.lyu.spring.service.impl.UserService&quot;&gt;       &lt;constructor-arg name=&quot;userDao&quot; ref=&quot;userDaoJdbc&quot;&gt;&lt;/constructor-arg&gt;       &lt;constructor-arg name=&quot;user&quot; ref=&quot;user&quot;&gt;&lt;/constructor-arg&gt;&lt;/bean&gt;&lt;!-- 注册实体User类，用于测试 --&gt;&lt;bean id=&quot;user&quot; class=&quot;com.lyu.spring.entity.User&quot;&gt;&lt;/bean&gt;&lt;!-- 注册jdbc实现的dao --&gt;&lt;bean id=&quot;userDaoJdbc&quot; class=&quot;com.lyu.spring.dao.impl.UserDaoJdbc&quot;&gt;&lt;/bean&gt;问题四：如果有多个构造方法，每个构造方法只有参数的顺序不同，那通过构造方法注入多个参数会注入到哪一个呢？1234567891011121314151617public class UserService implements IUserService {       private IUserDao userDao;       private User user;       public UserService(IUserDao userDao, User user) {           System.out.println(&quot;这是第二个构造方法&quot;);           this.userDao = userDao;           this.user = user;       }       public UserService(User user, IUserDao userDao) {           System.out.println(&quot;这是第一个构造方法&quot;);           this.userDao = userDao;           this.user = user;       }       public void loginUser() {           userDao.loginUser();       }}结果：哪个构造方法在前就注入哪一个，这种情况下就与构造方法顺序有关。2. setter方法注入配置文件如下：12345678910&gt;&lt;!-- 注册userService --&gt;&gt;&lt;bean id=&quot;userService&quot; class=&quot;com.lyu.spring.service.impl.UserService&quot;&gt;    &lt;!-- 写法一 --&gt;    &lt;!-- &lt;property name=&quot;UserDao&quot; ref=&quot;userDaoMyBatis&quot;&gt;&lt;/property&gt; --&gt;    &lt;!-- 写法二 --&gt;    &lt;property name=&quot;userDao&quot; ref=&quot;userDaoMyBatis&quot;&gt;&lt;/property&gt;&gt;&lt;/bean&gt;&gt;&lt;!-- 注册mybatis实现的dao --&gt;&gt;&lt;bean id=&quot;userDaoMyBatis&quot; class=&quot;com.lyu.spring.dao.impl.UserDaoMyBatis&quot;&gt;&lt;/bean&gt;注：上面这两种写法都可以,spring会将name值的每个单词首字母转换成大写，然后再在前面拼接上”set”构成一个方法名,然后去对应的类中查找该方法,通过反射调用,实现注入。切记：name属性值与类中的成员变量名以及set方法的参数名都无关，只与对应的set方法名有关，下面的这种写法是可以运行成功的123456789&gt;public class UserService implements IUserService {       private IUserDao userDao1;       public void setUserDao(IUserDao userDao1) {            this.userDao1 = userDao1;       }       public void loginUser() {            userDao1.loginUser();       }&gt;}还有一点需要注意：如果通过set方法注入属性，那么spring会通过默认的空参构造方法来实例化对象，所以如果在类中写了一个带有参数的构造方法，一定要把空参数的构造方法写上，否则spring没有办法实例化对象，导致报错。3. 基于注解的注入@Resource：java的注解，默认以byName的方式去匹配与属性名相同的bean的id，如果没有找到就会以byType的方式查找，如果byType查找到多个的话，使用@Qualifier注解（spring注解）指定某个具体名称的bean。1234&gt;@Resource&gt;@Qualifier(&quot;userDaoMyBatis&quot;)&gt;private IUserDao userDao;&gt;public UserService(){}@Resource装配顺序如果同时指定了name和type，则从Spring上下文中找到唯一匹配的bean进行装配，找不到则抛出异常如果指定了name，则从上下文中查找名称（id）匹配的bean进行装配，找不到则抛出异常如果指定了type，则从上下文中找到类型匹配的唯一bean进行装配，找不到或者找到多个，都会抛出异常如果既没有指定name，又没有指定type，则自动按照byName方式进行装配；如果没有匹配，则回退为一个原始类型进行匹配，如果匹配则自动装配@Autowired：spring注解，默认是以byType的方式去匹配与属性名相同的bean的id，如果没有找到，就通过byName的方式去查找。123&gt;@Autowired&gt;@Qualifier(&quot;userDaoJdbc&quot;)&gt;private IUserDao userDao;java配置类相关注解@Configuration 声明当前类为配置类，相当于xml形式的Spring配置（类上）@Bean 注解在方法上，声明当前方法的返回值为一个bean，替代xml中的方式（方法上）@Configuration 声明当前类为配置类，其中内部组合了@Component注解，表明这个类是一个bean（类上）@ComponentScan 用于对Component进行扫描，相当于xml中的（类上）@WishlyConfiguration 为@Configuration与@ComponentScan的组合注解，可以替代这两个注解切面(AOP)相关注解Spring支持AspectJ的注解式切面编程。@Aspect 声明一个切面（类上）使用@After、@Before、@Around定义建言（advice），可直接将拦截规则（切点）作为参数。@After 在方法执行之后执行（方法上）@Before 在方法执行之前执行（方法上）@Around 在方法执行之前与之后执行（方法上）@PointCut 声明切点在java配置类中使用@EnableAspectJAutoProxy注解开启Spring对AspectJ代理的支持（类上）@Bean的属性支持@Scope 设置Spring容器如何新建Bean实例（方法上，得有@Bean）其设置类型包括：Singleton （单例,一个Spring容器中只有一个bean实例，默认模式）,Protetype （每次调用新建一个bean）,Request （web项目中，给每个http request新建一个bean）,Session （web项目中，给每个http session新建一个bean）,GlobalSession（给每一个 global http session新建一个Bean实例）@StepScope 在Spring Batch中还有涉及@PostConstruct 由JSR-250提供，在构造函数执行完之后执行，等价于xml配置文件中bean的initMethod@PreDestory 由JSR-250提供，在Bean销毁之前执行，等价于xml配置文件中bean的destroyMethod@Value注解@Value 为属性注入值（属性上）支持如下方式的注入：》注入普通字符@Value(“Michael Jackson”)String name;》注入操作系统属性@Value(“#{systemProperties[‘os.name’]}”)String osName;》注入表达式结果@Value(“#{ T(java.lang.Math).random() * 100 }”) String randomNumber;》注入其它bean属性@Value(“#{domeClass.name}”)String name;》注入文件资源@Value(“classpath:com/hgs/hello/test.txt”)String Resource file;》注入网站资源@Value(“http://www.javastack.cn&quot;)Resource url;》注入配置文件Value(“${book.name}”)String bookName;注入配置使用方法： ① 编写配置文件（test.properties）book.name=《三体》② @PropertySource 加载配置文件(类上)@PropertySource(“classpath:com/hgs/hello/test/test.propertie”)③ 还需配置一个PropertySourcesPlaceholderConfigurer的bean。环境切换@Profile 通过设定Environment的ActiveProfiles来设定当前context需要使用的配置环境。（类或方法上）@Conditional Spring4中可以使用此注解定义条件话的bean，通过实现Condition接口，并重写matches方法，从而决定该bean是否被实例化。（方法上）异步相关@EnableAsync 配置类中，通过此注解开启对异步任务的支持，叙事性AsyncConfigurer接口（类上），点击这里了解使用详情。@Async 在实际执行的bean方法使用该注解来申明其是一个异步任务（方法上或类上所有的方法都将异步，需要@EnableAsync开启异步任务）定时任务相关@EnableScheduling 在配置类上使用，开启计划任务的支持（类上）@Scheduled 来申明这是一个任务，包括cron,fixDelay,fixRate等类型（方法上，需先开启计划任务的支持）@Enable*注解说明这些注解主要用来开启对xxx的支持。@EnableAspectJAutoProxy 开启对AspectJ自动代理的支持@EnableAsync 开启异步方法的支持@EnableScheduling 开启计划任务的支持@EnableWebMvc 开启Web MVC的配置支持@EnableConfigurationProperties 开启对@ConfigurationProperties注解配置Bean的支持@EnableJpaRepositories 开启对SpringData JPA Repository的支持@EnableTransactionManagement 开启注解式事务的支持@EnableTransactionManagement 开启注解式事务的支持@EnableCaching 开启注解式的缓存支持测试相关注解@RunWith 运行器，Spring中通常用于对JUnit的支持@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration 用来加载配置ApplicationContext，其中classes属性用来加载配置类@ContextConfiguration(classes={TestConfig.class})SpringMVC相关注解@EnableWebMvc 在配置类中开启Web MVC的配置支持，如一些ViewResolver或者MessageConverter等，若无此句，重写WebMvcConfigurerAdapter方法（用于对SpringMVC的配置）。@Controller 声明该类为SpringMVC中的Controller@RequestMapping 用于映射Web请求，包括访问路径和参数（类或方法上）@ResponseBody 支持将返回值放在response内，而不是一个页面，通常用户返回json数据（返回值旁或方法上）@RequestBody 允许request的参数在request体中，而不是在直接连接在地址后面。（放在参数前）@PathVariable 用于接收路径参数，比如@RequestMapping(“/hello/{name}”)申明的路径，将注解放在参数中前，即可获取该值，通常作为Restful的接口实现方法。@RestController 该注解为一个组合注解，相当于@Controller和@ResponseBody的组合，注解在类上，意味着，该Controller的所有方法都默认加上了@ResponseBody。@ControllerAdvice 通过该注解，我们可以将对于控制器的全局配置放置在同一个位置，注解了@Controller的类的方法可使用@ExceptionHandler、@InitBinder、@ModelAttribute注解到方法上，这对所有注解了 @RequestMapping的控制器内的方法有效。@ExceptionHandler 用于全局处理控制器里的异常@InitBinder 用来设置WebDataBinder，WebDataBinder用来自动绑定前台请求参数到Model中。@ModelAttribute 本来的作用是绑定键值对到Model里，在@ControllerAdvice中是让全局的@RequestMapping都能获得在此处设置的键值对。spring 三种装配机制隐式的自动装配bean在《Spring实战》这本书中将自动装配分为了两个部分（组件扫描和自动装配）：组件扫描：Spring会自动发现应用上下文（Spring容器）中所创建的bean；自动装配：Spring自动满足bean之间的依赖1、创建可被发现的bean1234&gt;package soundsystem;&gt;public interface CompactDisc{   void play();&gt;}创建带有@Component注解的CompactDisc接口的一个实现类，这个类会被Spring扫描到并自动创建bean1234567891011&gt;package soundsystem;&gt;import ...&gt;@Component&gt;public class SgtPeppers implements CompactDisc{       private String title = &quot;Sgt, Pepper's Lonely Hearts Club Band&quot;;       private String artist = &quot;The Beatles&quot;;       public void play(){            System.out.println(&quot;Playing &quot; + title + &quot; By &quot; + artist);       }&gt;}使用@ComponentScan注解启动组件扫描，@ComponentScan 默认会扫描与配置类相同的包以及其下所有的子包，这样，被@Component注解修饰的类就会被发现123456&gt;package soundsystem;&gt;import ...&gt;@Configuration&gt;@ComponentScan&gt;public class CDPlayerConfig{}也可以使用XML配置的方式来启动组件扫描12345&gt;&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&gt;&lt;beans&gt;       &lt;context:component-scan base-package=&quot;soundsystem&quot;/&gt;&gt;&lt;/beans&gt;2、为组件扫描的 bean 命名若直接使用 @Component 注解来声明一个 bean，bean 的名字默认为类名首字母小写。例如，如下 SgtPeppers 类的默认 bean 名称为 sgtPeppers1234&gt;@Component()&gt;public class SgtPeppers implements CompactDisc{   ...&gt;}可以在 @Component 注解中说明此 bean 的名称：1234&gt;@Component(&quot;lonelyHeartClub&quot;)&gt;public class SgtPeppers implements CompactDisc{   ...&gt;}此外，也可以使用另外一种为 bean 命名的方式：1234567&gt;package soundsystem;&gt;import javax.inject.Named;&gt;@Named(&quot;lonelyHeartClub&quot;)&gt;public class SgtPeppers implements CompactDisc{   ...&gt;}Spring 支持将 @Named 作为 @Component 注解的替代方案，两者之间有一些细微的差异，但在大多数场景下，它们是可以相互替换的。推荐使用 @Component 注解，因为 @Named 注解并不能清楚的表明它是做什么的3、设置组件扫描的基础包在之前的案列中，我们没有为 @ComponentScan 注解设置任何属性，这意味着，按照默认规则，它会以配置类所在包作为基础包（ base  package ）来扫描组件.在之前的案列中，我们没有为 @ComponentScan 注解设置任何属性，这意味着，按照默认规则，它会以配置类所在包作为基础包（ base package ）来扫描组件.有一个原因会促使我们要明确的设置基础包，那就是我们想要将配置类放在单独的包中，使其与其他的应用代码区分开来。为了指定不同的包，只需要在 @ComponentScan 的 value 属性中指明包的名称：123&gt;@Configuration&gt;@ComponentScan(&quot;soundsystem&quot;)&gt;public class CDPlayerConfig{}或者，如果你想更加明确的表名你所设置的是基础包，那么你可以通过 basePackages 属性进行配置：123&gt;@Configuration&gt;@ComponentScan(basePackages=&quot;soundsystem&quot;)&gt;public class CDPlayerConfig{}如果你想扫描多个包，只需要将 basePackages 属性的值设置为要扫描包的一个数组即可：123&gt;@Configuration&gt;@ComponentScan(basePackages={&quot;soundsystem&quot;,&quot;video&quot;})&gt;public class CDPlayerConfig{}注意到，basePackages 属性的值是一个 String 类型的数组，这样配置没有问题，但却是不安全的，如果要重构代码的话，这些包的名字可能会被修改，从而导致包扫描出现错误，除了将包设置为简单的 String 类型之外，@ComponentScan 还提供了另一种方法，那就是将其指定为要扫描包中所含的类或接口。123&gt;@Configuration&gt;@ComponentScan(basePackageClasses={CDPlayer.class, DVDPlayer.class})&gt;public class CDPlayerConfig{}你可以在包中设置一个专门用来进行包扫描的空标记接口，这样，可以避免对任何实际应用的代码进行重构后，包扫描出现错误。4、通过为 bean 添加注解实现自动装配如果我们的应用程序所有对象都是独立的， 彼此之间没有任何依赖，那么仅仅使用Spring的扫描功能就算完事了,它会自动的帮我们把Java对象创建.但是如果对象与对象之间如果存在相互依赖的联系时,我们就需要用的Spring给我们提供的自动装配了.1234567891011&gt;@Component&gt;public class CDPlayer implements MediaPlayer{       private CompactDisc cd;       @Autowired       public CDPlayer(CompacrDisc cd){            this.cd = cd;       }       public void play(){            cd.play();       }&gt;}以上示例的构造器上添加了 @Autowired 注解，这表明当 Spring 创建 CDPlayer bean 的时候，会通过这个构造器进行实例化并传入一个 CompactDisc 类型的 bean。@Autowired 注解不仅能够用在构造器上，还可以用在属性设置的 Setter 方法上。实际上，Setter 方法并没有什么特殊之处，@Autowired 注解可以用在类的任何方法上：如下，@Autowired 注解完全能够发挥作用1234&gt;@Autowired&gt;public void insertDisc(CompactDisc cd){       this.cd = cd;&gt;}不管是构造器、Setter 方法还是其他方法，Spring 都会尝试满足方法参数上所声明的依赖，假如有且仅有一个 bean 依赖需求的话，那么这个 bean 就会被装填进来。如果没有匹配的 bean，那么在应用上下文创建的时候，Spring 会抛出一个异常。为了避免异常的出现，你可以将 @Autowired 的 required 属性设置为 false：1234&gt;@Autowired(required=false)&gt;public CDPlayer(CompactDisc cd){       this.cd = cd;&gt;}如果有多个 bean 都能满足依赖关系的话，Spring 也将会抛出一个异常，表明没有明确指定要选择那个 bean 进行装配。总结一下，自动装配bean的过程：一、把需要被扫描的类，添加 @Component注解，使它能够被Spring自动发现。二、通过显示的设置Java代码 @ComponentScan注解或XML配置，让Spring开启组件扫描，并将扫描的结果类创建bean。三、@Autowried注解能偶实现bean的自动装配，实现依赖注入。处理自动装配的歧义性问题，Spring. 处理自动装配的歧义性通过java代码装配bean通XML中装配bean</content>
	</entry>
	<entry>
		<title>多线程_JUC线程池</title>
		<url>/article/5912654c.html</url>
		<content>JUC线程池1234567891011121314151617181920212223242526271. Executor它是&quot;执行者&quot;接口，它是来执行任务的。准确的说，Executor提供了execute()接口来执行已提交的 Runnable 任务的对象。Executor存在的目的是提供一种将&quot;任务提交&quot;与&quot;任务如何运行&quot;分离开来的机制。它只包含一个函数接口：void execute(Runnable command)2. ExecutorServiceExecutorService继承于Executor。它是&quot;执行者服务&quot;接口，它是为&quot;执行者接口Executor&quot;服务而存在的；准确的话，ExecutorService提供了&quot;将任务提交给执行者的接口(submit方法)&quot;，&quot;让执行者执行任务(invokeAll, invokeAny方法)&quot;的接口等等。3. AbstractExecutorServiceAbstractExecutorService是一个抽象类，它实现了ExecutorService接口。AbstractExecutorService存在的目的是为ExecutorService中的函数接口提供了默认实现。AbstractExecutorService函数列表由于它的函数列表和ExecutorService一样，这里就不再重复列举了。4. ThreadPoolExecutorThreadPoolExecutor就是大名鼎鼎的&quot;线程池&quot;。它继承于AbstractExecutorService抽象类。5. ScheduledExecutorServiceScheduledExecutorService是一个接口，它继承于于ExecutorService。它相当于提供了&quot;延时&quot;和&quot;周期执行&quot;功能的ExecutorService。ScheduledExecutorService提供了相应的函数接口，可以安排任务在给定的延迟后执行，也可以让任务周期的执行。6. ScheduledThreadPoolExecutorScheduledThreadPoolExecutor继承于ThreadPoolExecutor，并且实现了ScheduledExecutorService接口。它相当于提供了&quot;延时&quot;和&quot;周期执行&quot;功能的ScheduledExecutorService。ScheduledThreadPoolExecutor类似于Timer，但是在高并发程序中，ScheduledThreadPoolExecutor的性能要优于Timer。7. ExecutorsExecutors是个静态工厂类。它通过静态工厂方法返回ExecutorService、ScheduledExecutorService、ThreadFactory 和 Callable 等类的对象。线程池示例12345678910111213141516171819202122232425262728293031323334353637383940import java.util.concurrent.Executors;import java.util.concurrent.ExecutorService;public class ThreadPoolDemo1 {    public static void main(String[] args) {        // 创建一个可重用固定线程数的线程池        ExecutorService pool = Executors.newFixedThreadPool(2);        // 创建实现了Runnable接口对象，Thread对象当然也实现了Runnable接口        Thread ta = new MyThread();        Thread tb = new MyThread();        Thread tc = new MyThread();        Thread td = new MyThread();        Thread te = new MyThread();        // 将线程放入池中进行执行        pool.execute(ta);        pool.execute(tb);        pool.execute(tc);        pool.execute(td);        pool.execute(te);        // 关闭线程池        pool.shutdown();    }}class MyThread extends Thread {    @Override    public void run() {        System.out.println(Thread.currentThread().getName()+ &quot; is running.&quot;);    }}运行结果：pool-1-thread-1 is running.pool-1-thread-2 is running.pool-1-thread-1 is running.pool-1-thread-2 is running.pool-1-thread-1 is running.结果说明：主线程中创建了线程池pool，线程池的容量是2。即，线程池中最多能同时运行2个线程。紧接着，将ta,tb,tc,td,te这3个线程添加到线程池中运行。最后，通过shutdown()关闭线程池。线程池原理(上)ThreadPoolExecutor简介ThreadPoolExecutor是线程池类。对于线程池，可以通俗的将它理解为”存放一定数量线程的一个线程集合。线程池允许若个线程同时允许，允许同时运行的线程数量就是线程池的容量；当添加的到线程池中的线程超过它的容量时，会有一部分线程阻塞等待。线程池会通过相应的调度策略和拒绝策略，对添加到线程池中的线程进行管理。”1234567891011121314151617181920212223// 阻塞队列。private final BlockingQueue&lt;Runnable&gt; workQueue;// 互斥锁private final ReentrantLock mainLock = new ReentrantLock();// 线程集合。一个Worker对应一个线程。private final HashSet&lt;Worker&gt; workers = new HashSet&lt;Worker&gt;();// “终止条件”，与“mainLock”绑定。private final Condition termination = mainLock.newCondition();// 线程池中线程数量曾经达到过的最大值。private int largestPoolSize;// 已完成任务数量private long completedTaskCount;// ThreadFactory对象，用于创建线程。private volatile ThreadFactory threadFactory;// 拒绝策略的处理句柄。private volatile RejectedExecutionHandler handler;// 保持线程存活时间。private volatile long keepAliveTime;private volatile boolean allowCoreThreadTimeOut;// 核心池大小private volatile int corePoolSize;// 最大池大小private volatile int maximumPoolSize;workers    workers是HashSet类型，即它是一个Worker集合。而一个Worker对应一个线程，也就是说线程池通过workers包含了”一个线程集合”。当Worker对应的线程池启动时，它会执行线程池中的任务；当执行完一个任务后，它会从线程池的阻塞队列中取出一个阻塞的任务来继续运行。    wokers的作用是，线程池通过它实现了”允许多个线程同时运行”。workQueue    workQueue是BlockingQueue类型，即它是一个阻塞队列。当线程池中的线程数超过它的容量的时候，线程会进入阻塞队列进行阻塞等待。    通过workQueue，线程池实现了阻塞功能。mainLock    mainLock是互斥锁，通过mainLock实现了对线程池的互斥访问。corePoolSize和maximumPoolSize    corePoolSize是”核心池大小”，maximumPoolSize是”最大池大小”。它们的作用是调整”线程池中实际运行的线程的数量”。    例如，当新任务提交给线程池时(通过execute方法)。          — 如果此时，线程池中运行的线程数量&lt; corePoolSize，则创建新线程来处理请求。          — 如果此时，线程池中运行的线程数量&gt; corePoolSize，但是却&lt; maximumPoolSize；则仅当阻塞队列满时才创建新线程。          如果设置的 corePoolSize 和 maximumPoolSize 相同，则创建了固定大小的线程池。如果将 maximumPoolSize 设置为基本的无界值（如 Integer.MAX_VALUE），则允许池适应任意数量的并发任务。在大多数情况下，核心池大小和最大池大小的值是在创建线程池设置的；但是，也可以使用 setCorePoolSize(int) 和 setMaximumPoolSize(int) 进行动态更改。poolSize    poolSize是当前线程池的实际大小，即线程池中任务的数量。allowCoreThreadTimeOut和keepAliveTime    allowCoreThreadTimeOut表示是否允许”线程在空闲状态时，仍然能够存活”；而keepAliveTime是当线程池处于空闲状态的时候，超过keepAliveTime时间之后，空闲的线程会被终止。threadFactory    threadFactory是ThreadFactory对象。它是一个线程工厂类，”线程池通过ThreadFactory创建线程”。handler    handler是RejectedExecutionHandler类型。它是”线程池拒绝策略”的句柄，也就是说”当某任务添加到线程池中，而线程池拒绝该任务时，线程池会通过handler进行相应的处理”。综上所说，线程池通过workers来管理”线程集合”，每个线程在启动后，会执行线程池中的任务；当一个任务执行完后，它会从线程池的阻塞队列中取出任务来继续运行。阻塞队列是管理线程池任务的队列，当添加到线程池中的任务超过线程池的容量时，该任务就会进入阻塞队列进行等待。线程池调度说明：    在”图-01”中，线程池中有N个任务。”任务1”, “任务2”, “任务3”这3个任务在执行，而”任务3”到”任务N”在阻塞队列中等待。正在执行的任务，在workers集合中，workers集合包含3个Worker，每一个Worker对应一个Thread线程，Thread线程每次处理一个任务。    当workers集合中处理完某一个任务之后，会从阻塞队列中取出一个任务来继续执行，如图-02所示。图-02表示”任务1”处理完毕之后，线程池将”任务4”从阻塞队列中取出，放到workers中进行处理。线程池示例1234567891011121314151617181920212223242526272829303132333435363738import java.util.concurrent.Executors;import java.util.concurrent.ExecutorService;public class ThreadPoolDemo1 {    public static void main(String[] args) {        // 创建一个可重用固定线程数的线程池        ExecutorService pool = Executors.newFixedThreadPool(2);        // 创建实现了Runnable接口对象，Thread对象当然也实现了Runnable接口        Thread ta = new MyThread();        Thread tb = new MyThread();        Thread tc = new MyThread();        Thread td = new MyThread();        Thread te = new MyThread();        // 将线程放入池中进行执行        pool.execute(ta);        pool.execute(tb);        pool.execute(tc);        pool.execute(td);        pool.execute(te);        // 关闭线程池        pool.shutdown();    }}class MyThread extends Thread {    @Override    public void run() {        System.out.println(Thread.currentThread().getName()+ &quot; is running.&quot;);    }}运行结果：pool-1-thread-1 is running.pool-1-thread-2 is running.pool-1-thread-1 is running.pool-1-thread-2 is running.pool-1-thread-1 is running.示例中，包括了线程池的创建，将任务添加到线程池中，关闭线程池这3个主要的步骤。创建线程池123456789101112131415161718192021222324252627282930313233343536371. newFixedThreadPool()newFixedThreadPool()在Executors.java中定义，源码如下：public static ExecutorService newFixedThreadPool(int nThreads) {    return new ThreadPoolExecutor(nThreads, nThreads,                                  0L, TimeUnit.MILLISECONDS,                                  new LinkedBlockingQueue&lt;Runnable&gt;());}说明：newFixedThreadPool(int nThreads)的作用是创建一个线程池，线程池的容量是nThreads。newFixedThreadPool()在调用ThreadPoolExecutor()时，会传递一个LinkedBlockingQueue()对象，而LinkedBlockingQueue是单向链表实现的阻塞队列。在线程池中，就是通过该阻塞队列来实现&quot;当线程池中任务数量超过允许的任务数量时，部分任务会阻塞等待&quot;。2. ThreadPoolExecutor()ThreadPoolExecutor()在ThreadPoolExecutor.java中定义，源码如下：public ThreadPoolExecutor(int corePoolSize,                          int maximumPoolSize,                          long keepAliveTime,                          TimeUnit unit,                          BlockingQueue&lt;Runnable&gt; workQueue) {    this(corePoolSize, maximumPoolSize, keepAliveTime, unit, workQueue,         Executors.defaultThreadFactory(), defaultHandler);}说明：该函数实际上是调用ThreadPoolExecutor的另外一个构造函数。说明：在ThreadPoolExecutor()的构造函数中，进行的是初始化工作。corePoolSize, maximumPoolSize, unit, keepAliveTime和workQueue这些变量的值是已知的，它们都是通过newFixedThreadPool()传递而来。下面看看threadFactory和handler对象。2.1 ThreadFactory线程池中的ThreadFactory是一个线程工厂，线程池创建线程都是通过线程工厂对象(threadFactory)来完成的。上面所说的threadFactory对象，是通过 Executors.defaultThreadFactory()返回的。public static ThreadFactory defaultThreadFactory() {    return new DefaultThreadFactory();}defaultThreadFactory()返回DefaultThreadFactory对象。说明：ThreadFactory的作用就是提供创建线程的功能的线程工厂。它是通过newThread()提供创建线程功能的，下面简单说说newThread()。newThread()创建的线程对应的任务是Runnable对象，它创建的线程都是“非守护线程”而且“线程优先级都是Thread.NORM_PRIORITY”。2.2 RejectedExecutionHandlerhandler是ThreadPoolExecutor中拒绝策略的处理句柄。所谓拒绝策略，是指将任务添加到线程池中时，线程池拒绝该任务所采取的相应策略。线程池默认会采用的是defaultHandler策略，即AbortPolicy策略。在AbortPolicy策略中，线程池拒绝任务时会抛出异常。添加任务到线程池1234567891011121314151. execute()说明：execute()的作用是将任务添加到线程池中执行。它会分为3种情况进行处理：情况1 -- 如果&quot;线程池中任务数量&quot; &lt; &quot;核心池大小&quot;时，即线程池中少于corePoolSize个任务；此时就新建一个线程，并将该任务添加到线程中进行执行。情况2 -- 如果&quot;线程池中任务数量&quot; &gt;= &quot;核心池大小&quot;，并且&quot;线程池是允许状态&quot;；此时，则将任务添加到阻塞队列中阻塞等待。在该情况下，会再次确认&quot;线程池的状态&quot;，如果&quot;第2次读到的线程池状态&quot;和&quot;第1次读到的线程池状态&quot;不同，则从阻塞队列中删除该任务。情况3 -- 非以上两种情况。在这种情况下，尝试新建一个线程，并将该任务添加到线程中进行执行。如果执行失败，则通过reject()拒绝该任务。2. addWorker()说明：addWorker(Runnable firstTask, boolean core) 的作用是将任务(firstTask)添加到线程池中，并启动该任务。    core为true的话，则以corePoolSize为界限，若&quot;线程池中已有任务数量&gt;=corePoolSize&quot;，则返回false；core为false的话，则以maximumPoolSize为界限，若&quot;线程池中已有任务数量&gt;=maximumPoolSize&quot;，则返回false。    addWorker()会先通过for循环不断尝试更新ctl状态，ctl记录了&quot;线程池中任务数量和线程池状态&quot;。    更新成功之后，再通过try模块来将任务添加到线程池中，并启动任务所在的线程。    从addWorker()中，我们能清晰的发现：线程池在添加任务时，会创建任务对应的Worker对象；而一个Workder对象包含一个Thread对象。(01) 通过将Worker对象添加到&quot;线程的workers集合&quot;中，从而实现将任务添加到线程池中。 (02) 通过启动Worker对应的Thread线程，则执行该任务。3. submit()补充说明一点，submit()实际上也是通过调用execute()实现的关闭线程池1说明：shutdown()的作用是关闭线程池。线程池原理(下)1线程有5种状态：新建状态，就绪状态，运行状态，阻塞状态，死亡状态。线程池也有5种状态；然而，线程池不同于线程，线程池的5种状态是：Running, SHUTDOWN, STOP, TIDYING, TERMINATED。12345678910111213141516171819线程池状态定义代码如下：private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0));private static final int COUNT_BITS = Integer.SIZE - 3;private static final int CAPACITY = (1 &lt;&lt; COUNT_BITS) - 1;private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;private static final int STOP = 1 &lt;&lt; COUNT_BITS;private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS;private static int ctlOf(int rs, int wc) { return rs | wc; }说明：ctl是一个AtomicInteger类型的原子对象。ctl记录了&quot;线程池中的任务数量&quot;和&quot;线程池状态&quot;2个信息。ctl共包括32位。其中，高3位表示&quot;线程池状态&quot;，低29位表示&quot;线程池中的任务数量&quot;。RUNNING    -- 对应的高3位值是111。SHUTDOWN   -- 对应的高3位值是000。STOP       -- 对应的高3位值是001。TIDYING    -- 对应的高3位值是010。TERMINATED -- 对应的高3位值是011。线程状态切换RUNNING(01) 状态说明：线程池处在RUNNING状态时，能够接收新任务，以及对已添加的任务进行处理。(02) 状态切换：线程池的初始化状态是RUNNING。换句话说，线程池被一旦被创建，就处于RUNNING状态！道理很简单，在ctl的初始化代码中(如下)，就将它初始化为RUNNING状态，并且”任务数量”初始化为0。1private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0))SHUTDOWN(01) 状态说明：线程池处在SHUTDOWN状态时，不接收新任务，但能处理已添加的任务。(02) 状态切换：调用线程池的shutdown()接口时，线程池由RUNNING -&gt; SHUTDOWN。STOP(01) 状态说明：线程池处在STOP状态时，不接收新任务，不处理已添加的任务，并且会中断正在处理的任务。(02) 状态切换：调用线程池的shutdownNow()接口时，线程池由(RUNNING or SHUTDOWN ) -&gt; STOP。TIDYING(01) 状态说明：当所有的任务已终止，ctl记录的”任务数量”为0，线程池会变为TIDYING状态。当线程池变为TIDYING状态时，会执行钩子函数terminated()。terminated()在ThreadPoolExecutor类中是空的，若用户想在线程池变为TIDYING时，进行相应的处理；可以通过重载terminated()函数来实现。(02) 状态切换：当线程池在SHUTDOWN状态下，阻塞队列为空并且线程池中执行的任务也为空时，就会由 SHUTDOWN -&gt; TIDYING。    当线程池在STOP状态下，线程池中执行的任务为空时，就会由STOP -&gt; TIDYING。TEMINATED(01) 状态说明：线程池彻底终止，就变成TERMINATED状态。(02) 状态切换：线程池处在TIDYING状态时，执行完terminated()之后，就会由 TIDYING -&gt; TERMINATED。拒绝策略线程池的拒绝策略，是指当任务添加到线程池中被拒绝，而采取的处理措施。当任务添加到线程池中之所以被拒绝，可能是由于：第一，线程池异常关闭。第二，任务数量超过线程池的最大限制。线程池共包括4种拒绝策略，它们分别是：AbortPolicy, CallerRunsPolicy, DiscardOldestPolicy和DiscardPolicy。1234AbortPolicy         -- 当任务添加到线程池中被拒绝时，它将抛出 RejectedExecutionException 异常。CallerRunsPolicy    -- 当任务添加到线程池中被拒绝时，会在线程池当前正在运行的Thread线程池中处理被拒绝的任务。DiscardOldestPolicy -- 当任务添加到线程池中被拒绝时，线程池会放弃等待队列中最旧的未处理任务，然后将被拒绝的任务添加到等待队列中。DiscardPolicy       -- 当任务添加到线程池中被拒绝时，线程池将丢弃被拒绝的任务。线程池默认的处理策略是AbortPolicy！DiscardPolicy 示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546import java.lang.reflect.Field;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;import java.util.concurrent.ThreadPoolExecutor.DiscardPolicy;public class DiscardPolicyDemo {    private static final int THREADS_SIZE = 1;    private static final int CAPACITY = 1;    public static void main(String[] args) throws Exception {        // 创建线程池。线程池的&quot;最大池大小&quot;和&quot;核心池大小&quot;都为1(THREADS_SIZE)，&quot;线程池&quot;的阻塞队列容量为1(CAPACITY)。        ThreadPoolExecutor pool = new ThreadPoolExecutor(THREADS_SIZE, THREADS_SIZE, 0, TimeUnit.SECONDS,                new ArrayBlockingQueue&lt;Runnable&gt;(CAPACITY));        // 设置线程池的拒绝策略为&quot;丢弃&quot;        pool.setRejectedExecutionHandler(new ThreadPoolExecutor.DiscardPolicy());        // 新建10个任务，并将它们添加到线程池中。        for (int i = 0; i &lt; 10; i++) {            Runnable myrun = new MyRunnable(&quot;task-&quot;+i);            pool.execute(myrun);        }        // 关闭线程池        pool.shutdown();    }}class MyRunnable implements Runnable {    private String name;    public MyRunnable(String name) {        this.name = name;    }    @Override    public void run() {        try {            System.out.println(this.name + &quot; is running.&quot;);            Thread.sleep(100);        } catch (Exception e) {            e.printStackTrace();        }    }}运行结果：task-0 is running.task-1 is running.结果说明：线程池pool的&quot;最大池大小&quot;和&quot;核心池大小&quot;都为1(THREADS_SIZE)，这意味着&quot;线程池能同时运行的任务数量最大只能是1&quot;。线程池pool的阻塞队列是ArrayBlockingQueue，ArrayBlockingQueue是一个有界的阻塞队列，ArrayBlockingQueue的容量为1。这也意味着线程池的阻塞队列只能有一个线程池阻塞等待。根据&quot;&quot;中分析的execute()代码可知：线程池中共运行了2个任务。第1个任务直接放到Worker中，通过线程去执行；第2个任务放到阻塞队列中等待。其他的任务都被丢弃了！DiscardOldestPolicy 示例123456789101112131415161718192021222324252627282930313233343536373839404142434445import java.lang.reflect.Field;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;import java.util.concurrent.ThreadPoolExecutor.DiscardOldestPolicy;public class DiscardOldestPolicyDemo {    private static final int THREADS_SIZE = 1;    private static final int CAPACITY = 1;    public static void main(String[] args) throws Exception {        // 创建线程池。线程池的&quot;最大池大小&quot;和&quot;核心池大小&quot;都为1(THREADS_SIZE)，&quot;线程池&quot;的阻塞队列容量为1(CAPACITY)。        ThreadPoolExecutor pool = new ThreadPoolExecutor(THREADS_SIZE, THREADS_SIZE, 0, TimeUnit.SECONDS,                new ArrayBlockingQueue&lt;Runnable&gt;(CAPACITY));        // 设置线程池的拒绝策略为&quot;DiscardOldestPolicy&quot;        pool.setRejectedExecutionHandler(new ThreadPoolExecutor.DiscardOldestPolicy());        // 新建10个任务，并将它们添加到线程池中。        for (int i = 0; i &lt; 10; i++) {            Runnable myrun = new MyRunnable(&quot;task-&quot;+i);            pool.execute(myrun);        }        // 关闭线程池        pool.shutdown();    }}class MyRunnable implements Runnable {    private String name;    public MyRunnable(String name) {        this.name = name;    }    @Override    public void run() {        try {            System.out.println(this.name + &quot; is running.&quot;);            Thread.sleep(200);        } catch (Exception e) {            e.printStackTrace();        }    }}运行结果：task-0 is running.task-9 is running.结果说明：将&quot;线程池的拒绝策略&quot;由DiscardPolicy修改为DiscardOldestPolicy之后，当有任务添加到线程池被拒绝时，线程池会丢弃阻塞队列中末尾的任务，然后将被拒绝的任务添加到末尾。AbortPolicy 示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.lang.reflect.Field;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;import java.util.concurrent.ThreadPoolExecutor.AbortPolicy;import java.util.concurrent.RejectedExecutionException;public class AbortPolicyDemo {    private static final int THREADS_SIZE = 1;    private static final int CAPACITY = 1;    public static void main(String[] args) throws Exception {        // 创建线程池。线程池的&quot;最大池大小&quot;和&quot;核心池大小&quot;都为1(THREADS_SIZE)，&quot;线程池&quot;的阻塞队列容量为1(CAPACITY)。        ThreadPoolExecutor pool = new ThreadPoolExecutor(THREADS_SIZE, THREADS_SIZE, 0, TimeUnit.SECONDS,                new ArrayBlockingQueue&lt;Runnable&gt;(CAPACITY));        // 设置线程池的拒绝策略为&quot;抛出异常&quot;        pool.setRejectedExecutionHandler(new ThreadPoolExecutor.AbortPolicy());        try {            // 新建10个任务，并将它们添加到线程池中。            for (int i = 0; i &lt; 10; i++) {                Runnable myrun = new MyRunnable(&quot;task-&quot;+i);                pool.execute(myrun);            }        } catch (RejectedExecutionException e) {            e.printStackTrace();            // 关闭线程池            pool.shutdown();        }    }}class MyRunnable implements Runnable {    private String name;    public MyRunnable(String name) {        this.name = name;    }    @Override    public void run() {        try {            System.out.println(this.name + &quot; is running.&quot;);            Thread.sleep(200);        } catch (Exception e) {            e.printStackTrace();        }    }}(某一次)运行结果：java.util.concurrent.RejectedExecutionException    at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:1774)    at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:768)    at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:656)    at AbortPolicyDemo.main(AbortPolicyDemo.java:27)task-0 is running.task-1 is running.结果说明：将&quot;线程池的拒绝策略&quot;由DiscardPolicy修改为AbortPolicy之后，当有任务添加到线程池被拒绝时，会抛出RejectedExecutionException。CallerRunsPolicy 示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import java.lang.reflect.Field;import java.util.concurrent.ArrayBlockingQueue;import java.util.concurrent.ThreadPoolExecutor;import java.util.concurrent.TimeUnit;import java.util.concurrent.ThreadPoolExecutor.CallerRunsPolicy;public class CallerRunsPolicyDemo {    private static final int THREADS_SIZE = 1;    private static final int CAPACITY = 1;    public static void main(String[] args) throws Exception {        // 创建线程池。线程池的&quot;最大池大小&quot;和&quot;核心池大小&quot;都为1(THREADS_SIZE)，&quot;线程池&quot;的阻塞队列容量为1(CAPACITY)。        ThreadPoolExecutor pool = new ThreadPoolExecutor(THREADS_SIZE, THREADS_SIZE, 0, TimeUnit.SECONDS,                new ArrayBlockingQueue&lt;Runnable&gt;(CAPACITY));        // 设置线程池的拒绝策略为&quot;CallerRunsPolicy&quot;        pool.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy());        // 新建10个任务，并将它们添加到线程池中。        for (int i = 0; i &lt; 10; i++) {            Runnable myrun = new MyRunnable(&quot;task-&quot;+i);            pool.execute(myrun);        }        // 关闭线程池        pool.shutdown();    }}class MyRunnable implements Runnable {    private String name;    public MyRunnable(String name) {        this.name = name;    }    @Override    public void run() {        try {            System.out.println(this.name + &quot; is running.&quot;);            Thread.sleep(100);        } catch (Exception e) {            e.printStackTrace();        }    }}(某一次)运行结果：task-2 is running.task-3 is running.task-4 is running.task-5 is running.task-6 is running.task-7 is running.task-8 is running.task-9 is running.task-0 is running.task-1 is running.结果说明：将&quot;线程池的拒绝策略&quot;由DiscardPolicy修改为CallerRunsPolicy之后，当有任务添加到线程池被拒绝时，线程池会将被拒绝的任务添加到&quot;线程池正在运行的线程&quot;中取运行。Callable和FutureCallable 和 Future 是比较有趣的一对组合。当我们需要获取线程的执行结果时，就需要用到它们。Callable用于产生结果，Future用于获取结果。1234567891011121. CallableCallable 是一个接口，它只包含一个call()方法。Callable是一个返回结果并且可能抛出异常的任务。为了便于理解，我们可以将Callable比作一个Runnable接口，而Callable的call()方法则类似于Runnable的run()方法。Callable的源码如下：public interface Callable&lt;V&gt; {    V call() throws Exception;}说明：从中我们可以看出Callable支持泛型。2. FuturejavaFuture 是一个接口。它用于表示异步计算的结果。提供了检查计算是否完成的方法，以等待计算的完成，并获取计算的结果。说明：Future用于表示异步计算的结果。它的实现类是FutureTask，在讲解FutureTask之前，我们先看看Callable, Future, FutureTask它们之间的关系图，如下：123456说明：(01) RunnableFuture是一个接口，它继承了Runnable和Future这两个接口。RunnableFuture的源码如下：public interface RunnableFuture&lt;V&gt; extends Runnable, Future&lt;V&gt; {    void run();}(02) FutureTask实现了RunnableFuture接口。所以，我们也说它实现了Future接口。Callable和Future的基本用法123456789101112131415161718192021222324252627282930313233343536import java.util.concurrent.Callable;import java.util.concurrent.Future;import java.util.concurrent.Executors;import java.util.concurrent.ExecutorService;import java.util.concurrent.ExecutionException;class MyCallable implements Callable {    @Override     public Integer call() throws Exception {        int sum    = 0;        // 执行任务        for (int i=0; i&lt;100; i++)            sum += i;        //return sum;         return Integer.valueOf(sum);    } }public class CallableTest1 {    public static void main(String[] args)         throws ExecutionException, InterruptedException{        //创建一个线程池        ExecutorService pool = Executors.newSingleThreadExecutor();        //创建有返回值的任务        Callable c1 = new MyCallable();        //执行任务并获取Future对象         Future f1 = pool.submit(c1);        // 输出结果        System.out.println(f1.get());         //关闭线程池         pool.shutdown();     }}运行结果：4950结果说明：　　在主线程main中，通过newSingleThreadExecutor()新建一个线程池。接着创建Callable对象c1，然后再通过pool.submit(c1)将c1提交到线程池中进行处理，并且将返回的结果保存到Future对象f1中。然后，我们通过f1.get()获取Callable中保存的结果；最后通过pool.shutdown()关闭线程池。1234567891011121314151617181920211. submit()submit()在java/util/concurrent/AbstractExecutorService.java中实现说明：submit()通过newTaskFor(task)创建了RunnableFuture对象ftask。它的源码如下：protected &lt;T&gt; RunnableFuture&lt;T&gt; newTaskFor(Callable&lt;T&gt; callable) {    return new FutureTask&lt;T&gt;(callable);}2. FutureTask的构造函数public FutureTask(Callable&lt;V&gt; callable) {    if (callable == null)        throw new NullPointerException();    // callable是一个Callable对象    this.callable = callable;    // state记录FutureTask的状态    this.state = NEW;       // ensure visibility of callable}3. FutureTask的run()方法我们继续回到submit()的源码中。在newTaskFor()新建一个ftask对象之后，会通过execute(ftask)执行该任务。此时ftask被当作一个Runnable对象进行执行，最终会调用到它的run()方法；ftask的run()方法在java/util/concurrent/FutureTask.java中实现，说明：run()中会执行Callable对象的call()方法，并且最终将结果保存到result中，并通过set(result)将result保存。之后调用FutureTask的get()方法，返回的就是通过set(result)保存的值。</content>
	</entry>
	<entry>
		<title>多线程_JUC集合</title>
		<url>/article/81c2c0c1.html</url>
		<content>JUC集合List的实现类主要有: LinkedList, ArrayList, Vector, Stack。(01) LinkedList是双向链表实现的双端队列；它不是线程安全的，只适用于单线程。(02) ArrayList是数组实现的队列，它是一个动态数组；它也不是线程安全的，只适用于单线程。(03) Vector是数组实现的矢量队列，它也一个动态数组；不过和ArrayList不同的是，Vector是线程安全的，它支持并发。(04) Stack是Vector实现的栈；和Vector一样，它也是线程安全的。 Set的实现类主要有: HastSet和TreeSet。(01) HashSet是一个没有重复元素的集合，它通过HashMap实现的；HashSet不是线程安全的，只适用于单线程。(02) TreeSet也是一个没有重复元素的集合，不过和HashSet不同的是，TreeSet中的元素是有序的；它是通过TreeMap实现的；TreeSet也不是线程安全的，只适用于单线程。 Map的实现类主要有: HashMap，WeakHashMap, Hashtable和TreeMap。(01) HashMap是存储“键-值对”的哈希表；它不是线程安全的，只适用于单线程。(02) WeakHashMap是也是哈希表；和HashMap不同的是，HashMap的“键”是强引用类型，而WeakHashMap的“键”是弱引用类型，也就是说当WeakHashMap 中的某个键不再正常使用时，会被从WeakHashMap中被自动移除。WeakHashMap也不是线程安全的，只适用于单线程。(03) Hashtable也是哈希表；和HashMap不同的是，Hashtable是线程安全的，支持并发。(04) TreeMap也是哈希表，不过TreeMap中的“键-值对”是有序的，它是通过R-B Tree(红黑树)实现的；TreeMap不是线程安全的，只适用于单线程。更多关于这些集合类的介绍，可以参考“Java 集合系列目录(Category)”。123为了方便，我们将前面介绍集合类统称为”java集合包“。java集合包大多是“非线程安全的”，虽然可以通过Collections工具类中的方法获取java集合包对应的同步类，但是这些同步类的并发效率并不是很高。为了更好的支持高并发任务，并发大师Doug Lea在JUC(java.util.concurrent)包中添加了java集合包中单线程类的对应的支持高并发的类。例如，ArrayList对应的高并发类是CopyOnWriteArrayList，HashMap对应的高并发类是ConcurrentHashMap，等等。JUC包在添加”java集合包“对应的高并发类时，为了保持API接口的一致性，使用了”Java集合包“中的框架。例如，CopyOnWriteArrayList实现了“Java集合包”中的List接口，HashMap继承了“java集合包”中的AbstractMap类，等等。得益于“JUC包使用了Java集合包中的类”，如果我们了解了Java集合包中的类的思想之后，理解JUC包中的类也相对容易；理解时，最大的难点是，对JUC包是如何添加对“高并发”的支持的！1234567891011121314151617181. List和SetJUC集合包中的List和Set实现类包括: CopyOnWriteArrayList, CopyOnWriteArraySet和ConcurrentSkipListSet。(01) CopyOnWriteArrayList相当于线程安全的ArrayList，它实现了List接口。CopyOnWriteArrayList是支持高并发的。(02) CopyOnWriteArraySet相当于线程安全的HashSet，它继承于AbstractSet类。CopyOnWriteArraySet内部包含一个CopyOnWriteArrayList对象，它是通过CopyOnWriteArrayList实现的。2. MapJUC集合包中Map的实现类包括: ConcurrentHashMap和ConcurrentSkipListMap。(01) ConcurrentHashMap是线程安全的哈希表(相当于线程安全的HashMap)；它继承于AbstractMap类，并且实现ConcurrentMap接口。ConcurrentHashMap是通过“锁分段”来实现的，它支持并发。(02) ConcurrentSkipListMap是线程安全的有序的哈希表(相当于线程安全的TreeMap); 它继承于AbstractMap类，并且实现ConcurrentNavigableMap接口。ConcurrentSkipListMap是通过“跳表”来实现的，它支持并发。(03) ConcurrentSkipListSet是线程安全的有序的集合(相当于线程安全的TreeSet)；它继承于AbstractSet，并实现了NavigableSet接口。ConcurrentSkipListSet是通过ConcurrentSkipListMap实现的，它也支持并发。3. QueueJUC集合包中Queue的实现类包括: ArrayBlockingQueue, LinkedBlockingQueue, LinkedBlockingDeque, ConcurrentLinkedQueue和ConcurrentLinkedDeque。(01) ArrayBlockingQueue是数组实现的线程安全的有界的阻塞队列。(02) LinkedBlockingQueue是单向链表实现的(指定大小)阻塞队列，该队列按 FIFO（先进先出）排序元素。(03) LinkedBlockingDeque是双向链表实现的(指定大小)双向并发阻塞队列，该阻塞队列同时支持FIFO和FILO两种操作方式。(04) ConcurrentLinkedQueue是单向链表实现的无界队列，该队列按 FIFO（先进先出）排序元素。(05) ConcurrentLinkedDeque是双向链表实现的无界队列，该队列同时支持FIFO和FILO两种操作方式。CopyOnWriteArrayList123456它相当于线程安全的ArrayList。和ArrayList一样，它是个可变数组；但是和ArrayList不同的时，它具有以下特性：1. 它最适合于具有以下特征的应用程序：List 大小通常保持很小，只读操作远多于可变操作，需要在遍历期间防止线程间的冲突。2. 它是线程安全的。3. 因为通常需要复制整个基础数组，所以可变操作（add()、set() 和 remove() 等等）的开销很大。4. 迭代器支持hasNext(), next()等不可变操作，但不支持可变 remove()等操作。5. 使用迭代器进行遍历的速度很快，并且不会与其他线程发生冲突。在构造迭代器时，迭代器依赖于不变的数组快照。1234567891011说明：1. CopyOnWriteArrayList实现了List接口，因此它是一个队列。2. CopyOnWriteArrayList包含了成员lock。每一个CopyOnWriteArrayList都和一个互斥锁lock绑定，通过lock，实现了对CopyOnWriteArrayList的互斥访问。3. CopyOnWriteArrayList包含了成员array数组，这说明CopyOnWriteArrayList本质上通过数组实现的。下面从“动态数组”和“线程安全”两个方面进一步对CopyOnWriteArrayList的原理进行说明。1. CopyOnWriteArrayList的“动态数组”机制 -- 它内部有个“volatile数组”(array)来保持数据。在“添加/修改/删除”数据时，都会新建一个数组，并将更新后的数据拷贝到新建的数组中，最后再将该数组赋值给“volatile数组”。这就是它叫做CopyOnWriteArrayList的原因！CopyOnWriteArrayList就是通过这种方式实现的动态数组；不过正由于它在“添加/修改/删除”数据时，都会新建数组，所以涉及到修改数据的操作，CopyOnWriteArrayList效率很低；但是单单只是进行遍历查找的话，效率比较高。2. CopyOnWriteArrayList的“线程安全”机制 -- 是通过volatile和互斥锁来实现的。(01) CopyOnWriteArrayList是通过“volatile数组”来保存数据的。一个线程读取volatile数组时，总能看到其它线程对该volatile变量最后的写入；就这样，通过volatile提供了“读取到的数据总是最新的”这个机制的保证。(02) CopyOnWriteArrayList通过互斥锁来保护数据。在“添加/修改/删除”数据时，会先“获取互斥锁”，再修改完毕之后，先将数据更新到“volatile数组”中，然后再“释放互斥锁”；这样，就达到了保护数据的目的。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768// CopyOnWriteArrayList函数列表// 创建一个空列表。CopyOnWriteArrayList()// 创建一个按 collection 的迭代器返回元素的顺序包含指定 collection 元素的列表。CopyOnWriteArrayList(Collection&lt;? extends E&gt; c)// CopyOnWriteArrayList(E[] toCopyIn)// 创建一个保存给定数组的副本的列表。// 将指定元素添加到此列表的尾部。boolean add(E e)// 在此列表的指定位置上插入指定元素。void add(int index, E element)// 按照指定 collection 的迭代器返回元素的顺序，将指定 collection 中的所有元素添加此列表的尾部。boolean addAll(Collection&lt;? extends E&gt; c)// 从指定位置开始，将指定 collection 的所有元素插入此列表。boolean addAll(int index, Collection&lt;? extends E&gt; c)// 按照指定 collection 的迭代器返回元素的顺序，将指定 collection 中尚未包含在此列表中的所有元素添加列表的尾部。int addAllAbsent(Collection&lt;? extends E&gt; c)// 添加元素（如果不存在）。boolean addIfAbsent(E e)// 从此列表移除所有元素。void clear()// 返回此列表的浅表副本。Object clone()// 如果此列表包含指定的元素，则返回 true。boolean contains(Object o)// 如果此列表包含指定 collection 的所有元素，则返回 true。boolean containsAll(Collection&lt;?&gt; c)// 比较指定对象与此列表的相等性。boolean equals(Object o)// 返回列表中指定位置的元素。E get(int index)// 返回此列表的哈希码值。int hashCode()// 返回第一次出现的指定元素在此列表中的索引，从 index 开始向前搜索，如果没有找到该元素，则返回 -1。int indexOf(E e, int index)// 返回此列表中第一次出现的指定元素的索引；如果此列表不包含该元素，则返回 -1。int indexOf(Object o)// 如果此列表不包含任何元素，则返回 true。boolean isEmpty()// 返回以恰当顺序在此列表元素上进行迭代的迭代器。Iterator&lt;E&gt; iterator()// 返回最后一次出现的指定元素在此列表中的索引，从 index 开始向后搜索，如果没有找到该元素，则返回 -1。int lastIndexOf(E e, int index)// 返回此列表中最后出现的指定元素的索引；如果列表不包含此元素，则返回 -1。int lastIndexOf(Object o)// 返回此列表元素的列表迭代器（按适当顺序）。ListIterator&lt;E&gt; listIterator()// 返回列表中元素的列表迭代器（按适当顺序），从列表的指定位置开始。ListIterator&lt;E&gt; listIterator(int index)// 移除此列表指定位置上的元素。E remove(int index)// 从此列表移除第一次出现的指定元素（如果存在）。boolean remove(Object o)// 从此列表移除所有包含在指定 collection 中的元素。boolean removeAll(Collection&lt;?&gt; c)// 只保留此列表中包含在指定 collection 中的元素。boolean retainAll(Collection&lt;?&gt; c)// 用指定的元素替代此列表指定位置上的元素。E set(int index, E element)// 返回此列表中的元素数。int size()// 返回此列表中 fromIndex（包括）和 toIndex（不包括）之间部分的视图。List&lt;E&gt; subList(int fromIndex, int toIndex)// 返回一个按恰当顺序（从第一个元素到最后一个元素）包含此列表中所有元素的数组。Object[] toArray()// 返回以恰当顺序（从第一个元素到最后一个元素）包含列表所有元素的数组；返回数组的运行时类型是指定数组的运行时类型。&lt;T&gt; T[] toArray(T[] a)// 返回此列表的字符串表示形式。1234567891011121314151617181920212223242526271. 创建CopyOnWriteArrayList共3个构造函数。说明：这3个构造函数都调用了setArray()，说明：setArray()的作用是给array赋值；其中，array是volatile transient Object[]类型，即array是“volatile数组”。关于volatile关键字，我们知道“volatile能让变量变得可见”，即对一个volatile变量的读，总是能看到（任意线程）对这个volatile变量最后的写入。正在由于这种特性，每次更新了“volatile数组”之后，其它线程都能看到对它所做的更新。关于transient关键字，它是在序列化中才起作用，transient变量不会被自动序列化。transient不是本文关注的重点，了解即可。关于transient的更多内容，请参考：http://www.cnblogs.com/skywang12345/p/io_06.html2. 添加以add(E e)为例，来对“CopyOnWriteArrayList的添加操作”进行说明说明：add(E e)的作用就是将数据e添加到”volatile数组“中。它的实现方式是，新建一个数组，接着将原始的”volatile数组“的数据拷贝到新数组中，然后将新增数据也添加到新数组中；最后，将新数组赋值给”volatile数组“。在add(E e)中有两点需要关注。第一，在”添加操作“开始前，获取独占锁(lock)，若此时有需要线程要获取锁，则必须等待；在操作完毕后，释放独占锁(lock)，此时其它线程才能获取锁。通过独占锁，来防止多线程同时修改数据！3. 获取说明：get(int index)的实现很简单，就是返回”volatile数组“中的第index个元素。4. 删除以remove(int index)为例，来对“CopyOnWriteArrayList的删除操作”进行说明。说明：remove(int index)的作用就是将”volatile数组“中第index个元素删除。它的实现方式是，如果被删除的是最后一个元素，则直接通过Arrays.copyOf()进行处理，而不需要新建数组。否则，新建数组，然后将”volatile数组中被删除元素之外的其它元素“拷贝到新数组中；最后，将新数组赋值给”volatile数组“。和add(E e)一样，remove(int index)也是”在操作之前，获取独占锁；操作完成之后，释放独占是“；并且”在操作完成时，会通过将数据更新到volatile数组中“。5. 遍历以iterator()为例，来对“CopyOnWriteArrayList的遍历操作”进行说明。说明：iterator()会返回COWIterator对象。COWIterator实现额ListIterator接口，说明：COWIterator不支持修改元素的操作。例如，对于remove(),set(),add()等操作，COWIterator都会抛出异常！另外，需要提到的一点是，CopyOnWriteArrayList返回迭代器不会抛出ConcurrentModificationException异常，即它不是fail-fast机制的！CopyOnWriteArrayList示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import java.util.*;import java.util.concurrent.*;/* *   CopyOnWriteArrayList是“线程安全”的动态数组，而ArrayList是非线程安全的。 * *   下面是“多个线程同时操作并且遍历list”的示例 *   (01) 当list是CopyOnWriteArrayList对象时，程序能正常运行。 *   (02) 当list是ArrayList对象时，程序会产生ConcurrentModificationException异常。 * * @author skywang */public class CopyOnWriteArrayListTest1 {    // TODO: list是ArrayList对象时，程序会出错。    //private static List&lt;String&gt; list = new ArrayList&lt;String&gt;();    private static List&lt;String&gt; list = new CopyOnWriteArrayList&lt;String&gt;();    public static void main(String[] args) {        // 同时启动两个线程对list进行操作！        new MyThread(&quot;ta&quot;).start();        new MyThread(&quot;tb&quot;).start();    }    private static void printAll() {        String value = null;        Iterator iter = list.iterator();        while(iter.hasNext()) {            value = (String)iter.next();            System.out.print(value+&quot;, &quot;);        }        System.out.println();    }    private static class MyThread extends Thread {        MyThread(String name) {            super(name);        }        @Override        public void run() {                int i = 0;            while (i++ &lt; 6) {                // “线程名” + &quot;-&quot; + &quot;序号&quot;                String val = Thread.currentThread().getName()+&quot;-&quot;+i;                list.add(val);                // 通过“Iterator”遍历List。                printAll();            }        }    }}ta-1, tb-1, ta-1, tb-1, ta-1, ta-1, tb-1, tb-1, tb-2, tb-2, ta-1, ta-2, tb-1, ta-1, tb-2, tb-1, ta-2, tb-2, tb-3, ta-2, ta-1, tb-3, tb-1, ta-3, tb-2, ta-1, ta-2, tb-1, tb-3, tb-2, ta-3, ta-2, tb-4, tb-3, ta-1, ta-3, tb-1, tb-4, tb-2, ta-4, ta-2, ta-1, tb-3, tb-1, ta-3, tb-2, tb-4, ta-2, ta-4, tb-3, tb-5, ta-3, ta-1, tb-4, tb-1, ta-4, tb-2, tb-5, ta-2, ta-5, tb-3, ta-1, ta-3, tb-1, tb-4, tb-2, ta-4, ta-2, tb-5, tb-3, ta-5, ta-3, tb-6, tb-4, ta-4, tb-5, ta-5, tb-6, ta-6,结果说明：如果将源码中的list改成ArrayList对象时，程序会产生ConcurrentModificationException异常。ConcurrentSkipListMap123ConcurrentSkipListMap是线程安全的有序的哈希表，适用于高并发的场景。ConcurrentSkipListMap和TreeMap，它们虽然都是有序的哈希表。但是，第一，它们的线程安全机制不同，TreeMap是非线程安全的，而ConcurrentSkipListMap是线程安全的。第二，ConcurrentSkipListMap是通过跳表实现的，而TreeMap是通过红黑树实现的。关于跳表(Skip List)，它是平衡树的一种替代的数据结构，但是和红黑树不相同的是，跳表对于树的平衡的实现是基于一种随机化的算法的，这样也就是说跳表的插入和删除的工作是比较简单的。说明：先以数据“7,14,21,32,37,71,85”序列为例，来对跳表进行简单说明。跳表分为许多层(level)，每一层都可以看作是数据的索引，这些索引的意义就是加快跳表查找数据速度。每一层的数据都是有序的，上一层数据是下一层数据的子集，并且第一层(level 1)包含了全部的数据；层次越高，跳跃性越大，包含的数据越少。跳表包含一个表头，它查找数据时，是从上往下，从左往右进行查找。现在“需要找出值为32的节点”为例，来对比说明跳表和普遍的链表。情况1：链表中查找“32”节点情况2：跳表中查找“32”节点忽略索引垂直线路上路径的情况下，只需要2步(红色部分表示路径)。1234下面说说Java中ConcurrentSkipListMap的数据结构。(01) ConcurrentSkipListMap继承于AbstractMap类，也就意味着它是一个哈希表。(02) Index是ConcurrentSkipListMap的内部类，它与“跳表中的索引相对应”。HeadIndex继承于Index，ConcurrentSkipListMap中含有一个HeadIndex的对象head，head是“跳表的表头”。(03) Index是跳表中的索引，它包含“右索引的指针(right)”，“下索引的指针(down)”和“哈希表节点node”。node是Node的对象，Node也是ConcurrentSkipListMap中的内部类。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091// ConcurrentSkipListMap函数列表// 构造一个新的空映射，该映射按照键的自然顺序进行排序。ConcurrentSkipListMap()// 构造一个新的空映射，该映射按照指定的比较器进行排序。ConcurrentSkipListMap(Comparator&lt;? super K&gt; comparator)// 构造一个新映射，该映射所包含的映射关系与给定映射包含的映射关系相同，并按照键的自然顺序进行排序。ConcurrentSkipListMap(Map&lt;? extends K,? extends V&gt; m)// 构造一个新映射，该映射所包含的映射关系与指定的有序映射包含的映射关系相同，使用的顺序也相同。ConcurrentSkipListMap(SortedMap&lt;K,? extends V&gt; m)// 返回与大于等于给定键的最小键关联的键-值映射关系；如果不存在这样的条目，则返回 null。Map.Entry&lt;K,V&gt; ceilingEntry(K key)// 返回大于等于给定键的最小键；如果不存在这样的键，则返回 null。K ceilingKey(K key)// 从此映射中移除所有映射关系。void clear()// 返回此 ConcurrentSkipListMap 实例的浅表副本。ConcurrentSkipListMap&lt;K,V&gt; clone()// 返回对此映射中的键进行排序的比较器；如果此映射使用键的自然顺序，则返回 null。Comparator&lt;? super K&gt; comparator()// 如果此映射包含指定键的映射关系，则返回 true。boolean containsKey(Object key)// 如果此映射为指定值映射一个或多个键，则返回 true。boolean containsValue(Object value)// 返回此映射中所包含键的逆序 NavigableSet 视图。NavigableSet&lt;K&gt; descendingKeySet()// 返回此映射中所包含映射关系的逆序视图。ConcurrentNavigableMap&lt;K,V&gt; descendingMap()// 返回此映射中所包含的映射关系的 Set 视图。Set&lt;Map.Entry&lt;K,V&gt;&gt; entrySet()// 比较指定对象与此映射的相等性。boolean equals(Object o)// 返回与此映射中的最小键关联的键-值映射关系；如果该映射为空，则返回 null。Map.Entry&lt;K,V&gt; firstEntry()// 返回此映射中当前第一个（最低）键。K firstKey()// 返回与小于等于给定键的最大键关联的键-值映射关系；如果不存在这样的键，则返回 null。Map.Entry&lt;K,V&gt; floorEntry(K key)// 返回小于等于给定键的最大键；如果不存在这样的键，则返回 null。K floorKey(K key)// 返回指定键所映射到的值；如果此映射不包含该键的映射关系，则返回 null。V get(Object key)// 返回此映射的部分视图，其键值严格小于 toKey。ConcurrentNavigableMap&lt;K,V&gt; headMap(K toKey)// 返回此映射的部分视图，其键小于（或等于，如果 inclusive 为 true）toKey。ConcurrentNavigableMap&lt;K,V&gt; headMap(K toKey, boolean inclusive)// 返回与严格大于给定键的最小键关联的键-值映射关系；如果不存在这样的键，则返回 null。Map.Entry&lt;K,V&gt; higherEntry(K key)// 返回严格大于给定键的最小键；如果不存在这样的键，则返回 null。K higherKey(K key)// 如果此映射未包含键-值映射关系，则返回 true。boolean isEmpty()// 返回此映射中所包含键的 NavigableSet 视图。NavigableSet&lt;K&gt; keySet()// 返回与此映射中的最大键关联的键-值映射关系；如果该映射为空，则返回 null。Map.Entry&lt;K,V&gt; lastEntry()// 返回映射中当前最后一个（最高）键。K lastKey()// 返回与严格小于给定键的最大键关联的键-值映射关系；如果不存在这样的键，则返回 null。Map.Entry&lt;K,V&gt; lowerEntry(K key)// 返回严格小于给定键的最大键；如果不存在这样的键，则返回 null。K lowerKey(K key)// 返回此映射中所包含键的 NavigableSet 视图。NavigableSet&lt;K&gt; navigableKeySet()// 移除并返回与此映射中的最小键关联的键-值映射关系；如果该映射为空，则返回 null。Map.Entry&lt;K,V&gt; pollFirstEntry()// 移除并返回与此映射中的最大键关联的键-值映射关系；如果该映射为空，则返回 null。Map.Entry&lt;K,V&gt; pollLastEntry()// 将指定值与此映射中的指定键关联。V put(K key, V value)// 如果指定键已经不再与某个值相关联，则将它与给定值关联。V putIfAbsent(K key, V value)// 从此映射中移除指定键的映射关系（如果存在）。V remove(Object key)// 只有目前将键的条目映射到给定值时，才移除该键的条目。boolean remove(Object key, Object value)// 只有目前将键的条目映射到某一值时，才替换该键的条目。V replace(K key, V value)// 只有目前将键的条目映射到给定值时，才替换该键的条目。boolean replace(K key, V oldValue, V newValue)// 返回此映射中的键-值映射关系数。int size()// 返回此映射的部分视图，其键的范围从 fromKey 到 toKey。ConcurrentNavigableMap&lt;K,V&gt; subMap(K fromKey, boolean fromInclusive, K toKey, boolean toInclusive)// 返回此映射的部分视图，其键值的范围从 fromKey（包括）到 toKey（不包括）。ConcurrentNavigableMap&lt;K,V&gt; subMap(K fromKey, K toKey)// 返回此映射的部分视图，其键大于等于 fromKey。ConcurrentNavigableMap&lt;K,V&gt; tailMap(K fromKey)// 返回此映射的部分视图，其键大于（或等于，如果 inclusive 为 true）fromKey。ConcurrentNavigableMap&lt;K,V&gt; tailMap(K fromKey, boolean inclusive)// 返回此映射中所包含值的 Collection 视图。Collection&lt;V&gt; values()1234567891011121314151617181920212223242526272829303132331. 添加下面以put(K key, V value)为例，对ConcurrentSkipListMap的添加方法进行说明。实际上，put()是通过doPut()将key-value键值对添加到ConcurrentSkipListMap中的。说明：doPut() 的作用就是将键值对添加到“跳表”中。要想搞清doPut()，首先要弄清楚它的主干部分 —— 我们先单纯的只考虑“单线程的情况下，将key-value添加到跳表中”，即忽略“多线程相关的内容”。它的流程如下：第1步：找到“插入位置”。即，找到“key的前继节点(b)”和“key的后继节点(n)”；key是要插入节点的键。第2步：新建并插入节点。即，新建节点z(key对应的节点)，并将新节点z插入到“跳表”中(设置“b的后继节点为z”，“z的后继节点为n”)。第3步：更新跳表。即，随机获取一个level，然后在“跳表”的第1层～第level层之间，每一层都插入节点z；在第level层之上就不再插入节点了。若level数值大于“跳表的层次”，则新建一层。2. 删除下面以remove(Object key)为例，对ConcurrentSkipListMap的删除方法进行说明。实际上，remove()是通过doRemove()将ConcurrentSkipListMap中的key对应的键值对删除的。说明：doRemove()的作用是删除跳表中的节点。和doPut()一样，我们重点看doRemove()的主干部分，了解主干部分之后，其余部分就非常容易理解了。下面是“单线程的情况下，删除跳表中键值对的步骤”：第1步：找到“被删除节点的位置”。 即，找到“key的前继节点(b)”，“key所对应的节点(n)”，“n的后继节点f”；key是要删除节点的键。第2步：删除节点。 即，将“key所对应的节点n”从跳表中移除 -- 将“b的后继节点”设为“f”！第3步：更新跳表。 即，遍历跳表，删除每一层的“key节点”(如果存在的话)。如果删除“key节点”之后，跳表的层次需要-1；则执行相应的操作！ 3. 获取下面以get(Object key)为例，对ConcurrentSkipListMap的获取方法进行说明。说明：doGet()是通过findNode()找到并返回节点的。说明：findNode(key)的作用是在返回跳表中key对应的节点；存在则返回节点，不存在则返回null。先弄清函数的主干部分，即抛开“多线程相关内容”，单纯的考虑单线程情况下，从跳表获取节点的算法。第1步：找到“被删除节点的位置”。根据findPredecessor()定位key所在的层次以及找到key的前继节点(b)，然后找到b的后继节点n。第2步：根据“key的前继节点(b)”和“key的前继节点的后继节点(n)”来定位“key对应的节点”。具体是通过比较“n的键值”和“key”的大小。如果相等，则n就是所要查找的键。ConcurrentSkipListMap示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768import java.util.*;import java.util.concurrent.*;/* *   ConcurrentSkipListMap是“线程安全”的哈希表，而TreeMap是非线程安全的。 * *   下面是“多个线程同时操作并且遍历map”的示例 *   (01) 当map是ConcurrentSkipListMap对象时，程序能正常运行。 *   (02) 当map是TreeMap对象时，程序会产生ConcurrentModificationException异常。 * * @author skywang */public class ConcurrentSkipListMapDemo1 {    // TODO: map是TreeMap对象时，程序会出错。    //private static Map&lt;String, String&gt; map = new TreeMap&lt;String, String&gt;();    private static Map&lt;String, String&gt; map = new ConcurrentSkipListMap&lt;String, String&gt;();    public static void main(String[] args) {            // 同时启动两个线程对map进行操作！        new MyThread(&quot;a&quot;).start();        new MyThread(&quot;b&quot;).start();    }    private static void printAll() {        String key, value;        Iterator iter = map.entrySet().iterator();        while(iter.hasNext()) {            Map.Entry entry = (Map.Entry)iter.next();            key = (String)entry.getKey();            value = (String)entry.getValue();            System.out.print(&quot;(&quot;+key+&quot;, &quot;+value+&quot;), &quot;);        }        System.out.println();    }    private static class MyThread extends Thread {        MyThread(String name) {            super(name);        }        @Override        public void run() {                int i = 0;            while (i++ &lt; 6) {                // “线程名” + &quot;序号&quot;                String val = Thread.currentThread().getName()+i;                map.put(val, &quot;0&quot;);                // 通过“Iterator”遍历map。                printAll();            }        }    }}(a1, 0), (a1, 0), (b1, 0), (b1, 0),(a1, 0), (b1, 0), (b2, 0), (a1, 0), (a1, 0), (a2, 0), (a2, 0), (b1, 0), (b1, 0), (b2, 0), (b2, 0), (b3, 0), (b3, 0), (a1, 0), (a2, 0), (a3, 0), (a1, 0), (b1, 0), (a2, 0), (b2, 0), (a3, 0), (b3, 0), (b1, 0), (b4, 0), (b2, 0), (a1, 0), (b3, 0), (a2, 0), (b4, 0), (a3, 0), (a1, 0), (a4, 0), (a2, 0), (b1, 0), (a3, 0), (b2, 0), (a4, 0), (b3, 0), (b1, 0), (b4, 0), (b2, 0), (b5, 0), (b3, 0), (a1, 0), (b4, 0), (a2, 0), (b5, 0), (a3, 0), (a1, 0), (a4, 0), (a2, 0), (a5, 0), (a3, 0), (b1, 0), (a4, 0), (b2, 0), (a5, 0), (b3, 0), (b1, 0), (b4, 0), (b2, 0), (b5, 0), (b3, 0), (b6, 0), (b4, 0), (a1, 0), (b5, 0), (a2, 0), (b6, 0), (a3, 0), (a4, 0), (a5, 0), (a6, 0), (b1, 0), (b2, 0), (b3, 0), (b4, 0), (b5, 0), (b6, 0), 结果说明：示例程序中，启动两个线程(线程a和线程b)分别对ConcurrentSkipListMap进行操作。以线程a而言，它会先获取“线程名”+“序号”，然后将该字符串作为key，将“0”作为value，插入到ConcurrentSkipListMap中；接着，遍历并输出ConcurrentSkipListMap中的全部元素。 线程b的操作和线程a一样，只不过线程b的名字和线程a的名字不同。当map是ConcurrentSkipListMap对象时，程序能正常运行。如果将map改为TreeMap时，程序会产生ConcurrentModificationException异常。</content>
	</entry>
	<entry>
		<title>多线程_生产者消费者</title>
		<url>/article/75fde354.html</url>
		<content>生产者消费者121. Java生产者消费者的三种实现https://blog.csdn.net/xindoo/article/details/80004003 1234567Java生产者消费者是最基础的线程同步问题，java岗面试中还是很容易遇到的，之前没写过多线程的代码，面试中被问到很尬啊，面完回来恶补下。在网上查到大概有5种生产者消费者的写法，分别如下。用synchronized对存储加锁，然后用object原生的wait() 和 notify()做同步。用concurrent.locks.Lock，然后用condition的await() 和signal()做同步。直接使用concurrent.BlockingQueue。使用PipedInputStream/PipedOutputStream。使用信号量semaphore。synchronized版本123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172import java.util.LinkedList;import java.util.Queue;public class ProducerAndConsumer {    private final int MAX_LEN = 10;    private Queue&lt;Integer&gt; queue = new LinkedList&lt;Integer&gt;();    class Producer extends Thread {        @Override        public void run() {            producer();        }        private void producer() {            while(true) {                synchronized (queue) {                    while (queue.size() == MAX_LEN) {                        queue.notify();                        System.out.println(&quot;当前队列满&quot;);                        try {                            queue.wait();                        } catch (InterruptedException e) {                            e.printStackTrace();                        }                    }                    queue.add(1);                    queue.notify();                    System.out.println(&quot;生产者生产一条任务，当前队列长度为&quot; + queue.size());                    try {                        Thread.sleep(500);                    } catch (InterruptedException e) {                        e.printStackTrace();                    }                }            }        }    }    class Consumer extends Thread {        @Override        public void run() {            consumer();        }        private void consumer() {            while (true) {                synchronized (queue) {                    while (queue.size() == 0) {                        queue.notify();                        System.out.println(&quot;当前队列为空&quot;);                        try {                            queue.wait();                        } catch (InterruptedException e) {                            e.printStackTrace();                        }                    }                    queue.poll();                    queue.notify();                    System.out.println(&quot;消费者消费一条任务，当前队列长度为&quot; + queue.size());                    try {                        Thread.sleep(500);                    } catch (InterruptedException e) {                        e.printStackTrace();                    }                }            }        }    }    public static void main(String[] args) {        ProducerAndConsumer pc = new ProducerAndConsumer();        Producer producer = pc.new Producer();        Consumer consumer = pc.new Consumer();        producer.start();        consumer.start();    }}lock+condition的同步123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import java.util.LinkedList;import java.util.Queue;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;/** * version 1 doesn't use synchronized to improve performance */public class ProducerAndConsumer1 {    private final int MAX_LEN = 10;    private Queue&lt;Integer&gt; queue = new LinkedList&lt;Integer&gt;();    private final Lock lock = new ReentrantLock();    private final Condition condition = lock.newCondition();    class Producer extends Thread {        @Override        public void run() {            producer();        }        private void producer() {            while(true) {                lock.lock();                try {                    while (queue.size() == MAX_LEN) {                        System.out.println(&quot;当前队列满&quot;);                        try {                            condition.await();                        } catch (InterruptedException e) {                            e.printStackTrace();                        }                    }                    queue.add(1);                    condition.signal();                    System.out.println(&quot;生产者生产一条任务，当前队列长度为&quot; + queue.size());                    try {                        Thread.sleep(500);                    } catch (InterruptedException e) {                        e.printStackTrace();                    }                } finally {                    lock.unlock();                }            }        }    }    class Consumer extends Thread {        @Override        public void run() {            consumer();        }        private void consumer() {            while (true) {                lock.lock();                try {                    while (queue.size() == 0) {                        System.out.println(&quot;当前队列为空&quot;);                        try {                            condition.await();                        } catch (InterruptedException e) {                            e.printStackTrace();                        }                    }                    queue.poll();                    condition.signal();                    System.out.println(&quot;消费者消费一条任务，当前队列长度为&quot; + queue.size());                    try {                        Thread.sleep(500);                    } catch (InterruptedException e) {                        e.printStackTrace();                    }                } finally {                    lock.unlock();                }            }        }    }    public static void main(String[] args) {        ProducerAndConsumer pc = new ProducerAndConsumer();        Producer producer = pc.new Producer();        Consumer consumer = pc.new Consumer();        producer.start();        consumer.start();    }}BlockingQueue版实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import java.util.Random;import java.util.concurrent.BlockingQueue;import java.util.concurrent.LinkedBlockingQueue;public class ProducerAndConsumer {    private BlockingQueue&lt;Integer&gt; queue = new LinkedBlockingQueue&lt;Integer&gt;(10);    class Producer extends Thread {        @Override        public void run() {            producer();        }        private void producer() {            while(true) {                try {                    queue.put(1);                } catch (InterruptedException e) {                    e.printStackTrace();                }                System.out.println(&quot;生产者生产一条任务，当前队列长度为&quot; + queue.size());                try {                    Thread.sleep(new Random().nextInt(1000)+500);                } catch (InterruptedException e) {                    e.printStackTrace();                }            }        }    }    class Consumer extends Thread {        @Override        public void run() {            consumer();        }        private void consumer() {            while (true) {                try {                    queue.take();                } catch (InterruptedException e) {                    e.printStackTrace();                }                System.out.println(&quot;消费者消费一条任务，当前队列长度为&quot; + queue.size());                try {                    Thread.sleep(new Random().nextInt(1000)+500);                } catch (InterruptedException e) {                    e.printStackTrace();                }            }        }    }    public static void main(String[] args) {        ProducerAndConsumer pc = new ProducerAndConsumer();        Producer producer = pc.new Producer();        Consumer consumer = pc.new Consumer();        producer.start();        consumer.start();    }}Disruptor方式123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107// 摘自《Java高并发程序设计》// https://blog.csdn.net/sunhaoning/article/details/64131362// BlockigQueue用于实现生产者和消费者一个不错的选择。它可以很自然地实现作为生产者和消费者的内存缓冲区。但是BlockigQueue并不是一个高性能的实现，它完全使用锁和阻塞等待来实现线程间的同步。在高并发场合，它的性能并不是特别的优越。就像之前我已经提过的：ConcurrentLinkedQueue是一个高性能的队列，但是BlockingQueue只是为了方便数据共享。// 而ConcurrentLinkedQueue的秘诀就在于大量使用了无锁的CAS操作。同理，如果我们使用CAS来实现生产者-消费者模式，也同样可以获得可观的性能提升。不过正如大家所见，使用CAS进行编程是非常困难的，但有一个好消息是，目前有一个现成的Disruptor框架，它已经帮助我们实现了这一个功能。// Disruptor框架是由LMAX公司开发的一款高效的无锁内存队列。它使用无锁的方式实现了一个环形队列，非常适合于实现生产者和消费者模式，比如事件和消息的发布。在Disruptor中，别出心裁地使用了环形队列（RingBuffer）来代替普通线性队列，这个环形队列内部实现为一个普通的数组。对于一般的队列，势必要提供队列同步head和尾部tail两个指针，用于出队和入队，这样无疑就增加了线程协作的复杂度。但如果队列是环形的，则只需要对外提供一个当前位置cursor，利用这个指针既可以进入入队也可以进行出队操作。由于环形队列的缘故，队列的总大小必须事先指定，不能动态扩展。为了能够快速从一个序列（sequence）对应到数组的实际位置（每次有元素入队，序列就加1），Disruptor要求我们必须将数组的大小设置为2的整数次方。这样通过sequence &amp;(queueSize-1)就能立即定位到实际的元素位置index。这个要比取余（%）操作快得多。// 首先，我们还是需要一个代表数据的PCData：public class PCData{    private long value;    public void set(long value)    {        this.value = value;    }    public long get(){        return value;    }}// 消费者实现为WorkHandler接口，它来自Disruptor框架public class Consumer implements WorkHandler＜PCData＞ {    @Override    public void onEvent(PCData event) throws Exception {        System.out.println(Thread.currentThread().getId() + &quot;:Event: --&quot;                + event.get() * event.get() + &quot;--&quot;);    }}// 还需要一个产生PCData的工厂类。它会在Disruptor系统初始化时，构造所有的缓冲区中的对象实例public class PCDataFactory implements EventFactory＜PCData＞{    public PCData newInstance()    {        return new PCData();    }}// 生产者public class Producer{     private final RingBuffer＜PCData＞ ringBuffer;     public Producer(RingBuffer＜PCData＞ ringBuffer)     {         this.ringBuffer = ringBuffer;     }     public void pushData(ByteBuffer bb)     {         long sequence = ringBuffer.next();  // Grab the next sequence         try         {             PCData event = ringBuffer.get(sequence); // Get the entry in the Disruptor                                                         // for the sequence             event.set(bb.getLong(0));  // Fill with data         }         finally         {             ringBuffer.publish(sequence);         }     }}// 主函数public static void main(String[] args) throws Exception {     Executor executor = Executors.newCachedThreadPool();     PCDataFactory factory = new PCDataFactory();     // Specify the size of the ring buffer, must be power of 2.     int bufferSize = ;     Disruptor＜PCData＞ disruptor = new Disruptor＜PCData＞(factory,             bufferSize,             executor,             ProducerType.MULTI,             new BlockingWaitStrategy()             );     disruptor.handleEventsWithWorkerPool(             new Consumer(),             new Consumer(),             new Consumer(),             new Consumer());     disruptor.start();     RingBuffer＜PCData＞ ringBuffer = disruptor.getRingBuffer();     Producer producer = new Producer(ringBuffer);     ByteBuffer bb = ByteBuffer.allocate(8);     for (long l = 0; true; l++)     {         bb.putLong(0, l);         producer.pushData(bb);         Thread.sleep(0);         System.out.println(&quot;add data &quot;+l);     } } 8:Event: --0--add data 011:Event: --1--add data 110:Event: --4--add data 29:Event: --9--add data 3    根据Disruptor的官方报告，Disruptor的性能要比BlockingQueue至少高一个数量级以上。提高消费者的响应时间：选择合适的策略当有新数据在Disruptor的环形缓冲区中产生时，消费者如何知道这些新产生的数据呢？或者说，消费者如何监控缓冲区中的信息呢？为此，Disruptor提供了几种策略，这些策略由WaitStrategy接口进行封装，主要有以下几种实现。BlockingWaitStrategy：这是默认的策略。使用BlockingWaitStrategy和使用BlockingQueue是非常类似的，它们都使用锁和条件（Condition）进行数据的监控和线程的唤醒。因为涉及到线程的切换，BlockingWaitStrategy策略是最节省CPU，但是在高并发下性能表现最糟糕的一种等待策略。SleepingWaitStrategy：这个策略也是对CPU使用率非常保守的。它会在循环中不断等待数据。它会先进行自旋等待，如果不成功，则使用Thread.yield()让出CPU，并最终使用LockSupport.parkNanos(1)进行线程休眠，以确保不占用太多的CPU数据。因此，这个策略对于数据处理可能产生比较高的平均延时。它比较适合于对延时要求不是特别高的场合，好处是它对生产者线程的影响最小。典型的应用场景是异步日志。YieldingWaitStrategy：这个策略用于低延时的场合。消费者线程会不断循环监控缓冲区变化，在循环内部，它会使用Thread.yield()让出CPU给别的线程执行时间。如果你需要一个高性能的系统，并且对延时有较为严格的要求，则可以考虑这种策略。使用这种策略时，相当于你的消费者线程变身成为了一个内部执行了Thread.yield()的死循环。因此，你最好有多于消费者线程数量的逻辑CPU数量（这里的逻辑CPU，我指的是“双核四线程”中的那个四线程，否则，整个应用程序恐怕都会受到影响。BusySpinWaitStrategy：这个是最疯狂的等待策略了。它就是一个死循环！消费者线程会尽最大努力疯狂监控缓冲区的变化。因此，它会吃掉所有的CPU资源。你只有在对延迟非常苛刻的场合可以考虑使用它（或者说，你的系统真的非常繁忙）。因为在这里你等同开启了一个死循环监控，所以，你的物理CPU数量必须要大于消费者线程数。注意，我这里说的是物理CPU，如果你在一个物理核上使用超线程技术模拟两个逻辑核，另外一个逻辑核显然会受到这种超密集计算的影响而不能正常工作。CPU Cache的优化：解决伪共享问题​        什么是伪共享问题呢？我们知道，为了提高CPU的速度，CPU有一个高速缓存Cache。在高速缓存中，读写数据的最小单位为缓存行（Cache Line），它是从主存（memory）复制到缓存（Cache）的最小单位，一般为32字节到128字节。​        如果两个变量存放在一个缓存行中时，在多线程访问中，可能会相互影响彼此的性能。如图5.4所示，假设X和Y在同一个缓存行。运行在CPU1上的线程更新了X，那么CPU2上的缓存行就会失效，同一行的Y即使没有修改也会变成无效，导致Cache无法命中。接着，如果在CPU2上的线程更新了Y，则导致CPU1上的缓存行又失效（此时，同一行的X又变得无法访问）。这种情况反反复复发生，无疑是一个潜在的性能杀手。如果CPU经常不能命中缓存，那么系统的吞吐量就会急剧下降。并行流水线121. 借鉴《Java高并发程序设计》第5.6节2. 123456789101112131415计算(B+C)*B/2，那么这个运行过程就是无法并行的。原因是，如果B+C没有执行完成，则永远算不出(B+C)*B，这就是数据相关性。如果线程执行时，所需的数据存在这种依赖关系，那么，就没有办法将它们完美的并行化。那遇到这种情况时，有没有什么补救措施呢？答案是肯定的，那就是借鉴日常生产中的流水线思想。比如，现在要生产一批小玩偶。小玩偶的制作分为四个步骤，第一要组装身体，第二要在身体上安装四肢和头部，第三，给组装完成的玩偶穿上一件漂亮的衣服，第四，就可以包装出货了。为了加快制作玩具的进度，我们不可能叫四个人同时加工一个玩具，因为这四个步骤有着严重的依赖关系。如果没有身体，就没有地方安装四肢，如果没有组装完成，就不能穿衣服，如果没有穿上衣服，就不能包装发货。因此，找四个人来做一个玩偶是毫无意义的。但是，如果你现在要制作的不是1只玩偶，而是1万只玩偶，那情况就不同了。你可以找四个人，第一个人只负责组装身体，完成后交给第二个人；第二个人只负责安装头部和四肢，交付第三人；第三人只负责穿衣服，并交付第四人；第四人只负责包装发货。这样所有人都可以一起工作，共同完成任务，而整个时间周期也能缩短到原来的1/4左右，这就是流水线的思想。一旦流水线满载，每次只需要一步（假设一个玩偶需要四步）就可以产生一个玩偶类似的思想可以借鉴到程序开发中。即使(B+C)*B/2无法并行，但是如果你需要计算一大堆B和C的值，你依然可以将它流水化。首先将计算过程拆分为三个步骤：P1:A＝B+CP2:D＝A×BP3:D=D/2上述步骤中P1、P2和P3均在单独的线程中计算，并且每个线程只负责自己的工作。此时，P3的计算结果就是最终需要的答案。P1接收B和C的值，并求和，将结果输入给P2。P2求乘积后输入给P3。P3将D除以2得到最终值。一旦这条流水线建立，只需要一个计算步骤就可以得到(B+C)*B/2的结果。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778// 为了实现这个功能，我们需要定义一个在线程间携带结果进行信息交换的载体：public class Msg {    public double i;    public double j;    public String orgStr=null;}// P1计算的是加法： public class Plus implements Runnable {     public static BlockingQueue＜Msg＞ bq=new LinkedBlockingQueue＜Msg＞();     @Override     public void run() {         while(true){             try {                 Msg msg=bq.take();                 msg.j=msg.i+msg.j;                 Multiply.bq.add(msg);             } catch (InterruptedException e) {             }         }     } }// 上述代码中，P1取得封装了两个操作数的Msg，并进行求和，将结果传递给乘法线程P2（第9行）。当没有数据需要处理时，P1进行等待。// P2计算乘法： public class Multiply implements Runnable {     public static BlockingQueue＜Msg＞ bq = new LinkedBlockingQueue＜Msg＞();     @Override     public void run() {         while (true) {             try {                 Msg msg = bq.take();                 msg.i = msg.i * msg.j;                 Div.bq.add(msg);             } catch (InterruptedException e) {             }         }     } }//和P1非常类似，P2计算相乘结果后，将中间结果传递给除法线程P3。//P3计算除法： public class Div implements Runnable {     public static BlockingQueue＜Msg＞ bq = new LinkedBlockingQueue＜Msg＞();     @Override     public void run() {         while (true) {             try {                 Msg msg = bq.take();                 msg.i = msg.i / 2;                 System.out.println(msg.orgStr + &quot;=&quot; + msg.i);             } catch (InterruptedException e) {             }         }     } }// P3将结果除以2后输出最终的结果。// 最后是提交任务的主线程，这里，我们提交0万个请求，让线程组进行计算： public class PStreamMain {     public static void main(String[] args) {         new Thread(new Plus()).start();         new Thread(new Multiply()).start();         new Thread(new Div()).start();         for (int i = 1; i ＜= ; i++) {             for (int j = 1; j ＜= ; j++) {                 Msg msg = new Msg();                 msg.i = i;                 msg.j = j;                 msg.orgStr = &quot;((&quot; + i + &quot;+&quot; + j + &quot;)*&quot; + i + &quot;)/2&quot;;                 Plus.bq.add(msg);             }         }     } }上述代码第行，将数据提交给P1加法线程，开启流水线的计算。在多核或者分布式场景中，这种设计思路可以有效地将有依赖关系的操作分配在不同的线程中进行计算，尽可能利用多核优势。</content>
	</entry>
	<entry>
		<title>python多进程</title>
		<url>/article/30993.html</url>
		<content>python多进程基本概念Python中的多进程是通过multiprocessing包来实现的，和多线程的threading.Thread差不多，它可以利用multiprocessing.Process对象来创建一个进程对象这个进程对象的方法和线程对象的方法差不多也有start()，run()，join()等方法，其中有一个方法不同Thread线程对象中的守护线程方法是setDeamon，而Process进程对象的守护进程是通过设置daemon属性来完成的与多线程的共享式内存不同，由于各个进程都是相互独立的，因此进程间通信再多进程中扮演这非常重要的角色，Python中我们可以使用multiprocessing模块中的pipe、queue、Array、Value等等工具来实现进程间通讯和数据共享，但是在编写起来仍然具有很大的不灵活性任务类型同步与异步同步就是指一个进程在执行某个请求的时候，若该请求需要一段时间才能返回信息那么这个进程将会一直等待下去，直到收到返回信息才继续执行下去异步是指进程不需要一直等下去，而是继续执行下面的操作，不管其他进程的状态当有消息返回时系统会通知进程进行处理，这样可以提高执行的效率IO密集和计算密集对于IO密集型任务: python的多线程能够节省时间对于计算(CPU)密集型任务: Python的多线程并没有用处，建议使用多进程其他组合搭配python使用多核，即开多个进程方法一: 协程+多进程，使用方法简单，效率还可以，一般使用该方法协程yield是你自己写的，是自己定义什么时候切换进程方法二：IO多路复用，使用复杂，但效率很高，不常用多进程相关模块1234567891011121314# 创建管理进程模块：Process(用于创建进程):通过创建一个Process对象然后调用它的start()方法来生成进程。Process遵循threading.Thread的API。Pool(用于创建进程管理池)：可以创建一个进程池，该进程将执行与Pool该类一起提交给它的任务，当子进程较多需要管理时使用。Queue（用于进程通信，资源共享）：进程间通信，保证进程安全。Value，Array（用于进程通信，资源共享）：Pipe（用于管道通信）：管道操作。Manager（用于资源共享）：创建进程间共享的数据，包括在不同机器上运行的进程之间的网络共享。# 同步子进程模块：ConditionEvent：用来实现进程间同步通信。Lock：当多个进程需要访问共享资源的时候，Lock可以用来避免访问的冲突。RLockSemaphore：用来控制对共享资源的访问数量，例如池的最大连接数。python多线程低效原因GIL的全称是 Global Interpreter Lock(全局解释器锁)，来源是 Python 设计之初的考虑，为了数据安全所做的决定某个线程想要执行，必须先拿到 GIL，我们可以把 GIL 看作是“通行证”，并且在一个 Python 进程中，GIL 只有一个拿不到通行证的线程，就不允许进入 CPU 执行目前 Python 的解释器有多种，例如：CPython：CPython 是用C语言实现的 Python 解释器，作为官方实现，它是最广泛使用的 Python 解释器PyPy：PyPy 是用RPython实现的解释器。RPython 是 Python 的子集， 具有静态类型。这个解释器的特点是即时编译，支持多重后端(C, CLI, JVM)。PyPy 旨在提高性能，同时保持最大兼容性(参考 CPython 的实现)Jython：Jython 是一个将 Python 代码编译成 Java 字节码的实现，运行在JVM(Java Virtual Machine)上。另外，它可以像是用 Python 模块一样，导入 并使用任何Java类IronPython：IronPython 是一个针对 .NET 框架的 Python 实现。它 可以用 Python 和 .NET framewor k的库，也能将 Python 代码暴露给 .NET 框架中的其他语言GIL 只在 CPython 中才有，而在 PyPy 和 Jython 中是没有 GIL 的注意: 每次释放GIL锁，线程进行锁竞争、切换线程，会消耗资源这就导致打印线程执行时长，会发现耗时更长的原因并且由于 GIL 锁存在，Python 里一个进程永远只能同时执行一个线程(拿到 GIL 的线程才能执行)，这就是为什么在多核CPU上，Python 的多线程效率并不高的根本原因多进程实现方式Process普通Process12345678910111213141516from multiprocessing import  Processdef func(name):    print('测试%s多进程' %name)if __name__ == '__main__':    process_list = []    for i in range(5):  #开启5个子进程执行fun1函数        p = Process(target=func, args=('Python',)) #实例化进程对象        p.start()        process_list.append(p)    for i in process_list:        p.join()    print('结束测试')12345678测试Python多进程测试Python多进程测试Python多进程测试Python多进程测试Python多进程结束测试Process finished with exit code 0上面的代码开启了5个子进程去执行函数，我们可以观察结果，是同时打印的，这里实现了真正的并行操作，就是多个CPU同时执行任务。我们知道进程是python中最小的资源分配单元，也就是进程中间的数据，内存是不共享的，每启动一个进程，都要独立分配资源和拷贝访问的数据，所以进程的启动和销毁的代价是比较大了，所以在实际中使用多进程，要根据服务器的配置来设定。继承Process12345678910111213141516171819202122from multiprocessing import  Processclass MyProcess(Process): #继承Process类    def __init__(self,name):        super(MyProcess,self).__init__()        self.name = name    def run(self):        print('测试%s多进程' % self.name)if __name__ == '__main__':    process_list = []    for i in range(5):  #开启5个子进程执行fun1函数        p = MyProcess('Python') #实例化进程对象        p.start()        process_list.append(p)    for i in process_list:        p.join()    print('结束测试')12345678测试Python多进程测试Python多进程测试Python多进程测试Python多进程测试Python多进程结束测试Process finished with exit code 0通过类继承的方法来实现的，python多进程的第二种实现方式也是一样的，效果和第一种方式一样Process类的其他方法12345678910111213141516171819构造方法：Process([group [, target [, name [, args [, kwargs]]]]])　　group: 线程组 　　target: 要执行的方法　　name: 进程名　　args/kwargs: 要传入方法的参数实例方法：　　is_alive()：返回进程是否在运行,bool类型。　　join([timeout])：阻塞当前上下文环境的进程程，直到调用此方法的进程终止或到达指定的timeout（可选参数）。　　start()：进程准备就绪，等待CPU调度　　run()：strat()调用run方法，如果实例进程时未制定传入target，这star执行t默认run()方法。　　terminate()：不管任务是否完成，立即停止工作进程属性：　　daemon：和线程的setDeamon功能一样　　name：进程名字　　pid：进程号进程池123456789101112131415161718192021222324252627# apply_async：异步from  multiprocessing import Pool,cpu_countimport os, time, randomdef fun1(name):    print('Run task %s (%s)...' % (name, os.getpid()))    start = time.time()    time.sleep(random.random() * 3)    end = time.time()    print('Task %s runs %0.2f seconds.' % (name, (end - start)))    return f'{name}: {os.getpid()}'if __name__=='__main__':    results = []    pool = Pool(cpu_count()-1)    for i in range(4):        results.append(pool.apply_async(func=fun1, args=(i,)))    pool.close()    pool.join()    print()    for result in results:        print(result.get())    print('All Done!!!')    print('结束测试')1234567891011121314151617Run task 0 (30716)...Run task 1 (15020)...Run task 2 (23200)...Run task 3 (5884)...Task 0 runs 1.34 seconds.Task 2 runs 1.53 seconds.Task 1 runs 1.88 seconds.Task 3 runs 2.48 seconds.0: 307161: 150202: 232003: 5884All Done!!!结束测试Process finished with exit code 0对Pool对象调用join()方法会等待所有子进程执行完毕，调用join()之前必须先调用close()，调用close()之后就不能继续添加新的Process了123456789101112131415161718192021222324252627282930# map_async：异步from multiprocessing import Pool, cpu_count, Managerfrom functools import partialdef job(data, mgrDicTask, lock):    res = f'a+b = {data[0] + data[1]}'    lock.acquire()    # Manager对象无法监测到它引用的可变对象值的修改，需要通过触发__setitem__方法来让它获得通知    tempDic = list(mgrDicTask['result'])    tempDic.append(res)    mgrDicTask['result'] = tempDic    lock.release()    return resif __name__ == &quot;__main__&quot;:    data = [[2, 3], [3, 4], [2, 5]]    pool = Pool(processes=cpu_count() - 1)    mgr = Manager()    lock = mgr.Lock()    mgrDicTask = mgr.dict()    mgrDicTask['result'] = []    fun = partial(job, mgrDicTask=mgrDicTask, lock=lock)    pool.map_async(fun, data)    pool.close()    pool.join()    print(mgrDicTask['result'])    print('All Done!!!')1234['a+b = 7', 'a+b = 7', 'a+b = 5']All Done!!!Process finished with exit code 0其他进程池1234567891011121314151617181920212223242526272829303132# 进程池的另外一种创建方式，跟线程池的创建方式一样。其方法等也相同。def process_pool_test(url_list):    book_list = []    # 创建进程池    pool = ProcessPoolExecutor(max_workers=20)    start = time.time()    for url in url_list:        time.sleep(0.5)        result = pool.submit(get_book_info, url)        book_list.append(result)    pool.shutdown()    print('time: ', time.time() - start)    book_name_list = []    author_list = []    author_info_list = []    print('book_list: ', len(book_list))    for future in book_list:        book_name_list.extend(future.result()['name'])        author_list.extend(future.result()['author'])        author_info_list.extend(future.result()['info'])    ExcelUtils.write_data_to_excel('bookInfo', book_name_list, author_list, author_info_list)if __name__ == '__main__':    sys.setrecursionlimit(10000)    url_list = ['https://www.edge.org/library']    for i in range(1, 52):        url_list.append('https://www.edge.org/library?page=%s' % i)    thread_pool_test(url_list)多进程通信内容提取神器 beautiful Soup 的用法进程是系统独立调度核分配系统资源(CPU、内存)的基本单位，进程之间是相互独立的，每启动一个新的进程相当于把数据进行了一次克隆，子进程里的数据修改无法影响到主进程中的数据，不同子进程之间的数据也不能共享，这是多进程在使用中与多线程最明显的区别但是难道Python多进程中间难道就是孤立的吗？当然不是，python也提供了多种方法实现了多进程中间的通信和数据共享(可以修改一份数据)进程队列Queue123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v1.0@author: narutohyc@file: multiprocessing_queue.py@Description: 多进程队列使用示例@time: 2020/5/14 15:53&quot;&quot;&quot;from multiprocessing import Process, Queue, Managerfrom multiprocessing import cpu_countimport osimport timeclass Task:    def __init__(self, task_name: str, data: list, **kwargs):        self.task_name = task_name        self.data = data    def __repr__(self):        return f'task_name:{self.task_name} data:{self.data}'class MultiProcessingQueue:    def __init__(self):        # 进程数        self.num_of_worker = cpu_count()        # 进程队列大小，根据不同的任务需求        self.size_of_queue = 10    def start_work(self):        print(&quot;start_work 开始&quot;)        # 进程队列        process_list = []        # 新建一个大小为10的队列        work_queue = Queue(self.size_of_queue)        # 进程间共享列表, 其他的还有共享字典等，都是进程安全的        dealed_sample_lst = Manager().list()        # 一个生产者        sent = Process(target=self.productor, args=(work_queue, dealed_sample_lst,))        sent.start()        process_list.append(sent)        # 多个消费者        for _ in range(self.num_of_worker - 1):            process = Process(target=self.consumer, args=(work_queue, dealed_sample_lst,))            process.start()            process_list.append(process)        [process.join() for process in process_list]        print(&quot;start_work 结束&quot;)        return dealed_sample_lst    def productor(self, work_queue: Queue, dealed_sample_lst):        print(&quot;生产者开始工作&quot;)        for ii in range(100):            work_queue.put(Task(task_name=f'{str(os.getpid())}-{str(ii)}', data=[ii for _ in range(2)]))            if ii % 30 == 0:                time.sleep(1)                print(&quot;生产者休息ing&quot;)        '''        JoinableQueue 比Queue多了task_done() 与join()两个函数，多用于生产者消费者问题。        task_done()是用在get()后，发送通知说我get完了        join()是说Queue里所有的task都已处理。        '''        # 这里需要加入结束标识，还有就是JoinableQueue的方式        for _ in range(self.num_of_worker - 1):            work_queue.put(None)        print(&quot;生产者工作结束&quot;)    def consumer(self, work_queue: Queue, dealed_sample_lst):        while True:            task: Task = work_queue.get()            if task is None:                break            # 处理数据            task.data = [ii * 2 for ii in task.data]            dealed_sample_lst.append(task)            print(task)        print(f'进程{os.getpid()} 处理结束')def multiprocessing_queue_test():    multiprocessing_queue = MultiProcessingQueue()    dealed_sample_lst = multiprocessing_queue.start_work()    # for sample in dealed_sample_lst:    #     print(sample)    print(&quot;测试结束&quot;)if __name__ == '__main__':    multiprocessing_queue_test()123456789101112131415161718192021222324252627282930313233start_work 开始生产者开始工作task_name:28868-0 data:[0, 0]生产者休息ingtask_name:28868-1 data:[2, 2]...task_name:28868-6 data:[12, 12]生产者休息ingtask_name:28868-31 data:[62, 62]...task_name:28868-58 data:[116, 116]生产者休息ingtask_name:28868-61 data:[122, 122]...task_name:28868-64 data:[128, 128]...生产者休息ingtask_name:28868-91 data:[182, 182]...task_name:28868-96 data:[192, 192]生产者工作结束进程29208 处理结束task_name:28868-97 data:[194, 194]进程20632 处理结束进程28496 处理结束task_name:28868-98 data:[196, 196]进程30200 处理结束进程26512 处理结束进程29776 处理结束task_name:28868-99 data:[198, 198]进程30072 处理结束start_work 结束测试结束上面的代码结果可以看到我们主进程中可以通过Queue获取子进程中put的数据，实现进程间的通信JoinableQueue队列JoinableQueue([maxsize])：这就像是一个Queue对象，但队列允许项目的使用者通知生成者项目已经被成功处理通知进程是使用共享的信号和条件变量来实现的参数介绍：maxsize: 是队列中允许最大项数，省略则无大小限制方法介绍：q.task_done()：使用者使用此方法发出信号，表示q.get()的返回项目已经被处理如果调用此方法的次数大于从队列中删除项目的数量将引发ValueError异常q.join():生产者调用此方法进行阻塞，直到队列中所有的项目均被处理阻塞将持续到队列中的每个项目均调用q.task_done()方法为止示例代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v1.0@author: narutohyc@file: multiprocessing_queue.py@Description: 多进程队列使用示例@time: 2020/5/14 15:53&quot;&quot;&quot;from multiprocessing import Process, Queue, JoinableQueue, Managerfrom multiprocessing import cpu_countimport os, timeclass Task:    def __init__(self, task_name: str, data: list, **kwargs):        self.task_name = task_name        self.data = data    def __repr__(self):        return f'task_name:{self.task_name} data:{self.data}'class MultiProcessingJoinableQueue:    def __init__(self):        # 进程数        self.num_of_worker = cpu_count()        # 进程队列大小，根据不同的任务需求        self.size_of_queue = 10    def start_work(self):        print(&quot;start_work 开始&quot;)        # 进程队列        process_list = []        # 新建一个大小为10的队列        work_queue = JoinableQueue(self.size_of_queue)        # 进程间共享列表, 其他的还有共享字典等，都是进程安全的        dealed_sample_lst = Manager().list()        # 一个生产者        sent = Process(target=self.productor, args=(work_queue, dealed_sample_lst,))        process_list.append(sent)        # 多个消费者        for _ in range(self.num_of_worker - 1):            process = Process(target=self.consumer, args=(work_queue, dealed_sample_lst,))            process.daemon = True            process_list.append(process)        [process.start() for process in process_list]        # 这里需要注意的一点是，这里join只需要调用生产者(别调消费者的join，否则无法正常退出)        # 消费者不需要，个人感觉应该是生产者那边已经调用了work_queue.join()的方法        # 消费者结束后，整个程序退出        [process.join() for process in process_list[:1]]        print(&quot;start_work 结束&quot;)        return dealed_sample_lst    def productor(self, work_queue: Queue, dealed_sample_lst):        print(&quot;生产者开始工作&quot;)        for ii in range(100):            work_queue.put(Task(task_name=f'{str(os.getpid())}-{str(ii)}', data=[ii for _ in range(2)]))            if ii % 30 == 0:                time.sleep(1)                print(&quot;生产者休息ing&quot;)        print(&quot;生产者工作结束&quot;)        work_queue.join()    def consumer(self, work_queue: Queue, dealed_sample_lst):        while True:            task: Task = work_queue.get()            if task is None:                break            # 处理数据            task.data = [ii * 2 for ii in task.data]            dealed_sample_lst.append(task)            print(task)            work_queue.task_done()        print(f'进程{os.getpid()} 处理结束')def multiprocessing_joinablequeue_test():    multiprocessing_joinablequeue = MultiProcessingJoinableQueue()    dealed_sample_lst = multiprocessing_joinablequeue.start_work()    # for sample in dealed_sample_lst:    #     print(sample)    print(&quot;测试结束&quot;)if __name__ == '__main__':    multiprocessing_joinablequeue_test()结果输出12345678910111213141516171819202122start_work 开始生产者开始工作task_name:14608-0 data:[0, 0]生产者休息ingtask_name:14608-1 data:[2, 2]...task_name:14608-7 data:[14, 14]生产者休息ingtask_name:14608-31 data:[62, 62]...task_name:14608-60 data:[120, 120]生产者休息ingtask_name:14608-61 data:[122, 122...task_name:14608-90 data:[180, 180]生产者休息ing生产者工作结束task_name:14608-91 data:[182, 182]...task_name:14608-93 data:[186, 186]start_work 结束测试结束管道PipePipe的本质是进程之间的用管道数据传递，而不是数据共享，这和socket有点像pipe()返回两个连接对象分别表示管道的两端，每端都有send()和recv()函数如果两个进程试图在同一时间的同一端进行读取和写入那么，这可能会损坏管道中的数据管道是数据不安全的，多个进程同时收发数据可道引起数据异常，这时候就应该配合锁使用12345678910111213141516171819from multiprocessing import Process, Pipedef fun1(conn):    print('子进程发送消息：')    conn.send('你好主进程')    print('子进程接受消息：')    print(conn.recv())    conn.close()if __name__ == '__main__':    conn1, conn2 = Pipe() #关键点，pipe实例化生成一个双向管    p = Process(target=fun1, args=(conn2,)) #conn2传给子进程    p.start()    print('主进程接受消息：')    print(conn1.recv())    print('主进程发送消息：')    conn1.send(&quot;你好子进程&quot;)    p.join()    print('结束测试')123456789主进程接受消息：子进程发送消息：子进程接受消息：你好主进程主进程发送消息：你好子进程结束测试Process finished with exit code 0上面可以看到主进程和子进程可以相互发送消息ManagersQueue和Pipe只是实现了数据交互，并没实现数据共享，即一个进程去更改另一个进程的数据，那么就要用到Managers1234567891011121314151617181920212223from multiprocessing import Process, Managerdef fun1(dic,lis,index):    dic[index] = 'a'    dic['2'] = 'b'        lis.append(index)    #[0,1,2,3,4,0,1,2,3,4,5,6,7,8,9]    #print(l)if __name__ == '__main__':    with Manager() as manager:        dic = manager.dict()#注意字典的声明方式，不能直接通过{}来定义        l = manager.list(range(5))#[0,1,2,3,4]        process_list = []        for i in range(10):            p = Process(target=fun1, args=(dic,l,i))            p.start()            process_list.append(p)        for res in process_list:            res.join()        print(dic)        print(l)12{0: 'a', '2': 'b', 3: 'a', 1: 'a', 2: 'a', 4: 'a', 5: 'a', 7: 'a', 6: 'a', 8: 'a', 9: 'a'}[0, 1, 2, 3, 4, 0, 3, 1, 2, 4, 5, 7, 6, 8, 9]可以看到主进程定义了一个字典和一个列表，在子进程中，可以添加和修改字典的内容在列表中插入新的数据，实现进程间的数据共享，即可以共同修改同一份数据注意事项无法调用多层生成器(待验证)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v0.1@author: narutohyc@file: text.py@Description: @time: 2020/5/29 19:51&quot;&quot;&quot;from multiprocessing import Process, Queue, JoinableQueue, Managerfrom multiprocessing import cpu_countimport os, timefrom abc import (ABC,                 abstractmethod,                 ABCMeta)from comm.logger.logger_config import loggerclass SampleIterator(ABC, metaclass=ABCMeta):    def __init__(self):        pass    @abstractmethod    def __iter__(self):        '''        样本处理并返回        '''        passclass DataSource1(SampleIterator):    def __init__(self):        super(DataSource1, self).__init__()    def __iter__(self):        for ii in range(10):            yield iiclass DataSource2(SampleIterator):    def __init__(self, data_source):        super(DataSource2, self).__init__()        self.data_source = data_source    def __iter__(self):        for ii in self.data_source:            yield iiclass DataSource3(SampleIterator):    def __init__(self, data_source):        super(DataSource3, self).__init__()        self.data_source = data_source    def __iter__(self):        for ii in self.data_source:            yield iiclass DataSource4(SampleIterator):    def __init__(self, data_source):        super(DataSource4, self).__init__()        self.data_source = data_source    def __iter__(self):        for ii in self.data_source:            yield iiclass HUCY():    def __init__(self, data_source=None):        self.num_of_worker = cpu_count()        self.size_of_queue = 2        self.data_source = data_source    def start_work(self):        # 进程队列        process_list = []        # 新建一个大小为10的队列        work_queue = Queue(self.size_of_queue)        # 一个生产者        produce_num = 1        for _ in range(produce_num):            sent = Process(target=self.productor, args=(work_queue,))            sent.start()            process_list.append(sent)        # 多个消费者        for _ in range(self.num_of_worker - produce_num):            process = Process(target=self.consumer, args=(work_queue,))            process.start()            process_list.append(process)        # 这里需要加入结束标识，还有就是JoinableQueue的方式        [process.join() for process in process_list[:produce_num]]        for _ in range(self.num_of_worker - produce_num):            work_queue.put(None)        [process.join() for process in process_list[produce_num:]]        print(&quot;start_work 结束&quot;)    def productor(self, work_queue):        [work_queue.put(ii) for ii in self.data_source]        logger.info(&quot;生产者结束&quot;)    def consumer(self, work_queue):        while True:            data = work_queue.get()            if data is None:                break            logger.info(f&quot;数据: {data}&quot;)        logger.info(&quot;消费者结束&quot;)def hyc_test():    da1=DataSource2(DataSource3(DataSource4(DataSource1())))    da2 = DataSource2(DataSource3(DataSource4(da1)))    da3 = DataSource2(DataSource3(DataSource4(da2)))    da4 = DataSource2(DataSource3(DataSource4(da3)))    da5 = DataSource2(DataSource3(DataSource4(da4)))    da6 = DataSource2(DataSource3(DataSource4(DataSource2(da5))))    hucy = HUCY(da6)    hucy.start_work()if __name__ == '__main__':    hyc_test()几个问题:以上代码 若有多个生产者  就会各自拥有自己的数据生成器，导致数据重复有些定义的方法  好像会使程序卡住</content>
	</entry>
	<entry>
		<title>文件和目录访问</title>
		<url>/article/53591.html</url>
		<content>文件文件是数据的抽象和集合文件是存储在辅助存储器上的数据序列文件是数据存储的一种形式文件展现形态：文本文件和二进制文件文本文件 vs. 二进制文件文件文件和二进制文件只是文件的展示方式本质上，所有文件都是二进制形式存储形式上，所有文件采用两种方式展示文本文件由单一特定编码组成的文件，如UTF-8编码由于存在编码，也被看成是存储着的长字符串适用于例如：.txt文件、.py文件等二进制文件直接由比特0和1组成，没有统一字符编码一般存在二进制0和1的组织结构，即文件格式适用于例如：.png文件、.avi文件等基本使用文件打开和关闭基本使用123&lt;变量名&gt; = open(&lt;文件名&gt;, &lt;打开模式&gt;, encoding='utf-8')文件句柄 文件路径和名称 文本 or 二进制&lt;变量名&gt;.close()打开模式文件的打开模式描述‘r’只读模式，默认值，如果文件不存在，返回FileNotFoundError‘w’覆盖写模式，文件不存在则创建，存在则完全覆盖‘x’创建写模式，文件不存在则创建，存在则返回FileExistsError‘a’追加写模式，文件不存在则创建，存在则在文件最后追加内容‘b’二进制文件模式‘t’文本文件模式，默认值‘+’与r/w/x/a一同使用，在原功能基础上增加同时读写功能示例12345678910111213141516a = open(&quot;f.txt&quot;, &quot;rt&quot;)a.read(size)                  # - 一次读入，统一处理a.readline(size)              # - 按数量读入，逐步处理a.readlines(hint)             # - 一次读入，分行处理a.write(s)a.writelines(lines)a.seek(offset)f = open(&quot;f.txt&quot;)             # - 文本形式、只读模式、默认值f = open(&quot;f.txt&quot;, &quot;rt&quot;)       # - 文本形式、只读模式、同默认值f = open(&quot;f.txt&quot;, &quot;w&quot;)        # - 文本形式、覆盖写模式f = open(&quot;f.txt&quot;, &quot;a+&quot;)       # - 文本形式、追加写模式+ 读文件f = open(&quot;f.txt&quot;, &quot;x&quot;)        # - 文本形式、创建写模式f = open(&quot;f.txt&quot;, &quot;b&quot;)        # - 二进制形式、只读模式f = open(&quot;f.txt&quot;, &quot;wb&quot;)       # - 二进制形式、覆盖写模式文件内容的读取1234567891011121314151617181920# 读入全部内容，如果给出参数，读入前size长度# &lt;f&gt;.read(size=-1)&gt;&gt;&gt;s = f.read(2)中国# 读入一行内容，如果给出参数，读入该行前size长度# &lt;f&gt;.readline(size=-1)&gt;&gt;&gt;s = f.readline()中国是一个伟大的国家！# 读入文件所有行，以每行为元素形成列表# 如果给出参数，读入前hint行# &lt;f&gt;.readlines(hint=-1)while True:lines = f.readlines(100000)if not lines:breakfor line in lines:pass # do something数据的文件写入1234567891011121314151617181920212223242526272829# 1）将嵌套列表或字典转为json格式数据import jsonf=open('a1.txt','w',encoding='utf-8')dic={1:'张三',2:'李四'}dic2={11:'张三三',2:'李思思'}a=json.dumps(dic)f.write(a+'\n')b=json.dumps(dic2)f.write(b+'\n')f.close()# 2)将文件中的列表或字典json字符串读取出来import jsonf=open('a1.txt','r',encoding='utf-8')for i in f:    print(json.loads(i))f.close()# 改变当前文件操作指针的位置，offset含义如下：# 0 – 文件开头； 1 – 当前位置； 2 – 文件结尾# seek（offset [,from]）方法改变当前文件的位置。Offset变量表示要移动的字节数。From变量指定开始移动字节的参考位置。# 如果from被设为0，这意味着将文件的开头作为移动字节的参考位置。如果设为1，则使用当前的位置作为参考位置。如果它被设为2，那么该文件的末尾将作为参考位置。&gt;&gt;&gt;f.seek(0) #回到文件开头# 查看指针位置f.tell()ls = [&quot;中国&quot;, &quot;法国&quot;, &quot;美国&quot;]sfo.writelines(ls)File对象的属性属性描述file.closed返回true如果文件已被关闭，否则返回false。file.mode返回被打开文件的访问模式。file.name返回文件的名称。file.softspace如果用print输出后，必须跟一个空格符，则返回false。否则返回true。高阶使用临时文件和目录一日一技：在Python中创建临时文件用于记录临时数据tempfile —- 生成临时文件和目录当我们在做数据分析的时候，可能会由于数据量过大导致内存不足。如果我们没有条件使用更高配置的电脑，也没有办法优化数据，那么我们可以先把计算的中间值存放在一个文本文件中。 这种方案虽然有效，但是中间数据写成的临时文件如果不清理，时间一长就会占用大量硬盘空间。当然你也可以每一次都覆盖临时文件，这样它虽然不会堆积，但当你的分析程序已经停止的时候，临时文件还在硬盘上占用空间。 tempfile 模块专门用于创建临时文件和临时目录既可以在 UNIX 平台上运行良好，也可以在 Windows 平台上运行良好入门示例123456789101112from tempfile import TemporaryFilewith TemporaryFile('w+t', encoding='utf-8') as f:           # 生成中间数据           f.write('中间数据')           f.write('另一部分中间数据')          # 其他计算过程          # 下面开始读取临时文件           f.seek(0)           f.read()       # 退出with上下文，临时文件自动被删除 由于临时文件被关闭就会被删除，所以需要实现同时读写文件，因此文件模式为 w+t使用 TemporaryFile，你没法知道这个临时文件叫做什么名字。如果你想知道文件名，甚至想让另一个程序打开这个临时文件，那么你可以使用 NamedTemporaryFile： 1234567import redisfrom tempfile import NamedTemporaryFilewith NamedTemporaryFile('w+t') as f:           # 把文件名通过某种方式传给其他程序           client = redis.Redis()           client.set('temp_file', f.name)           # 后续操作 只要这个临时文件还没有被关闭，那么另一个程序就可以读取这个临时文件123&gt;# 通过with语句创建临时目录with tempfile.TemporaryDirectory() as tmpdirname:print('创建临时目录', tmpdirname)常用的函数tempfile 模块函数功能描述tempfile.TemporaryFile(mode=’w+b’, buffering=None, encoding=None, newline=None, suffix=None, prefix=None, dir=None)创建临时文件。返回一个类文件对象，也就是支持文件 I/Otempfile.NamedTemporaryFile(mode=’w+b’, buffering=None, encoding=None, newline=None, suffix=None, prefix=None, dir=None, delete=True)创建临时文件。功能与上一个函数的功能大致相同，只是它生成的临时文件在文件系统中有文件名tempfile.SpooledTemporaryFile(max_size=0, mode=’w+b’, buffering=None, encoding=None, newline=None, suffix=None, prefix=None, dir=None)创建临时文件。与 TemporaryFile 函数相比，当程序向该临时文件输出数据时，会先输出到内存中，直到超过 max_size 才会真正输出到物理磁盘中。tempfile.TemporaryDirectory(suffix=None, prefix=None, dir=None)生成临时目录tempfile.gettempdir()获取系统的临时目录tempfile.gettempdirb()与 gettempdir() 相同，只是该函数返回字节串tempfile.gettempprefix()返回用于生成临时文件的前缀名tempfile.gettempprefixb()与 gettempprefix() 相同，只是该函数返回字节串提示：表中有些函数包含很多参数，但这些参数都具有自己的默认值因此如果没有特殊要求，可以不对其传参tempfile 模块还提供了 tempfile.mkstemp() 和 tempfile.mkdtemp() 两个低级别的函数上面介绍的 4 个用于创建临时文件和临时目录的函数都是高级别的函数，高级别的函数支持自动清理，而且可以与 with 语句一起使用而这两个低级别的函数则不支持，因此一般推荐使用高级别的函数来创建临时文件和临时目录此外，tempfile 模块还提供了 tempfile.tempdir 属性，通过对该属性赋值可以改变系统的临时目录数据解压缩gzip 格式读取压缩文件示例：123import gzipwith gzip.open('/home/joe/file.txt.gz', 'rb') as f:    file_content = f.read()创建GZIP 文件示例：1234import gzipcontent = b&quot;Lots of content here&quot;with gzip.open('/home/joe/file.txt.gz', 'wb') as f:    f.write(content)使用 GZIP 压缩已有的文件示例：12345import gzipimport shutilwith open('/home/joe/file.txt', 'rb') as f_in:    with gzip.open('/home/joe/file.txt.gz', 'wb') as f_out:        shutil.copyfileobj(f_in, f_out)使用 GZIP 压缩二进制字符串示例：123import gzips_in = b&quot;Lots of content here&quot;s_out = gzip.compress(s_in)ZIP格式tar格式如何将整个 tar 归档提取到当前工作目录:1234import tarfiletar = tarfile.open(&quot;sample.tar.gz&quot;)tar.extractall()tar.close()如何通过 TarFile.extractall() 使用生成器函数而非列表来提取一个 tar 归档的子集:1234567891011import osimport tarfiledef py_files(members):    for tarinfo in members:        if os.path.splitext(tarinfo.name)[1] == &quot;.py&quot;:            yield tarinfotar = tarfile.open(&quot;sample.tar.gz&quot;)tar.extractall(members=py_files(tar))tar.close()如何基于一个文件名列表创建未压缩的 tar 归档:12345import tarfiletar = tarfile.open(&quot;sample.tar&quot;, &quot;w&quot;)for name in [&quot;foo&quot;, &quot;bar&quot;, &quot;quux&quot;]:    tar.add(name)tar.close()使用 with 语句的同一个示例:1234import tarfilewith tarfile.open(&quot;sample.tar&quot;, &quot;w&quot;) as tar:    for name in [&quot;foo&quot;, &quot;bar&quot;, &quot;quux&quot;]:        tar.add(name)如何读取一个 gzip 压缩的 tar 归档并显示一些成员信息:1234567891011import tarfiletar = tarfile.open(&quot;sample.tar.gz&quot;, &quot;r:gz&quot;)for tarinfo in tar:    print(tarinfo.name, &quot;is&quot;, tarinfo.size, &quot;bytes in size and is &quot;, end=&quot;&quot;)    if tarinfo.isreg():        print(&quot;a regular file.&quot;)    elif tarinfo.isdir():        print(&quot;a directory.&quot;)    else:        print(&quot;something else.&quot;)tar.close()如何创建一个归档并使用 TarFile.add() 中的 filter 形参来重置用户信息:12345678import tarfiledef reset(tarinfo):    tarinfo.uid = tarinfo.gid = 0    tarinfo.uname = tarinfo.gname = &quot;root&quot;    return tarinfotar = tarfile.open(&quot;sample.tar.gz&quot;, &quot;w:gz&quot;)tar.add(&quot;foo&quot;, filter=reset)tar.close()os库基本概念os库提供通用的、基本的操作系统交互功能，是Python标准库包含几百个函数，常用路径操作、进程管理、环境参数等几类路径操作：os.path子库，处理文件路径及信息进程管理：启动系统中其他程序环境参数：获得系统软硬件信息等环境参数path子库导入123from os.path import joinfrom IPython.core.interactiveshell import InteractiveShell # InteractiveShell.ast_node_interactivity = &quot;all&quot; # 可同时输出多个结果文件操作判断是否是文件  filepath最后没有/ 默认取//123456filepath = '../res/file_op/folder/''is file' if os.path.isfile(filepath) else 'is folder''is dir' if os.path.isdir(filepath) else 'not dir''is folder''is dir判断两个路径都指向相同的文件或目录12# 如果两个路径都指向相同的文件或目录，则返回 Trueos.path.samefile(path1, path2)增删文件[夹]创建文件夹1234# 2. 创建、删除文件与文件夹newfolder = '../res/file_op/mkfolder/'os.mkdir(newfolder) # 创建文件夹,不会递归创建不存在的父目录os.makedirs(newfolder) # 父目录不存在，则递归创建父目录删除文件夹123os.rmdir(newfolder) # 如果要删除目录，请使用rmdir().import shutilshutil.rmtree(path)    # 递归删除文件夹删除文件1234if os.path.exists(join(filepath,'file_0.txt')):    os.remove(join(filepath,'file_0.txt')) # 删除文件    # join('savepath', 'png/demo')   #正确    # join('savepath', '/png/demo')  #错误目录遍历python中os.walk的用法详解列出目录下的所有子目录12345path = '../res/file_op'for dirpath, dirnames, filenames in os.walk(path):    for dirname in dirnames:        print(os.path.join(dirpath, dirname))../res/file_op\folder列出目录下的所有文件1234567891011for dirpath, dirnames, filenames in os.walk(path):    for filename in filenames:        print(os.path.join(dirpath, filename))../res/file_op\file_ico.png../res/file_op\folder\file_1.txt  filepath = '../res/file_op'fileList = os.listdir(filepath) # 列出所有文件for fl in fileList:    print(fl)file_1.txt分割文件目录/文件名和后缀123456file_path = &quot;D:/test/test.py&quot;(filepath, tempfilename) = os.path.split(file_path)# 等价于 (filepath, tempfilename) = os.path.dirname(path),os.path.basename(path)==&gt; D:/test test.py(filename, extension) = os.path.splitext(tempfilename)==&gt; test .py目录操作修改目录名称1234os.rename('d0/d10/d100', 'D100')    # 会导致目录移动！os.path.abspath(path)                   # 获取一个路径的完全路径os.path.splitext(path)                  # 分离文件名和扩展名os.path.normpath( &quot;D://PYE//file.txt&quot; ) # 归一化path的表示形式，统一用\\分隔路径返回当前程序与文件之间的相对路径 (relative path)1234path = 'E:\\PycharmWS\\sj\\text-mining-framework'os.getcwd() # 'E:\\PycharmWS\\sj\\text-mining-framework\\tests\\others'In [3]: os.path.relpath(path)Out[3]: '..\\..'返回 path 的大小，以字节为单位，该文件不存在或不可访问时，抛出 OSError 异常1os.path.getsize( &quot;D:/PYE/file.txt&quot; )    # 返回path对应文件的大小，以字节为单位返回序列 paths 中每个路径名称的最长共同有效子路径12# 如果 paths 同时包含绝对和相对路径名称或者如果 paths 为空则会引发 ValueErroros.path.commonpath(paths)高阶文件操作复制和删除复制文件[夹]1234567import shutil# shutil.copy(srcfile, dstfile) : 复制srcfile文件，并命名为dstfileshutil.copy('d0/file1.txt', 'd0/file2.txt')# shutil.copy(srcfile, path) : 复制srcfile文件到path路径下shutil.copy('d0/file1.txt', '..')     # shutil.copytree(srcDir, dstDir) : 递归地复制srcDir目录，并命名为dstDir；dstDir不能是已经存在的！shutil.copytree('d0/', 'D0')移动文件[夹]123456# shutil.move(srcfile, dstfile) : 移动srcfile文件并命名为dstfile，相当于文件重命名shutil.move('d0/file1.txt', 'd0/FILE1.txt')# shutil.move(srcfile, path) ： 移动srcfile文件到path路径下shutil.move('d0/file2.txt', '.')# shutil.move(srcDir, dstDir) ： 移动srcDir目录(包括其子目录和文件)到dstDir目录下shutil.move('d0', 'D0')返回路径和最后一个文件名组成的元组1234# 返回路径和最后一个文件名组成的元组_, filename = os.path.split(os.getcwd())print(filename)Out[7]: 'python'磁盘使用统计磁盘使用统计数据，形式为一个 named tuple，其中包含 total, used 和 free 属性分别表示总计、已使用和未使用空间的字节数。在 Windows 上，path 必须是一个目录；在 Unix 上，它可以是一个文件或一个目录123import shutilshutil.disk_usage(path)Out[19]: usage(total=223217709056, used=82961846272, free=140255862784)os库之环境参数获取或改变系统环境信息123456# 修改当前程序操作的路径&gt;&gt;&gt;os.chdir(&quot;D:&quot;)# 返回程序的当前路径&gt;&gt;&gt;os.getcwd()'D:\\'获取操作系统环境信息1234567# 获得当前系统登录用户名称&gt;&gt;&gt;os.getlogin()'hyc'# 获得当前系统的CPU数量&gt;&gt;&gt;os.cpu_count()8获取操作系统环境信息123# 获得n个字节长度的随机字符串，通常用于加解密运算&gt;&gt;&gt;os.urandom(10)b'7\xbe\xf2!\xc1=\x01gL\xb3'其他使用os库之进程管理12345# 执行程序或命令command# 在Windows系统中，返回值为cmd的调用返回信息import osos.system(&quot;C:\\Windows\\System32\\calc.exe&quot;)os.system(&quot;C:\\Windows\\System32\\mspaint.exe D:\\PYECourse\\grwordcloud.png&quot;)查看剩余空间大小(ubuntu)1234567def checkDis(path):    import os    hd = {}    disk = os.statvfs(path)    percent = (disk.f_blocks - disk.f_bfree) * 100 / \                (disk.f_blocks - disk.f_bfree + disk.f_bavail) + 1    return 100 - percent</content>
	</entry>
	<entry>
		<title>Pythonic的几个办法</title>
		<url>/article/31834.html</url>
		<content>Pythonic的几个办法Python难点解析—-高级篇2.PythonicPython 实用冷门知识整理这些年来，Python 开发者用Pythonic这个形容词来描述那种符合特定风格的代码。这种Pythonice风格，既不是非常严密的规范，也不是由编译器强加给开发者的规则，而是大家在使用Python语言协同工作的过程中逐渐形成的习惯。确认python版本1python --version12345import sysprint(sys.version)3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]print(sys.version_info)sys.version_info(major=3, minor=7, micro=6, releaselevel='final', serial=0)有很多种流行的Python运行时环境，例如，CPython、 Jython、 IronPython 以及PyPy等。enumerate迭代enumerate可以把各种迭代器包装为生成器，以便稍后产生输出值。可以给enumerate提供第二个参数，以指定开始计数时所用的值(默认为0)。1234567891011121314151617181920212223&gt;&gt;&gt; numbers = [45, 22, 14, 65, 97, 72]&gt;&gt;&gt; for i, num in enumerate(numbers):...     if num % 3 == 0 and num % 5 == 0:...         numbers[i] = 'fizzbuzz'...     elif num % 3 == 0:...         numbers[i] = 'fizz'...     elif num % 5 == 0:...         numbers[i] = 'buzz'...&gt;&gt;&gt; numbers['fizzbuzz', 22, 14, 'buzz', 97, 'fizz']# 对于每个元素，enumerate()返回一个计数器和元素值。计数器默认为0，也是元素的索引。不想在0开始你的计数？只需使用可选的start参数来设置偏移量&gt;&gt;&gt; numbers = [45, 22, 14, 65, 97, 72]&gt;&gt;&gt; for i, num in enumerate(numbers, start=52):...     print(i, num)...52 4553 2254 1455 6556 9757 72列表递推式列表推导是构建列表（list）的快捷方式，而生成器表达式则可以用来创建其他任何类型的序列。通常的原则是，只用列表推导来创建新的列表，并且尽量保持简短。如果列表推导的代码超过了两行，你可能就要考虑是不是得用 for 循环重写了。在 Python 3 中都有了自己的局部作用域，就像函数似的。表达式内部的变量和赋值只在局部起作用，表达式的上下文里的同名变量还可以被正常引用，局部变量并不会影响到它们。列表推导可以帮助我们把一个序列或是其他可迭代类型中的元素过滤或是加工，然后再新建一个列表。列表推导的作用只有一个：生成列表。如果想生成其他类型的序列，生成器表达式就派上了用场。1234567891011121314151617181920&gt;&gt;&gt; numbers = [4, 2, 1, 6, 9, 7]&gt;&gt;&gt; def square(x):...     return x*x...&gt;&gt;&gt; list(map(square, numbers))[16, 4, 1, 36, 81, 49]&gt;&gt;&gt; [square(x) for x in numbers][16, 4, 1, 36, 81, 49]# 使用map()和列表推导的两种方法都返回相同的值，但列表推导更容易阅读和理解。&gt;&gt;&gt; def is_odd(x):...    return bool(x % 2)...&gt;&gt;&gt; list(filter(is_odd, numbers))[1, 9, 7]&gt;&gt;&gt; [x for x in numbers if is_odd(x)][1, 9, 7]# ，filter和列表推导方法返回相同的值，但列表推导更容易理解。12345# 列表推导也支持多个if条件。处在同-循环级别中的多项条件，彼此之间默认形成and表达式。# 例如，要从数字列表中选出大于4的偶数，那么下面这两种列表推导方式是等效的。a = [i for i in range(10)]b = [x for x in a if x&gt;4 if x%2==0]c = [x for x in a if x&gt;4 and x%2==0]生成器表达式生成器表达式背后遵守了迭代器协议，可以逐个地产出元素，而不是先建立一个完整的列表，然后再把这个列表传递到某个构造函数里。生成器表达式的语法跟列表推导差不多，只不过把方括号换成圆括号而已。如果生成器表达式是一个函数调用过程中的唯一参数，那么不需要额外再用括号把它围起来。生成器表达式就可以帮忙省掉运行 for 循环的开销123456789101112# 前面提到，列表推导是方便的工具，但有时会导致不必要的内存使用。# 错误方式&gt;&gt;&gt; sum([i * i for i in range(1, 1001)])333833500# 正确方式&gt;&gt;&gt; sum((i * i for i in range(1, 1001)))333833500# 换出括号会将列表推导更改为生成器表达式。当你知道要从序列中检索数据，但不需要同时访问所有数据的时候，生成器表达式非常适合。 # 生成器表达式返回生成器对象，而不是创建列表。该对象知道它在当前状态中的位置（例如，i = 49）并且仅在被要求时计算下一个值。 # 因此，当sum通过重复调用.__ next __()来迭代生成器对象时，生成器检查i等于多少，计算i * i，在内部递增i，并将正确的值返回到sum。该设计允许生成器用于大量数据序列，因为一次只有一个元素存在于内存中。列表展平一日一技：如何把多层嵌套的列表展平1234567891011def flat(deep_list, result):    for element in deep_list:        if isinstance(element, list):            flat(element, result)        else:            result.append(element)a = [1, 2, [3, 4, [5, 6, 7], 8], 9, [10, 11]]result = []flat(a, result)print(result)12345678910111213141516def list_flat(deep_list, ignore_types=(str, bytes)) -&gt; List:    &quot;&quot;&quot;    列表嵌套展平为一维列表    @param deep_list: 嵌套列表    @param ignore_types: 不做展平的类型    @rtype: 一维列表    &quot;&quot;&quot;    for element in deep_list:        if isinstance(element, list) and not isinstance(element, ignore_types):            yield from list_flat(element, ignore_types=ignore_types)        else:            yield elementa = [1, 2, [3, 4, [5, 6, 7], 8], 9, [10, 11]]result = [x for x in list_flat(a)]print(result)所以，当代码运行到1[x for x in flat(a)]的时候，每一次循环都会进入到 flat生成器里面。在 flat里面，对传入的参数使用for循环进行迭代，如果拿到的元素不是列表，那么就直接抛出，送到上一层。如果当前已经是最上层了，那么就再一次抛出给外面的列表推导式。如果当前元素是列表，那么继续生成一个生成器，并对这个新的生成器进行迭代，并把每一个结果继续往上层抛出。最终，每一个数字都会被一层一层往上抛出给列表推导式，从而获得需要的结果。字典展平1234567891011nest_dict = {    'a': 1,    'b': {        'c': 2,        'd': 3,        'e': {'f': 4}    },    'g': {'h': 5},    'i': 6,    'j': {'k': {'l': {'m': 8}}}}使用yield关键字来实现这个需求，在不炫技的情况下，只需要8行代码。在炫技的情况下，只需要3行代码。要快速地把这个嵌套字典压扁，我们需要从下向上来处理字段。例如对于b-&gt;e-&gt;f-&gt;4这条路径，我们首先把最里面的{'f': 4}转换为一个元组('f', 4)。然后，把这个元组向上抛出，于是得到了元组('e', ('f', 4))。我们把 e拼接到f的前面，变为：('e_f', 4)，继续往上抛出，得到('b', ('e_f', 4))。再把b拼接到e_f上面，得到('b_e_f', 4)。完成一条线路的组装。这个逻辑如果使用yield关键字来实现，就是：12345678910111213def map_flat(deep_map, full_key: bool = True) -&gt; Dict:    &quot;&quot;&quot;    字典嵌套展平为一维字典    @param deep_map: 嵌套字典    @param full_key: 是否使用完整的key    @rtype: 一维字典    &quot;&quot;&quot;    for key, value in deep_map.items():        if isinstance(value, dict):            for k, v in map_flat(value, full_key):                yield (f'{key}_{k}', v) if full_key else (k, v)        else:            yield key, value12345678910{k:v for k,v in map_flat(nest_dict)}{    'a': 1,    'b_c': 2,    'b_d': 3,    'b_e_f': 4,    'g_h': 5,    'i': 6,    'j_k_l_m': 8}通过使用 yield关键字，字典的key会像是在流水线上一样，一层一层从内向外进行组装，从而形成完整的路径。找list的最值索引12345def max_idx(lst):    return max(range(len(lst)), key=lst.__getitem__)max_idx([1,5,7,2,3])Out[62]: 2list去重并保留顺序123456789from collections import OrderedDicta = ['heelo','world','world','a','hello']list(OrderedDict.fromkeys(a).keys())Out[65]: ['heelo', 'world', 'a', 'hello']a = ['hello','world','world','a','hello']list(OrderedDict.fromkeys(a).keys())Out[67]: ['hello', 'world', 'a']代码里面调用 pip说到安装 Python 的第三方库，会 Python 的同学都知道，在终端使用pip install xxx即可。那么如果我想在代码里面安装第三方库怎么办呢？可能有人想到使用 os 模块：123import ospackage_name = 'requests'os.system(f'pip install {package_name}')这种方法确实可行，并且即使你在虚拟环境中使用这种方式安装，也确实不会安装到系统的 Python 环境中。但是这种方式总感觉有点奇怪。而且如果这个package_name字符串经过精心构造，可以执行任意系统命令，例如：123import ospackage_name = 'requests &amp;&amp; rm -rf *'os.system(f'pip install {package_name}')为了防止这种情况发生，我们可以直接调用pip这个 Python 包：12from pip._internal import mainmain.main(['install', '第三方库名'])命令行下面的参数都可以通过转换为列表的形式执行，例如：123from pip._internal import mainmain.main(['install', '-r', 'requirements.txt'])使用f-Strings格式化字符串123456# f-strings支持使用字符串格式化迷你语言，以及强大的字符串插值。这些功能允许你添加变量甚至有效的Python表达式，并在添加到字符串之前在运行时对它们进行评估：&gt;&gt;&gt; def get_name_and_decades(name, age):...     return f&quot;My name is {name} and I'm {age / 10:.5f} decades old.&quot;...&gt;&gt;&gt; get_name_and_decades(&quot;Maria&quot;, 31)My name is Maria and I'm 3.10000 decades old.集合论集合的本质是许多唯一对象的聚集。因此，集合可以用于去重集合中的元素必须是可散列的，set 类型本身是不可散列的，但是 frozenset 可以。因此可以创建一个包含不同 frozenset 的 set。给定两个集合 a 和 b，a | b 返回的是它们的合集，a &amp; b 得到的是交集，而 a - b 得到的是差集。除空集之外，集合的字面量——{1}、{1, 2}，等等——看起来跟它的数学形式一模一样。如果是空集，那么必须写成 set() 的形式。在 Python 3 里面，除了空集，集合的字符串表示形式总是以 {...} 的形式出现。像 {1, 2, 3} 这种字面量句法相比于构造方法（set([1, 2, 3])）要更快且更易读。后者的速度要慢一些，因为 Python 必须先从 set 这个名字来查询构造方法，然后新建一个列表，最后再把这个列表传入到构造方法里。但是如果是像 {1, 2, 3} 这样的字面量，Python 会利用一个专门的叫作 BUILD_SET 的字节码来创建集合。数学符号python运算符方法描述set的实现以及导致的结果set 和 frozenset 的实现也依赖散列表，但在它们的散列表里存放的只有元素的引用（就像在字典里只存放键而没有相应的值）。在 set 加入到 Python 之前，我们都是把字典加上无意义的值当作集合来用的。这些特点总结如下。集合里的元素必须是可散列的。集合很消耗内存。可以很高效地判断元素是否存在于某个集合。元素的次序取决于被添加到集合里的次序。往集合里添加元素，可能会改变集合里已有元素的次序。1234567891011&gt;&gt;&gt; import random&gt;&gt;&gt; all_words = &quot;all the words in the world&quot;.split()&gt;&gt;&gt; def get_random_word():...    return random.choice(all_words)&gt;&gt;&gt; def get_unique_words():...     words = set()...     for _ in range(1000):...         words.add(get_random_word())...     return words&gt;&gt;&gt; get_unique_words(){'world', 'all', 'the', 'words'}使用字符串常量访问公共字符串组12345678910111213141516171819202122232425# 可以使用is_upper()，它返回字符串中的所有字符是否都是大写字母：&gt;&gt;&gt; import string&gt;&gt;&gt; def is_upper(word):...     for letter in word:...         if letter not in string.ascii_uppercase:...             return False...     return True...&gt;&gt;&gt; is_upper('Thanks Geir')False&gt;&gt;&gt; is_upper('LOL')True# is_upper()迭代word中的字母，并检查字母是否为string.ascii_大写字母的一部分。如果你打印出string.ascii_大写，你会发现它只是一个字符串，该值设置为文本“ABCDEFGHIJKLMNOPQRSTUVWXYZ”。所有字符串常量都只是经常引用的字符串值的字符串。其中包括以下内容：string.ascii_lettersstring.ascii_uppercasestring.ascii_lowercasestring.digitsstring.hexdigitsstring.octdigitsstring.punctuationstring.printablestring.whitespace使用Itertools生成排列和组合12345678910111213# itertools.permutations()构建所有排列的列表，这意味着它是输入值的每个可能分组的列表，其长度与count参数匹配。r关键字参数允许我们指定每个分组中有多少值：&gt;&gt;&gt; import itertools&gt;&gt;&gt; friends = ['Monique', 'Ashish', 'Devon', 'Bernie']&gt;&gt;&gt; list(itertools.permutations(friends, r=2))[('Monique', 'Ashish'), ('Monique', 'Devon'), ('Monique', 'Bernie'),('Ashish', 'Monique'), ('Ashish', 'Devon'), ('Ashish', 'Bernie'),('Devon', 'Monique'), ('Devon', 'Ashish'), ('Devon', 'Bernie'),('Bernie', 'Monique'), ('Bernie', 'Ashish'), ('Bernie', 'Devon')]# itertools.combinations()生成组合。这些也是输入值的可能分组，但现在值的顺序无关紧要。&gt;&gt;&gt; list(itertools.combinations(friends, r=2))[('Monique', 'Ashish'), ('Monique', 'Devon'), ('Monique', 'Bernie'),('Ashish', 'Devon'), ('Ashish', 'Bernie'), ('Devon', 'Bernie')]漂亮的打印出JSON123456789101112131415161718192021&gt;&gt;&gt; import json&gt;&gt;&gt; print(json.dumps(data))  # No indention{&quot;status&quot;: &quot;OK&quot;, &quot;count&quot;: 2, &quot;results&quot;: [{&quot;age&quot;: 27, &quot;name&quot;: &quot;Oz&quot;, &quot;lactose_intolerant&quot;: true}, {&quot;age&quot;: 29, &quot;name&quot;: &quot;Joe&quot;, &quot;lactose_intolerant&quot;: false}]}&gt;&gt;&gt; print(json.dumps(data, indent=2))  # With indention{  &quot;status&quot;: &quot;OK&quot;,  &quot;count&quot;: 2,  &quot;results&quot;: [    {      &quot;age&quot;: 27,      &quot;name&quot;: &quot;Oz&quot;,      &quot;lactose_intolerant&quot;: true    },    {      &quot;age&quot;: 29,      &quot;name&quot;: &quot;Joe&quot;,      &quot;lactose_intolerant&quot;: false    }  ]}with上下文管理with模块with 语句会设置一个临时的上下文，交给上下文管理器对象控制，并且负责清理上下文。这么做能避免错误并减少样板代码，因此 API 更安全，而且更易于使用。上下文管理器对象存在的目的是管理 with 语句，就像迭代器的存在是为了管理 for 语句一样。with 语句的目的是简化 try/finally 模式。finally 子句中的代码通常用于释放重要的资源，或者还原临时变更的状态。上下文管理器协议包含 __enter__ 和 __exit__ 两个方法。with 语句开始运行时，会在上下文管理器对象上调用__enter__ 方法。with 语句运行结束后，会在上下文管理器对象上调用__exit__方法，以此扮演 finally 子句的角色。with的行为执行 with 后面的表达式得到的结果是上下文管理器对象，不过，把值绑定到目标变量上（as 子句）是在上下文管理器对象上调用 __enter__ 方法的结果。__enter__ 方法除了返回上下文管理器之外，还可能返回其他对象。不管控制流程以哪种方式退出 with 块，都会在上下文管理器对象上调用 __exit__ 方法，而不是在 __enter__ 方法返回的对象上调用。with 语句的 as 子句是可选的。对 open 函数来说，必须加上 as 子句，以便获取文件的引用。如果 `__exit__` 方法返回 `None`，或者 `True` 之外的值，`with` 块中的任何异常都会向上冒泡。解释器调用 __enter__ 方法时，除了隐式的 self 之外，不会传入任何参数。传给 __exit__ 方法的三个参数列举如下。1exc_type　　异常类（例如 ZeroDivisionError）。1exc_value　　异常实例。有时会有参数传给异常构造方法，例如错误消息，这些参数可以使用 exc_value.args 获取。1traceback　　traceback 对象。可以手动调用 `__enter__` 和 `__exit__` 方法。with 不仅能管理资源，还能用于去掉常规的设置和清理代码，或者在另一个过程前后执行的操作123456with allocate_resource() as resource:    resource.use()# 打开/关闭文件with open('data.txt') as f:    data = f.read()上下文装饰器contextlib 模块中的实用工具closing: 如果对象提供了 close() 方法，但没有实现 __enter__/__exit__ 协议，那么可以使用这个函数构建上下文管理器。suppress: 构建临时忽略指定异常的上下文管理器。@contextmanager: 这个装饰器把简单的生成器函数变成上下文管理器，这样就不用创建类去实现管理器协议了。ContextDecorator: 这是个基类，用于定义基于类的上下文管理器。这种上下文管理器也能用于装饰函数，在受管理的上下文中运行整个函数。ExitStack: 这个上下文管理器能进入多个上下文管理器。with 块结束时，ExitStack 按照后进先出的顺序调用栈中各个上下文管理器的 __exit__ 方法。如果事先不知道 with 块要进入多少个上下文管理器，可以使用这个类。例如，同时打开任意一个文件列表中的所有文件。显然，在这些实用工具中，使用最广泛的是 @contextmanager 装饰器，因此要格外留心。这个装饰器也有迷惑人的一面，因为它与迭代无关，却要使用 yield 语句。重点介绍下**@contextmanager**@contextmanager 装饰器能减少创建上下文管理器的样板代码量，因为不用编写一个完整的类，定义 __enter__ 和 __exit__ 方法，而只需实现有一个 yield 语句的生成器，生成想让 __enter__ 方法返回的值。在使用 @contextmanager 装饰的生成器中，yield 语句的作用是把函数的定义体分成两部分：yield 语句前面的所有代码在 with 块开始时（即解释器调用 __enter__ 方法时）执行， yield 语句后面的代码在 with 块结束时（即调用 __exit__ 方法时）执行。123456789101112import contextlib@contextlib.contextmanager  ➊def looking_glass():    import sys    original_write = sys.stdout.write  ➋    def reverse_write(text):  ➌        original_write(text[::-1])    sys.stdout.write = reverse_write  ➍    yield 'JABBERWOCKY'  ➎    sys.stdout.write = original_write  ➏ # 这里相当于__exit__这个类的 __enter__ 方法有如下作用。(1) 调用生成器函数，保存生成器对象（这里把它称为 gen）。(2) 调用 next(gen)，执行到 yield 关键字所在的位置。(3) 返回 next(gen) 产出的值，以便把产出的值绑定到 with/as 语句中的目标变量上。with 块终止时，`__exit__` 方法会做以下几件事：检查有没有把异常传给 exc_type；如果有，调用 gen.throw(exception)，在生成器函数定义体中包含 yield 关键字的那一行抛出异常。否则，调用 next(gen)，继续执行生成器函数定义体中 yield 语句之后的代码。上文示例有一个严重的错误：如果在 with 块中抛出了异常，Python 解释器会将其捕获，然后在 looking_glass 函数的 yield 表达式里再次抛出。但是，那里没有处理错误的代码，因此 looking_glass 函数会中止，永远无法恢复成原来的 sys.stdout.write 方法，导致系统处于无效状态。以下代码是基于生成器的上下文管理器，而且实现了异常处理——从外部看，行为与前文一样1234567891011121314151617181920import contextlib@contextlib.contextmanagerdef looking_glass():    import sys    original_write = sys.stdout.write    def reverse_write(text):        original_write(text[::-1])    sys.stdout.write = reverse_write    msg = ''  ➊    try:        yield 'JABBERWOCKY'    except ZeroDivisionError:  ➋        msg = 'Please DO NOT divide by zero!'    finally:        sys.stdout.write = original_write  ➌        if msg:            print(msg)  ➍前面说过，为了告诉解释器异常已经处理了，__exit__ 方法会返回 True，此时解释器会压制异常。如果 __exit__ 方法没有显式返回一个值，那么解释器得到的是 None，然后向上冒泡异常。使用 @contextmanager 装饰器时，默认的行为是相反的：装饰器提供的__exit__ 方法假定发给生成器的所有异常都得到处理了，因此应该压制异常。6 如果不想让 @contextmanager 压制异常，必须在被装饰的函数中显式重新抛出异常。在 @contextmanager 装饰器装饰的生成器中，yield 与迭代没有任何关系。@contextmanager 装饰器能把包含一个 yield 语句的简单生成器变成上下文管理器——这比定义一个至少包含两个方法的类要更简洁。for/else块`else` 子句不仅能在 if 语句中使用，还能在 for、while 和 try 语句中使用。`for/else`、`while/else` 和 `try/else` 的语义关系紧密，不过与 `if/else` 差别很大。在循环中，else 的语义恰好相反：“运行这个循环，然后做那件事。”else 子句的行为如下。for: 仅当 for 循环运行完毕时（即 for 循环没有被 break 语句中止）才运行 else 块。while: 仅当 while 循环因为条件为假值而退出时（即 while 循环没有被 break 语句中止）才运行 else 块。try: 仅当 try 块中没有异常抛出时才运行 else 块。官方文档还指出：“else 子句抛出的异常不会由前面的 except 子句处理。”在所有情况下，如果异常或者 return、break 或 continue 语句导致控制权跳到了复合语句的主块之外，else 子句也会被跳过。在这些语句中使用 else 子句通常能让代码更易于阅读，而且能省去一些麻烦，不用设置控制标志或者添加额外的 if 语句。12345678# for else 是 Python 中特有的语法格式，else 中的代码在 for 循环遍历完所有元素之后执行# 如果for循环正常结束，else中语句执行。如果是break的，则不执行。for i in mylist:    if i == theflag:break    process(i)else:    raise ValueError(&quot;List argument missing terminal flag.&quot;)12345678# 方式一flag = Falsefor x in xx:if some condition:flag = Truebreakif flag:print 'no break'123456# 方式二for x in xx:if some condition:breakelse:print 'no break'1234567891011121314151617181920212223242526# 判断质数/素数——我知道的最快的方法# https://blog.csdn.net/songyunli1111/article/details/78690447# Exampledef is_prime(n = 20):    is_p = False    for ii in range(2 if n&gt;=2 else 1, int(math.sqrt(n))+1):        if n%ii == 0:            break    else:        is_p = True    return is_p# 或者直接写成def is_prime(n = 20):    for ii in range(2 if n&gt;=2 else 1, int(math.sqrt(n))+1):        if n%ii == 0:            return False    else:        return True# 获取n=50以内的素数列表import mathn = 50data = [ii for ii in range(n)]res = itertools.compress(data,[is_prime(da) for da in data])print([r for r in res])Out[30]: [0, 2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]合理使用列表123456from collections import dequenames = deque(['raymond', 'rachel', 'matthew', 'roger',               'betty', 'melissa', 'judith', 'charlie'])names.popleft()               Out[10]: 'raymond'列表对象（list）是一个查询效率高于更新操作的数据结构，删除和插入需要对剩下的元素做移动操作deque 是一个双向队列的数据结构，删除元素和插入元素会很快序列解包元组拆包可以应用到任何可迭代对象上，唯一的硬性要求是，被可迭代对象中的元素数量必须要跟接受这些元素的元组的空档数一致。除非我们用 * 来表示忽略多余的元素。os.path.split() 函数就会返回以路径和最后一个文件名组成的元组 (path, last_part)在平行赋值中，* 前缀只能用在一个变量名前面，但是这个变量可以出现在赋值表达式的任意位置123456789101112p = 'vttalk', 'female', 30, 'python@qq.com'name, gender, age, email = pname, gender, age, emailOut[13]: ('vttalk', 'female', 30, 'python@qq.com')    num_list = [100, 19, 20, 98]first, *left_num_list, last = num_listprint(first, left_num_list, last)Out[14]: 100 [19, 20] 98    string = 'xuexiao 4 fuzhou daxue'tag,start_index,value = string.split(' ',2)链式比较操作12if 18 &lt; age &lt; 60:    print(&quot;yong man&quot;)assert用法1234&gt;&gt;&gt; assert mul(2, 3) == 7, 'This statement is wrong!!!!!!'Traceback (most recent call last):  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;AssertionError: This statement is wrong!!!!!!slots优化内存使用slots使用了100M内存，比使用dict存储属性值节省了2倍。其实使用collection模块的namedtuple也可以实现slots相同的功能。namedtuple其实就是继承自tuple，同时也因为slots的值被设置成了一个空tuple以避免创建dictcollection 和普通创建类方式相比，也节省了不少的内存。所在在确定类的属性值固定的情况下，可以使用slots方式对内存进行优化。但是这项技术不应该被滥用于静态类或者其他类似场合，那不是python程序的精神所在。12345678910111213141516171819# 未使用__slots__# -*- coding: utf-8 -*from memory_profiler import profileclass Foobar(object):    # __slots__ = ('x')    def __init__(self, x):        self.x = x@profiledef main():    f = [Foobar(42) for i in range(1000000)]if __name__ == &quot;__main__&quot;:    main()    Line #    Mem usage    Increment   Line Contents================================================   137     45.7 MiB     45.7 MiB   @profile   138                             def main():   139    215.9 MiB      0.9 MiB       f = [Foobar(42) for i in range(1000000)]12345678910111213141516171819# 使用__slots__# -*- coding: utf-8 -*from memory_profiler import profileclass Foobar(object):    __slots__ = ('x')    def __init__(self, x):        self.x = x@profiledef main():    f = [Foobar(42) for i in range(1000000)]if __name__ == &quot;__main__&quot;:    main()Line #    Mem usage    Increment   Line Contents================================================   132     45.7 MiB     45.7 MiB   @profile   133                             def main():   134     99.8 MiB      0.4 MiB       f = [Foobar(42) for i in range(1000000)]12# 使用__slots__要注意，__slots__定义的属性仅对当前类起作用，对继承的子类是不起作用的# 除非在子类中也定义__slots__，这样，子类允许定义的属性就是自身的__slots__加上父类的__slots__。暂留</content>
	</entry>
	<entry>
		<title>python网络编程</title>
		<url>/article/55054.html</url>
		<content>网络编程socket和urllib的关系提供对多种不同类型套接字的低级访问，您可以使用这些套接字通过任何端口和协议进行通信。例如，您可以将其用于电子邮件、SSH、远程桌面等，也可以用于侦听端口（对于服务器）。几乎所有Python网络库，包括urllib，都以某种方式使用socket。在urllib专门用于套接字的特定用途，即HTTP（和可选的TLS）和FTP协议的客户端，通常（但不总是）使用端口80、443或21。Python urllib、urllib2、urllib3用法及区别urllib、urllib2是老版本，urllib3是新版本，requests是基于urllib3写的。其中urllib和urllib2是内置库模块urllib和urllib2的功能差不多，简单来说urllib2是urllib的增强——urllib2更好一些，但是urllib中有urllib2中所没有的函数。对于简单的下载， urllib绰绰有余。如果需要实现HTTP身份验证或Cookie亦或编写扩展来处理自己的协议，urllib2可能是更好的选择。在Python2.x中主要为urllib和urllib2，这两个标准库是不可相互替代的。但是在Python3.x中将urllib2合并到了urllib，这一点值得注意。urllib支持设置编码的函数urllib.urlencode，在模拟登陆的时候经常需要传递经过post编码之后的参数，如果不想使用第三方库完成模拟登录，就必须使用到标准库中的urllib。urllib提供一些比较原始基础的方法而urllib2并没有，比如urllib中的urlencode方法用来GET查询字符串的产生。urllib2比较有优势的地方在于urllib2.openurl中可以接受一个Request类的实例来设置Request参数，来修改/设置Header头从而达到控制HTTP Request的header部分的目的，也可以修改用户代理,设置cookie等，但urllib仅可以接受URL。如果你访问一个网站想更改User Agent(可以伪装你的浏览器)，你就需要使用urllib2。urllib2模块没有加入urllib.urlretrieve函数以及urllib.quote等一系列quote和unquote功能，这个时候就需要urllib的辅助。socket网络7层协议，4层，5层？理清容易混淆的几个概念爬虫遇到 Socket，莫慌，肝就完了！Python3中的SocketServersocket是应用层与TCP/IP协议族通信的中间软件抽象层，它是一组接口。在设计模式中，Socket其实就是一个门面模式，它把复杂的TCP/IP协议族隐藏在Socket接口后面，对用户来说，一组简单的接口就是全部，让Socket去组织数据，以符合指定的协议。Socket 被称为套接字，是对 TCP/IP 协议的封装，它是传输层和应用层间的抽象层相比 HTTP 的短连接通信方式，Socket 可实现客户端和服务器的长连接通信Fiddler、Charles 只能抓取应用层的数据，如果你想抓其他层，比如：网络层、传输层、数据链路层的数据，强烈建议使用：Wireshark在标准的OIS模型中并没有规定说必须有socket层，也就是说不使用socket也能完成通讯，是的，的确如此那为什么需要socket呢？一个字懒，程序员都是懒的我们发现还没有开始实现应用程序逻辑，就需要花大把时间来实现各种协议，太费事了，就有人专门把协议中一堆复杂的事情进行了封装，于是socket就诞生了有了socket以后，无需自己编写代码实现三次握手，四次挥手，ARP请求，打包数据等等，socket已经封装好了，只需要遵循socket的规定去编程，写出的程序自然就是遵循tcp/udp标准的socket的发展套接字起源于20世纪70年代加利福尼亚大学伯克利分校版本的 Unix，即人们所说的 BSD Unix因此，有时人们也把套接字称为“伯克利套接字”或“BSD 套接字”一开始,套接字被设计用在同 一台主机上多个应用程序之间的通讯，这也被称进程间通讯或 IPC套接字有两种(或者称为有两个种族)，分别是基于文件型的和基于网络型的基于文件类型的套接字家族：AF_UNIXunix一切皆文件，基于文件的套接字调用的就是底层的文件系统来取数据，两个套接字进程运行在同一机器，可以通过访问同一个文件系统间接完成通信基于网络类型的套接字家族：AF_INET还有AF_INET6被用于ipv6，还有一些其他的地址家族，不过，他们要么是只用于某个平台，要么就是已经被废弃，或者是很少被使用，或者是根本没有实现，所有地址家族中，AF_INET是使用最广泛的一个，python支持很多种地址家族，但是由于大部通讯都是网络通讯，所以大部分时候使用AF_INETsocket示例123456789101112131415# 1.导入socket模块import socket# 2.创建socket对象 函数定义如下socket.socket(socket_family,socket_type,protocal=0)#socket_family 可以是 AF_UNIX 或 AF_INET。#socket_type 可以是 SOCK_STREAM表示TCP协议 或 SOCK_DGRAM表示UDP协议。#protocol 一般不填,默认值为 0。   # 2.1获取TCP 套接字tcpSock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)# 或者 后面的参数都有默认值,可以不写,默认创建的是TCP协议sockettcpSock = socket.socket()# 2.2获取udp/ip套接字udpSock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)服务端套接字函数服务端套接字函数含义s.bind()绑定(主机,端口号)到套接字s.listen()开始TCP监听s.accept()被动接受TCP客户的连接,(阻塞式)等待连接的到来客户端套接字函数客户端套接字函数含义s.connect()主动初始化TCP服务器连接s.connect_ex()connect()函数的扩展版本,出错时返回出错码,而不是抛出异常公共用途的套接字函数公共用途的套接字函数含义s.recv()接收TCP数据s.send()发送TCP数据(send在待发送数据量大于己端缓存区剩余空间时,数据丢失,不会发完)s.sendall()发送完整的TCP数据(本质就是循环调用sendsendall在待发送数据量大于己端缓存区剩余空间时，数据不丢失，循环调用send直到发完)s.recvfrom()接收UDP数据s.sendto()发送UDP数据s.getpeername()连接到当前套接字的远端的地址s.getsockname()当前套接字的地址s.getsockopt()返回指定套接字的参数s.setsockopt()设置指定套接字的参数s.close()关闭套接字面向锁的套接字方法面向锁的套接字方法含义s.setblocking()设置套接字的阻塞与非阻塞模式s.settimeout()设置阻塞套接字操作的超时时间s.gettimeout()得到阻塞套接字操作的超时时间TCP通讯流程TCP的通讯流程与打电话的过程非常相似买手机 == socket()装进手机卡 == bind()待机 == listen()电话来了、接受通话 == accept()听 == read()说 == write()挂电话 ==  close()TCP服务端1234567891011121314import socketip_port = ('127.0.0.1', 9000)  # 电话卡BUFSIZE = 1024  # 收发消息的尺寸s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)  # 买手机s.bind(ip_port)  # 手机插卡s.listen(5)  # 手机待机conn, addr = s.accept()  # 手机接电话print('接到来自%s的电话' % addr[0])msg = conn.recv(BUFSIZE)  # 听消息,听话print(msg, type(msg))conn.send(msg.upper())  # 发消息,说话conn.close()  # 挂电话s.close()  # 手机关机TCP客户端12345678910import socketip_port = ('127.0.0.1', 9000)BUFSIZE = 1024s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)s.connect_ex(ip_port)  # 拨电话s.send('linhaifeng nb'.encode('utf-8'))  # 发消息,说话(只能发送字节类型)feedback = s.recv(BUFSIZE)  # 收消息,听话print(feedback.decode('utf-8'))s.close()  # 挂电话注意TCP中必须先启动服务器再启动客户端,否则客户端由于无法链接服务器,直接报错!如上就完成了一个最基本的TCP通讯，但是建立是为了传输数据，二传输数据很多时候并不是一次性就传输完成了，需要多次收发过程，所以需要给代码加上循环改进版服务器端1234567891011121314151617import socketip_port=('127.0.0.1',8081)#电话卡BUFSIZE=1024s=socket.socket(socket.AF_INET,socket.SOCK_STREAM) #买手机s.bind(ip_port) #手机插卡s.listen(5)     #手机待机while True:                         #新增接收链接循环,可以不停的接电话    conn,addr=s.accept()            #手机接电话    # print(conn)    # print(addr)    print('接到来自%s的电话' �dr[0])    while True:                         #新增通信循环,可以不断的通信,收发消息        msg=conn.recv(BUFSIZE)             #听消息,听话        print(msg,type(msg))        conn.send(msg.upper())          #发消息,说话    conn.close()                    #挂电话s.close()                       #手机关机改进版客户端12345678910111213import socketip_port=('127.0.0.1',8081)BUFSIZE=1024s=socket.socket(socket.AF_INET,socket.SOCK_STREAM)s.connect_ex(ip_port)           #拨电话while True:                             #新增通信循环,客户端可以不断发收消息    msg=input('&gt;&gt;: ').strip()    if len(msg) == 0:continue    s.send(msg.encode('utf-8'))         #发消息,说话(只能发送字节类型)       feedback=s.recv(BUFSIZE)                           #收消息,听话    print(feedback.decode('utf-8'))s.close()                                       #挂电话基于UDP的socketUDP通讯流程与对讲机非常类似(由于不需要建立连接所以省去TCP的listen()和accept()这两步)买传呼机 ==  socket()固定对讲频道 == bind()收信号 == recvfrom()发信号 == sendto()UDP服务器端1234567891011import socketip_port=('127.0.0.1',9000) # 固定通讯频道BUFSIZE=1024#在TCP中socket的初始化参数可以省略, 因为默认创建的就是TCP协议的socket#而UDP则必须手动指定相关参数udp_server_client=socket.socket(socket.AF_INET,socket.SOCK_DGRAM) # 买对讲机udp_server_client.bind(ip_port)while True:    msg,addr=udp_server_client.recvfrom(BUFSIZE) #收信息    print(msg,addr)    udp_server_client.sendto(msg.upper(),addr) # 发信息UDP客户端123456789import socketip_port=('127.0.0.1',9000) #确定通讯频道BUFSIZE=1024udp_server_client=socket.socket(socket.AF_INET,socket.SOCK_DGRAM) # 买对讲机while True:    msg=input('&gt;&gt;: ').strip()    if not msg:continue    udp_server_client.sendto(msg.encode('utf-8'),ip_port) # 发消息    back_msg,addr=udp_server_client.recvfrom(BUFSIZE) #收消息udp是无链接的，先启动哪一端都不会报错，即使对方地址根本不存在也不会报错，强制关闭任何一方也没有任何问另外，由于无连接的特点，服务器不需要针对摸个客户端进行循环，只要循环的接收即可，谁发来的消息都可以被处理，基于这个特点我们可以编写一个UDP程序，实现多个客户端同时与服务器通讯UDP聊天服务器123456789import socketip_port=('127.0.0.1',8081)udp_server_sock=socket.socket(socket.AF_INET,socket.SOCK_DGRAM) #买手机udp_server_sock.bind(ip_port)while True:    qq_msg,addr=udp_server_sock.recvfrom(1024)    print('来自[%s:%s]的一条消息:\033[1;44m%s\033[0m' %(addr[0],addr[1],qq_msg.decode('utf-8')))    back_msg=input('回复消息: ').strip()    udp_server_sock.sendto(back_msg.encode('utf-8'),addr)UDP聊天客户端1234567891011121314151617181920import socketBUFSIZE = 1024udp_client_socket = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)qq_name_dic = {    '狗哥': ('127.0.0.1', 8081),    '天线宝宝': ('127.0.0.1', 8081),    '巴拉巴拉小魔女': ('127.0.0.1', 8081),    '王尼玛': ('127.0.0.1', 8081),}while True:    qq_name = input('请选择聊天对象: ').strip()    while True:        msg = input('请输入消息,回车发送: ').strip()        if msg == 'quit': break        if not msg or not qq_name or qq_name not in qq_name_dic: continue        udp_client_socket.sendto(msg.encode('utf-8'), qq_name_dic[qq_name])        back_msg, addr = udp_client_socket.recvfrom(BUFSIZE)        print('来自[%s:%s]的一条消息:\033[1;44m%s\033[0m' % (addr[0], addr[1], back_msg.decode('utf-8')))udp_client_socket.close()粘包问题粘包指的是数据与数据之间没有明确的分界线，导致不能正确读取数据！要理解粘包问题，需要先了解TCP协议传输数据时的具体流程，TCP协议也称之为流式协议(UDP称为数据报协议)应用程序无法直接操作硬件，应用程序想要发送数据则必须将数据交给操作系统，而操作系统需要需要同时为所有应用程序提供数据传输服务，也就意味着，操作系统不可能立马就能将应用程序的数据发送出去，就需要为应用程序提供一个缓冲区，用于临时存放数据，具体流程如下：发送方：当应用程序调用send函数时，应用程序会将数据从应用程序拷贝到操作系统缓存，再由操作系统从缓冲区读取数据并发送出去接收方：对方计算机收到数据也是操作系统先收到，至于应用程序何时处理这些数据，操作系统并不清楚，所以同样需要将数据先存储到操作系统的缓冲区中，当应用程序调用recv时，实际上是从操作系统缓冲区中将数据拷贝到应用程序的过程上述过程对于TCP与UDP都是相同的不同之处在于：UDP: UDP在收发数据时是基于数据包的，即一个包一个包的发送，包与包之间有着明确的分界，到达对方操作系统缓冲区后也是一个一个独立的数据包，接收方从操作系统缓冲区中将数据包拷贝到应用程序这种方式存在的问题：发送方发送的数据长度每个操作系统会有不同的限制，数据超过限制则无法发送接收方接收数据时如果应用程序的提供的缓存容量小于数据包的长度将造成数据丢失，而缓冲区大小不可能无限大TCP: 当我们需要传输较大的数据，或需要保证数据完整性时，最简单的方式就是使用TCP协议了与UDP不同的是，TCP增加了一套校验规则来保证数据的完整性，会将超过TCP包最大长度的数据拆分为多个TCP包 并在传输数据时为每一个TCP数据包指定一个顺序号，接收方在收到TCP数据包后按照顺序将数据包进行重组，重组后的数据全都是二进制数据，且每次收到的二进制数据之间没有明显的分界基于这种工作机制TCP在三种情况下会发送粘包问题当单个数据包较小时接收方可能一次性读取了多个包的数据当整体数据较大时接收方可能一次仅读取了一个包的一部分内容另外TCP协议为了提高效率，增加了一种优化机制，会将数据较小且发送间隔较短的数据合并发送，该机制也会导致发送方将两个数据包粘在一起发送粘包的解决方案解决方案：在发送数据前先发送数据长度上述方案看起来解决了粘包问题，但是由于negle优化机制的存在，长度信息和数据还是有可能会粘包，而接受方并不知道长度信息具体几个字节，所以现在的问题是如何能够长度信息做成一个固定长度的bytes数据我们可以将字符串拼接为一个固定长度的字符 但是这样太麻烦，struct模块为我们提供了一个功能，可以将整数类型转换为固定长度的bytes，此时就派上用场了自定义报头解决粘包上述方案已经完美解决了粘包问题，但是扩展性不高，例如我们要实现文件上传下载，不光要传输文件数据，还需要传输文件名字，md5值等等，如何能实现呢，具体思路：发送端：先将所有的额外信息打包到一个头中然后先发送头部数据最后发送真实数据接收端：接收固定长度的头部长度数据根据长度数据获取头部数据根据头部数据获取真实数据客户端：12345678910111213141516171819202122232425import socketimport structimport jsonc = socket.socket()c.connect((&quot;127.0.0.1&quot;,8888))while True:    cmd = input(&quot;&gt;&gt;&gt;:&quot;).strip()    c.send(cmd.encode(&quot;utf-8&quot;))     # 头部数据    data = c.recv(4)    head_length = struct.unpack(&quot;i&quot;,data)[0]    head_data = c.recv(head_length).decode(&quot;utf-8&quot;)    head = json.loads(head_data)    print(head)    # 真实数据长度    data_length = head[&quot;data_size&quot;]    #接收真实数据    size = 0    res = b&quot;&quot;    while size &lt; data_length:        temp = c.recv(1024)        size += len(temp)        res += temp    print(res.decode(&quot;gbk&quot;))服务器：12345678910111213141516171819202122232425import socketimport subprocessimport structimport jsonserver = socket.socket()server.bind((&quot;127.0.0.1&quot;,8888))server.listen()while True:    client, addr = server.accept()    while True:        cmd = client.recv(1024).decode(&quot;utf-8&quot;)        p = subprocess.Popen(cmd,shell=True,stdout=-1,stderr=-1)          # 真实数据        data = p.stdout.read() + p.stderr.read()               # 头部数据        head = {&quot;data_size&quot;:len(data),&quot;额外信息&quot;:&quot;额外的值&quot;}        head_data = json.dumps(head).encode(&quot;utf-8&quot;)        #头部长度        head_len = struct.pack(&quot;i&quot;,len(head_data))        #逐个发送        client.send(head_len)        client.send(head_data)        client.send(data)urlliburllib—URL handling modulespython爬虫从入门到放弃（三）之 Urllib库的基本使用基本概述urllib是python内置的HTTP请求库，包括以下模块urllib.request: 最基本的HTTP请求模块，用来发起请求，就和人们在浏览器上输入网址来访问网页一样urllib.error: 异常处理模块，如果在请求时出现错误，用这个模块来抓住异常，保证程序不会因为抛出异常而挂掉urllib.parse url: 工具模块，提供了许多URL处理方法，比如URL的拆分、合并等等urllib.robotparser: 用来识别目标网站的robot.txt文件(基本用不上)重要模块urlopen关于urllib.request.urlopen参数的介绍：1urllib.request.urlopen(url, data=None, [timeout, ]*, cafile=None, capath=None, cadefault=False, context=None)请求urlopen一般常用的有三个参数，它的参数如下：urllib.requeset.urlopen(url,data,timeout)1234567891011121314151617181920import urllib.request# Get请求response = urllib.request.urlopen('http://www.baidu.com')# response.read()可以获取到网页的内容print(response.read().decode('utf-8'))# Post请求import urllib.parseimport urllib.request# 通过bytes(urllib.parse.urlencode())可以将post数据进行转换放到urllib.request.urlopen的data参数中。这样就完成了一次post请求data = bytes(urllib.parse.urlencode({'word': 'hello'}), encoding='utf8')print(data)response = urllib.request.urlopen('http://httpbin.org/post', data=data)print(response.read())# Timeout参数# 在某些网络情况不好或者服务器端异常的情况会出现请求慢的情况，或者请求异常# 所以这个时候我们需要给请求设置一个超时时间，而不是让程序一直在等待结果response = urllib.request.urlopen('http://httpbin.org/get', timeout=1)print(response.read())如果我们添加data参数的时候就是以post请求方式请求，如果没有data参数就是get请求方式响应通过response.status、response.getheaders().response.getheader(“server”)，获取状态码以及头部信息response.read()获得的是响应体的内容12345678print(response.status)200print(response.getheaders())[('Connection', 'close'), ('Content-Length', '49589'), ('Server', 'nginx'), ('Content-Type', 'text/html; charset=utf-8'), ('X-Frame-Options', 'DENY'), ('Via', '1.1 vegur, 1.1 varnish, 1.1 varnish'), ('Accept-Ranges', 'bytes'), ('Date', 'Wed, 18 Aug 2021 07:37:14 GMT'), ('Age', '1496'), ('X-Served-By', 'cache-bwi5120-BWI, cache-nrt18321-NRT'), ('X-Cache', 'HIT, HIT'), ('X-Cache-Hits', '2, 1285'), ('X-Timer', 'S1629272234.003034,VS0,VE0'), ('Vary', 'Cookie'), ('Strict-Transport-Security', 'max-age=63072000; includeSubDomains')]print(response.getheader(&quot;server&quot;))nginxprint(response.read())响应体的内容request上述的urlopen只能用于一些简单的请求，因为它无法添加一些header信息，如果后面写爬虫我们可以知道，很多情况下我们是需要添加头部信息去访问目标站的，这个时候就用到了urllib.requesturllib中，request这个模块主要负责构造和发起网络请求，并在其中加入Headers、Proxy等。利用它可以模拟浏览器的一个请求发起过程设置Headers有很多网站为了防止程序爬虫爬网站造成网站瘫痪，会需要携带一些headers头部信息才能访问最长见的有user-agent参数12345678910111213141516171819from urllib import request, parse# 方式一url = 'http://httpbin.org/post'headers = {    'User-Agent': 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)',    'Host': 'httpbin.org'}dict = {'name': 'hyc'}data = bytes(parse.urlencode(dict), encoding='utf8')req = request.Request(url=url, data=data, headers=headers, method='POST')response = request.urlopen(req)# 方式二data = bytes(parse.urlencode(dict), encoding='utf8')req = request.Request(url=url, data=data, method='POST')# 好处是自己可以定义一个请求头字典，然后循环进行添加req.add_header('User-Agent', 'Mozilla/4.0 (compatible; MSIE 5.5; Windows NT)')response = request.urlopen(req)urlretrieve下载文件urlretrieve()方法直接将远程数据下载到本地。1urlretrieve(url, filename=None, reporthook=None, data=None)url：下载链接地址filename：指定了保存本地路径(如果参数未指定，urllib会生成一个临时文件保存数据)reporthook：是一个回调函数，当连接上服务器、以及相应的数据块传输完毕时会触发该回调，我们可以利用这个回调函数来显示当前的下载进度data：指post导服务器的数据，该方法返回一个包含两个元素的(filename, headers) 元组，filename 表示保存到本地的路径，header表示服务器的响应头123456789101112131415161718import osfrom six.moves import urllibimport sysDATA_URL = 'http://www.python.org/ftp/python/2.7.5/Python-2.7.5.tar.bz2'filename = DATA_URL.split('/')[-1]def _progress(block_num, block_size, total_size):    '''回调函数       @block_num: 已经下载的数据块       @block_size: 数据块的大小       @total_size: 远程文件的大小    '''    sys.stdout.write('\r&gt;&gt; Downloading %s %.1f%%' % (filename,                     float(block_num * block_size) / float(total_size) * 100.0))    sys.stdout.flush()filepath, _ = urllib.request.urlretrieve(DATA_URL, filename, _progress)handlerProxyHandler通过rulllib.request.ProxyHandler()可以设置代理,网站它会检测某一段时间某个IP 的访问次数，如果访问次数过多，它会禁止你的访问,所以这个时候需要通过设置代理来爬取数据123456789import urllib.requestproxy_handler = urllib.request.ProxyHandler({    'http': 'http://127.0.0.1:9743',    'https': 'https://127.0.0.1:9743'})opener = urllib.request.build_opener(proxy_handler)response = opener.open('http://httpbin.org/get')print(response.read())HTTPCookiProcessorcookie中保存中我们常见的登录信息，有时候爬取网站需要携带cookie信息访问,这里用到了http.cookijar，用于获取cookie以及存储cookie1234567891011121314151617181920212223242526272829303132333435import http.cookiejar, urllib.request# 方式一：直接打印cookie = http.cookiejar.CookieJar()handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')for item in cookie:    print(item.name+&quot;=&quot;+item.value)    # 方式二：MozillaCookieJar()方式import http.cookiejar, urllib.requestfilename = &quot;cookie.txt&quot;cookie = http.cookiejar.MozillaCookieJar(filename)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')cookie.save(ignore_discard=True, ignore_expires=True)# 方式三：LWPCookieJar()方式import http.cookiejar, urllib.requestfilename = 'cookie.txt'cookie = http.cookiejar.LWPCookieJar(filename)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')cookie.save(ignore_discard=True, ignore_expires=True)# 读取cookieimport http.cookiejar, urllib.requestcookie = http.cookiejar.LWPCookieJar()cookie.load('cookie.txt', ignore_discard=True, ignore_expires=True)handler = urllib.request.HTTPCookieProcessor(cookie)opener = urllib.request.build_opener(handler)response = opener.open('http://www.baidu.com')print(response.read().decode('utf-8'))excepturllib库实现了三个异常类：URLError里只有一个属性：reason,即抓异常的时候只能打印错误信息HTTPError里有三个属性：code,reason,headersContentTooShortError里有三个属性：reason,content123456789101112131415from urllib import request,errortry:    response = request.urlopen(&quot;http://pythonsite.com/1111.html&quot;)except error.HTTPError as e:    print(e.reason)    print(e.code)    print(e.headers)except error.URLError as e:    print(e.reason)    # e.reason其实也可以在做深入的判断    if isinstance(e.reason,socket.timeout):        print(&quot;time out&quot;)else:    print(&quot;reqeust successfully&quot;)url parse相关urlparse123456# urllib.parse.urlparse(urlstring, scheme='', allow_fragments=True)from urllib.parse import urlparseresult = urlparse(&quot;http://www.baidu.com/index.html;user?id=5#comment&quot;)print(result)urlunpars功能和urlparse的功能相反，它是用于拼接1234from urllib.parse import urlunparsedata = ['http','www.baidu.com','index.html','user','a=123','commit']print(urlunparse(data))urljoin这个的功能其实是做拼接的1234567891011121314151617181920from urllib.parse import urljoinprint(urljoin('http://www.baidu.com', 'FAQ.html'))print(urljoin('http://www.baidu.com', 'https://pythonsite.com/FAQ.html'))print(urljoin('http://www.baidu.com/about.html', 'https://pythonsite.com/FAQ.html'))print(urljoin('http://www.baidu.com/about.html', 'https://pythonsite.com/FAQ.html?question=2'))print(urljoin('http://www.baidu.com?wd=abc', 'https://pythonsite.com/index.php'))print(urljoin('http://www.baidu.com', '?category=2#comment'))print(urljoin('www.baidu.com', '?category=2#comment'))print(urljoin('www.baidu.com#comment', '?category=2'))# 从拼接的结果我们可以看出，拼接的时候后面的优先级高于前面的urlhttp://www.baidu.com/FAQ.htmlhttps://pythonsite.com/FAQ.htmlhttps://pythonsite.com/FAQ.htmlhttps://pythonsite.com/FAQ.html?question=2https://pythonsite.com/index.phphttp://www.baidu.com?category=2#commentwww.baidu.com?category=2#commentwww.baidu.com?category=2urlencode这个方法可以将字典转换为url参数123456from urllib.parse import urlencodeparams = {&quot;name&quot;:&quot;hyc&quot;, &quot;age&quot;:28}base_url = &quot;http://www.baidu.com?&quot;url = base_url+urlencode(params)print(url)cookie登录后，得到cookie，带着这个cookie可以访问登录后的内容123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v1.0@author: huangyc@file: cookie_test.py@Description: @time: 2022/11/3 11:34&quot;&quot;&quot;import urllib.parseimport urllib.parseimport urllib.requestimport urllib.requestfrom http import cookiejardef cookie_test():    # 定义请求头    headers = {&quot;content-type&quot;: &quot;application/x-www-form-urlencoded&quot;,               &quot;Users-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.130 Safari/537.36&quot;}    # 登录的网址    login_url = &quot;https://xx.hycbook.com/xx.php&quot;    # 登录的参数    login_form_data = {&quot;fm_usr&quot;: &quot;xxx&quot;, &quot;fm_pwd&quot;: &quot;xxx&quot;}    # 使用urllib.request时  post方法所携带的参数不能是字典形式    login_form_data_final = urllib.parse.urlencode(login_form_data)    # 发送登录请求POST, 自动保存cookie    cook_jar = cookiejar.MozillaCookieJar()    # 定义有添加 cookie功能的处理器    cook_handler = urllib.request.HTTPCookieProcessor(cook_jar)    # 根据处理器生成opener    opener = urllib.request.build_opener(cook_handler)    # 此时headers里面还没有cookie, 还没有登录    login_request = urllib.request.Request(login_url, headers=headers, data=login_form_data_final.encode(&quot;utf-8&quot;))    # 如果登录成功. cookjar自动保存cookie, opener里面有cookjar, 所以opener里有cookie    resp = opener.open(login_request)    res_text = resp.read().decode('utf-8')    cook_jar.save(&quot;cookie.txt&quot;)    # 创建管理器    cookie_handler = urllib.request.HTTPCookieProcessor(cook_jar)    http_handler = urllib.request.HTTPHandler()    https_handler = urllib.request.HTTPSHandler()    # 创建请求求管理器, 此处将登录过的cookie带进去    opener = urllib.request.build_opener(cookie_handler, http_handler, https_handler)    look_url = f&quot;https://xx.hycbook.com&quot;    req = urllib.request.Request(look_url)    # 发起请求    response = opener.open(req)    print(&quot;结束&quot;)urllib3requests</content>
	</entry>
	<entry>
		<title>collection模块</title>
		<url>/article/54754.html</url>
		<content>Python的collections模块collections —- 容器数据类型Python3 collections模块使用详解collections — Container datatypes  namedtuple(): 生成可以使用名字来访问元素内容的tuple子类 deque: 双端队列，可以快速的从另外一侧追加和推出对象 Counter: 计数器，主要用来计数 OrderedDict: 有序字典 defaultdict: 带有默认值的字典       ChainMap    类似字典(dict)的容器类，将多个映射集合到一个视图里面       UserDict    封装了字典对象，简化了字典子类化       UserList    封装了列表对象，简化了列表子类化       UserString    封装了列表对象，简化了字符串子类化 collections.abc  抽象基类tuple的几个特性：不可变，iterable拆包tuple不可变不是绝对的tuple比list好的地方 4.1 immutable的重要性：性能优化(元素全为immutable的会作为常量在编译时确定)、线程安全、可以作为dict的key，拆包特性 4.2 tuple类似struct，list类似array容器序列　　list、tuple 和 collections.deque 这些序列能存放不同类型的数据。扁平序列　　str、bytes、bytearray、memoryview 和 array.array，这类序列只能容纳一种类型。容器序列存放的是它们所包含的任意类型的对象的引用，而扁平序列里存放的是值而不是引用。换句话说，扁平序列其实是一段连续的内存空间。由此可见扁平序列其实更加紧凑，但是它里面只能存放诸如字符、字节和数值这种基础类型。序列类型还能按照能否被修改来分类。可变序列　　list、bytearray、array.array、collections.deque 和 memoryview。不可变序列　　tuple、str 和 bytes。具名元组namedtuple概念命名的元组，意味给元组中的每个位置赋予含义，意味着代码可读性更强，namedtuple可以在任何常规元素使用的地方使用，而且它可以通过名称来获取字段信息而不仅仅是通过位置索引。用以构建只有少数属性但是没有方法的对象，比如数据库条目。collections.namedtuple 是一个工厂函数，它可以用来构建一个带字段名的元组和一个有名字的用 namedtuple 构建的类的实例所消耗的内存跟元组是一样的，因为字段名都被存在对应的类里面创建一个具名元组需要两个参数，一个是类名，另一个是类的各个字段的名字。几个最有用的：_fields 类属性、类方法 _make(iterable) 和实例方法 _asdict()。❶ _fields 属性是一个包含这个类所有字段名称的元组。❷ 用 _make() 通过接受一个可迭代对象来生成这个类的一个实例，它的作用跟 City(*delhi_data) 是一样的。❸ _asdict() 把具名元组以 collections.OrderedDict 的形式返回，我们可以利用它来把元组里的信息友好地呈现出来。12345678from collections import namedtupleUser = namedtuple(&quot;USER&quot;, [&quot;name&quot;, &quot;age&quot;, &quot;city&quot;, &quot;height&quot;])user_1 = User(name=&quot;雷鸣&quot;, age=21, city=&quot;北京&quot;, height=&quot;175&quot;)print(&quot;user_1&quot;, user_1.name, user_1.age, user_1.city, user_1.height)user_1 雷鸣 21 北京 175user_1._fieldsOut[4]: ('name', 'age', 'city', 'height')使用定义代码12345678910111213141516171819202122232425262728293031323334# namedtuple： 生成可以使用名字来访问元素内容的tuple子类# -*- coding: utf-8 -*from collections import namedtupleUser = namedtuple(&quot;USER&quot;, [&quot;name&quot;, &quot;age&quot;, &quot;city&quot;, &quot;height&quot;])user_1 = User(name=&quot;雷鸣&quot;, age=21, city=&quot;北京&quot;, height=&quot;175&quot;)print(&quot;user_1&quot;, user_1.name, user_1.age, user_1.city, user_1.height)user_tuple = (&quot;雷姆&quot;, 17, '异世界')user_2 = User(*user_tuple, &quot;172&quot;)print(&quot;user_2&quot;, user_2.name, user_2.age, user_2.city, user_2.height)user_dict = {    &quot;name&quot;: &quot;雷玖&quot;,    &quot;age&quot;: &quot;17&quot;,    &quot;city&quot;: &quot;异世界&quot;,}user_3 = User(**user_dict, height=&quot;172&quot;)print(&quot;user_3&quot;, user_3.name, user_3.age, user_3.city, user_3.height)user_tuple_2 = (&quot;雷姆&quot;, 17, '异世界', &quot;172&quot;)user_list_2 = [&quot;雷姆&quot;, 17, '异世界', &quot;172&quot;]user_dict_2 = {    &quot;name&quot;: &quot;雷玖&quot;,    &quot;age&quot;: &quot;17&quot;,    &quot;city&quot;: &quot;异世界&quot;,    &quot;height&quot;: &quot;172&quot;}user_4 = User._make(user_dict_2.values())print(&quot;user_4&quot;, user_4.name, user_4.age, user_4.city, user_4.height)user_5 = User._make(user_tuple_2)print(&quot;user_5&quot;, user_5.name, user_5.age, user_5.city, user_5.height)user_6 = User._make(user_list_2)print(&quot;user_6&quot;, user_6.name, user_6.age, user_6.city, user_6.height)输出123456user_1 雷鸣 21 北京 175user_2 雷姆 17 异世界 172user_3 雷玖 17 异世界 172user_4 雷玖 17 异世界 172user_5 雷姆 17 异世界 172user_6 雷姆 17 异世界 172访问代码123456789from collections import namedtupleUser = namedtuple(&quot;USER&quot;, [&quot;name&quot;, &quot;age&quot;, &quot;city&quot;, &quot;height&quot;])user_tuple = (&quot;雷姆&quot;, 17, '异世界')user_2 = User(*user_tuple, &quot;172&quot;)print(user_2._asdict())item = ('a', 'b', 'a', 'a', 5)for it in set(item):    print(f'{it} 出现了{item.count(it)}次')输出1234OrderedDict([('name', '雷姆'), ('age', 17), ('city', '异世界'), ('height', '172')])a 出现了3次5 出现了1次b 出现了1次字典介绍dict 类型不但在各种程序里广泛使用，它也是 Python 语言的基石。跟它有关的内置函数都在 __builtins__.__dict__模块中。正是因为字典至关重要，Python 对它的实现做了高度优化，而散列表则是字典类型性能出众的根本原因。collections.abc 模块中有 Mapping 和 MutableMapping 这两个抽象基类然而，非抽象映射类型一般不会直接继承这些抽象基类，它们会直接对 dict 或是 collections.UserDict 进行扩展。标准库里的所有映射类型都是利用 dict 来实现的，因此它们有个共同的限制，即只有可散列的数据类型才能用作这些映射里的键关于可散列类型的定义有这样一段话：如果一个对象是可散列的，那么在这个对象的生命周期中，它的散列值是不变的，而且这个对象需要实现 __hash__() 方法。另外可散列对象还要有 __qe__() 方法，这样才能跟其他键做比较。如果两个可散列对象是相等的，那么它们的散列值一定是一样的一般来讲用户自定义的类型的对象都是可散列的，散列值就是它们的 id() 函数的返回值，所以所有这些对象在比较的时候都是不相等的创建字典的不同方式：1234567&gt;&gt;&gt; a = dict(one=1, two=2, three=3)&gt;&gt;&gt; b = {'one': 1, 'two': 2, 'three': 3}&gt;&gt;&gt; c = dict(zip(['one', 'two', 'three'], [1, 2, 3]))&gt;&gt;&gt; d = dict([('two', 2), ('one', 1), ('three', 3)])&gt;&gt;&gt; e = dict({'three': 3, 'one': 1, 'two': 2})&gt;&gt;&gt; a == b == c == d == eTrue除了这些字面句法和灵活的构造方法之外，字典推导（dict comprehension）也可以用来建造新 dict字典推导（dictcomp）可以从任何以键值对作为元素的可迭代对象中构建出字典。映射类型的方法其实很丰富。表 3-1 为我们展示了 dict、defaultdict 和 OrderedDict 的常见方法，后面两个数据类型是 dict 的变种，位于 collections 模块内。字典的变种：collections.OrderedDict： 这个类型在添加键的时候会保持顺序，因此键的迭代次序总是一致的。OrderedDict的 popitem 方法默认删除并返回的是字典里的最后一个元素，但是如果像 my_odict.popitem(last=False) 这样调用它，那么它删除并返回第一个被添加进去的元素。collections.ChainMap： 该类型可以容纳数个不同的映射对象，然后在进行键查找操作的时候，这些对象会被当作一个整体被逐个查找，直到键被找到为止。collections.Counter： 这个映射类型会给键准备一个整数计数器。每次更新一个键的时候都会增加这个计数器。所以这个类型可以用来给可散列表对象计数，或者是当成多重集来用——多重集合就是集合里的元素可以出现不止一次。跟 OrderedDict、ChainMap 和 Counter 这些开箱即用的类型不同，UserDict 是让用户继承写子类的。dict的实现及其导致的结果键必须是可散列的一个可散列的对象必须满足以下要求。(1) 支持 hash() 函数，并且通过 __hash__() 方法所得到的散列值是不变的。(2) 支持通过 __eq__() 方法来检测相等性。(3) 若 a == b 为真，则 hash(a) == hash(b) 也为真。所有由用户自定义的对象默认都是可散列的，因为它们的散列值由 id() 来获取，而且它们都是不相等的。字典在内存上的开销巨大由于字典使用了散列表，而散列表又必须是稀疏的，这导致它在空间上的效率低下。在用户自定义的类型中，__slots__ 属性可以改变实例属性的存储方式，由 dict变成 tuple键查询很快dict 的实现是典型的空间换时间：字典类型有着巨大的内存开销，但它们提供了无视数据量大小的快速访问——只要字典能被装在内存里键的次序取决于添加顺序当往 dict 里添加新键而又发生散列冲突的时候，新键可能会被安排存放到另一个位置。往字典里添加新键可能会改变已有键的顺序无论何时往字典里添加新的键，Python 解释器都可能做出为字典扩容的决定。扩容导致的结果就是要新建一个更大的散列表，并把字典里已有的元素添加到新表里。defaultdict默认值词典错误方式代码12345678910111213141516# 错误方式一student_grades = {}grades = [('elliot', 91), ('neelam', 98),          ('bianca', 81), ('elliot', 88)]for name, grade in grades:if name not in student_grades:    student_grades[name] = []    student_grades[name].append(grade)print(student_grades)# 错误方式二student_grades = {}for name, grade in grades:student_grades.setdefault(name, [])    student_grades[name].append(grade)print(student_grades)输出12{'elliot': [91, 88], 'neelam': [98], 'bianca': [81]}{'elliot': [91, 88], 'neelam': [98], 'bianca': [81]}正确方式在这种情况下，你将创建一个defaultdict，它使用不带参数的list构造函数作为默认方法。没有参数的list返回一个空列表，因此如果名称不存在则defaultdict调用list()，然后再把学生成绩添加上。如果你想更炫一点，你也可以使用lambda函数作为值来返回任意常量。在用户创建 defaultdict 对象的时候，就需要给它配置一个为找不到的键创造默认值的方法。实例化一个 defaultdict 的时候，需要给构造方法提供一个可调用对象把 list 构造方法作为 default_factory 来创建一个 defaultdict。defaultdict 里的 default_factory 只会在 __getitem__ 里被调用，在其他的方法里完全不会发挥作用。比如，dd 是个 defaultdict，k 是个找不到的键， dd[k] 这个表达式会调用 default_factory 创造某个默认值，而 dd.get(k) 则会返回 None。所有的映射类型在处理找不到的键的时候，都会牵扯到 __missing__ 方法。这也是这个方法称作“missing”的原因如果有一个类继承了 dict，然后这个继承类提供了 __missing__ 方法，那么在 __getitem__ 碰到找不到的键的时候，Python 就会自动调用它__missing__ 方法只会被 __getitem__ 调用像 k in my_dict.keys() 这种操作在 Python 3 中是很快的，而且即便映射类型对象很庞大也没关系。这是因为 dict.keys() 的返回值是一个“视图”。视图就像一个集合，而且跟字典类似的是，在视图里查找一个元素的速度很快。代码12345# 正确方式from collections import defaultdictstudent_grades = defaultdict(list)[student_grades[name].append(grade) for name, grade in grades]print(student_grades)输出1defaultdict(&lt;class 'list'&gt;, {'elliot': [91, 88], 'neelam': [98], 'bianca': [81]})统计缺失多少键1234567891011121314151617class CountMissing:   ...:     def __init__(self):   ...:         self.added = 0   ...:     def missing(self):   ...:         self.added += 1    ...:         return 0   ...:     current = {'green':12, 'blue':3}increments = [('red',5),('blue',17),('orange',9)]counter = CountMissing()from collections import defaultdictresult = defaultdict(counter.missing, current)for key,amount in increments:   ...:     result[key] += amount   ...:     print(counter.added)2其它默认值代码1234567891011121314151617181920212223242526# 错误方式cowboy = {'age': 32, 'horse': 'mustang', 'hat_size': 'large'}if 'name' in cowboy:name = cowboy['name']else:name = 'The Man with No Name'print(name)# 如果你想在仍然访问name的key时使用默认值更新字典# 错误方式if 'name' not in cowboy:cowboy['name'] = 'The Man with No Name'name = cowboy['name']print(name)# 正确方式# get()执行与第一种方法相同的操作，但现在它们会自动处理。如果key存在，则返回适当的值。否则，将返回默认值name = cowboy.get('name', 'The Man with No Name')print(name)# 正确方式name = cowboy.setdefault('name', 'The Man with No Name')print(name)from collections import defaultdictdic = defaultdict(lambda :'0a')dic['a']Out[41]: '0a'输出1234The Man with No NameThe Man with No NameThe Man with No NameThe Man with No Name有序词典OrderedDict概念OrderedDict类似于正常的词典，只是它记住了元素插入的顺序，当在有序的词典上迭代时，返回的元素就是它们第一次添加的顺序。class collections.OrderedDict，返回已给dict的子类，支持常规的dict的方法，OrderedDict是一个记住元素首次插入顺序的词典，如果一个元素重写已经存在的元素，那么原始的插入位置保持不变，如果删除一个元素再重新插入，那么它就在末尾。使用使用dict时，Key是无序的。在对dict做迭代时，我们无法确定Key的顺序。如果要保持Key的顺序，可以用OrderedDict，OrderedDict的Key会按照插入的顺序排列，不是Key本身排序例子1代码123456789101112131415161718# 例子1from collections import OrderedDictodd0 = {'banana': 3, 'apple': 4}od1 = OrderedDict({'banana': 3, 'apple': 4})od2 = OrderedDict({'apple': 4, 'banana': 3})print(od1 == od2)print(od1 == odd0)# 例子2from collections import OrderedDictod1 = OrderedDict({'banana': 3, 'apple': 4})# OrderedDict.popitem(last=True)，popitem方法返回和删除一个(key,value)对，如果last=True，就以LIFO方式执行，否则以FIFO方式执行。od1.popitem(False)print(od1)od1.pop('apple')  # 这里必须提供keyprint(od1)输出1234FalseTrueOrderedDict([('apple', 4)])OrderedDict()例子212345678910111213141516171819202122232425# 例子3from collections import OrderedDictod1 = OrderedDict({'banana': 3, 'apple': 4})od1.move_to_end('banana')print(od1)# 例子4# OrderedDict可以实现一个FIFO（先进先出）的dict，当容量超出限制时，先删除最早添加的Keyfrom collections import OrderedDictclass LastUpdatedOrderedDict(OrderedDict):    def __init__(self, capacity):        super(LastUpdatedOrderedDict, self).__init__()        self._capacity = capacity    def __setitem__(self, key, value):        containsKey = 1 if key in self else 0        if len(self) - containsKey &gt;= self._capacity:            last = self.popitem(last=False)            print('remove:', last)        if containsKey:            del self[key]            print('set:', (key, value))        else:            print('add:', (key, value))        OrderedDict.__setitem__(self, key, value)1OrderedDict([('apple', 4), ('banana', 3)])ChainMap原始的映射对象被存放在一个列表中构成一个字典序列self.maps在 ChainMap 中查询某个键时，会对原始的映射对象依次查询，直至找到这个键，若未找到，则默认引发 KeyError 异常。在 ChainMap 中进行插入、更新、删除时，只会对原始映射中的第一个映射进行操作ChainMap 对象除了 maps 属性外，还具有一个 parents 属性和 new_child(m=None) 方法。parents 属性返回了一个不包含原始映射中的第一个映射的 ChainMap 对象，对应源码中 self.class(self.maps[1:]) ，其效果和 ChainMap(d.maps[1:]) 相同new_child(m=None) 方法返回一个包含指定映射 m（未指定时，为空字典）及其他原有映射的 ChainMap 对象，其中指定映射位于底层 maps 列表首位，在其他原有映射之前。1234567891011121314151617181920212223242526272829303132# ChainMap 类是为了将多个映射快速的链接到一起，这样它们就可以作为一个单元处理。它通常比创建一个新字典和多次调用 update() 要快很多from collections import ChainMapa = {'a': 1, 'b': 2}b = {'c': 3}c = ChainMap(a, b) # 如果有重复的key，以第一次为准print(c)# 将数据以列表形式展现，实际上是将链接指向原来的实现print(c.maps)print(c.maps[0]['b'])ChainMap({'a': 1, 'b': 2}, {'c': 3})[{'a': 1, 'b': 2}, {'c': 3}]2# 简单应用一个Python命令中，如果在命令行中输入参数则使用该参数，没有则从OS环境变量中获取，如果还没有再取自定义默认值import osimport argparsefrom collections import ChainMapdefaults = {'color': 'red', 'user': 'guest'}parser = argparse.ArgumentParser()parser.add_argument('-u', '--user')parser.add_argument('-c', '--color')namespace = parser.parse_args()command_line_args = {k: v for k, v in vars(namespace).items() if v}combined = ChainMap(command_line_args, os.environ, defaults)print(combined['color'])print(combined['user'])redguest使用 ChainMap 对象作为嵌套上下文我们在上面提到了 new_child(m=None) 可用于创建子上下文，这个是一个非常便捷的方法，可以使用一个空字典或其他指定的映射来创建一个新的 ChainMap 对象。针对这个新对象的修改不会对原有的 ChainMap 对象（可以将其理解为底层的数据结构或者基础上下文）产生影响。12345678910111213&gt;&gt;&gt; d_1 = {'name': 'bob', 'age': 25}&gt;&gt;&gt; d_2 = {'height': '175', 'weight': 120}&gt;&gt;&gt; c = ChainMap(d_1, d_2)  # 创建一个基础上下文&gt;&gt;&gt; c_nc = c.new_child()  # 创建一个嵌套的子上下文&gt;&gt;&gt; c_nc['skill'] = 'Python'  # 在子上下文环境中进行赋值&gt;&gt;&gt; cChainMap({'name': 'bob', 'age': 25}, {'height': '175', 'weight': 120})&gt;&gt;&gt; c_ncChainMap({'skill': 'Python'}, {'name': 'bob', 'age': 25}, {'height': '175', 'weight': 120})&gt;&gt;&gt; list(c_nc)['height', 'weight', 'name', 'age', 'skill']&gt;&gt;&gt; dict(c_nc){'height': '175', 'weight': 120, 'name': 'bob', 'age': 25, 'skill': 'Python'}计数器CounterPs: Counter仅支持Hashable对象进行统计定义假如你有一长串没有标点符号或大写字母的单词，你想要计算每个单词出现的次数。你可以使用字典或defaultdict增加计数，但collections.Counter提供了一种更清晰，更方便的方法。 Counter是dict的子类，它使用0作为任何缺失元素的默认值，并且更容易计算对象的出现次数：1234567from collections import Counterwords = &quot;if there was there was but if there was not there was not&quot;.split()# 或者  counts = Counter(if=2,there=4,was=4,not=2,but=1) # 传进元组counts = Counter(words)print(counts)&gt;&gt;&gt; Counter({'there': 4, 'was': 4, 'if': 2, 'not': 2, 'but': 1})遍历元素遍历所有元素12345678910# 当你将单词列表传递给Counter时，它会存储每个单词以及该单词在列表中出现的次数。# 如果你好奇两个最常见的词是什么？只需使用.most_common（）：print(counts.most_common(2))# 遍历所有元素for k, v in counts.items():print(k, v)# 遍历打印所有元素print(sorted(counts.elements()))123456789[('there', 4), ('was', 4)]if 2there 4was 4but 1not 2['but', 'if', 'if', 'not', 'not', 'there', 'there', 'there', 'there', 'was', 'was', 'was', 'was']更新元素123456789101112# update(增加元素)counts.update(&quot;if you are here&quot;.split())print(counts)# subtract(原来的元素减去新传入的元素)counts.subtract(['if'])  # if的次数减一print(counts)# 删除元素counts['but'] = 0del counts['but']print(counts)123Counter({'there': 4, 'was': 4, 'if': 3, 'not': 2, 'but': 1, 'you': 1, 'are': 1, 'here': 1})Counter({'there': 4, 'was': 4, 'if': 2, 'not': 2, 'but': 1, 'you': 1, 'are': 1, 'here': 1})Counter({'there': 4, 'was': 4, 'if': 2, 'not': 2, 'you': 1, 'are': 1, 'here': 1})类集合操作123456c = Counter(a=3, b=1)d = Counter(a=1, b=2)print(c + d)  # 相加print(c - d)  # 相减，如果小于等于0，删去print(c &amp; d)  # 求最小print(c | d)  # 求最大1234Counter({'a': 4, 'b': 3})Counter({'a': 2})Counter({'a': 1, 'b': 1})Counter({'a': 3, 'b': 2})子类化UserDict就创造自定义映射类型来说，以 UserDict 为基类，总比以普通的 dict 为基类要来得方便。而更倾向于从 UserDict 而不是从 dict 继承的主要原因是，后者有时会在某些方法的实现上走一些捷径，导致我们不得不在它的子类中重写这些方法，但是 UserDict 就不会带来这些问题。值得注意的地方是，UserDict 并不是 dict 的子类，但是 UserDict 有一个叫作 data 的属性，是 dict 的实例，这个属性实际上是 UserDict 最终存储数据的地方。双向队列deque定义deque是栈和队列的一种广义实现，deque是”double-end queue”的简称；deque支持线程安全、有效内存地以近似O(1)的性能在deque的两端插入和删除元素，尽管list也支持相似的操作，但是它主要在固定长度操作上的优化，从而在pop(0)和insert(0,v)（会改变数据的位置和大小）上有O(n)的时间复杂度。常用方法deque支持如下方法，append(x)， 将x添加到deque的右侧；appendleft(x)， 将x添加到deque的左侧；clear()， 将deque中的元素全部删除，最后长度为0；count(x)， 返回deque中元素等于x的个数；extend(iterable)， 将可迭代变量iterable中的元素添加至deque的右侧；extendleft(iterable)， 将变量iterable中的元素添加至deque的左侧，往左侧添加序列的顺序与可迭代变量iterable中的元素相反；pop()， 移除和返回deque中最右侧的元素，如果没有元素，将会报出IndexError；popleft()， 移除和返回deque中最左侧的元素，如果没有元素，将会报出IndexError；remove(value)， 移除第一次出现的value，如果没有找到，报出ValueError；reverse()， 反转deque中的元素，并返回None；rotate(n)， 从右侧反转n步，如果n为负数，则从左侧反转，d.rotate(1)等于d.appendleft(d.pop())；maxlen， 只读的属性，deque的最大长度，如果无解，就返回None；除了以上的方法之外，deque还支持迭代、序列化、len(d)、reversed(d)、copy.copy(d)、copy.deepcopy(d)，通过in操作符进行成员测试和下标索引，索引的时间复杂度是在两端是O(1)，在中间是O(n)，为了快速获取，可以使用list代替。​    index（查找某个元素的索引位置）insert（在指定位置插入元素）12345678910111213141516171819202122232425262728293031323334353637383940414243444546&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; d = deque('ghi')# 新建一个deque，有三个元素&gt;&gt;&gt; for ele in d:# 遍历deque...     print ele.upper()...     ... GHI&gt;&gt;&gt; d.append('j')# deque右侧添加一个元素&gt;&gt;&gt; d.appendleft('f')# deque左侧添加一个元素&gt;&gt;&gt; d# 打印dequedeque(['f', 'g', 'h', 'i', 'j'])&gt;&gt;&gt; d.pop()# 返回和移除最右侧元素'j'&gt;&gt;&gt; d.popleft()# 返回和移除最左侧元素'f'&gt;&gt;&gt; list(d)# 以列表形式展示出deque的内容['g', 'h', 'i']&gt;&gt;&gt; d[0]# 获取最左侧的元素'g'&gt;&gt;&gt; d[-1]# 获取最右侧的元素'i'&gt;&gt;&gt; list(reversed(d))# 以列表形式展示出倒序的deque的内容['i', 'h', 'g']&gt;&gt;&gt; 'h' in d# 在deque中搜索True&gt;&gt;&gt; d.extend('jkl')# 一次添加多个元素&gt;&gt;&gt; ddeque(['g', 'h', 'i', 'j', 'k', 'l'])&gt;&gt;&gt; d.rotate(1)# 往右侧翻转&gt;&gt;&gt; ddeque(['l', 'g', 'h', 'i', 'j', 'k'])&gt;&gt;&gt; d.rotate(-1)# 往左侧翻转&gt;&gt;&gt; ddeque(['g', 'h', 'i', 'j', 'k', 'l'])&gt;&gt;&gt; deque(reversed(d))# 以逆序新建一个dequedeque(['l', 'k', 'j', 'i', 'h', 'g'])&gt;&gt;&gt; d.clear()# 清空deque&gt;&gt;&gt; d.pop()# 不能在空的deque上popTraceback (most recent call last):  File &quot;&lt;input&gt;&quot;, line 1, in &lt;module&gt;IndexError: pop from an empty deque&gt;&gt;&gt; d.extendleft('abc')# 以输入的逆序向左扩展&gt;&gt;&gt; ddeque(['c', 'b', 'a'])12345678910111213# deque是为了高效实现插入和删除操作的双向列表，适合用于队列和栈：# 使用list存储数据时，按索引访问元素很快，但是插入和删除元素就很慢了，因为list是线性存储，数据量大的时候，插入和删除效率很低。&gt;&gt;&gt; from collections import deque&gt;&gt;&gt; q = deque(['a', 'b', 'c'])&gt;&gt;&gt; q.append('x')&gt;&gt;&gt; q.appendleft('y')&gt;&gt;&gt; qdeque(['y', 'a', 'b', 'c', 'x'])# deque除了实现list的append()和pop()外，还支持appendleft()和popleft()，这样就可以非常高效地往头部添加或删除元素# deque是线程安全的，有GIL保护# append和popleft都是原子操作multiprocessing：这个包实现了自己的Queue，它和queue.Queue类似，是设计给进程间通信使用的。同时还有个multiprocessing.JoinableQueue类型，可以让任务管理变得更方便。asyncio：python3.4新提供的包，里面有Queue、LifoQueue、PriorityQueue和JoinableQueue，这些类受到queue和mulitiprocessing模块的影响，但是为异步编程里的任务管理提供了便利。子类化UserDictUserDict封装了字典对象，简化了字典子类化UserList封装了列表对象，简化了列表子类化UserString封装了列表对象，简化了字符串子类化</content>
	</entry>
	<entry>
		<title>logging模块</title>
		<url>/article/12909.html</url>
		<content>python的日志模块loggingPython Logging 指南Python日志库logging总结-可能是目前为止将logging库总结的最好的一篇文章python logging日志模块以及多进程日志Python配置日志的几种方式logging 模块在部署项目时，不可能直接将所有的信息都输出到控制台中，我们可以将这些信息记录到日志文件中这样不仅方便我们查看程序运行时的情况，也可以在项目出现故障时根据运行时产生的日志快速定位问题出现的位置logging框架组成：Loggers: 日志，暴露函数给应用程序，基于日志记录器和过滤器级别决定哪些日志有效。LogRecord ：日志记录器，将日志传到相应的处理器处理。Handlers: 处理器, 将(日志记录器产生的)日志记录发送至合适的目的地。常用类型有StreamHandler、FileHandler、NullHandlerFilters: 过滤器, 提供了更好的粒度控制,它可以决定输出哪些日志记录。Formatters: 格式化器, 指明了最终输出中日志记录的布局。logging日志级别：每个logger都有一个日志的级别。logging中定义了如下级别LevelNumeric value解释NOTSET0意指不设置 所以按照父logger级别来过滤日志DEBUG10详细信息，通常仅在诊断问题时才有意义INFO20确认事情按预期工作WARNING30表明发生了意外情况，或表明在不久的将来出现了一些问题（例如 “磁盘空间不足”）。但是该软件仍在按预期工作ERROR40由于更严重的问题，该软件无法执行某些功能CRITICAL50严重错误，表明程序本身可能无法继续运行注意事项：但是当发生异常时，直接使用无参数的 debug()、info()、warning()、error()、critical() 方法并不能记录异常信息需要设置 exc_info 参数为 True 才可以，或者使用 exception() 方法，还可以使用 log() 方法，但还要设置日志级别和 exc_info 参数12345678910111213import logginglogging.basicConfig(filename=&quot;test.log&quot;, filemode=&quot;w&quot;, format=&quot;%(asctime)s %(name)s:%(levelname)s:%(message)s&quot;,                    datefmt=&quot;%d-%M-%Y %H:%M:%S&quot;, level=logging.DEBUG)a = 5b = 0try:    c = a / bexcept Exception as e:    # 下面三种方式三选一，推荐使用第一种    logging.exception(&quot;Exception occurred&quot;)    logging.error(&quot;Exception occurred&quot;, exc_info=True)    logging.log(level=logging.DEBUG, msg=&quot;Exception occurred&quot;, exc_info=True)logging 流程判断 Logger 对象对于设置的级别是否可用，如果可用，则往下执行，否则，流程结束。创建 LogRecord 对象，如果注册到 Logger 对象中的 Filter 对象过滤后返回 False，则不记录日志，流程结束，否则，则向下执行。LogRecord 对象将 Handler 对象传入当前的 Logger 对象，（图中的子流程）如果 Handler 对象的日志级别大于设置的日志级别，再判断注册到 Handler 对象中的 Filter 对象过滤后是否返回 True 而放行输出日志信息，否则不放行，流程结束。如果传入的 Handler 大于 Logger 中设置的级别，也即 Handler 有效，则往下执行，否则，流程结束。判断这个 Logger 对象是否还有父 Logger 对象，如果没有（代表当前 Logger 对象是最顶层的 Logger 对象 root Logger），流程结束。否则将 Logger 对象设置为它的父 Logger 对象，重复上面的 3、4 两步，输出父类 Logger 对象中的日志输出，直到是 root Logger 为止。Logger 使用logging 配置python代码使用Python代码显式的创建loggers，handlers和formatters并分别调用它们的配置函数通过简单方式进行配置，使用 basicConfig() 函数直接进行配置；代码Logger 对象和 Handler 对象都可以设置级别，而默认 Logger 对象级别为 30 ，也即 WARNING，默认 Handler 对象级别为 0，也即 NOTSET。logging 模块这样设计是为了更好的灵活性，比如有时候我们既想在控制台中输出DEBUG 级别的日志，又想在文件中输出WARNING级别的日志。可以只设置一个最低级别的 Logger 对象，两个不同级别的 Handler 对象，示例代码如下：123456789101112131415161718192021222324252627282930import loggingimport logging.handlerslogger = logging.getLogger(&quot;logger&quot;)handler1 = logging.StreamHandler()handler2 = logging.FileHandler(filename=&quot;test.log&quot;, encoding=&quot;utf-8&quot;)logger.setLevel(logging.DEBUG)handler1.setLevel(logging.WARNING)handler2.setLevel(logging.DEBUG)formatter = logging.Formatter(&quot;%(asctime)s %(name)s %(levelname)s %(message)s&quot;)handler1.setFormatter(formatter)handler2.setFormatter(formatter)logger.addHandler(handler1)logger.addHandler(handler2)# 分别为 10、30、30# print(handler1.level)# print(handler2.level)# print(logger.level)logger.debug('This is a customer debug message')logger.info('This is an customer info message')logger.warning('This is a customer warning message')logger.error('This is an customer error message')logger.critical('This is a customer critical message')或者123456789logging.basicConfig(filename=&quot;config.log&quot;,                    filemode=&quot;w&quot;,                    format=&quot;%(asctime)s-%(name)s-%(levelname)s-%(message)s&quot;,                    level=logging.INFO)logging.basicConfig(level=log_level,                    format='%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(message)s',                    datefmt='%a, %d %b %Y %H:%M:%S',                    filename='parser_result.log',                    filemode='w')参数说明参数名称参数描述filename日志输出到文件的文件名filemode文件模式，r[+]、w[+]、a[+]format日志输出的格式datefat日志附带日期时间的格式style格式占位符，默认为 “%” 和 “{}”level设置日志输出级别stream定义输出流，用来初始化 StreamHandler 对象不能 filename 参数一起使用，否则会ValueError 异常handles定义处理器，用来创建 Handler 对象，不能和 filename 、stream 参数一起使用，否则也会抛出 ValueError 异常输出123452018-05-06 11:05:34,486 - simple_logger - DEBUG - debug message.2018-05-06 11:05:34,487 - simple_logger - INFO - info message.2018-05-06 11:05:34,487 - simple_logger - WARNING - warning message.2018-05-06 11:05:34,487 - simple_logger - ERROR - error message.2018-05-06 11:05:34,487 - simple_logger - CRITICAL - critical message.配置文件创建一个日志配置文件，然后使用fileConfig()函数来读取该文件的内容相对于第一种配置方式的优点在于，它将配置信息和代码分离了，这一方面降低了日志的维护成本，同时还使得非开放人员也能够很容易的修改日志配置通过配置文件进行配置，使用fileConfig()函数读取配置文件常见的配置文件有 ini 格式、yaml 格式、JSON 格式，或者从网络中获取都是可以的配置文件logging.conf配置文件中一定要包含loggers、handlers、formatters这些section，它们通过keys这个option来指定该配置文件中已经定义好的loggers、handlers和formatters，多个值之间用逗号分隔；另外loggers这个section中的keys一定要包含root这个值；loggers、handlers、formatters中所所指定的日志器、处理器和格式器都需要在下面单独的section中进行定义。section的命名规则为[logger_loggerName]、[handler_handlerName]、[formatter_formatterName]；定义logger的section必须指定level和handlers这两个option，level的可取值为DEBUG、INFO、WARNING、ERROR、CRITICAL、NOTSET，其中NOTSET表示所有级别的日志消息都要记录，包括用户定义级别；handlers的值是以逗号分隔的handler名字列表，这里出现的handler必须出现在[handlers]这个section中，并且相应的handler必须在配置文件中有对应的section定义；对于非root logger来说，除了level和handlers这两个option之外，还需要一些额外的option，其中qualname是必须提供的option，它表示在logger层级中的名字，在应用代码中通过这个名字得到logger；propagate是可选的，其默认值为1，表示消息将会传递给高层次logger的handler，通常我们需要指定其值为0，这个可以看下面的例子；另外，对于非root logger的level如果设置为NOTSET，系统将会查找高层次的logger来决定此logger的有效level；定义handler的section中必须指定class和args这两个option，level和formatter为可选option；class表示用于创建handler的类名，args表示传递给class所指定的handler类初始化方法参数，它必须是一个元组(tuple)的形式，即便只有一个参数值也需要是一个元组的形式；level与logger中的level一样，而formatter指定的是该处理器所使用的格式器，这里指定的格式器名称必须出现在formatters这个section总，且在配置文件中必须要有这个formatter的section定义；如果不指定formatter则该handler将会以消息本身作为日志消息进行记录，而不添加额外的时间、日志器名称等信息；定义formatter的section中的option都是可选的，其中包括format用于指定格式字符串，默认为消息字符串本身；datefmt用于指定asctime的时间格式，默认为”%Y-%m-%d %H:%M:%S”；class用于指定格式器类名，默认为logging.Formatter；每一个logger或者handler或者formatter都有一个key名字，以logger为例，首先需要在[loggers]配置中加上key名字代表了这个logger然后用[loggers_xxxx]其中xxxx为key名来具体配置这个logger，在log02中我配置了level和一个handler名，当然你可以配置多个hander根据这个handler名再去 [handlers]里面去找具体handler的配置，以此类推1234567891011121314151617181920212223242526272829303132333435363738##############################################[loggers]keys=root, log02[logger_root]level=INFOhandlers=handler01[logger_log02]level=DEBUGhandler=handler02qualname=log02##############################################[handlers]keys=handler01,handler02[handler_handler01]class=FileHandlerlevel=INFOformatter=form01args=('../log/cv_parser_gm_server.log',&quot;a&quot;)[handler_handler02]class=StreamHandlerlevel=NOTSETformatter=form01args=(sys.stdout,)##############################################[formatters]keys=form01,form02[formatter_form01]format=%(asctime)s %(filename)s[line:%(lineno)d] %(levelname)s %(process)d %(message)sdatefmt=[%Y-%m-%d %H:%M:%S][formatter_form02]format=(message)s##############################################代码该函数实际上是对configparser模块的封装，函数定义：该函数定义在logging.config模块下1logging.config.fileConfig(fname, defaults=None, disable_existing_loggers=True)参数说明：fname：表示配置文件的文件名或文件对象；defaults：指定传给ConfigParser的默认值；disable_existing_loggers：这是一个布尔值，默认值为True(为了向后兼容)表示禁用已经存在的logger，除非它们或它们的祖先明确的出现在日志配置中；如果该值为False，则对已存在的loggers保持启动状态1234567891011121314import logging.config# 读取日志配置文件内容logging.config.fileConfig(&quot;logging.conf&quot;)# 创建一个日志器loggerlogger = logging.getLogger(&quot;simpleExample&quot;)# 日志输出logger.debug(&quot;debug message.&quot;)logger.info(&quot;info message.&quot;)logger.warning(&quot;warning message.&quot;)logger.error(&quot;error message.&quot;)logger.critical(&quot;critical message.&quot;)输出123452018-05-06 12:29:24,849 - simpleExample - DEBUG - debug message.2018-05-06 12:29:24,849 - simpleExample - INFO - info message.2018-05-06 12:29:24,849 - simpleExample - WARNING - warning message.2018-05-06 12:29:24,849 - simpleExample - ERROR - error message.2018-05-06 12:29:24,849 - simpleExample - CRITICAL - critical message.配置文件test.yaml123456789101112131415161718version: 1formatters:  simple:    format: '%(asctime)s - %(name)s - %(levelname)s - %(message)s'handlers:  console:    class: logging.StreamHandler    level: DEBUG    formatter: simple  loggers:  simpleExample:    level: DEBUG    handlers: [console]    propagate: noroot:  level: DEBUG  handlers: [console]代码123456789import logging.config# 需要安装 pyymal 库import yamlwith open('test.yaml', 'r') as f:    config = yaml.safe_load(f.read())    logging.config.dictConfig(config)logger = logging.getLogger(&quot;sampleLogger&quot;)字典配置(推荐)创建一个包含配置信息的dict，然后把它传递给dictConfig()函数通过配置字典进行配置，使用 dictConfig() 函数读取配置信息logging.FileHandler: 文件handle, 多线程下安全logging.handlers.RotatingFileHandler: 轮循文件handle, 多线程下安全, 可以限制文件大小, 设置历史日志数量concurrent_log_handler.ConcurrentRotatingFileHandler: 多进程多线程下安全, 可以限制文件大小, 设置历史日志数量12pip install redis==4.2.2pip install concurrent-log-handler==0.9.20logging_config.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v0.1@author: narutohyc@file: logger_config.py@Description: 日志配置字典 + 定义logger句柄供项目使用              Python日志库logging总结-可能是目前为止将logging库总结的最好的一篇文章:                  https://www.jianshu.com/p/7b5e4752932e@time: 2020/5/23 21:34&quot;&quot;&quot;import loggingimport osfrom enum import Enum, uniquefrom logging import config as logging_configfrom os.path import join# 获取日志目录from basic_support.data_access.config.project_config import project_configbase_path = project_config.PROJECT_PATHbase_log_path = join(base_path, 'logs')# 文件目录不存在时, 创建该目录if not os.path.exists(base_log_path):    os.makedirs(base_log_path)@uniqueclass LogLevel(Enum):    &quot;&quot;&quot; 日志等级枚举类 &quot;&quot;&quot;    CRITICAL = '致命'  # 严重错误,表明程序本身可能无法继续运行    ERROR = '错误'  # 由于更严重的问题,该软件无法执行某些功能    WARNING = '警告'  # 表明发生了意外情况,或表明在不久的将来出现了一些问题 (例如 '磁盘空间不足')。但是该软件仍在按预期工作    INFO = '普通'  # 确认事情按预期工作    DEBUG = '详细'  # 详细信息,通常仅在诊断问题时才有意义    NOTSET = '不设置'  # 意指不设置,所以按照父logger级别来过滤日志# 日志相关配置LOGGING_CONFIG = {    'version': 1,    'loggers': {  # 日志，暴露函数给应用程序，基于日志记录器和过滤器级别决定哪些日志有效        '': {  # root logger            'level': LogLevel.INFO.name,  # 日志等级            'handlers': ['console_handler', 'info_file_handler', 'error_file_handler'],        }    },    'handlers': {  # 处理器, 将(日志记录器产生的)日志记录发送至合适的目的地        'console_handler': {            'level': LogLevel.INFO.name,  # 控制台日志等级 和 最终等级=max(当前等级,loggers)            'formatter': 'info',            'class': 'logging.StreamHandler',  # 日志类            'stream': 'ext://sys.stdout',  # 日志流        },        'info_file_handler': {            'level': LogLevel.INFO.name,  # 信息日志等级            'formatter': 'info',            'class': &quot;concurrent_log_handler.ConcurrentRotatingFileHandler&quot;,  # 多进程下多线程安全            'filename': os.path.join(base_log_path, 'info.log'),  # 信息日志文件输出目录            'mode': 'a+',  # 日志文件模型 a表示追加  w是覆盖写            'encoding': 'utf-8',            'backupCount': 4,  # 4 = 自己+历史的3个            'maxBytes': 1024 * 1024 * 50,  # 单个日志文件大小限制在 50MB内            'use_gzip': False,        },        'error_file_handler': {            'level': LogLevel.WARNING.name,  # 错误日志等级            'formatter': 'error',            'class': 'logging.FileHandler',            # 'class': 'logging.handlers.RotatingFileHandler',            'filename': os.path.join(base_log_path, 'error.log'),  # 错误日志文件输出目录            'mode': 'a+',            'encoding': 'utf-8'        }    },    'formatters': {  # 格式化器, 指明了最终输出中日志记录的布局        'info': {            'format': '%(asctime)s %(module)s:%(lineno)d %(levelname)s: %(message)s',  # 日志输出格式化            'datefmt': '%Y-%m-%d %H:%M:%S'  # 日期格式化        },        'error': {            # 'format': '%(asctime)s-%(levelname)s-%(name)s-%(process)d::%(module)s|%(lineno)s:: %(message)s',            'format': '%(asctime)s %(module)s|%(lineno)s %(levelname)s|%(process)d:  %(message)s',            'datefmt': '%Y-%m-%d %H:%M:%S'        },    },}# 获取日志实例logging_config.dictConfig(LOGGING_CONFIG)logger = logging.getLogger(__name__)使用12345678from comm.config.logger_config import logger# 日志输出logger.debug(&quot;debug message.&quot;)logger.info(&quot;info message.&quot;)logger.warning(&quot;warning message.&quot;)logger.error(&quot;error message.&quot;)logger.critical(&quot;critical message.&quot;)输出123452020-05-24 10:29:30 text_augmentation_script DEBUG:  debug message.2020-05-24 10:29:30 text_augmentation_script INFO:  info message.2020-05-24 10:29:30 text_augmentation_script WARNING:  warning message.2020-05-24 10:29:30 text_augmentation_script ERROR:  error message.2020-05-24 10:29:30 text_augmentation_script CRITICAL:  critical message.Handler 子类StreamHandler 实例将消息发送到流（类文件对象）。FileHandler 实例将消息发送到磁盘文件。BaseRotatingHandler 是在某个点切割日志文件的处理器的基类。它并不意味着直接实例化。而是使用 RotatingFileHandler 或 TimedRotatingFileHandler。RotatingFileHandler 实例将消息发送到磁盘文件，支持最大日志文件大小和日志文件切割。TimedRotatingFileHandler 实例将消息发送到磁盘文件，以特定的时间间隔切割日志文件。SocketHandler 实例将消息发送到 TCP/IP 套接字。从 3.4 开始，也支持 Unix 域套接字。DatagramHandler 实例将消息发送到 UDP 套接字。从 3.4 开始，也支持 Unix 域套接字。SMTPHandler 实例将消息发送到指定的电子邮件地址。SysLogHandler 实例将消息发送到 Unix syslog 守护程序，可以是在远程计算机上。NTEventLogHandler 实例将消息发送到 Windows NT/2000/XP 事件日志。MemoryHandler 实例将消息发送到内存中的缓冲区，只要满足特定条件，就会刷新内存中的缓冲区。HTTPHandler 实例使用 GET 或 POST 语义将消息发送到 HTTP 服务器。WatchedFileHandler 实例监视他们要记录的文件。如果文件发生更改，则会关闭该文件并使用文件名重新打开。此处理程序仅在类 Unix 系统上有用; Windows 不支持使用的基础机制。QueueHandler 实例将消息发送到队列，例如队列或多处理模块中实现的队列。NullHandler 实例不会对错误消息执行任何操作。NullHandler ，StreamHandler 和 FileHandler 类在核心日志包中定义。其他处理程序在子模块 logging.handlers 中定义。（还有另一个子模块 logging.config，用于配置功能。）日志文件按照时间划分或者按照大小划分如果将日志保存在一个文件中，那么时间一长，或者日志一多，单个日志文件就会很大，既不利于备份，也不利于查看。我们会想到能不能按照时间或者大小对日志文件进行划分呢？答案肯定是可以的，并且还很简单，logging 考虑到了我们这个需求。logging.handlers 文件中提供了 TimedRotatingFileHandler 和 RotatingFileHandler 类分别可以实现按时间和大小划分。打开这个 handles 文件，可以看到还有其他功能的 Handler 类，它们都继承自基类 BaseRotatingHandler。自定义 Logger可能的问题 logging 库是线程安全的，但在多进程、多线程、多进程多线程环境中仍然还有值得考虑的问题，比如，如何将日志按照进程（或线程）划分为不同的日志文件，也即一个进程（或线程）对应一个文件。可以使用多进程安全的日志类concurrent-log-handler</content>
	</entry>
	<entry>
		<title>Spring注解_Spring入门篇</title>
		<url>/article/1ddf4748.html</url>
		<content>Spring入门篇那些年，让我们一起着迷的Springhttp://spring.iohttps://spring.io/projects/spring-framework概述spring是一个开源框架，是一个轻量级的控制反转(IOC)和面向切面（AOP)的容器框架 -从大小与开销两方面而言Spring都是轻量的 -通过控制反转（IoC）的技术达到松耦合的目的 -提供了面向切面编程的丰富支持，允许通过分离应用的业务逻辑与系统级服务进行内聚性的开发 -包含并管理应用对象的配置和生命周期，这个意义上是一种容器 -将简单的组件配置、组合成为复杂的应用，这个意义上是框架 &gt;Java 程序员必须掌握的 5 个注解！Spring作用容器 ·提供了对多种技术的支持 -JMS -MQ支持 -UnitTest AOP（事务管理、日志等） ·提供了众多方便应用的辅助类（JDBC Template等） ·对主流应用框架（Hibernate等）提供了良好的支持适用范围构建企业应用（SpringMVC+Spring+Hibernate/ibatis) 单独使用Bean容器（Bean管理） 单独使用AOP进行切面处理 其他的Spring功能，如：对消息的支持等 在互联网中的应用什么是框架·框架的特点 -半成品 -封装了特定的处理流程和控制逻辑 -成熟的、不断升级改进的软件 ·框架与类库的区别 -框架一般是封装了逻辑、高内聚的，类库则是松散的工具组合 -框架专注于某一领域，类库则是更通用的 为什么使用框架·软件系统日趋复杂 ·重用度高，开发效率和质量提高 ·软件设计人员要专注于对领域的了解，使需求分析更充分 ·易于上手、快速解决问题 Spring IOC容器接口及面向接口编程接口·用于沟通的中介物的抽象化 ·实体把自己提供给外界的一种抽象化说明，用以由内部操作分 离出外部沟通方法，使其能被修改内部而不影响外界其他实体 与其交互的方式 ·对应Java接口即声明，声明了哪些方法是对外公开提供的 ·在Java8中，接口可以拥有方法体 面向接口编程结构设计中，分清层次及调用关系，每层只向外（上层）提供 一组功能接口，各层间仅依赖接口而非实现类 接口实现的变动不影响各层间的调用，这一点在公共服务中尤为重要 “面向接口编程”中的“接口”是用于隐藏具体实现和实现多态性的组件什么是IOCIOC:控制反转，控制权的转移，应用程序本身不负责依赖对象的创建和维护，而是由外部容器负责创建和维护 DI（依赖注入）是其一种实现方式 目的：创建对象并且组装对象之间的关系 &gt;扩展理解2004年，Martin Fowler探讨了同一个问题，既然IOC是控制 反转，那么到底是“哪些方面的控制被反转了呢？”，经过详 细地分析和论证后，他得出了答案：“获得依赖对象的过程被 反转了”。控制被反转之后，获得依赖对象的过程由自身管理 变为了由IOC容器主动注入。于是，他给“控制反转”取了一 个更合适的名字叫做“依赖注入（Dependency Injection) “。他的这个答案，实际上给出了实现IOC的方法：注入。所 谓依赖注入，就是由IOC容器在运行期间，动态地将某种依赖 关系注入到对象之中。单元测试下载junit-*.jar并引入工程 创建UnitTestBase类，完成对Spring配置文件的加载、销毁 所有的单元测试类都继承自UnitTestBase，通过它的 getBean方法获取想要得到的对象 子类（具体执行单元测试的类）加注解： @RunWith(BlockJUnit4ClassRunner.class) 单元测试方法加注解：@Test 右键选择要执行的单元测试方法执行或者执行一个类的全部单 元测试方法 Bean容器初始化基础：两个包 -org.springframework.beans -org.springframework.context -BeanFactory提供配置结构和基本功能，加载并初始化Bean -ApplicationContext保存了Bean对象并在Spring中被广泛使用 方式，ApplicationContext -本地文件 -Classpath -Web应用中依赖servlet或Listener Bean容器初始化·文件 1FileSystemXmlApplicationContext context= new FileSystemXmlApplicationContext(&quot;F:/workspace/appcontext.xml&quot;); ·Classpath 1caspatxnlApplicstionc ontext context= new ClaspathxmlAplicationcontextrclasspathispring-contextxml);.Web应用 123456789101112&lt;listener&gt; &lt;listener-class&gt;        org.springframework.web.context.ContextLoaderListener    &lt;/listener-class&gt; &lt;/listener&gt; &lt;servlet&gt;     &lt;servlet-name&gt;context&lt;/servlet-name&gt;     &lt;servlet-class&gt;        org.springframework.web.context.ContextLoaderServlet    &lt;/servlet-class&gt;     &lt;load-on-startup&gt;1&lt;/load-on-startup&gt; &lt;/servlet&gt; Spring的常用注入方式Spring注入Spring注入是指在启动Spring容器加载bean配置的时候，完成对变量的赋值行为 常用的两种注入方式 -设值注入 1234567891011 &lt;?xml version=&quot;1.0&quot;encoding=&quot;UTF-8?&gt;  &lt;beans xmlns=&quot;http:/www.springframework.org/schema/beans&quot;  xmlns:xsi=&quot;htto://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http:/www.springframework.org/schema/beans  http:/www.springframeworkorg/schema/beans/spring-beans.xsd&quot;&gt;     &lt;bean id=&quot;injectionService&quot;class='com.imooc.ioc.injection.service.InjiectionServicelmp/&gt;    &lt;property name=&quot;injection0AO&quot;ref=tipjection0Ag'1p&lt;/property&gt;     &lt;/bean&gt;     &lt;bean id=ipjection0AO&quot;class=&quot;com.imooc.iodinjiection.dao.lpjection0AOlmp/&gt;&lt;/bean&gt; &lt;/beans&gt;12345678910111213141516import com.imooc.ioc.injection.dao.InjectionDAO; public class InjectionServicelmpl implements InjectionService{     private InjectionDAO injectionDAO;     //设值注入     public void setlnjectionDAO(InjectionDAO injectionDAO)    {         this.injectionDAO=injectionDAO;     }     public void save(String arg)    {         //模拟业务操作         System.outprintin(CService接收参数：+arg);         arg=arg +&quot;:&quot;+this.hashCode0;         injectionDAO.save(arg);    }}​    -构造注入 1234567891011 &lt;?xml version=&quot;1.0&quot;encoding=&quot;UTF-8?&gt;  &lt;beans xmlns=&quot;http:/www.springframework.org/schema/beans&quot;  xmlns:xsi=&quot;htto://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http:/www.springframework.org/schema/beans  http:/www.springframeworkorg/schema/beans/spring-beans.xsd&quot;&gt;     &lt;bean id=&quot;injectionService&quot;class='com.imooc.ioc.injection.service.InjiectionServicelmp/&gt;    &lt;constructor-arg name=&quot;injection0AO&quot;ref=iniection0AQ&gt;I&lt;/constructor-arg&gt;      &lt;/bean&gt;     &lt;bean id=ipjection0AO&quot;class=&quot;com.imooc.iodinjiection.dao.lpjection0AOlmp/&gt;&lt;/bean&gt; &lt;/beans&gt;1234567891011121314151617import com.imooc.ioc.injection.dao.InjectionDAO; public class InjectionServicelmpl implements InjectionService{     private InjectionDAO injectionDAO;     //构造器注入     public InjectionServicelmpl(InjectionDAO injectionDAO)    {         this.injectionDAO=injectionDAO;    }    public void save(String arg)    {         //模拟业务操作         System.outprintin(CService接收参数：+arg);         arg=arg +&quot;:&quot;+this.hashCode0;         injectionDAO.save(arg);    }}Spring Bean装配Bean装配，从Spring到Spring BootBean配置项Class Scope Constructor arguments Properties Autowiring mode lazy-initialization mode Initialization/destruction method Bean的作用域singleton:单例，指一个Bean容器中只存在一份 prototype：每次请求（每次使用）创建新的实例，destroy 方式不生效 request:每次http请求创建一个实例且仅在当前request内 有效 session:同上，每次http请求创建，当前session内有效 global session:基于portlet的web中有效（portlet定义了 global session),如果是在web中，同session 1&lt;bean id=&quot;beanScope&quot;class=&quot;com.imooc.bean.BeanScope&quot; scope=singletor'&quot;&gt;&lt;/bean&gt; Bean的生命周期生命周期-定义 -初始化-实现org.springframework.beans.factory.InitializingBean接口 ，覆盖afterPropertiesSet方法 123456public class ExamplelnitializingBean implements InitializingBean{     @Override     public void afterPropertiesSet0 throws Exception{         //do something    }}-配置init-method1&lt;bean id=&quot;exampleInitBean&quot; class=&quot;examples.ExampleBean&quot; init-method=&quot;init&quot;/〉  12345public class ExampleBean {     public void init(){         //do some initialization work     }}-使用 -销毁 -实现org.springframework.beans.factory.DisposableBean接口 ，覆盖destroy方法 123456public class ExampleDisposableBean implements DisposableBean{     @Override     public void destroy0 throws Exception{         //do something    }}配置destroy-method 1&lt;bean id=&quot;exampleInitRean&quot; class=&quot;examples.ExampleBean&quot;destroy-method=&quot;cleanup&quot;/&gt; 12345public class ExampleBean {     public void cleanup(){         //do some destruction work (like releasing pooled connections)     }}配置全局默认初始化、销毁方法 1234567&lt;?xml version=1.0&quot;encoding='UTF-8?&gt;&lt;beans xmlns=&quot;http:/www.springframework.org/schema/beans&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XML Schema-instance&quot; xsi:schemaLocation=&quot;http://www.springframeworkorg/schema/beans http:/www.springframework.org/schema/beans/spring-beans.xsd&quot;default-init-method=&quot;init&quot;default-destroy-method=&quot;destroys&quot;&gt;&lt;/beans&gt;优先顺序  InitializingBean和DisposableBean(定义完需实现)配置init-method和destroy-method(定义完需实现)默认(可选)Aware接口Spring中提供了一些以Aware结尾的接口，实现了Aware接 口的bean在被初始化之后，可以获取相应资源 通过Aware接口，可以对Spring相应资源进行操作（一定要慎 重） 为对Spring进行简单的扩展提供了方便的入口 Spring中的aware接口1234567891011121314151617181920&gt;import org.springframework.beans.BeansException;&gt;import org.springframework.beans.factory.BeanNameAware;&gt;import org.springframework.context.ApplicationContext;&gt;import org.springframework.context.ApplicationContextAware;&gt;public class MoocBeanName implements BeanNameAware, ApplicationContextAware {private String beanName;@Overridepublic void setBeanName(String name) {this.beanName = name;System.out.println(&quot;MoocBeanName : &quot; + name);}@Overridepublic void setApplicationContext(ApplicationContext applicationContext)throws BeansException {System.out.println(&quot;setApplicationContext : &quot; + applicationContext.getBean(this.beanName).hashCode());}&gt;}Bean的自动装配Bean的自动装配（Autowiring) No:不做任何操作 byName:根据属性名自动装配。此选项将检查容器并根据名字 查找与属性完全一致的bean，并将其与属性自动装配 123456789&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;    xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans    http://www.springframework.org/schema/beans/spring-beans.xsd&quot;     default-autowire=&quot;byName&quot;&gt;    &lt;bean id=&quot;autoWiringService&quot; class=&quot;com.imooc.autowiring.AutoWiringService&quot; &gt;&lt;/bean&gt;    &lt;bean id=&quot;autoWiringDAO&quot; class=&quot;com.imooc.autowiring.AutoWiringDAO&quot; &gt;&lt;/bean&gt;&lt;/beans&gt;12345public class AutoWiringDAO {public void say(String word) {System.out.println(&quot;AutoWiringDAO : &quot; + word);}}12345678910public class AutoWiringService {    private AutoWiringDAO autoWiringDAO;    public void setAutoWiringDAO(AutoWiringDAO autoWiringDAO) {        System.out.println(&quot;setAutoWiringDAO&quot;);        this.autoWiringDAO = autoWiringDAO;    }    public void say(String word) {        this.autoWiringDAO.say(word);    }}byType:如果容器中存在一个与指定属性类型相同的bean，那 么将与该属性自动装配；如果存在多个该类型bean，那么抛出 异常，并指出不能使用byType方式进行自动装配；如果没有找 到相匹配的bean，则什么事都不发生 12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;    xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans    http://www.springframework.org/schema/beans/spring-beans.xsd&quot;     default-autowire=&quot;byType&quot;&gt;    &lt;bean id=&quot;autoWiringService&quot; class=&quot;com.imooc.autowiring.AutoWiringService&quot; &gt;&lt;/bean&gt;    &lt;bean id=&quot;autoWiringDAO&quot; class=&quot;com.imooc.autowiring.AutoWiringDAO&quot; &gt;&lt;/bean&gt;&lt;/beans&gt;&lt;!--autoWiringDAO  也可以是其它名字不影响， 甚至删除也可以--&gt;Constructor:与byType方式类似，不同之处在于它应用于构造器参数。如果容器中没有找到与构造器参数类型一致的bean，那么抛出异常 123456789&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;    xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans    http://www.springframework.org/schema/beans/spring-beans.xsd&quot;     default-autowire=&quot;constructor&quot;&gt;    &lt;bean id=&quot;autoWiringService&quot; class=&quot;com.imooc.autowiring.AutoWiringService&quot; &gt;&lt;/bean&gt;    &lt;bean class=&quot;com.imooc.autowiring.AutoWiringDAO&quot; &gt;&lt;/bean&gt;&lt;/beans&gt;12345678910public class AutoWiringService {    private AutoWiringDAO autoWiringDAO;    public AutoWiringService(AutoWiringDAO autoWiringDAO) {        System.out.println(&quot;AutoWiringService&quot;);        this.autoWiringDAO = autoWiringDAO;    }    public void say(String word) {        this.autoWiringDAO.say(word);    }}Bean的Resources针对于资源文件的统一接口Resources-UrlResource：URL对应的资源，根据一个URL地址即可构建 -ClassPathResource：获取类路径下的资源文件 -FileSystemResource:获取文件系统里面的资源 -ServletContextResource:ServletContext封装的资源，用于访问ServletContext环境下的资源 -InputStreamResource:针对于输入流封装的资源 -ByteArrayResource:针对于字节数组封装的资源 ResourceLoader 所有的application contexts都实现了ResourceLoader接口，因此在Resource实例中都可以使用application contexts。123456public interface ResourceLoader{    Resource getResource(String location);}Resource template=ctx.getResource(&quot;some/resource/path/myTemplate.txt&quot;);Resource template=ctx.getResource(&quot;classpath:some/resource/path/myTemplate.txt&quot;);Resource template=ctx.getResource(&quot;file:/some/resource/path/myTemplate.txt&quot;);12345678910111213141516171819import java.io.IOException;import org.springframework.beans.BeansException;import org.springframework.context.ApplicationContext;import org.springframework.context.ApplicationContextAware;import org.springframework.core.io.Resource;public class MoocResource implements ApplicationContextAware  {private ApplicationContext applicationContext;@Overridepublic void setApplicationContext(ApplicationContext applicationContext)throws BeansException {this.applicationContext = applicationContext;}public void resource() throws IOException {Resource resource = applicationContext.getResource(&quot;config.txt&quot;);System.out.println(resource.getFilename());System.out.println(resource.contentLength());}}Bean的定义及作用域Classpath扫描与组件管理从Spring3.0开始，Spring JavaConfig项目提供了很多特性 ，包括使用java而不是XML定义bean，比如@Configuration,@Bean,@Import,@DependsOn @Component是一个通用注解，可用于任何bean @Repository,@Service,@Controller是更有针对性的注解 -@Repository通常用于注解DAO类，即持久层 -@Service通常用于注解Service类，即服务层 -@Controller通常用于Controller类，即控制层（MVC）类的自动检测及Bean的注册·Spring可以自动检测类并注册Bean到ApplicationContext中123456789101112@Service bublic class SimpleMovielister{ private MovieFinder movieFinder; @Autovired public SimpleMovieLister(MovieFinder movieFinder){     this.movieFinder=movieFinder;}}@Repository public class JpaMovieFinder implements MovieFinder{ //implementation elided for clarity}context:annotation-config 通过在基于XML的Spring配置如下标签（请注意包含上下文 命名空间） \仅会查找在同一个applicatcontext中的Bean注解12345678910&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;    xmlns:context=&quot;http://www.springframework.org/schema/context&quot;    xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans        http://www.springframework.org/schema/beans/spring-beans.xsd        http://www.springframework.org/schema/context        http://www.springframework.org/schema/context/spring-context.xsd&quot; &gt;    &lt;context:annotation-config/&gt;&lt;/beans&gt;类的自动检测及Bean的注册为了能够检测这些类并注册相应的Bean，需要下面内容123&lt;beans&gt;   &lt;context:component-scan base-package=&quot;org.example&quot;/&gt;&lt;/beans&gt;\包含，通常在使用前者后，不用再使用后者，前者包含后者全部功能AutowiredAnnotationBeanPostProcessor和 CommonAnnotationBeanPostProcessor也会被包含进来使用过滤器进行自定义扫描默认情况下，类被自动发现并注册bean的条件是：使用 @Component,@Repository,@Service,@Controller注解或者使用@Component的自定义注解 可以通过过滤器修改上面的行为，如:下面例子的XML配置忽略所有的@Repository注解并用“Stub”代替12345&lt;beans&gt;    &lt;context:component-scan base-package=&quot;org.example&quot;&gt;    &lt;context:include-filter type=&quot;regex&quot; expression=&quot;.*Stub.*Repository&quot;/&gt;    &lt;context:exclude-filter type=&quot;annotation&quot; expression=&quot;org.springframework.stereotype.Repository&quot;/&gt; &lt;/context:component-scan&gt; &lt;/beans&gt;还可使用use-default-filters=”false”禁用自动发现与注册定义Bean扫描过程中组件被自动检测，那么Bean名称是由 BeanNameGenerator生成的（@Component, @Repository,@Service,@Controller都会有个name属性用 于显式设置Bean Name) 1234@Service(&quot;myMovieLister&quot;)Repository public class SimpleMovieLister {}@Repositorypublic class MovieFinderImpl implements MovieFinder{}可自定义bean命名策略，实现BeanNameGenerator接口， 并一定要包含一个无参数构造函器123&lt;beans&gt;    &lt;context:component-scan base-package=&quot;org.example&quot; name-generator=&quot;org.example.MyNameGenerator&quot;/&gt;&lt;/beans&gt;作用域（Scope)通常情况下自动查找的Spring组件，其scope是singleton，Spring2.5提供了一个标识scope的注解@Scope123@Scope(&quot;prototype&quot;) @Repository public class MovieFinderImpl implements MovieFinder{}也可以自定义scope策略，实现ScopeMetadataResolver接 口并提供一个无参构造器，比如在多线程中123&lt;beans&gt; &lt;context:component-scan base-package=&quot;org.example” scope-resolver=&quot;org.example.MyScopeResolver&quot;/&gt; &lt;/beans&gt; proxyMode属性12@Scope(&quot;prototype&quot; proxyMode=ScopedProxyMode.TARGET_CLASS )public class MovieFinderImpl implements MovieFinder{}Autowired注解说明@Required@Required注解适用于bean属性的setter方法 这个注解仅仅表示，受影响的bean属性必须在配置时被填充， 通过在bean定义或通过自动装配一个明确的属性值 1234567public class SimpleMovieLister {private MovieFinder movieFinder;     @Required     public void setMovieFinder(MovieFinder movieFinder){         this.movieFinder=movieFinder;     }}@Autowired可以将@Autowired注解为“传统”的setter方法 1234567private MovieFinder movieFinder;     @Autovired     public void setMovieFinder(MovieFinder movieFinder)    {     this.moviesinder=movieFinder;     }}可用于构造器或成员变量 1234567@Autovired private MovieCatalog movieCatalog; private CustomerPreferenceDao customerPreferenceDao; @Autovired public MovieRecommender(CustomerPreferenceDao customerPreferenceDao){     this.customerPreferenceDao=customerPreferenceDao;}默认情况下，如果因找不到合适的bean将会导致autowiring 失败抛出异常，可以通过下面的方式避免1234567public class simpleMovieLister{     private MovieFinder movieFinder;     @Autovired(required=false)     public void setMovieFinder(MovieFinder movieFinder){         this.movieFinder=movieFinder;    }}每个类只能有一个构造器被标记为required=true @Autowired的必要属性，建议使用@Required注解 1234567891011&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;    xmlns:context=&quot;http://www.springframework.org/schema/context&quot;    xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans        http://www.springframework.org/schema/beans/spring-beans.xsd        http://www.springframework.org/schema/context        http://www.springframework.org/schema/context/spring-context.xsd&quot; &gt;&lt;context:component-scan base-package=&quot;com.imooc.beanannotation&quot;&gt;    &lt;/context:component-scan&gt; &lt;/beans&gt;12345678910111213141516171819202122232425262728import org.springframework.beans.factory.annotation.Autowired;import org.springframework.stereotype.Service;import com.imooc.beanannotation.injection.dao.InjectionDAO;    @Servicepublic class InjectionServiceImpl implements InjectionService {    // 方式一// @Autowiredprivate InjectionDAO injectionDAO;        // 方式二@Autowiredpublic InjectionServiceImpl(InjectionDAO injectionDAO) {this.injectionDAO = injectionDAO;}        // 方式三// @Autowiredpublic void setInjectionDAO(InjectionDAO injectionDAO) {this.injectionDAO = injectionDAO;}        @Overridepublic void save(String arg) {System.out.println(&quot;Service接收参数：&quot; + arg);injectionDAO.save(arg + &quot;:&quot; + this.hashCode());}}可以使用@Autowired注解那些众所周知的解析依赖性接口， 比如：BeanFactory，ApplicationContext,Environment, ResourceLoader,ApplicationEventPublisher,and MessageSource 1234public class MovieRecommender { @Autovired private ApplicationContext context;     public MovieRecommender(){}} 可以通过添加注解给需要该类型的数组的字段或方法，以提供ApplicationContext中的所有特定类型的bean12345private Set&lt;Moviecatalog&gt; movieCatalogs; @Autovired public void seMovieCatalogs(Set&lt;MovieCatalog&gt; movieCatalogs){( this.movieCatalogs=movieCatalogs;}可以用干装配kew为string的Map12345private Map&lt;String,Movlecatalog&gt; moviecatalogs; @Aucovired pablic void setMovieCatalogs(Map&lt;String,MovieCatalog &gt; movieCatalogs){this.movieCatalogs=movieCatalogs;}如果希望数组有序，可以让bean实现 org.springframework.core.Ordered接口或使用的@Order注解，并且只针对数组有效@Autowired是由Spring BeanPostProcessor处理的，所以不能在自己的BeanPostProcessor或 BeanFactoryPostProcessor类型应用这些注解，这些类型必须通过XML或者Spring的@Bean注解加载123456789101112public interface BeanInterface {}import org.springframework.core.annotation.Order;import org.springframework.stereotype.Component;@Order(2)@Componentpublic class BeanImplOne implements BeanInterface {}@Order(1)@Componentpublic class BeanImplTwo implements BeanInterface {}1234567891011121314151617181920212223242526import java.util.List;import java.util.Map;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.stereotype.Component;@Componentpublic class BeanInvoker {@Autowiredprivate List&lt;BeanInterface&gt; list;public void say() {if (null != list &amp;&amp; 0 != list.size()) {System.out.println(&quot;list...&quot;);for (BeanInterface bean : list) {System.out.println(bean.getClass().getName());}} else {System.out.println(&quot;List&lt;BeanInterface&gt; list is null&quot;);}}}list...com.imooc.beanannotation.multibean.BeanImplTwocom.imooc.beanannotation.multibean.BeanImplOne1234567891011121314151617181920212223242526import java.util.List;import java.util.Map;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.stereotype.Component;@Componentpublic class BeanInvoker {@Autowiredprivate Map&lt;String, BeanInterface&gt; map;public void say() {if (null != map &amp;&amp; 0 != map.size()) {System.out.println(&quot;map...&quot;);for (Map.Entry&lt;String, BeanInterface&gt; entry : map.entrySet()) {System.out.println(entry.getKey() + &quot;      &quot; + entry.getValue().getClass().getName());}} else {System.out.println(&quot;Map&lt;String, BeanInterface&gt; map is null&quot;);}}}map...beanImplTwo  com.imooc.beanannotation.multibean.BeanImplTwobeanImplOne  com.imooc.beanannotation.multibean.BeanImplOne@Qualifier注解按类型自动装配可能多个bean实例的情况，可以使用Spring的@Qualifier注解缩小范围(或指定唯一)，也可以用于指定单独的构造器参数或方法参数 可用于注解集合类型变量主要由xml定义和java注解实现这两种方式123456&lt;bean class=&quot;example.SimpleMovieCatalog&quot;&gt; &gt;&lt;qualifier value=&quot;main&quot;/&gt; &lt;/bean&gt; &gt;&lt;bean class=&quot;example.SimpleMovieCatalog&quot;&gt; &lt;qualifier value=&quot;action&quot;/&gt;&lt;/bean&gt; 如果通过名字进行注解注入，主要使用的不是@Autowired( 即使在技术上能够通过@Qualifier定bean的名字），替代方式是使用JSR-250@Resource注解，它是通过其独特的名称 来定义来识别特定的目标（这是一个与所声明的类型是无关的 匹配过程）因语义差异，集合或Map类型的bean无法通过@Autowired 来注入，因为没有类型匹配到这样的bean，为这些bean使用 @Resource注解，通过唯一名称引用集合或Map的bean @Autowired适用于fields,constructors,multi-argument methods这些允许在参数级别使用@Qualifier注解缩小范围的情况 @Resource适用于成员变量、只有一个参数的setter方法，所以在目标是构造器或一个多参数方法时，最好的方式是使用qualifiers可以定义自己的Qualifier123456789101112131415161718192021222324252627@Componentpublic class BeanImplOne implements BeanInterface {}@Componentpublic class BeanImplTwo implements BeanInterface {}import org.springframework.beans.factory.annotation.Autowired;&gt;import org.springframework.beans.factory.annotation.Qualifier;import org.springframework.stereotype.Component;@Componentpublic class BeanInvoker {@Autowired@Qualifier(&quot;beanImplTwo&quot;)private BeanInterface beanInterface;public void say() {if (null != beanInterface) {System.out.println(beanInterface.getClass().getName());} else {System.out.println(&quot;beanInterface is null...&quot;);}}}com.imooc.beanannotation.multibean.BeanImplTwo基于Java的容器注解@Bean标识一个用于配置和初始化一个由SpringloC容器管理的新对象的方法，类似于XML配置文件的 可以在Spring的@Component注解的类中使用@Bean注解 任何方法（仅仅是可以）上一点中，通常使用的是@Configuration 1&lt;beans&gt; &lt;bean id=&quot;myService&quot; class=&quot;com.acme.services.MyServiceImp1&quot;/&gt; &lt;/beans&gt;等价于123456789101112import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;    @Configurationpublic class StoreConfig {    // 若没有指明name，则默认为方法名    // @Bean@Bean(name=&quot;stringStore&quot; initMethod=&quot;init&quot; destoryMethod=&quot;cleanup&quot;)public StringStore stringStore() {return new StringStore();}}使用@ImportResource和@Value 注解进行资源文件读取12345678910&lt;beans&gt;    &lt;context:annotation-config/&gt;     &lt;context:property-placeholder location=&quot;classpath:/com/acme/jdbc.properties&quot;/&gt;     &lt;bean class=&quot;com.acme.AppConfig&quot;/&gt;     &lt;bean class=lWorg.springframework.jdbc.datasource.DriverManagerDataSource&quot;&gt;         &lt;property name=&quot;url&quot; value=&quot;${jdbc.url}&quot;/&gt;         &lt;property name=&quot;username&quot; value=&quot;${jdbc.username}&quot;/&gt;         &lt;property name=&quot;password&quot; value=&quot;S{jdbc.password)&quot;/&gt; &lt;/bean&gt; &lt;/beans&gt; 等价于123456789101112131415@Configuration ImportResource(&quot;classpath:/com/acme/properties-config.xm1&quot;) public class AppConfig{    @Value(&quot;$(jdbc.url)&quot;)     private String url;     @Value(&quot;$(jdbc.username)&quot;)     private String username;     @Value(&quot;jdbc.password)&quot;)     private String password;         @Bean     public DataSource dataSource(){         return new DriverManagerDataSource(url,username,password);     }}12345# properties-config.xm1jdbc.username=rootpassword=rooturl=127.00.1基于泛型的装配&gt;&gt;123456789101112131415161718192021222324252627282930313233&gt;public interface Store&lt;T&gt; {}&gt;public class StringStore implements Store&lt;String&gt; {}&gt;public class IntegerStore implements Store&lt;Integer&gt; {}&gt;import org.springframework.beans.factory.annotation.Autowired;&gt;import org.springframework.context.annotation.Bean;&gt;import org.springframework.context.annotation.Configuration;&gt;import org.springframework.context.annotation.ImportResource;&gt;@Configuration&gt;@ImportResource(&quot;classpath:config.xml&quot;)&gt;public class StoreConfig {@Autowiredprivate Store&lt;String&gt; s1;@Autowiredprivate Store&lt;Integer&gt; s2;@Beanpublic StringStore stringStore() {return new StringStore();}@Beanpublic IntegerStore integerStore() {return new IntegerStore();}   @Bean(name = &quot;stringStoreTest&quot;)public Store stringStoreTest() {System.out.println(&quot;s1 : &quot; + s1.getClass().getName());System.out.println(&quot;s2 : &quot; + s2.getClass().getName());return new StringStore();}&gt;}CustomAutowireConfigurerCustomAutowireConfigurer是 BeanFactoryPostProcessor的子类，通过它可以注册自己的 qualifier注解类型（即使没有使用Spring的@Qualifier 注解）1234567&lt;bean id=&quot;customAutowireConfigurer&quot; class=&quot;org.springframework.beans.factory.annotation.CustomAutowireConfigurer&quot;&gt;     &lt;property name=&quot;customQualifierTypes&quot;&gt;         &lt;set&gt;             &lt;value&gt;example.CustomQualifier&lt;/value&gt;         &lt;/set&gt;     &lt;/property&gt; &lt;/bean&gt;该AutowireCandidateResolver决定自动装配的候选者-每个bean定义的autowire-candidate值 -任何中的default-autowire-candidates -@Qualifier注解及使用CustomAutowireConfigurer的自定义类型JSR支持说明Spring AOP基本概念Spring AOP是什么?你都拿它做什么?AOP基本概念和特点什么是AOPAOP：Aspect Oriented Programming的缩写，意为：面向切面编程，通过预编译方式和运行期动态代理实现程序功能 的统一维护的一种技术 主要的功能是：日志记录，性能统计，安全控制，事务处理， 异常处理等等 &gt;AOP实现方式预编译 -AspectJ 运行期动态代理（JDK动态代理、CGLib动态代理） -SpringAOPJbossAOP AOP几个相关概念 名称说明切面(Aspect)一个关注点的模块化，这个关注点可能会横切多个对象连接点(Joinpoint)程序执行过程中的某个特定的点通知(Advice)在切面的某个特定的连接点上执行的动作切入点(Pointcut)匹配连接点的断言，在AOP中通知和一个切入点表达式关联引入(Introduction)在不修改类代码的前提下，为类添加新的方法和属性目标对象(Target Object)被一个或者多个切面所通知的对象AOP代理(AOP Proxy)AOP框架创建的对象，用来实现切面契约(aspect contract)(包括通知方法执行等功能）织入(Weaving)把切面连接到其它的应用程序类型或者对象上，并创建一个被通知的对象，分为：编译时织入、类加载时织入、执行时织入Advice的类型名称说明前置通知(Before advice)在某连接点(join point)之前执行的通 知，但不能阻止连接点前的执行(除非它抛出一个异常）返回后通知(After returning advice)(After returning advice)在某连接点(join point)正常完成后执行的通知抛出异常后通知(After throwing advice)在方法抛出异常退出时执行的通知后通知(After((finally) advice)当某连接点退出的时候执行的通知(不论是正常返回还是异常退出）环绕通知(Around Advice)包围一个连接点(join point)的通知Spring框架中AOP的用途提供了声明式的企业服务，特别是EJB的替代服务的声明 允许用户定制自己的方面，以完成OOP与AOP的互补使用 Spring的AOP实现纯java实现，无需特殊的编译过程，不需要控制类加载器层次 目前只支持方法执行连接点(通知Spring Bean的方法执行) 不是为了提供最完整的AOP实现(尽管它非常强大)；而是侧重于提供一种AOP实现和Spring IoC容器之间的整合，用于帮助解决企业应用中的常见问题 Spring AOP不会与AspectU竞争，从而提供综合全面的AOP解决方案 有接口和无接口的Spring AOP实现区别Spring AOP默认使用标准的JavaSE动态代理作为AOP代理， 这使得任何接口(或者接口集)都可以被代理 Spring AOP中也可以使用CGLIB代理(如果一个业务对象并没有实现一个接口)配置切面和切入点Schema-based AOPSpring所有的切面和通知器都必须放在一个内 （可以配置包含多个元素），每一个 \可以包含pointcut，advisor和aspect元素 （它们必须按照这个顺序进行声明 )\风格的配置大量使用了Spring的自动代理机制123456&lt;bean id=&quot;moocAspect&quot; class=&quot;com.imooc.aop.schema.advice.MoocAspect&quot;&gt;&lt;/bean&gt; &lt;bean id=&quot;aspectBiz&quot; class=&quot;com.imooc.aop.schema.advice.biz.AspectBiz&quot;&gt;&lt;/bean&gt; &lt;aop:config&gt;     &lt;aop:aspect id=&quot;moocAspect4op&quot; ref=&quot;moocAspect&quot;&gt;        &lt;aop:pointcut expression=&quot;execution(* com.imooc.aop.schema.advice.biz.*Biz(..))&quot; id=&quot;moocPiontcut&quot;/&gt;    &lt;/aop:aspect&gt;&lt;/aop:config&gt; | 表达式                                                       | 含义                                                         || —————————————————————————————— | —————————————————————————————— || execution(public*(..))                                      | 切入点为执行所有public方法时                                 || execution( set(..))                                        | 切入点为执行所有set开始的方法时                              || execution( com.service.Service.(..))                       | 切入点为执行AccountService类中的所有方法时                   || execution( com.xyz.service..(.))                            | 切入点为执行com.xyz.service包下的所有方法时                  || execution( com.xyz.service…(..))                          | 切入点为执行com.xyz.service包及其子包下的所有方法时          || within(com.xyz.service.)                                    | (only in Spring AOP)                                         || within(com.xyz.service..*)                                   | (only in Spring AOP) within用于匹配指定类型内的方法执行 || this(com.xyz.service.AccountService)                         | (only in Spring AOP)this用于匹配当前AOP代理对象类型的执行方法 || target(com.xyz.service.AccountService)                       | (only in Spring AOP)target 用于匹配当前目标对象类型的执行方法 || @annotation(org.springframework.transaction.annota tion.Transactional) | (only in Spring AOP)args 用于匹配当前执行的方法传入的参数为指定类型的执行方法 |Advice应用123456&gt;public class AspectBiz {public void biz() {System.out.println(&quot;AspectBiz biz.&quot;);&gt;//throw new RuntimeException();}&gt;}1234567891011121314151617&gt;import org.aspectj.lang.ProceedingJoinPoint;&gt;public class MoocAspect {public void before() {System.out.println(&quot;MoocAspect before.&quot;);}public void afterReturning() {System.out.println(&quot;MoocAspect afterReturning.&quot;);}public void afterThrowing() {System.out.println(&quot;MoocAspect afterThrowing.&quot;);}public void after() {System.out.println(&quot;MoocAspect after.&quot;);}&gt;}12345678910&gt;&lt;bean id=&quot;moocAspect&quot; class=&quot;com.imooc.aop.schema.advice.MoocAspect&quot;&gt;&lt;/bean&gt; &gt;&lt;bean id=&quot;aspectBiz&quot; class=&quot;com.imooc.aop.schema.advice.biz.AspectBiz&quot;&gt;&lt;/bean&gt; &lt;aop:config&gt;    &lt;aop:aspect id=&quot;moocAspect4op&quot; ref=&quot;moocAspect&quot;&gt;       &lt;aop:pointcut expression=&quot;execution(* com.imooc.aop.schema.advice.biz.*Biz(..))&quot; id=&quot;moocPiontcut&quot;/&gt;       &lt;aop:before method=&quot;before&quot; pointcut-ref=&quot;moocPiontcut&quot;/&gt; &lt;aop:after-returning method=&quot;afterReturning&quot; pointcut-ref=&quot;moocPiontcut&quot;/&gt; &lt;aop:after-throwing method=&quot;afterThrowing&quot; pointcut-ref=&quot;moocPiontcut&quot;/&gt; &lt;aop:after method=&quot;after&quot; pointcut-ref=&quot;moocPiontcut&quot;/&gt;    &lt;/aop:aspect&gt;&gt;&lt;/aop:config&gt; 12345&gt;// 输出&gt;MoocAspect before.&gt;AspectBiz biz.&gt;MoocAspect afterReturning.&gt;MoocAspect afterAround advice通知方法的第一个参数必须是ProceedingJoinPoint类型 1&lt;aop:around method=&quot;around&quot; pointcut-ref=&quot;moocPiontcut&quot;/&gt;1234567891011public Object around(ProceedingJoinPoint pjp) {Object obj = null;try {System.out.println(&quot;MoocAspect around 1.&quot;);obj = pjp.proceed();System.out.println(&quot;MoocAspect around 2.&quot;);} catch (Throwable e) {e.printStackTrace();}return obj;}Advice parameters12&gt;&lt;aop:around method=&quot;aroundInit&quot;            pointcut=&quot;execution(* com.imooc.aop.schema.advice.biz.AspectBiz.init(String, int)) and args(bizName, times)&quot;/&gt; 12345678&gt;public class AspectBiz {public void biz() {System.out.println(&quot;AspectBiz biz.&quot;);}public void init(String bizName, int times) {System.out.println(&quot;AspectBiz init : &quot; + bizName + &quot;   &quot; + times);}&gt;}123456789101112131415&gt;import org.aspectj.lang.ProceedingJoinPoint;&gt;public class MoocAspect {public Object aroundInit(ProceedingJoinPoint pjp, String bizName, int times) {System.out.println(bizName + &quot;   &quot; + times);Object obj = null;try {System.out.println(&quot;MoocAspect aroundInit 1.&quot;);obj = pjp.proceed();System.out.println(&quot;MoocAspect aroundInit 2.&quot;);} catch (Throwable e) {e.printStackTrace();}return obj;}&gt;}Introduction应用允许一个切面声明一个实现指定接口的通知对象，并且提供了一个接口实现类来代表这些对象 由\中的\元素声明该元素用于声明所匹配的类型拥有一个新的parent（因此得名）schema-defined aspects只支持singleton model 12345678&lt;aop:config&gt;    &lt;aop:aspect id=&quot;moocAspectAOP&quot; ref=&quot;moocAspect&quot;&gt;        &lt;aop:declare-parents         types-matching=&quot;com.imooc.aop.schema.advice.biz.*(+)&quot;            implement-interface=&quot;com.imooc.aop.schema.advice.Fit&quot;            default-impl=&quot;com.imooc.aop.schema.advice.FitImpl&quot;/&gt;    &lt;/aop:aspect&gt;&lt;/aop:config&gt;12345678910111213141516171819public interface Fit {void filter();}public class FitImpl implements Fit {@Overridepublic void filter() {System.out.println(&quot;FitImpl filter.&quot;);}}@Test public void testFit0{     Fit fit=(Fit)super.getBean(&quot;aspectBiz&quot;);     fit.filter0; }// 输出FitImpl filter.Advisorsadvisor就像一个小的自包含的方面，只有一个advice 切面自身通过一个bean表示，并且必须实现某个advice 接口，同时，advisor也可以很好的利用AspectJ的切入 点表达式 Spring通过配置文件中\元素支持advisor 实际使用中，大多数情况下它会和transactional advice 配合使用 为了定义一个advisor的优先级以便让advice可以有序， 可以使用order属性来定义advisor的顺序 123456789101112131415161718192021222324252627282930&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;beans xmlns=&quot;http://www.springframework.org/schema/beans&quot;    xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot;    xmlns:context=&quot;http://www.springframework.org/schema/context&quot;    xmlns:aop=&quot;http://www.springframework.org/schema/aop&quot;    xsi:schemaLocation=&quot;http://www.springframework.org/schema/beans         http://www.springframework.org/schema/beans/spring-beans.xsd          http://www.springframework.org/schema/context        http://www.springframework.org/schema/context/spring-context.xsd        http://www.springframework.org/schema/aop         http://www.springframework.org/schema/aop/spring-aop.xsd&quot;&gt;&lt;context:component-scan base-package=&quot;com.imooc.aop.schema&quot;&gt;    &lt;/context:component-scan&gt;&lt;aop:config&gt;&lt;aop:aspect id=&quot;concurrentOperationRetry&quot;                     ref=&quot;concurrentOperationExecutor&quot;&gt;&lt;aop:pointcut id=&quot;idempotentOperation&quot;            expression=&quot;execution(* com.imooc.aop.schema.advisors.service.*.*(..)) &quot; /&gt;&lt;aop:around pointcut-ref=&quot;idempotentOperation&quot;                         method=&quot;doConcurrentOperation&quot; /&gt;&lt;/aop:aspect&gt;&lt;/aop:config&gt;&lt;bean id=&quot;concurrentOperationExecutor&quot;           class=&quot;com.imooc.aop.schema.advisors.ConcurrentOperationExecutor&quot;&gt;&lt;property name=&quot;maxRetries&quot; value=&quot;3&quot; /&gt;&lt;property name=&quot;order&quot; value=&quot;100&quot; /&gt;&lt;/bean&gt; &lt;/beans&gt;1234567891011121314151617181920212223242526272829303132333435import org.aspectj.lang.ProceedingJoinPoint;import org.springframework.core.Ordered;import org.springframework.dao.PessimisticLockingFailureException;public class ConcurrentOperationExecutor implements Ordered {private static final int DEFAULT_MAX_RETRIES = 2;private int maxRetries = DEFAULT_MAX_RETRIES;private int order = 1;public void setMaxRetries(int maxRetries) {this.maxRetries = maxRetries;}public int getOrder() {return this.order;}public void setOrder(int order) {this.order = order;}public Object doConcurrentOperation(ProceedingJoinPoint pjp) throws Throwable {int numAttempts = 0;PessimisticLockingFailureException lockFailureException;do {numAttempts++;System.out.println(&quot;Try times : &quot; + numAttempts);try {return pjp.proceed();} catch (PessimisticLockingFailureException ex) {lockFailureException = ex;}} while (numAttempts &lt;= this.maxRetries);System.out.println(&quot;Try error : &quot; + numAttempts);throw lockFailureException;}}123456789101112import org.springframework.dao.PessimisticLockingFailureException;import org.springframework.stereotype.Service;@Servicepublic class InvokeService {public void invoke() {System.out.println(&quot;InvokeService ......&quot;);}public void invokeException() {throw new PessimisticLockingFailureException(&quot;&quot;);}}1234567@Test public void testSave0{     InvokeService service=super.getBeaninvokeService&quot;);     service.invoke0;     System.out printin0.    service.invokeException0;}123456789// 输出Try times:1 InvokeService...     Try times:1 Try times:2 Try times:3 Try times:4 Try error:4 </content>
	</entry>
	<entry>
		<title>机器学习_一元函数微积分(1)</title>
		<url>/article/10706.html</url>
		<content>写在前面，本系列主要是对下面这本书做的学习笔记常用数学符号的 LaTeX 表示方法Markdown 常用数学符号和公式极限与连续极限是微积分中最基本的概念，也是理解导数与积分等概念的基础。可数集和不可数集初等数学已经对元素数有限的集合进行的系统系统阐述，对无限集有些概念和规则不再适用，即使是常用的自然数集和实数集其性质，也需要重新定义基数或势集合的元素数量称为其基数或势，记为，对于集合，其基数为基数为有限值的集合称为有限集；基数为无限值的集合称为无限集对于两个有限集，如果集合是集合的真子集，即，则有，而无限集的基数为，因此不能直接使用这个规则进行基数比较考虑正整数集，令集合为所有正奇数组成的集合，集合为所有正偶数组成的集合。因此有但这并不意味着双射函数对于集合和，如果集合的任意元素，在集合都有唯一的元素通过某种映射关系与之对应，即存在如下的双射函数(一对一映射函数) 则称这两个集合的基数相等【1】正整数集与正偶数集的基数相等，因为它们的元素之间存在如下的双射关系【2】集合和集合的基数也相等，因为【3】集合与整个实数集合等势，因为集合实数集合的元素间存在如下的双射函数此函数可以将区间拉升至，且存在反函数图 集合A到实数集的映射【4】区间与区间是等价的，因为存在双射函数反过来实数与区间也是等价的，因为存在双射函数此函数称为logistic函数或sigmoid函数，有着优良的性质，在机器学习与深度学习中被广泛使用。可数集和不可数集无限集，可进一步分为可数集与不可数集，可数集中的每个元素可以用正整数进行编号，即与正整数等势正整数是可数集，它的每个元素可以写成可数集定义：如果存在从正整数集到集合$A$的双射关系，则集合是可数的整数集是可数的，因为存在双射函数，将其映射到正整数集有理数集也是可数集，因为所有的有理数都可以写成两个整数相除的形式整数集和有理数集，这里的离散和可数等价，任何可数集在数轴上的”长度”为0。不可数集：无理数、实数集和长度不为0的实数区间都是不可数的，其中的元素是连续的，它们在数轴上是稠密的或者说连续的，如，圆周率以及以自然对数底数都是无理数，不可数集在数轴上的”长度”不为0。可数和不可数的概念将用于定积分中函数的可积性，以及概率论中的离散型和连续型随机变量等概念中数列的极限极限概念与定义数列极限数列的极限反映了当数列元素下标趋向于时数列项取值的趋势数列极限定义: 对于数列以及某一实数，如果对于任意给定的都存在正整数，使得对于任意满足的都有下面的不等式成立则称此数列的极限为，或称其收敛于，数列的极限记为直观解释: 当增加时数列的值无限趋近于，可接近到任意指定的程度，由控制，当数列的极限不存在，则称该数列发散，如果数列的极限存在，则其值必唯一证明数列极限存在且为某一值的方法是证明公式(1)成立并非所有的数列都存在极限数列，当时，数列的值在-1与1之间振荡数列，当时，数列的值趋向于，极限也不存在数列极限的四则运算计算极限，根据数列极限的四则运算有极限存在判定法则单调收敛定理夹逼定理数列上界和下界上界: 对于数列，如果它的任意元素都满足，则称为数列的上界，上界不唯一下界: 对于数列，如果它的任意元素都满足，则称为数列的下界单调收敛定理: 如果数列单调递增且存在上界，则极限存在，如果数列单调递减且有下界，则极限存在，即单调有界的数列收敛根据单调收敛定理，可以得到微积分中一个重要极限，对于数列的极限为图 单调有界的数列收敛其中为自然对数的底数，约为，是数学中最重要的常数之一证明: 证明数列有界和单调递增，收敛于数列的极限为这个极限可以用来表示对个样本进行次有放回等概率抽样，当样本量趋于无穷大时，每个样本一次都没被抽中的概率单调有界是数列收敛的充分条件而非必要条件图 不满足单调有界条件但收敛的数列夹逼定理: 如果对于有且，则有，这一结论称为夹逼定理如果数列无界，则必定发散，有界是数列收敛的必要条件而非充分条件，如数列有界但不收敛函数的极限函数极限的严格定义由法国数学家柯西给出: 即当前广泛使用的$\epsilon \mbox{-} s $定义领域: 点的领域是指满足不等式的所有构成的集合，即区间，称为领域的半径去心领域: 去心领域是指满足上式且去掉点的点构成的集合函数极限: 对于函数，如果对于任意的，均存在的去心领域，使得去心领域内所有的都有则称函数在点处的极限为，函数在点处的极限记为直观解释: 当自变量的值无限接近于时，函数值无限接近于即在内的函数值都在区间内证明函数极限的方法和数列类似，核心是证明在区新领域内使得公式(1)成立左右极限一维数轴上有两个方向，变量可以从左侧趋近于，也可以从右侧趋近于因此函数的极限分为左极限和右极限，左极限和右极限分别记为夹逼定理假设是包含点的区间，、、为定义在该区间上的函数，如果对所有属于但不等于点的点都有，且则有，这一结论称为夹逼定理几个重要极限令，则，因此有因为有最后可以得到函数的连续性与间断点连续性函数的连续型通过极限定义，是最基本的性质之一函数连续的直观表现: 如果自变量的改变很小，则因变量的改变也非常小，函数值不会突然发生跳跃函数连续的定义如果函数满足，则称它在点处连续几何解释函数连续在该点处的函数曲线没有”断”连续的重要性质基本初等函数在其定义域内是连续的，包括多项式函数、有理分式函数、指数函数、对数函数、三角函数、反三角函数绝对值函数在其定义域内也是连续的由基本初等函数经过有限次四则运算和复合而形成的函数，在其定义域内连续，这样的函数被称为初等函数如果函数和在定义域内连续，则复合函数在定义域内连续第一类间断点第二类间断点跳跃间断点跳跃间断点: 函数在处的左极限和右极限都存在，但不相等例子:  其中为跳跃间断点图 跳跃间断点可去间断点可去间断点: 或者左右极限相等，但不等于该点处的函数值例子:  其中为可去间断点函数在点处的左极限和右极限至少有一个不存在例子: 正切函数，在处极限值不存在，为第二类间断点图 正切函数的第二类间断点例子: 反比例函数，为函数的第二类间断点，在该点处函数的极限不存在图 反比例函数的第二类间断点连续函数的性质连续函数具有很多优良的性质闭区间上的连续函数一定存在极大值和极小值，使得对于该区间任意的有开区间上的连续函数则不能保证存在极大值和极小值，如反比例函数介值定理介值定理： 如果函数在闭区间内连续，是介于和之间的一个数，则存在中的某个点，使得，考虑下图图  介值定理对于区间，在左右端点处的函数值分别为和，对于任意的，均存在至少一个点，使得如果，则曲线必然存在一点使得，这就是方程的根，事实上，满足此条件介值定理几何意义函数在区间内一定与直线至少有一个交点，其中介于和之间介值定理保证至少存在一点使得，满足此条件的可能有多个连续性假设机器学习与深度学习算法所使用的绝大多数模型，假设函数是连续函数，以保证输入变化小的变化不至于导致预测值的突变，这称为连续性假设，连续性通常能够保证机器学习算法有更好的泛化性能上确界与下确界上确界与下确界可看作是集合最大值与最小值的推广上确界下确界上确界也称为最小上界，对于的非空子集，如果该集合中的任意元素均有，即是集合的一个上界，且满足吃不等是此不等式条件的最小值，则为集合的上确界，记为如果满足此条件的不存在，则称为集合的上确界为，如果集合的上确界存在，则必定唯一如果集合的元素存在最大值，则最大值为其上确界，如闭区间的上确界为1集合可能不存在最大值，如开区间，其上确界为，但不是集合的最大值例子: 集合是有限集，其上确界为集合元素的最大值，其值为集合是无限可数集，并且该数列单调递增，其上确界为数列的极限集合是无限不可数集，其上确界为该函数的极大值充分必要条件集合存在上确界的充分必要条件是集合有上界，实数集的任意非空有界子集均存在上确界下确界: 下确界也称为最大下界，是集合的一个下界，即对中的任意元素，均有，且是最大的下界，则称为的下确界，记住$inf(S)$对于闭区间，下确界为，对于开区间，其下确界也为例子:集合的下确界为，在时取得集合的下确界为，是函数的最小值李普希茨连续李普希茨连续李普希茨连续是比连续更强的条件，不但保证了函数值不间断，而且限定了函数的变化速度给定函数，如果对于区间任意两点、都存在常数使得下面的不等式成立则称函数在区间内满足李普希茨条件，也称为李普希茨连续使得公式(1)成立的最小值称为李普希茨常数，其值与具体的函数有关如果，则称函数为压缩映射几何意义在任意两点，处函数割线斜率的绝对值均不大于在任一点处，曲线均夹在直线与之间，如图所示图 李普希茨连续性向右上倾斜的直线与想做上倾斜的直线分别为与例子一次函数在内是李普希茨连续，因为对于任意的和都有因此该函数李普希茨连续且李普希茨常数为二次函数在内不是李普希茨连续，因为对于内任意的和都有显然不存在常数使得任意和都满足，因此该函数不是李普希茨连续函数在区间内李普希茨连续对于区间内任意的和都有区间内不是李普希茨连续，对于该区间内任意的和都有当时，不存在常数满足机器学习中的意义李普希茨连续要求函数在区间上不能有超过线性的变化速度，对于分析和确保机器学习算法的稳定性有重要的作用无穷小量考虑函数极限值为的情况定义: 如果函数在的某去心领域内，有定义且则称为的无穷小量虽然它们的极值均为0，但它们之间比值的极限却有几种情况高阶无穷小: ，该比值也是无穷小量，例如同阶无穷小(等价无穷小): ，比值的极限为非$0$的有界变量，例如低阶无穷小: ，比值的极限为无界变量(称为无穷大量)，例如其中高阶无穷小记为，等价无穷小记为直观来看这些比值反映了无穷小接近于0时的变化速度快慢典型的等价无穷小()等价无穷小在计算极限时起着重要的作用导数与微分导数是微分学中的核心概念，它决定了可导函数的基本性质，包括单调性、极值与凹凸性。在机器学习中，绝大多数算法可以归纳为求解最优化的问题，对于连续型优化问题在求解时一般需要使用导数一阶导数定义导数定义函数的自变量变化值趋向于时，函数值的变化量与自变量变化量比值的极限，在点处的导数为如果式(1)的极限存在，则称函数在点$x$处可导除了用表示之外，导数也可写成，即上面的定义也可以写成另一种形式左右导数类似于极限，导数也可分为左导数和右导数左导数是从左侧趋向于时的极限右导数为自变量从右侧趋向于时的极限函数可导的充分必要条件是左右导数均存在且相等，其必要条件是函数连续如果导数不存在，则称函数不可导几何和物理意义几何意义函数在定义域 内所有点处的导数值构成的函数称为导函数，简称函数导数的几何意义是函数在点处切线的斜率，反映了函数值在此点处变化的快慢函数在处，根据导数的定义有因此在该点处的切线斜率为，由于切线经过，因此切线的方程为，即，下图为该曲线和对应的切线图  导数的几何意义物理意义导数典型的物理意义是瞬时速度差分公式单侧差分公式如果的值接近于，则在点处的导数可以用下面的公式近似计算称为单侧差分公式中心差分公式根据导数的定义有其中为接近于的正数，这称为中心差分公式，用于数值计算导数值，因此可用下面的公式近似计算点处的一阶导数值基本函数的求导下表)列出了各种基本初等函数的求导公式表 基本函数的求导公式基本函数求导公式幂函数指数函数指数函数三角函数三角函数三角函数三角函数对数函数反三角函数反三角函数反三角函数证明四则运算的求导公式基本运算求导公式加法减法数乘乘法除法倒数机器学习中常用的函数logistic函数可用作神经网络的激活函数softplus函数，可看作是函数Relu即$max(0,x)$的光滑近似，该函数的导数为logistic函数函数的导数为：Relu函数可用作神经网络的激活函数，在$0$处该函数不可导，在该点处左导数为0，右导数为1去掉0点，该函数的导数为：绝对值函数，在$0$点处改函数不可导，左导数为-1，右导数为1，去掉该点，此函数的导数为绝对值函数常用于构造机器学习算法训练目标函数的正则化项sgn符号函数，在$0$点处不连续，因此不可导，去掉该点，此函数在所有点处的导数均为0符号函数常用于分类器的预测函数，表示二值化的分类结果，在支持向量机、logstic回归中都被使用如果一个函数所有不可导点的集合为有限集或无限可数集，则称该函数几乎处处可导，本节中的绝对值函数、Relu函数、符号函数均是几乎处处可导的函数，处处可导保证了在训练时能够使用梯度下降法求解函数的极值高阶导数定义对导数连续求导可以得到高阶导数，二阶导数的是一阶导数的导数，记为n阶导数记为计算$f(x) = x^{m}$的n阶导数其一阶导数为$f’(x) = mx^{m-1}$，二阶导数为$f’’(x) = m(m-1)x^{m-2}$，以此类推，n阶导数为如果$m=n$，则$f^{(n)}(x) = n!$计算$f(x) = \frac {1}{1-x}$的n阶导数计算$f(x) = ln(1+x)$的n阶导数计算$f(x) = e^x$的n阶导数计算f(x)= sin(x)的n阶导数根据此规律有类似地，对于余弦函数$f(x)=cos(x)$有二阶导数的物理意义是加速度，如果$f(t)$为位移函数，其二阶导数为$t$时刻的加速度其中$a(t)$为$t$时刻的加速度，$v(t)$为$t$时刻的速度在python语言中，sympy库提供了求导的功能123456from sympy import *x = symbols('x')r = diff(cos(x), x)print(r)&gt;&gt;&gt;&gt; -sin(x)微分函数$y=f(x)$在某一区间上有定义，对于区间内的点$x_0$，当$x$变为$x_0 + \Delta x$时，如果函数的增量$\Delta y = f(x_0 + \Delta)-f(x_0)$可以表示成其中$A$是不依赖于$\Delta x$的常数，$o(\Delta x)$是$\Delta x$的高阶无穷小，则称函数在$x_0$处可微$A \Delta x$称为函数在$x_0$处的微分，记为$dy$，即$dy = A \Delta x$，$dy$为$\Delta y$的线性主部通常把$\Delta x$称为自变量的微分，记为$dx$，如果函数可微，则导数和微分的关系为$dy = f(x)dx$微分的几何意义是在点$(x_0, f(x_0))$处自变量增加$\Delta x$时切线函数$y = f’(x_0)(x-x_0)+f(x_0)$举例说明微分的计算，对于函数$y=sin(x^2)$其导数为$y’ = 2x \cdot cos(x^2)$，于是可以得到其微分为$dy = 2x \cdot cos(x^2) dx$考虑下复合函数的微分，对于复合函数$z = f(y), y=g(x)$根据复合函数求导公式有$\frac{dz}{dx}=f’(y)g’(x)$，因此其微分为$dz = f’(y)g’(x)dx$由于有$dy = g’(x)dx$，因为也可以写成$dz = f’(y)dy$导数与函数的单调性导数决定了可导函数的重要性质，包括单调性与极值，是研究函数性质的有力工具，由于导数是函数变化率的极限因此如果在$x$点处它的值为正，则在该点处自变量增大时函数值也增大，如果为负，则自变量增大时函数值减小假设函数$f(x)$在区间$[a,b]$内连续，在区间$(a,b)$内可导。如果在$(a,b)$内$f’(x) \gt 0$，则函数在$[a,b]$内单调递增；如果在$(a,b)$内$f’(x) \t 0$，则函数在$[a,b]$内单调递减，可以通过拉格朗日中值定理证明对于函数$f(x) = x^3 + 4x^2 -10x +1$其一阶导数为$f’(x) = 3x^2 +8x -10$，方程$f’(x)=0$的根为$x = \frac{-8 \pm \sqrt{64+120}}{6} = \frac{-4 \pm \sqrt{46}}{3}$在区间$(- \infty , \frac{-4-\sqrt{46}}{3} )$内$f’(x) \gt 0$，函数单调递增在区间$(\frac{-4-\sqrt{46}}{3} , \frac{-4+\sqrt{46}}{3} )$内$f’(x) \lt 0$，函数单调递减在区间$(\frac{-4+\sqrt{46}}{3} , +\infty )$内$f’(x) \gt 0$，函数单调递增因此在$x= \frac{-4-\sqrt{46}}{3}$处函数有极大值，在$ x = \frac{-4+\sqrt{46}}{3}$处函数有极小值，函数的曲线为利用导数可以证明某些不等式，其思路是证明函数在某一区间内单调，因此在区间的端点处取得极值证明当$x&gt;0$时不等式$ln(x) \le x-1 $成立构造函数$f(x) = x-1-ln(x)$，其导数为$f’(x) = 1- \frac{1}{x}$当$x \lt 1$时有$f’(x) \lt 0$，函数单调递减当$x \gt 1$时有$f’(x) \gt 0$，函数单调递增1是函数的极小值点，且$f(1)=0$，因此不等式成立极值判别法则极值定义，这里指的是局部极值函数$f(x)$在区间$I$内有定义，$x_0$是该区间内的一个点如果存在$x_0$的一个$\delta $领域，对于该领域内任意一点$x$都有$f(x_0) \ge f(x)$，则称$x_0$是该函数的极大值如果领域内任意一点$x$都有$f(x_0) \le f(x)$，则称$x_0$是该函数的极小值极大值和极小值统称为极值如果存在$x_0$的一个$\delta $领域，对于该领域内任意一点$x$都有$f(x_0) \gt f(x)$，则称$x_0$是该函数的严格极大值如果领域内任意一点$x$都有$f(x_0) \lt f(x)$，则称$x_0$是该函数的严格极小值费马定理下的充分条件二阶导数下的充分条件费马定理假设函数$f(x)$在$x_0$点处可导，如果在$x_0$点处取得极值，则定有$f’(x) = 0$，这一结论称为费马定理，它给出了可导函数取极值的一阶必要条件导数等于0的点称为函数的驻点(Stationary Point)最优化算法一般通过寻找函数的驻点而求解函数的极值问题，导数为0是函数取得极值的必要条件而非充分条件下面给出在费马定理下的函数取极值的充分条件，假设函数$f(x)$在$x_0$点的一个领域内可导，且有$f’(x) = 0$，考察在$x_0$去心领域内的函数值符号，有三种情况在$x_0$的左侧$f’(x) \gt 0$，在$x_0$的右侧$f’(x) \lt 0$，则函数在$x_0$取严格极大值在$x_0$的左侧$f’(x) \lt 0$，在$x_0$的右侧$f’(x) \gt 0$，则函数在$x_0$取严格极小值在$x_0$的左侧和右侧$f’(x)$同号，则$x_0$不是极值点对于第一种情况，函数在$x_0$的左侧单调增，在右侧单递减，因此$x_0$是极大值点；对于第二种情况，函数在$x_0$的左侧单递减，在右侧单递增，因此$x_0$是极小值点；对于第三种情况，函数在$x_0$的两侧均单调增或者单调减，因此$x_0$不是极值点根据此结论，求函数的极值点的方法首先是求解方程$f’(x)=0$得到函数的所有驻点，然后判断驻点两侧一阶导数值的符号利用二阶导数的信息给出函数极值的充分条件，假设$x_0$为函数的驻点，且在该点处二阶可导，对于驻点处二阶导数的符号，可分为三种情况$f’’(x) \gt 0$，则$x_0$为函数$f(x)$的严格极小值点$f’’(x) \lt 0$，则$x_0$为函数$f(x)$的严格极大值点$f’’(x) = 0$，则不定，$x_0$可能是极值点也可能不是极值点，需进一步讨论情况一：如果n是偶数，则$x_0$是极值点，当$f^{(n)}{x_0} \gt 0$是$f(x)$de 严格极小值点，当$f^{(n)}{x_0} \lt 0$是$f(x)$de 严格极大值点情况二：如果n是技术，则$x_0$不是$f(x)$的极值点该充分条件可以用泰勒公式证明例子考虑函数$f(x)=x^2$其一阶导数为$f’(x)=2x$，令$f’(x)=0$可以解得其驻点为$x=0$，由于$f’’(x)=2 \gt 0$，该点是函数的极小值点对于函数$f(x) = -x^2$其一阶导数为$f’(x) = -2x $，令$f’(x)=0$可以解得其驻点为$x=0$，由于$f’’(x)=-2 \lt 0$，该点是函数的极大值点对于函数$f(x)=x^3$其一阶导数为$f’(x)=3x^2$，令$f’(x)=0$可以解得其驻点为$x=0$其二阶导数为$f’’(x)=6x$其三阶导数为$f^{(3)}(x)=6$由于$f’’(0)=0$，$f^{(3)}(0)=6$，因此该点不是极值点最后一种情况称为鞍点，会导致数值优化算法，如梯度下降无法找到真正的极值点导数与函数的凹凸性凹凸性是函数的另一个重要性质，它与单调性共同决定了函数曲线的形状凸函数定义凹函数定义对于函数$f(x)$在它的定义域内有两点$x$、$y$，如果对于任意的实数$ 0 \le \theta \le 1$都满足如下不等式即函数为凸函数，从图像上看，如果函数为凸函数，那么它是向下凸的，用直线连接函数上任何两点(即两点的割线)，线段上的点都在函数曲线的上方，如图所示如果满足不等式则称为凹函数，需要强调的是，这里遵循的是欧美国家的定义，与国内某些高等数学教材定义刚好相反证明凸函数定义对于$ \forall x,y $以及$ 0 \le \theta \le 1$有显然有因此有如果把式中的等号去掉，则称函数式严格凸函数，类似的可以定义严格凹函数二阶可导函数是凸函数和凹函数的充分必要条件假设$f(x)$在区间$[a,b]$内连续，在区间$(a,b)$内一阶导数和二阶导数均存在如果在$(a,b)$内$f’’(x) \ge 0$，则称函数在$[a,b]$内为凸函数如果在$(a,b)$内$f’’(x) \le 0$，则称函数在$[a,b]$内为凹函数如果在$(a,b)$内$f’’(x) &gt; 0$，则称函数在$[a,b]$内为严格凸函数如果在$(a,b)$内$f’’(x) &lt; 0$，则称函数在$[a,b]$内为严格凹函数拐点函数凹凸性的分界点称为拐点，如果函数二阶可导，则在拐点处有$f’’(x)=0$，且在拐点处两侧二阶导数值异号对于函数$f(x)=x^3$，其二阶导数为$6x$，$0$点处二阶导数值为$0$，且在$0$点两侧的二阶导数值异号，因此$0$为其拐点凸函数有良好的的性质，可以保证优化算法找到函数的极小值点</content>
	</entry>
	<entry>
		<title>数据库相关操作</title>
		<url>/article/59006.html</url>
		<content>Elasticsearch数据库环境配置安装环境pip install elasticsearch==7.6.0EsDao包装类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186# -- coding: utf-8 --&quot;&quot;&quot;@version: v1.0@author: huangyc@file: EsDao.py@Description: Es统一操作类@time: 2020/4/27 10:22&quot;&quot;&quot;from elasticsearch.helpers import bulkfrom elasticsearch import Elasticsearchimport pandas as pdclass EsDao(object):    &quot;&quot;&quot;    ElasticSearch的数据操作类    &quot;&quot;&quot;    # 查询批次大小    DEFAULT_BATCH_SIZE = 1000    # 写入批次大小    BULK_BATCH_SIZE = 10000    def __init__(self, hosts, timeout=3600*24):        self.hosts = hosts        self.timeout = timeout        self.es = Elasticsearch(hosts, timeout=self.timeout)    def save_data_list(self, index_name, data_list):        &quot;&quot;&quot;        保存数据列表到es的指定索引中        :param index_name: 索引名称        :param data_list: 数据列表，列表元素代表一行数据，元素类型为dict        :return:        &quot;&quot;&quot;        bulk_data_lst = [            data_list[i:i + self.BULK_BATCH_SIZE]            for i in range(0, len(data_list), self.BULK_BATCH_SIZE)        ]        if len(data_list) &gt; 0 and '_id' in data_list[0]:            for bulk_data in bulk_data_lst:                actions = [{                    &quot;_index&quot;: index_name,                    &quot;_type&quot;: index_name,                    &quot;_id&quot;: data.pop(&quot;_id&quot;),                    &quot;_source&quot;: data                }                    for data in bulk_data                ]                bulk(self.es, actions, index=index_name, raise_on_error=True)        else:            for bulk_data in bulk_data_lst:                actions = [{                    &quot;_index&quot;: index_name,                    &quot;_type&quot;: index_name,                    &quot;_source&quot;: data                }                    for data in bulk_data                ]                bulk(self.es, actions, index=index_name, raise_on_error=True)    def is_index_exists(self, index_name):        &quot;&quot;&quot;        判断指定索引是否存在        :param index_name: 索引名称        :return:        &quot;&quot;&quot;        return self.es.indices.exists(index=index_name)    def delete_by_query(self, index_name, query_body):        &quot;&quot;&quot;        按查询结果删除数据        :param index_name:        :param query_body:        :return:        &quot;&quot;&quot;        return self.es.delete_by_query(index_name, query_body)    def clear_index_data(self, index_name):        &quot;&quot;&quot;        清空指定索引的数据        :param index_name:        :return:        &quot;&quot;&quot;        return self.delete_by_query(            index_name=index_name,            query_body={                &quot;query&quot;: {                    &quot;match_all&quot;: {}                }            }        )    def save_df_data(self, index_name, df):        &quot;&quot;&quot;        保存pandas的DataFrame到es的指定索引中        :param index_name: 索引名称        :param df: 要保存的dataframe        :return:        &quot;&quot;&quot;        col_lst = df.columns.tolist()        dic_lst = [dict([(c, v) for c, v in zip(col_lst, r)]) for r in df.values.tolist()]        self.save_data_list(index_name=index_name, data_list=dic_lst)    def create_index(self, index_name, mapping_properties):        &quot;&quot;&quot;        创建索引        :param index_name: 索引名称        :param mapping_properties: 索引mapping中的属性列表        :return:        &quot;&quot;&quot;        if not self.es.indices.exists(index=index_name):            mapping = {                &quot;mappings&quot;: {                    index_name: {                        &quot;properties&quot;: mapping_properties                    }                }            }            res = self.es.indices.create(index=index_name, body=mapping)            if res is not None and 'acknowledged' in res:                return res.get('acknowledged')        return False    def _search_with_scroll(self, index_name, query_body):        if &quot;size&quot; not in query_body:            query_body[&quot;size&quot;] = self.DEFAULT_BATCH_SIZE        response = self.es.search(            index=index_name,            body=query_body,            search_type=&quot;dfs_query_then_fetch&quot;,            scroll=&quot;120m&quot;,            timeout=&quot;60m&quot;        )        scroll_id = response[&quot;_scroll_id&quot;]        while True:            sources = [doc[&quot;_source&quot;] for doc in response[&quot;hits&quot;][&quot;hits&quot;]]            if len(sources) == 0:                break            yield sources            response = self.es.scroll(scroll_id=scroll_id, scroll=&quot;60m&quot;)    def query_for_df(self, index_name, query_body):        &quot;&quot;&quot;        执行查询并获取pandas.DataFrame格式的返回值        :param index_name: 索引名称        :param query_body: 查询条件        :return:        &quot;&quot;&quot;        sources = []        for sub_source in self._search_with_scroll(index_name=index_name, query_body=query_body):            sources.extend(sub_source)        return pd.DataFrame(sources)    def query_for_df_with_batch(self, index_name, query_body, batch_size=DEFAULT_BATCH_SIZE):        &quot;&quot;&quot;        按批次大小查询并返回pandas.DataFrame的generator格式的返回值        :param index_name: 索引名称        :param query_body: 查询条件        :param batch_size: 批次大小        :return:        &quot;&quot;&quot;        if &quot;size&quot; not in query_body:            query_body[&quot;size&quot;] = batch_size        for sub_source in self._search_with_scroll(index_name=index_name, query_body=query_body):            yield pd.DataFrame(sub_source)    def get_first_row_with_df(self, index_name):        &quot;&quot;&quot;        获取指定索引的首行数据，格式为pandas.DataFrame        可用于获取索引的元信息        :param index_name: 索引名称        :return:        &quot;&quot;&quot;        query_body = {            &quot;size&quot;: 1,            &quot;query&quot;: {                &quot;match_all&quot;: {}            }        }        for sub_source in self._search_with_scroll(index_name=index_name, query_body=query_body):            return pd.DataFrame(sub_source)使用案例123456789101112131415161718192021222324class TaskMeta:    '''    数据元类    '''    def __init__(self, text, doc_id, sentence_id, reg_lst, flag, has_reg, text_source=&quot;primitive&quot;):        self.text = text        self.doc_id = doc_id        self.sentence_id = sentence_id        self.reg_lst = reg_lst        self.flag = flag        self.has_reg = has_reg        self.text_source = text_source    def __repr__(self):        return f'{self.text} {self.doc_id} {self.sentence_id} {self.reg_lst} {self.flag} {self.has_reg} {self.text_source}'    def to_dict(self):        return {&quot;text&quot;: self.text,                &quot;doc_id&quot;: self.doc_id,                &quot;sentence_id&quot;: self.sentence_id,                &quot;reg_lst&quot;: self.reg_lst,                &quot;flag&quot;: self.flag,                &quot;has_reg&quot;: self.has_reg,                &quot;text_source&quot;: self.text_source}1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556def create_index(target_es_dao, index_name, mapping):    '''    创建es索引    :return: 是否创建成功    '''    if not target_es_dao.is_index_exists(index_name):        target_es_dao.create_index(index_name, mapping)    else:        target_es_dao.clear_index_data(index_name)        print(f&quot;索引{index_name}已存在, 已清除数据&quot;)def writer_fun(target_es_dao, target_index, sample_lst):    '''    写数据到es库    '''    df_sample_lst = []    [df_sample_lst.append(sample.to_dict()) for sample in sample_lst]    df_sample_lst = pd.DataFrame(df_sample_lst)    target_es_dao.save_df_data(target_index, df_sample_lst)    print(f'写入数据{len(sample_lst)}条')def es_cal_test():    # 获取连接    source_es_dao = EsDao(f&quot;http://{aug_config.SOURCE_IP}:{aug_config.SOURCE_PORT}/&quot;)    query_condition = {        &quot;query_string&quot;: {            &quot;default_field&quot;: &quot;has_reg&quot;,            &quot;query&quot;: &quot;true&quot;        }    }    query_body = {        &quot;query&quot;: query_condition    }    # 查询数据    datas = source_es_dao.query_for_df(index_name=aug_config.SOURCE_INDEX, query_body=query_body)    records = datas.to_dict(orient='record')    sample_lst = []    for record in records:        sample_lst.append(            TaskMeta(                text=record[&quot;text&quot;],                doc_id=record[&quot;doc_id&quot;],                sentence_id=record[&quot;sentence_id&quot;],                reg_lst=record[&quot;reg_lst&quot;],                flag=record[&quot;flag&quot;],                has_reg=record[&quot;has_reg&quot;]            )        )    # 创建索引    create_index(target_es_dao, aug_config.TARGET_INDEX, aug_config.MAPPING)    # 写入数据    writer_fun(target_es_dao, aug_config.TARGET_INDEX, sample_lst=sample_lst)if __name__ == '__main__':    es_cal_test()Oracle数据库Python操作Oracle数据库：cx_Oracle环境配置Linux上Python连接Oracle解决报错cx_Oracle.DatabaseError: DPI-1047安装库1pip install cx-Oracle链接库准备，需要将oci.dll、oraocci11.dll、oraociei11.dll复制到sitepackages路径下，oracle client下载链接，并配置到系统环境变量，链接中没有的自己去官网（win64、所有平台、linux64）注册一个账号下载对应的版本12-- 查看oracle版本SELECT * FROM v$version;没有配置会报如下的错：12345# Windows下报错cx_Oracle.DatabaseError: DPI-1047: Cannot locate a 64-bit Oracle Client library: &quot;D:\software\win_or# Linux下报错cx_Oracle.DatabaseError: DPI-1047: Cannot locate a 64-bit Oracle Client library: &quot;libclntsh.so: cannot open shared object file: No such file or directory&quot;. See https://cx-oracle.readthedocs.io/en/latest/user_guide/installation.html for helpwindows下安装完客户端后，配置oracle客户端的环境变量1D:\software\win_oracle_dlls\instantclient_11_2linux下可以使用rpm安装包安装1sudo rpm -ivh oracle-instantclient11.2-basic-11.2.0.4.0-1.x86_64.rpm然后将环境变量配置到/etc/profile1234567891011# 配置oracle客户端export ORACLE_CLIENT_HOME=/lib/oracle/11.2/client64export TNS_ADMIN=$ORACLE_CLIENT_HOMEexport LD_LIBRARY_PATH=$ORACLE_CLIENT_HOME/libexport ORABIN=$ORACLE_CLIENT_HOME/binPATH=$PATH:$ORABINexport PATHexport PATH=$ORACLE_HOME:$PATHexport PATH=$PATH:$HOME/bin:$ORACLE_CLIENT_HOME/bin其他类似找不到libclntsh.sod的错误，如果出现这个错误，请进行软连接挂载文件，让系统的路径能正确的获取到该文件，操作如下：123sudo sh -c &quot;/usr/lib/oracle/instantclient_11_1 &gt; /etc/ld.so.conf.d/oracle-instantclient.conf&quot;sudo ldconfigsql基础建表123456789--blob字段插入实例create table blob_table_tmp(  id number primary key,  blob_cl blob not null,clob_cl clob not null);insert into blob_table_tmp values(1,rawtohex('11111000011111'),'增加一条记录时，碰到插入blob类型数据出错');insert into blob_table_tmp values(3,rawtohex('4561888'),'增加一条记录时，碰到插入blob类型数据出错');insert into blob_table_tmp values(4,rawtohex('增加一条记录时333'),'增加一条记录时，碰到插入blob类型数据出错');查询获取连接123456789101112131415FINANCE_DB_HOST = &quot;192.168.x.x&quot;FINANCE_DB_PORT = 1521FINANCE_DB_USER = &quot;hyc&quot;FINANCE_DB_PASSWORD = &quot;123456&quot;FINANCE_DB_DB = &quot;ORCL&quot;class OracleConn():    config_path = ''    @staticmethod    def get_conn(conn_name, encoding=&quot;UTF-8&quot;):        conn_str = str(eval(&quot;%s_DB_USER&quot; % (OracleConn.config_path, conn_name))) + &quot;/&quot; + str(eval(&quot;%s.%s_DB_PASSWORD&quot; % (OracleConn.config_path, conn_name)))        conn_str += &quot;@&quot; + str(eval(&quot;%s_DB_HOST&quot; % (OracleConn.config_path, conn_name)))        conn_str += &quot;:&quot; + str(eval(&quot;%s_DB_PORT&quot; % (OracleConn.config_path, conn_name)))        conn_str += &quot;/&quot; + str(eval(&quot;%s_DB_DB&quot; % (OracleConn.config_path, conn_name)))        return ora.connect(conn_str, encoding=encoding, nencoding=encoding)读写数据库1234567891011121314151617181920212223242526def oracle_test():    # 获取数据库连接    conn = OracleConn.get_conn(&quot;FINANCE&quot;)    cur = conn.cursor()    # 查询数据    sql = &quot;select id,blob_cl,clob_cl from FINANCE.blob_table_tmp&quot;    datas = []    r = cur.execute(sql)    # 假设name是clob字段类型    [datas.append((gg[0], gg[1].read().decode('utf-8'), gg[2].read())) for gg in r]    # 写入数据    insert_sql = &quot;INSERT INTO new_table(id,new_name) VALUES (:ID,:NEW_NAME)&quot;    res = []    [res.append((data[0], data[1])) for data in datas]    cur.executemany(insert_sql, res)    cur.execute('commit')    cur.close()    conn.close()    print(&quot;写入结束&quot;)if __name__ == '__main__':    oracle_test()相关操作关于数据库的连接，查询和写入1234567891011121314151617181920212223242526272829303132333435import cx_Oracleclass Setting:    DB_USER = 'narutohyc'    DB_PASSWORD = 'hyc'    DB_IP = '192.168.0.1'    DB_PORT = ''    DB_SERVICE = 'dataBaseName'setting = Setting()def oracle_test():    # 获取数据库连接    conn = cx_Oracle.connect('%s/%s@%s/%s' % (setting.DB_USER, setting.DB_PASSWORD, setting.DB_IP, setting.DB_SERVICE), encoding='utf-8')    cur = conn.cursor()    # 查询数据    sql = &quot;select ID, name from hyc_database&quot;    datas = []    r = cur.execute(sql)    # 假设name是clob字段类型    [datas.append((gg[0], gg[1].read())) for gg in r]    # 写入数据    insert_sql = &quot;INSERT INTO new_table(id,new_name) VALUES (:ID,:NEW_NAME)&quot;    res = []    [res.append((data[0], data[1])) for data in datas]    cur.executemany(insert_sql, res)    cur.execute('commit')    cur.close()    conn.close()    print(&quot;写入结束&quot;)if __name__ == '__main__':    oracle_test()Postgresql数据库官方文档Documentation → PostgreSQL 16查询数据类型我终于学会了使用python操作postgresql保姆级 CentOS 7离线安装PostgreSQL 14教程易百_PostgreSQL教程离线安装数据库先从centos7-pg_14.2下载下载rpm包(微云下载centos7.6_PostgreSQL14.2)，或者直接官方下载安装教程安装，如果离线安装就下载rpm包12345# 离线安装执行以下命令安装rpm -ivh postgresql14-libs-14.2-1PGDG.rhel7.x86_64.rpmrpm -ivh postgresql14-14.2-1PGDG.rhel7.x86_64.rpmrpm -ivh postgresql14-server-14.2-1PGDG.rhel7.x86_64.rpmrpm -ivh postgresql14-contrib-14.2-1PGDG.rhel7.x86_64.rpm出现OSError: Python library not found: libpython3.6mu.so.1.0, libpython3.6m.so.1.0, libpython3.6.so.1.0, libpython3.6m.so的解决办法1yum install python3-devel创建数据库data和log文件夹123456# 创建数据库data和log文件夹mkdir -p /home/postgres/pgsql_datamkdir -p /home/postgres/pgsql_log# 创建日志文件touch /home/postgres/pgsql_log/pgsql.log授权给安装数据时自动创建的postgres用户12chown -R postgres:postgres /home/postgres/pgsql_datachown -R postgres:postgres /home/postgres/pgsql_log切换到安装数据时自动创建的postgres用户1su - postgres初始化数据库到新建数据目录1/usr/pgsql-14/bin/initdb -D /home/postgres/pgsql_data启动服务器(初始化数据库日志文件)123/usr/pgsql-14/bin/pg_ctl -D  /home/postgres/pgsql_data/ -l /home/postgres/pgsql_log/pgsql.log start# 查看状态/usr/pgsql-14/bin/pg_ctl -D /home/postgres/pgsql_data/ -l /home/postgres/pgsql_log/pgsql.log status切换到管理员开启端口并重启防火墙123su rootfirewall-cmd --zone=public --add-port=5432/tcp --permanentfirewall-cmd --reload修改配置文件实现远程访问vi /home/postgres/pgsql_data/postgresql.conf123456# 修改监听地址listen_addresses = '*'# 修改最大连接数（按需）max_connections = 1000# 修改密码认证password_encryption = md5修改可访问的用户IP段123vi /home/pgsql_data/pg_hba.conf（a进入编辑模式，esc退出编辑模式，:wq并按回车保存）IPV4下修改为或新增host    all             all             0.0.0.0/0               trustpostgres用户重启数据库服务12su - postgres/usr/pgsql-14/bin/pg_ctl -D  /home/postgres/pgsql_data/ -l /home/postgres/pgsql_log/pgsql.log restart数据库安装结束，管理员postgres，默认密码123456使用navicat连接pg库后新建数据库环境配置pip install psycopg2sql语法数据库连接12345678-- 获取数据库实例连接数select count(*) from pg_stat_activity;-- 获取数据库最大连接数show max_connections;-- 查询当前连接数详细信息select * from pg_stat_activity;-- 查询数据库中各个用户名对应的数据库连接数select usename, count(*) from pg_stat_activity group by usename;数据库信息1234567891011121314151617181920212223242526-- 查询数据库大小select pg_size_pretty (pg_database_size('pg_fac_stk'));-- 查询各表磁盘占用SELECT    table_schema || '.' || table_name AS table_full_name,    pg_size_pretty(pg_total_relation_size('&quot;' || table_schema || '&quot;.&quot;' || table_name || '&quot;')) AS sizeFROM information_schema.tables where table_name like 'finance_%'ORDER BY    pg_total_relation_size('&quot;' || table_schema || '&quot;.&quot;' || table_name || '&quot;') DESC;    -- 获取各个表中的数据记录数select relname as TABLE_NAME, reltuples as rowCounts from pg_class where relkind = 'r' order by rowCounts desc;-- 查看数据库表对应的数据文件select pg_relation_filepath('product');-- 查看数据库实例的版本select version();-- 分析评估SQL执行情况EXPLAIN ANALYZE SELECT * FROM t_cfg_opinfo;-- 获取数据库当前的回滚事务数以及死锁数select datname,xact_rollback,deadlocks from pg_stat_database;数据备份与恢复使用pgdump备份数据库pgdump是PostgreSQL官方提供的备份工具，可以将数据库的数据和架构保存到一个文件中，使用pgdump备份的优点包括：备份数据可以保持原有的结构和特性，还原时可以保证数据准确性备份文件可以跨平台传输，方便进行远程备份备份文件可以进行压缩，减小文件大小，方便传输和存储可以新建数据库，建几张表做测试1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162# 学生表CREATE TABLE students (    id SERIAL PRIMARY KEY,    name VARCHAR(100) NOT NULL,    gender VARCHAR(10) NOT NULL,    age INTEGER NOT NULL,    class VARCHAR(20) NOT NULL);# 学科表CREATE TABLE subjects (    id SERIAL PRIMARY KEY,    name VARCHAR(100) NOT NULL);# 成绩表CREATE TABLE scores (    id SERIAL PRIMARY KEY,    student_id INTEGER NOT NULL,    subject_id INTEGER NOT NULL,    score INTEGER NOT NULL,    FOREIGN KEY (student_id) REFERENCES students (id),    FOREIGN KEY (subject_id) REFERENCES subjects (id));# 插入一些测试数据INSERT INTO students (name, gender, age, class)VALUES    ('Alice', 'Female', 18, 'Class A'),    ('Bob', 'Male', 17, 'Class B'),    ('Charlie', 'Male', 19, 'Class A'),    ('Diana', 'Female', 18, 'Class B');# 插入学科表数据INSERT INTO subjects (name)VALUES    ('Mathematics'),    ('English'),    ('Science');-- Alice 的成绩INSERT INTO scores (student_id, subject_id, score)VALUES    (1, 1, 90),    (1, 2, 85),    (1, 3, 92);-- Bob 的成绩INSERT INTO scores (student_id, subject_id, score)VALUES    (2, 1, 78),    (2, 2, 80),    (2, 3, 75);-- Charlie 的成绩INSERT INTO scores (student_id, subject_id, score)VALUES    (3, 1, 88),    (3, 2, 92),    (3, 3, 90);-- Diana 的成绩INSERT INTO scores (student_id, subject_id, score)VALUES    (4, 1, 95),    (4, 2, 88),    (4, 3, 92);备份使用pgdump备份数据库非常简单，只需要在终端中输入相应的命令即可备份整个数据库1234pg_dump -h &lt;数据库地址&gt; -p &lt;数据库端口&gt; -U &lt;数据库用户名&gt; -F c -b -v -f &lt;备份文件路径&gt; &lt;数据库名称&gt;# 示例/usr/pgsql-14/bin/pg_dump -h 127.0.0.1 -U postgres -p 5432 -F t -b -v -f build_hyc_test.sql.tar hyc_test备份指定表或数据1234567pg_dump -h &lt;数据库地址&gt; -p &lt;数据库端口&gt; -U &lt;数据库用户名&gt; -F c -b -v -t &lt;表名1&gt; -t &lt;表名2&gt; -f &lt;备份文件路径&gt; &lt;数据库名称&gt;# 示例-- 备份指定表到sql文件-- '-c --if-exists' 会生成 'drop table if exist' 命令-- '--no-owner' 是一个选项，用于指定在导出数据库时不包括拥有者信息pg_dump --verbose --host=192.168.xx.xx --port=5432 --username=postgres --file /home/huangyc/pg_bak_test/bak_hyc.sql --encoding=UTF-8 -t &quot;public.tushare_wz_index&quot; -t &quot;public.tushare_us_basic&quot; -t &quot;public.dim_fund&quot; -t &quot;public.dim_index&quot; -c --if-exists --no-owner pg_fac_stk具体参数的含义如下：-h：数据库服务所在主机地址，可以是本地地址localhost或者IP地址-p：数据库服务的监听端口，一般为默认端口5432-U：连接数据库的用户名-F：备份文件的格式，包括自定义格式c，纯文本格式p和归档格式t-b：在备份文件中包含备份的数据库的模式信息-v：备份过程中输出详细的信息-f：备份文件的保存路径和文件名-t：只备份指定的表和数据12345-- 备份postgres库并tar打包pg_dump -h 127.0.0.1 -p 5432 -U postgres -f postgres.sql.tar -Ft;-- 备份postgres库，转储数据为带列名的INSERT命令pg_dumpall -d postgres -U postgres -f postgres.sql --column-inserts;还原使用备份文件进行恢复也非常简单，只需要在终端中输入相应的命令即可恢复整个库1234pg_restore -h &lt;数据库地址&gt; -p &lt;数据库端口&gt; -U &lt;数据库用户名&gt; -d &lt;数据库名称&gt; &lt;备份文件路径&gt;# 示例/usr/pgsql-14/bin/pg_restore -h 127.0.0.1 -U postgres -p 5432 -d hyc_test_bak build_hyc_test.sql.tar恢复指定数据123456pg_restore -h &lt;数据库地址&gt; -p &lt;数据库端口&gt; -U &lt;数据库用户名&gt; -t &lt;表名1&gt; -t &lt;表名2&gt; -d &lt;数据库名称&gt; &lt;备份文件路径&gt;# 示例-- 对于pg_dump备份出来的sql文件，直接执行sql文件即可恢复-- 还原指定sql文件到bak_test库(需要自己建库)psql --host=192.168.xx.xx --port=5432 --username=postgres -d bak_test --file /home/huangyc/pg_bak_test/bak_hyc.sql.tar具体参数的含义如下：-h：数据库服务所在主机地址，可以是本地地址localhost或者IP地址-p：数据库服务的监听端口，一般为默认端口5432-U：连接数据库的用户名-d：恢复数据的目标数据库名称-t：只恢复指定的表和数据命令详解1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465[postgres@pg01 ~]$ pg_dump --help用法:  pg_dump [选项]... [数据库名字]**一般选项**:  -f, --file=FILENAME          输出文件或目录名  -F, --format=c|d|t|p         输出文件格式 (c=custom, d=directory, t=tar,p=plain,plain就是sql纯文本 (默认值))  -j, --jobs=NUM               执行多个并行任务进行备份转储工作  -v, --verbose                详细模式  -V, --version                输出版本信息，然后退出  -Z, --compress=0-9           被压缩格式的压缩级别，0表示不压缩  --lock-wait-timeout=TIMEOUT  在等待表锁超时后操作失败  --no-sync                    不用等待变化安全写入磁盘  -?, --help                   显示此帮助, 然后退出**控制输出内容选项(常用)**:  -a, --data-only              只转储数据,不包括模式,只对纯文本输出有意义  -s, --schema-only            只转储模式, 不包括数据  -c, --clean                  在重新创建之前，先清除（删除）数据库对象，如drop table。只对纯文本输出有意义  -C, --create                 指定输出文件中是否生成create database语句,只对纯文本输出有意义  -n, --schema=PATTERN         指定要导出的schema，不指定则导出所有的非系统schema  -N, --exclude-schema=PATTERN 排除导出哪些schema  -O, --no-owner               在明文格式中, 忽略恢复对象所属者  -t, --table=PATTERN          指定导出的表、视图、序列，可以使用多个-t匹配多个表，使用-t之后，-n和-N就失效了  -T, --exclude-table=PATTERN  排除表  -x, --no-privileges          不要转储权限 (grant/revoke)  --disable-triggers           在只恢复数据的过程中禁用触发器  --exclude-table-data=PATTERN do NOT dump data for the specified table(s)  --if-exists                  当删除对象时使用IF EXISTS  --inserts                    以INSERT命令，而不是COPY命令的形式转储数据，使用该选项可以把数据加载到非pg数据库，会使恢复非常慢                               该选项为每行生成1个单独的insert命令，?在恢复过程中遇到错误，将会丢失1行而不是全部表数据  --column-inserts             以带有列名的INSERT命令形式转储数据，例如insert into table_name(column,...) values(value1,...)  --load-via-partition-root    通过根表加载分区  --no-comments                不转储注释  --no-tablespaces             不转储表空间分配信息  --no-unlogged-table-data     不转储没有日志的表数据  --on-conflict-do-nothing     将ON CONFLICT DO NOTHING添加到INSERT命令**控制输出内容选项(不常用)**:  -S, --superuser=NAME         指定关闭触发器时需要用到的超级用户名。 它只有在使用了--disable-triggers时才有影响。一般情况下，最好不要输入该参数，而是用 超级用户启动生成的脚本。  -b, --blobs                  在转储中包括大对象  -B, --no-blobs               排除转储中的大型对象  -E, --encoding=ENCODING      转储以ENCODING形式编码的数据  --binary-upgrade             只能由升级工具使用  --enable-row-security        启用行安全性（只转储用户能够访问的内容）  --extra-float-digits=NUM     覆盖extra_float_digits的默认设置  --disable-dollar-quoting     取消美元 (符号) 引号, 使用 SQL 标准引号  --no-publications            不转储发布  --no-security-labels         不转储安全标签的分配  --no-subscriptions           不转储订阅  --no-synchronized-snapshots  在并行工作集中不使用同步快照  --quote-all-identifiers      所有标识符加引号，即使不是关键字  --rows-per-insert=NROWS      每个插入的行数；意味着--inserts  --section=SECTION            备份命名的节 (数据前, 数据, 及 数据后)  --serializable-deferrable    等到备份可以无异常运行  --snapshot=SNAPSHOT          为转储使用给定的快照  --strict-names               要求每个表和(或)schema包括模式以匹配至少一个实体  --use-set-session-authorization                               使用 SESSION AUTHORIZATION 命令代替                               ALTER OWNER 命令来设置所有权**联接选项**:  -d, --dbname=DBNAME      对数据库 DBNAME备份  -h, --host=主机名        数据库服务器的主机名或套接字目录  -p, --port=端口号        数据库服务器的端口号  -U, --username=名字      以指定的数据库用户联接  -w, --no-password        永远不提示输入口令  -W, --password           强制口令提示 (自动)  --role=ROLENAME          在转储前运行SET ROLE对于pg_dump的自定义备份custom和tar类型的备份，需要使用pg_restore进行恢复，pg_restore语法如下1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556[postgres@pg01 pg_backup]$ pg_restore --helppg_restore 从一个归档中恢复一个由 pg_dump 创建的 PostgreSQL 数据库.用法:  pg_restore [选项]... [文件名]一般选项:  -d, --dbname=名字        连接数据库名字  -f, --file=文件名       输出文件名(- 对于stdout)  -F, --format=c|d|t       备份文件格式(应该自动进行)  -l, --list               打印归档文件的 TOC 概述  -v, --verbose            详细模式  -V, --version            输出版本信息, 然后退出  -?, --help               显示此帮助, 然后退出恢复控制选项:  -a, --data-only              只恢复数据, 不包括模式  -c, --clean                  在重新创建之前，先清除（删除）数据库对象  -C, --create                 创建目标数据库  -e, --exit-on-error          发生错误退出, 默认为继续  -I, --index=NAME             恢复指定名称的索引  -j, --jobs=NUM               执行多个并行任务进行恢复工作  -L, --use-list=FILENAME      从这个文件中使用指定的内容表排序                               输出  -n, --schema=NAME            在这个模式中只恢复对象  -N, --exclude-schema=NAME    不恢复此模式中的对象  -O, --no-owner               不恢复对象所属者  -P, --function=NAME(args)    恢复指定名字的函数  -s, --schema-only            只恢复模式, 不包括数据  -S, --superuser=NAME         使用指定的超级用户来禁用触发器  -t, --table=NAME             恢复命名关系（表、视图等）  -T, --trigger=NAME           恢复指定名字的触发器  -x, --no-privileges          跳过处理权限的恢复 (grant/revoke)  -1, --single-transaction     作为单个事务恢复  --disable-triggers           在只恢复数据的过程中禁用触发器  --enable-row-security        启用行安全性  --if-exists                  当删除对象时使用IF EXISTS  --no-comments                不恢复注释  --no-data-for-failed-tables  对那些无法创建的表不进行                               数据恢复  --no-publications            不恢复发行  --no-security-labels         不恢复安全标签信息  --no-subscriptions           不恢复订阅  --no-tablespaces             不恢复表空间的分配信息  --section=SECTION            恢复命名节 (数据前、数据及数据后)  --strict-names               要求每个表和(或)schema包括模式以匹配至少一个实体  --use-set-session-authorization                               使用 SESSION AUTHORIZATION 命令代替                               ALTER OWNER 命令来设置所有权联接选项:  -h, --host=主机名        数据库服务器的主机名或套接字目录  -p, --port=端口号        数据库服务器的端口号  -U, --username=名字      以指定的数据库用户联接  -w, --no-password        永远不提示输入口令  -W, --password           强制口令提示 (自动)  --role=ROLENAME          在恢复前执行SET ROLE操作选项 -I, -n, -N, -P, -t, -T, 以及 --section 可以组合使用和指定多次用于选择多个对象.如果没有提供输入文件名, 则使用标准输入.表空间新建表空间1234# 新建表空间目录 t_fac_tsmkdir /home/huangyc/t_fac_ts# 修改表空间的用户权限chown postgres /home/huangyc/t_fac_tspg库新建表空间1create tablespace t_fac_ts owner postgres location '/home/huangyc/t_fac_ts';表空间有关的一些语法123456# 删除表空间 (需要先drop表空间所有的表, 或者将该空间下所有的表移除才能drop表空间)DROP TABLESPACE t_fac_ts;# 修改具体的表到指定表空间下ALTER TABLE t_fac_tushare_stock_basic SET TABLESPACE t_fac_ts;# 修改指定库到指定表空间下ALTER DATABASE name SET TABLESPACE new_tablespace;锁表处理pg锁表解锁查看被锁的表123select a.locktype,a.database,a.pid,a.mode,a.relation,b.relnamefrom pg_locks ajoin pg_class b on a.relation = b.oid where relname='t_opt_strhdk_blsj';杀死被锁的pid1select pg_terminate_backend(pid);表结构修改12345678910-- 修改表名alter table &quot;user&quot; rename to &quot;ts_user&quot;;-- 添加新字段alter table table_name add column col_name varchar(50);-- 丢弃某列alter table table_name drop column col_name;-- 添加主键alter table table_name add primary key(&quot;col_name&quot;);-- 修改字段名alter table table_name rename column old_col_name to new_col_name;数据更新和查询设置某字段的值12345678-- 设置某字段的值update table_name set col_name=new_value;-- 更新某个字段并关联其他表UPDATE table1SET field_to_update = table2.new_valueFROM table2WHERE table1.common_column = table2.common_column;删除表中重复数据12345678910-- 查询[旧表]数据的重复情况select col1,col2,count(*) from old_table group by col1,col2;-- 所有字段都一样的情况create table bak_table as select distinct * from table_name;-- 查询[新表]数据的重复情况select col1,col2,count(*) from bak_table group by col1,col2;truncate table old_table;insert into old_table (col1,col2) select col1,col2 from bak_table;不存在插入，存在更新1insert into ... on conflict(column_name) do update set ...conflict(column_name): column_name字段是判断要查找的数据是否存在，作为判断条件column_name必须是主键或者其他具有唯一性的字段(如唯一键或排他键)1234insert into user(id,username,address,create_date,create_by) values('1','刘德华','香港',now(),'system') on conflict(id) do update set address='中国',update_date=now(),update_by='system';12# 批量的方式insert into testunnest(id, age, name) values (unnest(array[1,3]), unnest(array[18,10]), unnest(array['valupdated', 'val3'])) on conflict (id) do update set age = excluded.age, name = excluded.name;数据和结构复制12345-- [复制表和数据] 复制表结构和数据 自动建表，不会复制主键什么的create table new_table as select * from old_table [WITH NO DATA];-- [复制数据] 复制数据到 新表 表需要提前建，并且表字段要一致，不会复制主键什么的insert into new_table (col_0, col_1) select col_0, col_1 from old_table;视图普通视图视图是一个虚拟表，它是根据一个或多个基本表的查询结果动态生成的，每次查询视图时都会执行相应的查询123456CREATE VIEW view_name ASSELECT column1, column2, ...FROM table_nameWHERE condition;drop view view_name;物化视图物化视图是一个实际存储数据的表，它的数据定期刷新，不像普通视图那样每次查询都重新计算。123456CREATE MATERIALIZED VIEW materialized_view_name ASSELECT column1, column2, ...FROM table_nameWHERE condition;drop MATERIALIZED VIEW materialized_view_name需要注意的是，物化视图需要定期手动或自动刷新以更新数据，你可以使用 REFRESH MATERIALIZED VIEW 命令来进行刷新分页查询1select * from table_name limit 10000 offset 20000;删除重复记录postgresql 常用的删除重复数据方法12345678910111213141516-- 初始化数据create table hyc_tmp_del_test(id int, name varchar(255));create table hyc_tmp_del_test_bk (like hyc_tmp_del_test);insert into hyc_tmp_del_test select generate_series(1, 10000), 'huangyc';insert into hyc_tmp_del_test select generate_series(1, 10000), 'huangyc';insert into hyc_tmp_del_test_bk select * from hyc_tmp_del_test;-- 最容易想到的方法就是判断数据是否重复，对于重复的数据只保留ctid最小（或最大）的数据，删除其他的-- id相同的数据，保留ctid最小的，其他的删除explain analyse delete from hyc_tmp_del_test_bk a where a.ctid &lt;&gt; (select min(t.ctid) from hyc_tmp_del_test_bk t where a.id=t.id); -- 17.112s-- group by方法通过分组找到ctid最小的数据，然后删除其他数据explain analyse delete from hyc_tmp_del_test_bk a where a.ctid not in (select min(ctid) from hyc_tmp_del_test_bk group by id); -- 0.052s-- 高效删除方法explain analyze delete from hyc_tmp_del_test_bk a where a.ctid = any(array (select ctid from (select row_number() over (partition by id), ctid from hyc_tmp_del_test_bk) t where t.row_number &gt; 1)); -- 0.055s第二种和第三种感觉差不多，原文说是第三种快不少，这里pg库是14.x版本关键pg中每个表都有几个系统隐藏列：tableoid， xmin， xmax，cmin，cmax，ctid其中tableoid表示表的oid，cmin、cmax、xmin和xmax是mvcc的实现有关ctid表示行版本在表中的物理位置: 它属于对象标识符类型(oid，Object Identifier Types)，是一种行标识符，它的数据使用的元组标识符(tid，tuple identifier)。元组ID是一对(块号，块内的元组索引)，用于标识当前行的物理位置。引申用法假设我们有一个表格 products，包含产品名称和产品类别。我们希望从中找出每个产品类别中的前两个产品123456789101112131415SELECT  *FROM  (    SELECT      product_name,      category,      row_number() OVER (PARTITION BY SUBSTR(p.category, 0, LENGTH(p.category)-2) ORDER BY product_name) AS ranking    FROM      products p    WHERE      p.category LIKE 'Electronics:%'  ) subqueryWHERE  ranking &lt;= 2;在这个例子中，我们有一个产品表 products，其中包含 product_name 和 category 列我们想要找出每个以 ‘Electronics:’ 开头的产品类别中的前两个产品子查询部分：子查询从 products 表中选择了产品名称 product_name 和产品类别 category我们只关注以 ‘Electronics:’ 开头的产品类别，即 p.category LIKE 'Electronics:%'使用 row_number() 函数，对每个以 ‘Electronics:’ 开头的类别进行编号，按产品名称排序主查询部分：在主查询中，我们选择了子查询的所有列 *我们进一步筛选了结果，只选择了编号小于等于2的记录，以获取每个类别中的前两个产品索引12345678-- 获取数据库表中的索引select * from pg_indexes where tablename = 't_cfg_opinfo'; -- 创建索引create index index_name on table_name (col_0, col_1);-- 查询索引select * from pg_indexes where tablename='table_name';-- 删除索引drop index index_name;什么情况下要避免使用索引？虽然索引的目的在于提高数据库的性能，但这里有几个情况需要避免使用索引使用索引时，需要考虑下列准则：索引不应该使用在较小的表上索引不应该使用在有频繁的大批量的更新或插入操作的表上索引不应该使用在含有大量的 NULL 值的列上索引不应该使用在频繁操作的列上实用sql12-- 查询库中的最大版本SELECT (CASE WHEN MAX(version) IS NULL THEN -1 ELSE MAX(version) END) + 1 AS version FROM table_name其他语法筛选某列，逗号拼接123select string_agg(bs_org_id,',') as bs_org_ids   from bs_org   where par_org_id ='100'日期转换12select to_char(col_name,'yyyyMMDD')-interval '2 day' from table_name-- -interval '2 day' 表示往前2天转时间戳12select '2011-01-06 09:57:59'::timestamp;TO_TIMESTAMP('2011-01-06 09:57:59', 'YYYY-MM-DD HH24:MI:S')postgresql 获取分组第一条数据 窗口函数给数据分组并排名，使用 row_number() over (partition by 分组的字段名 order by 排序规则) as 排名从上述第一步中取出，排名为第一的数据，即为第一条数据 select * from 上述第一步 where 排名=1获取前N名的数据，将一中第二步的条件换成where 排名 &lt; N+1 distributed key12alter table table_name set distributed by (id);alter table table_name add primary key (id);ORM框架ORM框架比较一文了解 Python 的三种数据源架构模式SQLAlchemy 和其他的 ORM 框架SQLObject优点：采用了易懂的ActiveRecord 模式一个相对较小的代码库缺点：方法和类的命名遵循了Java 的小驼峰风格不支持数据库session隔离工作单元Storm优点：清爽轻量的API，短学习曲线和长期可维护性不需要特殊的类构造函数，也没有必要的基类缺点：迫使程序员手工写表格创建的DDL语句，而不是从模型类自动派生Storm的贡献者必须把他们的贡献的版权给Canonical公司Django’s ORM优点：易用，学习曲线短和Django紧密集合，用Django时使用约定俗成的方法去操作数据库缺点：不好处理复杂的查询，强制开发者回到原生SQL紧密和Django集成，使得在Django环境外很难使用peewee优点：Django式的API，使其易用轻量实现，很容易和任意web框架集成缺点：不支持自动化 schema 迁移多对多查询写起来不直观SQLAlchemy优点：企业级API，使得代码有健壮性和适应性灵活的设计，使得能轻松写复杂查询缺点：工作单元概念不常见重量级API，导致长学习曲线相比其他的ORM， SQLAlchemy 意味着，无论你何时写SQLAlchemy代码， 都专注于工作单元的前沿概念 。DB Session 的概念可能最初很难理解和正确使用，但是后来你会欣赏这额外的复杂性，这让意外的时序提交相关的数据库bug减少到0。在SQLAlchemy中处理多数据库是棘手的， 因为每个DB session 都限定了一个数据库连接。但是，这种类型的限制实际上是好事， 因为这样强制你绞尽脑汁去想在多个数据库之间的交互， 从而使得数据库交互代码很容易调试。SQLAlchemySQLAlchemy 1.4 Documentationsqlalchemy操作数据库sqlalchemy外键和relationship查询SQLALlchemy数据查询小集合SQLAlchemy 的连接池机制SQLAlchemy 中的 Session、sessionmaker、scoped_sessionContextual/Thread-local SessionsSQLAlchemy(常用的SQLAlchemy列选项)查询官网例子Object Relational Tutorial (1.x API)sqlalchemy外键和relationship查询session和scoped_sessionsession用于创建程序和数据库之间的会话，所有对象的载入和保存都需通过session对象 。通过sessionmaker调用创建一个工厂，并关联Engine以确保每个session都可以使用该Engine连接资源scoped_session 实现了一个线程的隔离，保证不同的线程拿到不同的session, 同一个线程拿到的session 是同一个值1234567s1 = Session()s2 = Session()s1.add(person)s1.commit()# 必须先close，s2才能继续操作persons1.close()s2.add(person)session 和scoped_session本质上都是用来操作数据库的，只是session 只适合在单线程下面使用官方文档提到了scoped_session的正确使用方法。request结束后要调用scoped_session.remove()Engine Configuration使用 create_engine创建我们需要的DB starting point1234from sqlalchemy import create_enginescheme = 'mysql+pymysql://root:123456@localhost:3306/dev_shopping?charset=utf8'engine = create_engine(scheme, pool_size=10 , max_overflow=-1, pool_recycle=1200)create_engine 函数常用参数：pool_size=10 # 连接池的大小，0表示连接数无限制pool_recycle=-1 # 连接池回收连接的时间，如果设置为-1，表示没有no timeout, 注意，mysql会自动断开超过8小时的连接，所以sqlalchemy沿用被mysql断开的连接会抛出MySQL has gone awaymax_overflow=-1 # 连接池中允许‘溢出’的连接个数，如果设置为-1，表示连接池中可以创建任意数量的连接pool_timeout=30 # 在连接池获取一个空闲连接等待的时间echo=False # 如果设置True, Engine将会记录所有的日志，日志默认会输出到sys.stdout创建Engine之后，接下来的问题，就是如何使用Engine在单进程中，建议在在初始化的模块的时候创建Engine, 使Engine成为全局变量， 而不是为每个调用Engine的对象或者函数中创建, Engine不同于connect, connect函数会创建数据库连接的资源，Engine是管理connect创建的连接资源在多进程中，为每个子进程都创建各自的Engine, 因为进程之间是不能共享Engine几种操作方式Working with Engines and ConnectionsSqlAlchemy的Engine，Connection和Session 区别？适合什么时候用？Engine方式Engine是SQLAlchemy中连接数据库最底层级别的对象，它维护了一个连接池，可以在应用程序需要和数据库对话时使用。在Engine.execute(close_with_result=True) close_with_result=True 表示连接自动关闭；123456result = engine.execute('SELECT * FROM tablename;') conn = engine.connect(close_with_result=True)result = conn.execute('SELECT * FROM tablename;')for row in result:    print(result['columnname']result.close()Connection方式Connection，实际上是执行SQL查询的工作，每当你想更好的控制连接的属性，如何时关闭等都建议使用这个操作；比如在一个事务中，要控制它提交commit的时间，在connection控制中就可以运行多个不同的SQL语句，如果其中一个出现问题，则其他所有的语句都会撤销更改；123456789connection = engine.connect()trans = connection.begin()try:    connection.execute(&quot;INSERT INTO films VALUES ('Comedy', '82 minutes');&quot;)    connection.execute(&quot;INSERT INTO datalog VALUES ('added a comedy');&quot;)    trans.commit()except:    trans.rollback()    raiseSession方式Session，一般都是用于ORM中，因为在ORM中，会自动生成SQL语句以及自动连接数据库（自己配置），使用session.execute（）也是个编辑的方法，可以将会话绑定到任何对象；如果你确定使用ORM，就建议使用session来处理execute(),否则还是使用connection更好方便；总结: 从应用角度来看，可以把这三类分为两种：直接使用Engine.execute() 或Connection.execute()，更加灵活，可以使用原生SQL语句使用Session处理交易类型的数据，因为方便使用session.add(), session.rollback(), session.commit(), session.close()等，它是使用ORM时推荐的一种和数据库交互的方式</content>
	</entry>
	<entry>
		<title>正则表达式学习</title>
		<url>/article/13105.html</url>
		<content>正则表达式概念python正则表达式深入理解正则表达式环视的概念与用法资源 | 正则表达式的功法大全在线正则表达式验证网站概念定义使用单个字符串来描述匹配某个句法规则的字符串，是对字符串操作的一种逻辑公式应用场景处理文本和数据，提高复杂文本分析的效率正则表达式过程依次拿出表达式和文本中的字符比较，如果每一个字符都能匹配，则匹配成功；否则匹配失败123456789101112# -*- coding: utf-8 -*import refrom IPython.core.interactiveshell import InteractiveShellInteractiveShell.ast_node_interactivity = &quot;all&quot;  # 可同时输出多个结果pattern_s = r'imooc'  # 定义正则表达式pattern_r = re.compile(pattern_s)  # 编译正则表达式str1 = 'imooc book'  # 需要查找的原始字符串match_r = pattern_r.match(str1)print(f'match_r.group(): {match_r.group()}')match_r.group(): imooc正则元字符内容匹配正则表达式代表的匹配字符.匹配任意字符（不包括换行符）[0-9]0-9的数字[a-z]小写字母[A-Z]大写字母\d匹配数字，等同于[0-9]\D匹配非数字，等同于[\^0-9]\w匹配大小写字母、数字和下划线，等同于[a-z0-9A-Z_]\W匹配非大小写字母、数字和下划线，等同于[\^a-z0-9A-Z_]\s匹配空白\S匹配非空白\u4e00-\u9fff匹配中文字符个数匹配正则表达式代表的匹配字符*匹配前面的字符或者子表达式0次或多次+匹配前一个字符或子表达式一次或多次?匹配前一个字符或子表达式0次或1次重复{n}匹配前一个字符或子表达式n次{m,n}匹配前一个字符或子表达式m至n次{n,}匹配前一个字符或者子表达式至少n次*? / +? / ??惰性匹配上一个位置匹配正则表达式代表的匹配字符^匹配字符串开头, 多行模式下匹配每一行的开始$匹配字符串结尾, 多行模式下匹配每一行的结束\A / \Z指定字符串必须出现在开头/结尾\b匹配位于单词开始或结束位置的空字符串\B匹配不位于单词开始或结束位置的空字符串分组匹配正则表达式代表的匹配字符|匹配左右任意一个表达式(ab)括号里的表达式作为一个分组\&lt;number&gt;引用编号为num的分组匹配到的字符串(?P&lt;name&gt;)分组起别名(?P=name)引用别名为name的分组匹配字符串[ ]可匹配其中任意一个字符转义匹配正则表达式代表的匹配字符\转义字符，如\.只能匹配.，不能再匹配任意字符基本使用预编译compile 函数用于编译正则表达式，生成一个正则表达式(Pattern)对象，供 match() 和 search() 这两个函数使用如果重复多次地使用正则表达式，最好是使用compile函数把正则表达式编译成对象re.Pattern，这样会大大地提高搜索的效率re.compile(pattern, flags=0)属性说明flags：编译时指定的模式groupindex：以正则表达式中有别名的组的别名为键、以该组对应的编号为值的字典，没有别名的组不包含在内。groups：正则表达式中分组的数量pattern：编译时用的正则表达式方法说明findall、finditer、match、search、split、sub、subn 等函数例子123456789101112131415161718192021222324252627# -*- coding: utf-8 -*import res = 'Hello, Mr.Gumby : 2016/10/26'pat = '''(?:        # 构造一个不捕获分组 用于使用 |              (?P&lt;name&gt;\w+\.\w+)    # 匹配 Mr.Gumby              |     # 或              (?P&lt;no&gt;\s+\.\w+) # 一个匹配不到的命名分组              )              .*? # 匹配  :               (\d+) # 匹配 2016              '''p = re.compile(pat, re.X)print(f'p.flags: {p.flags}')print(f'p.groupindex: {p.groupindex}')print(f'p.groups: {p.groups}')print(f'p.pattern: {p.pattern}')p.flags: 96p.groupindex: {'name': 1, 'no': 2}p.groups: 3p.pattern: (?:        # 构造一个不捕获分组 用于使用 |              (?P&lt;name&gt;\w+\.\w+)    # 匹配 Mr.Gumby              |     # 或              (?P&lt;no&gt;\s+\.\w+) # 一个匹配不到的命名分组              )              .*? # 匹配  :               (\d+) # 匹配 2016匹配模式Python正则表达式，请不要再用re.compile了在正则表达式中，采用预编译的优势就是可以节约时间，提高效率re.compile(pattern, flags=0)给定一个正则表达式 pattern指定使用的模式 flags 默认为0 即不使用任何模式，然后会返回一个 SRE_Pattern对象参数说明pattern: 一个字符串形式的正则表达式flags: 可选，表示匹配模式，比如忽略大小写，多行模式等，使用按位或运算符 | 同时添加多个模式,具体参数为：re.I：数值2，忽略大小写re.L：数值4，表示特殊字符集 \w, \W, \b, \B, \s, \S 依赖于当前环境re.M：数值8，多行模式re.S：数值16，即为 . 并且包括换行符在内的任意字符（. 不包括换行符）re.U：数值32，表示特殊字符集 \w, \W, \b, \B, \d, \D, \s, \S 依赖于 Unicode 字符属性数据库re.X：数值64，为了增加可读性，忽略空格和 # 后面的注释忽略大小写re.I123456789101112131415# I: IGNORECASE， 忽略大小写的匹配模式# -*- coding: utf-8 -*import restr1 = 'hello World!'pattern_s = &quot;hello world!&quot;pattern_r = re.compile(pattern_s, re.I)print(f'pattern_r.match(str1).group(): {pattern_r.match(str1).group()}')# 或在正则表达式中指定模式以及注释pattern_s = &quot;(?#注释)(?i)hello world!&quot;pattern_r = re.compile(pattern_s)print(f'pattern_r.match(str1).group(): {pattern_r.match(str1).group()}')pattern_r.match(str1).group(): hello World!pattern_r.match(str1).group(): hello World!字符集本地化re.L12# L: LOCALE， 字符集本地化。这个功能是为了支持多语言版本的字符集使用环境的，比如在转义符\w，在英文环境下，它代表[a-zA-Z0-9_]，即所以英文字符和数字。# 如果在一个法语环境下使用，缺省设置下，不能匹配&quot;é&quot; 或&quot;ç&quot;。加上这L选项和就可以匹配了。不过这个对于中文环境似乎没有什么用，它仍然不能匹配中文字符。多行模式re.M1234567891011121314151617181920212223# M: MULTILINE，多行模式, 改变 ^ 和 $ 的行为# -*- coding: utf-8 -*import restr1 = '''first line1second line2third line3'''# ^regex_start = re.compile(&quot;^\w+&quot;)print(f'regex_start.findall(str1): {regex_start.findall(str1)}')regex_start_m = re.compile(&quot;^\w+&quot;, re.M)print(f'regex_start_m.findall(str1): {regex_start_m.findall(str1)}')# $regex_end = re.compile(&quot;\w+$&quot;)print(f'regex_end.findall(str1): {regex_end.findall(str1)}')regex_end_m = re.compile(&quot;\w+$&quot;, re.M)print(f'regex_end_m.findall(str1): {regex_end_m.findall(str1)}')regex_start.findall(str1): ['first']regex_start_m.findall(str1): ['first', 'second', 'third']regex_end.findall(str1): ['line3']regex_end_m.findall(str1): ['line1', 'line2', 'line3']所有字符re.S123456789101112131415# S: DOTALL，此模式下 '.' 的匹配不受限制，可匹配任何字符，包括换行符# -*- coding: utf-8 -*import restr2 = '''first line1second line2third line3'''# 此时.无法匹配换行符regex = re.compile(&quot;.+&quot;)print(f'regex.findall(str2): {regex.findall(str2)}')# 此时.匹配换行符regex_dotall = re.compile(&quot;.+&quot;, re.S)print(f'regex_dotall.findall(str2): {regex_dotall.findall(str2)}')regex.findall(str2): ['first line1', 'second line2', 'third line3']regex_dotall.findall(str2): ['first line1\nsecond line2\nthird line3']冗余模式re.X12345678910# X: VERBOSE，冗余模式， 忽略正则表达式中的空白和#号注释，如写一个匹配邮箱的正则表达式# -*- coding: utf-8 -*import reemail_regex = re.compile(&quot;[\w+\.]+@[a-zA-Z\d]+\.(com|cn)&quot;)email_regex = re.compile(&quot;&quot;&quot;[\w+\.]+  # 匹配@符前的部分                            @  # @符                            [a-zA-Z\d]+  # 邮箱类别                            \.(com|cn)   # 邮箱后缀  &quot;&quot;&quot;, re.X)UNICODE解析re.U1# U: UNICODE，使用 \w, \W, \b, \B 这些元字符时将按照 UNICODE 定义的属性.re.Match类若匹配成功，match/search返回的是Match对象，finditer返回的也是Match对象的迭代器获取匹配结果需要调用Match对象的group()、groups或group(index)方法属性说明endpos: 本次搜索结束位置索引lastgroup: 本次搜索匹配到的最后一个分组的别名lastindex: 本次搜索匹配到的最后一个分组的索引pos: 本次搜索开始位置索引re: 本次搜索使用的 SRE_Pattern 对象regs: 列表，元素为元组，包含本次搜索匹配到的所有分组的起止位置方法说明end([group=0])：返回指定分组的结束位置，默认返回正则表达式所匹配到的最后一个字符的索引expand(template)：根据模版返回相应的字符串，类似与 sub 函数里面的 repl， 可使用 \1 或者 \g 来选择分组group([group1, …])：根据提供的索引或名字返回响应分组的内容默认返回 start() 到 end() 之间的字符串， 提供多个参数将返回一个元组groupdict([default=None])：返回一个包含所有匹配到的命名分组的字典，没有命名的分组不包含在内key 为组名， value 为匹配到的内容，参数 default 为没有参与本次匹配的命名分组提供默认值groups([default=None])：以元组形式返回每一个分组匹配到的字符串，包括没有参与匹配的分组，其值为 defaultspan([group])：返回指定分组的起止位置组成的元组，默认返回由 start() 和 end() 组成的元组start([group])：返回指定分组的开始位置，默认返回正则表达式所匹配到的第一个字符的索引123456789101112131415161718192021222324252627# -*- coding: utf-8 -*import res = 'Hello, Mr.Gumby : 2016/10/26'm = re.search(', (?P&lt;name&gt;\w+\.\w+).*?(\d+)', s)# 本次搜索的结束位置索引print(f'm.endpos: {m.endpos}')# 本次搜索匹配到的最后一个分组的别名# 本次匹配最后一个分组没有别名print(f'm.lastgroup: {m.lastgroup}')# 本次搜索匹配到的最后一个分组的索引print(f'm.lastindex: {m.lastindex}')# 本次搜索开始位置索引print(f'm.pos: {m.pos}')# 本次搜索使用的 SRE_Pattern 对象print(f'm.re: {m.re}')# 列表，元素为元组，包含本次搜索匹配到的所有分组的起止位置 第一个元组为正则表达式匹配范围print(f'm.regs: {m.regs}')# 本次搜索操作的字符串print(f'm.string: {m.string}')m.endpos: 28m.lastgroup: Nonem.lastindex: 2m.pos: 0m.re: re.compile(', (?P&lt;name&gt;\\w+\\.\\w+).*?(\\d+)')m.regs: ((5, 22), (7, 15), (18, 22))m.string: Hello, Mr.Gumby : 2016/10/26123456789101112131415161718192021222324252627282930313233343536373839# -*- coding: utf-8 -*import res = 'Hello, Mr.Gumby : 2016/10/26'm = re.search('''(?:        # 构造一个不捕获分组 用于使用 |              (?P&lt;name&gt;\w+\.\w+)    # 匹配 Mr.Gumby              |     # 或              (?P&lt;no&gt;\s+\.\w+) # 一个匹配不到的命名分组              )              .*? # 匹配  :               (\d+) # 匹配 2016              ''',              s, re.X)# 返回指定分组的结束位置，默认返回正则表达式所匹配到的最后一个字符的索引print(f'm.end(): {m.end()}')# 根据模版返回相应的字符串，类似sub()的 repl,可使用 \1 或者 \g&lt;name&gt; 来选择分组mExpend = m.expand('my name is \\1')print(f&quot;m.expand('my name is \\1'): {mExpend}&quot;)# 根据提供的索引或名字返回响应分组的内容，默认返回 start() 到 end() 之间的字符串，# 提供多个参数将返回一个元组print(f'm.group(): {m.group()}')print(f'm.group(1, 2): {m.group(1, 2)}')# 返回一个包含所有匹配到的命名分组的字典，没有命名的分组不包含在内，key 为组名，# value 为匹配到的内容，参数 default 为没有参与本次匹配的命名分组提供默认值m.groupdict('default_string')# 以元组形式返回每一个分组匹配到的字符串，包括没有参与匹配的分组，其值为 defaultprint(f&quot;m.groups('default_string'): {m.groups('default_string')}&quot;)# 返回指定分组的起止未知组成的元组，默认返回由 start() 和 end() 组成的元组print(f'm.span(3): {m.span(3)}')# 返回指定分组的开始位置，默认返回正则表达式所匹配到的第一个字符的索引print(f'm.start(3): {m.start(3)}')m.end(): 22m.expand('my name is \1'): my name is Mr.Gumbym.group(): Mr.Gumby : 2016m.group(1, 2): ('Mr.Gumby', None)m.groups('default_string'): ('Mr.Gumby', 'default_string', '2016')m.span(3): (18, 22)m.start(3): 18匹配matchre.match(pattern, string[, flags])从首字母开始匹配，string如果包含pattern子串，则匹配成功，返回Match对象，失败则返回None，若要完全匹配，pattern要以$结尾参数说明pattern：匹配的正则表达式string：要匹配的字符串。flags：标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：正则表达式修饰符 - 可选标志match函数在string的开始位置匹配，如果匹配则返回对象，否则返回None12345678910111213141516#! /usr/bin/env python# -*- coding=utf-8 -*-   import re   text = 'python world'm = re.match(r&quot;\w+&quot;, text)print(m.group(0)) if m else print('not match')text = '#python world'm = re.match(r&quot;\w+&quot;, text)print(m.group(0)) if m else print('not match')    output: pythonoutput: not match12345678# re.match()函数# -*- coding: utf-8 -*import restr1 = '333STR1666STR299'pattern_s = r'([A-Z]+(\d))'print(f're.match(pattern_s, str1): {re.match(pattern_s, str1)}')  # str1的开头不符合正则，所以结果为Nonere.match(pattern_s, str1): Nonesearchre.search(pattern, string[, flags])若string中包含pattern子串，则返回Match对象，否则返回None，注意，如果string中存在多个pattern子串，只返回第一个参数说明pattern：匹配的正则表达式string：要匹配的字符串flags：标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等search会扫描整个string查找匹配，如果找到则返回一个相应的匹配对象，否则返回None12345678910111213141516#! /usr/bin/env python# -*- coding=utf-8 -*-import retext = 'python world'm = re.search(r&quot;\w+&quot;, text)print(m.group(0)) if m else print('not match')text = '#python world'm = re.search(r&quot;\w+&quot;, text)print(m.group(0)) if m else print('not match')output: pythonoutput: python1234567891011121314151617181920# re.search()函数# -*- coding: utf-8 -*import restr1 = '333STR1666STR299'pattern_s = r'([A-Z]+(\d))'match_r = re.search(pattern_s, str1)# match_r[0]是regex所代表的整个字符串print(f'match_r.group(0): {match_r.group(0)}')  # match_r.group()# match_r[1]是第一个()中的内容print(f'match_r.group(1): {match_r.group(1)}')# match_r[2]是第二对()中的内容print(f'match_r.group(2): {match_r.group(2)}')# 所有group组成的一个元组print(f'match_r.groups(): {match_r.groups()}')match_r.group(0): STR1match_r.group(1): STR1match_r.group(2): 1match_r.groups(): ('STR1', '1')检索findallre.findall(pattern, string[, flags])返回string中所有与pattern相匹配的全部字串，返回形式为数组参数说明string: 待匹配的字符串pos: 可选参数，指定字符串的起始位置，默认为 0endpos: 可选参数，指定字符串的结束位置，默认为字符串的长度findall返回所有匹配的指定模式的文本子串到列表中，一个元素一个匹配串123456789101112# re.findall()函数# -*- coding: utf-8 -*import restr1 = '333STR1666STR299'pattern_s = r'([A-Z]+(\d))'match_r = re.findall(pattern_s, str1)for m in match_r:    print(f'm[0], m[1]: {m[0], m[1]}')    m[0], m[1]: ('STR1', '1')m[0], m[1]: ('STR2', '2')finditerre.finditer(pattern, string[, flags])返回string中所有与pattern相匹配的全部字串，返回形式为迭代器参数说明pattern：匹配的正则表达式string：要匹配的字符串。flags：标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：正则表达式修饰符 - 可选标志finditer函数跟findall函数类似，但返回的是一个迭代器123456789101112# re.finditer()函数# -*- coding: utf-8 -*import restr1 = '333STR1666STR299'pattern_s = r'([A-Z]+(\d))'match_r = re.finditer(pattern_s, str1)for m in match_r:    print(f'm.group(0), m.group(1), m.group(2): {m.group(0), m.group(1), m.group(2)}')  # 字符串    m.group(0), m.group(1), m.group(2): ('STR1', 'STR1', '1')m.group(0), m.group(1), m.group(2): ('STR2', 'STR2', '2')替换和分割sub替换re.sub(pattern, repl, string, count=0, flags=0)将字符串中匹配正则表达式的部分替换为其他值Return the string obtained by replacing the leftmost non-overlapping occurrences of the pattern in string by the replacement repl.repl can be either a string or a callable; if a string, backslash escapes in it are processed. If it is a callable, it’s passed the Match object and must return a replacement string to be used.参数说明pattern: 正则中的模式字符串。repl: 替换的字符串，也可为一个函数。string: 要被查找替换的原始字符串。count: 模式匹配后替换的最大次数，默认 0 表示替换所有的匹配。代码12345678910111213141516# -*- coding: utf-8 -*import redef add_one(match):    val = match.group()    num = int(val) + 1    return str(num)str2 = 'imooc videonum=200'reSub_1 = re.sub(r'\d+', add_one, str2)print(f&quot;re.sub(r'\d+', add_one, str2): {reSub_1}&quot;)reSub_2 = re.sub(r'\d+', '203', str2)print(f&quot;re.sub(r'\d+', '203', str2): {reSub_2}&quot;)sample_text = '2020-05-20 10:59:23 hello world 2020-05-21 10:59:24 hello kitty'sample_pattern = r'(?P&lt;year&gt;\d{4})-(?P&lt;month&gt;\d{2})-(?P&lt;day&gt;\d{2})'sample_repl = r'\g&lt;month&gt;/\g&lt;day&gt;/\g&lt;year&gt;'print(re.sub(sample_pattern, sample_repl, sample_text))输出123re.sub(r'\d+', add_one, str2): imooc videonum=201re.sub(r'\d+', '203', str2): imooc videonum=20305/20/2020 10:59:23 hello world 05/21/2020 10:59:24 hello kitty高级用法一日一技：如何正确使用 re.sub 的第二个参数re.sub第二个参数可以是函数设想有一个字符串abc18123456794xyz123，这个字符串中有两段数字，并且长短是不一样的第一个数字是11位的手机号。我想把字符串替换为：`abc[隐藏手机号]xyz*不是手机号的数字，每一位数字逐位替换为星号1234567891011import redef test(repl): if len(repl.group(0)) == 11:     return '[隐藏手机号]' else:     return '*' * len(repl.group(0))a = 'abc18123456794xyz123'b = re.sub('\d+', test, a)print(b)subn替换subn(pattern, repl, string, count=0, flags=0)作用与函数 sub 一样， 唯一不同之处在于返回值为一个元组，第一个值为替换后的字符串，第二个值为发生替换的次数split分割re.split(pattern, string, maxsplit=0, flags=0)根据匹配分割字符串，返回分割字符串组成的列表Split the source string by the occurrences of the pattern, returning a list containing the resulting substrings.If capturing parentheses are used in pattern, then the text of all groups in the pattern are also returned as part of the resulting list. If maxsplit is nonzero, at most maxsplit splits occur, and the remainder of the string is returned as the final element of the list.参数说明pattern：匹配的正则表达式string：要匹配的字符串。maxsplit：分隔次数，maxsplit=1 分隔一次，默认为 0，不限制次数。flags：标志位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。参见：正则表达式修饰符 - 可选标志代码12345# -*- coding: utf-8 -*import restr3 = 'imooc:C C++ Java Python,c#'resub_3 = re.split(r':| |,',str3)print(f&quot;re.split(r':| |,',str3):{resub_3}&quot;)输出1re.split(r':| |,',str3):['imooc', 'C', 'C++', 'Java', 'Python', 'c#']正则缓存当你在程序中使用 re 模块，无论是先使用 compile 还是直接使用比如 findall 来使用正则表达式操作文本re 模块都会将正则表达式先编译一下， 并且会将编译过后的正则表达式放到缓存中这样下次使用同样的正则表达式的时候就不需要再次编译， 因为编译其实是很费时的，这样可以提升效率而默认缓存的正则表达式的个数是 100，当你需要频繁使用少量正则表达式的时候，缓存可以提升效率而使用的正则表达式过多时，缓存带来的优势就不明显了这个re.purge()函数的作用是清除缓存中的正则表达式，可能在你需要优化占用内存的时候会用到爬虫例子代码123456789101112131415161718192021# -*- coding: utf-8 -*import urllib.requestimport reimport osurl = r'https://www.imooc.com/course/list?c=ai'res = urllib.request.urlopen(url)html = res.read().decode('utf-8')listurl = re.findall(r'src=&quot;//img\d.+?jpg&quot;', html)lurl = ['http:%s' % x[5:-1] for x in listurl]print(lurl)basepath = './res'if not os.path.exists(basepath):    os.makedirs(basepath)imgspath = os.path.join(basepath, 're_op')if not os.path.exists(imgspath):    os.mkdir(imgspath)for ii, uu in enumerate(lurl):    savepath = os.path.join(imgspath, '%d.jpg' % (ii))    res = urllib.request.urlretrieve(uu, savepath)输出1['http://img4.mukewang.com/5bd8157a0001a7a506000336-240-135.jpg', 'http://img3.mukewang.com/5bc6e6b80001434f06000338-240-135.jpg', 'http://img2.mukewang.com/5ba2386600013d3705980337-240-135.jpg', 'http://img3.mukewang.com/5b9105800001288905400300-240-135.jpg', 'http://img4.mukewang.com/5b7f737a0001cfb706000336-240-135.jpg', 'http://img4.mukewang.com/5abc6159000142f706000338-240-135.jpg', 'http://img1.mukewang.com/5a40c6370001d13c06000338-240-135.jpg']进阶用法非贪婪匹配匹配模式贪婪匹配：贪婪匹配在匹配字符串时总是尝试匹配尽可能多的字符非贪婪匹配：与贪婪匹配相反，非贪婪匹配在匹配字符串时总是尝试匹配尽可能少的字符Python里数量词默认是贪婪模式的，使用*? / +? / ??可使贪婪模式变成非贪婪模式1234567891011121314151617181920212223242526272829303132333435363738# -*- coding: utf-8 -*import restr1 = '9abc'  # 需要查找的原始字符串pattern_s = r'[0-9][a-z]*?'  # 定义正则表达式pattern_r = re.compile(pattern_s)  # 编译正则表达式match_r = pattern_r.match(str1)print(f'match_r.group(): {match_r.group()}')pattern_s = r'[0-9][a-z]*'  # 定义正则表达式pattern_r = re.compile(pattern_s)  # 编译正则表达式match_r = pattern_r.match(str1)print(f'match_r.group(): {match_r.group()}')pattern_s = r'[0-9][a-z]+?'  # 定义正则表达式pattern_r = re.compile(pattern_s)  # 编译正则表达式match_r = pattern_r.match(str1)print(f'match_r.group(): {match_r.group()}')pattern_s = r'[0-9][a-z]+'  # 定义正则表达式pattern_r = re.compile(pattern_s)  # 编译正则表达式match_r = pattern_r.match(str1)print(f'match_r.group(): {match_r.group()}')pattern_s = r'[0-9][a-z]??'  # 定义正则表达式pattern_r = re.compile(pattern_s)  # 编译正则表达式match_r = pattern_r.match(str1)print(f'match_r.group(): {match_r.group()}')pattern_s = r'[0-9][a-z]?'  # 定义正则表达式pattern_r = re.compile(pattern_s)  # 编译正则表达式match_r = pattern_r.match(str1)print(f'match_r.group(): {match_r.group()}')match_r.group(): 9match_r.group(): 9abcmatch_r.group(): 9amatch_r.group(): 9abcmatch_r.group(): 9match_r.group(): 9a分组匹配(捕获组)12345678import regex as repat = re.compile(r&quot;有(?P&lt;zj&gt;(许可证[件]?|执照))的?(?P&lt;xf_type&gt;(暂扣|吊销).{0,4}(?P=zj))&quot;)doc = &quot;有许可证的吊销许可证，并给予严厉惩罚&quot;res = list(re.finditer(pat, doc))res[0].groupdict()Out[16]: {'zj': '许可证', 'xf_type': '吊销许可证'}123456789# 分组匹配之 \&lt;number&gt;# -*- coding: utf-8 -*import restr1 = '&lt;book&gt;python&lt;/book&gt;'pattern_s = r'&lt;([\w]+&gt;)[\w]+&lt;/\1'print(re.match(pattern_s, str1).group())&lt;book&gt;python&lt;/book&gt;1234567891011# 分组匹配之 别名 (?P&lt;name&gt;) 和 (?P=name)# -*- coding: utf-8 -*import restr1 = '&lt;book&gt;python&lt;/book&gt;'pattern_s = r'&lt;(?P&lt;mark1&gt;[\w]+&gt;)[\w]+&lt;/(?P=mark1)'print(re.match(pattern_s, str1).group())Out[12]: &lt;book&gt;python&lt;/book&gt;re.match(pattern_s, str1).groupdict()Out[13]: {'mark1': 'book&gt;'}12345678910111213141516# 分组匹配之group()和groups()group() / group(0): 母串中与模式pattern匹配的子串group(index): 第index个group匹配成功的子串groups(): 所有group组成的一个元组，与pattern中的()有关# -*- coding: utf-8 -*import rep = re.compile('\d-\d-\d')m = p.match('2-3-1')print(f'm.group(): {m.group()}')print(f'm.group(0): {m.group(0)}')print(f'm.groups(): {m.groups()}')m.group(): 2-3-1m.group(0): 2-3-1m.groups(): ()1234567891011# -*- coding: utf-8 -*import rep = re.compile('(\d)-(\d)-(\d)')m = p.match('2-3-1d5-4-3')print(f'm.group(): {m.group()}')print(f'm.group(0): {m.group(0)}')print(f'm.groups(): {m.groups()}')m.group(): 2-3-1m.group(0): 2-3-1m.groups(): ('2', '3', '1')1234567891011# -*- coding: utf-8 -*import rep = re.compile('(\d)-(\d)-(\d)')m = re.match(p,'2-3-1d5-4-3')print(f'm.group(): {m.group()}')print(f'm.group(0): {m.group(0)}')print(f'm.groups(): {m.groups()}')m.group(): 2-3-1m.group(0): 2-3-1m.groups(): ('2', '3', '1')转义匹配替代函数使用python的过程中，你肯定对转义字符的使用苦恼过，因为有的时候我们需要使用一些特殊符号如”$ * . ^”等的原意有时候需要被转义后的功能，并且转义字符地使用很繁琐，容易出错re.escape(pattern)转义: 如果你需要操作的文本中含有正则的元字符，你在写正则的时候需要将元字符加上反斜扛 \ 去匹配自身而当这样的字符很多时，写出来的正则表达式就看起来很乱而且写起来也挺麻烦的，这个时候你可以使用这个函数可以对字符串中所有可能被解释为正则运算符的字符进行转义的应用函数123456789101112# -*- coding: utf-8 -*import restr2 = &quot;.+\d123&quot;regex_str = re.escape(&quot;.+\d123&quot;)# 查看转义后的字符print(f'regex_str: {regex_str}')# 查看匹配到的结果for g in re.findall(regex_str, str2):    print(g)    regex_str: \.\+\\d123.+\d123re增强的包装主要目的是，增强re的匹配能力，为match对象添加了start: int=0参数，start指定了当前字符串的起始位置，默认的起始位置是0导入库，regex是re的增强版，需要pip安装，它支持更强的正则，比如一个表达式中可以出现两个一样的捕获组名字12from typing import Iterable, Listimport regex as re包装match对象为ReMatch类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495class ReMatch:    def __init__(self, match, start: int = 0):        &quot;&quot;&quot;        指定起始位置的Match类        :param match: re.Match对象        :param start:        &quot;&quot;&quot;        self.match = match        self.start_idx = start    @property    def endpos(self):        &quot;&quot;&quot; 本次搜索结束位置索引 &quot;&quot;&quot;        return self.match.endpos + self.start_idx    @property    def lastgroup(self):        &quot;&quot;&quot; 本次搜索匹配到的最后一个分组的别名 &quot;&quot;&quot;        return self.match.lastgroup    @property    def lastindex(self):        &quot;&quot;&quot; 本次搜索匹配到的最后一个分组的索引 &quot;&quot;&quot;        return self.match.lastindex + self.start_idx if type(self.match.lastindex) == int else self.match.lastindex    @property    def pos(self):        &quot;&quot;&quot; 本次搜索开始位置索引 &quot;&quot;&quot;        return self.match.pos + self.start_idx    @property    def re(self):        &quot;&quot;&quot; 本次搜索使用的SRE_Pattern对象 &quot;&quot;&quot;        return self.match.re    @property    def regs(self):        &quot;&quot;&quot; 列表，元素为元组，包含本次搜索匹配到的所有分组的起止位置 &quot;&quot;&quot;        return tuple([(s + self.start_idx, e + self.start_idx) for s, e in self.match.regs])    def end(self, *args, **kwargs):        &quot;&quot;&quot; 返回指定分组的结束位置，默认返回正则表达式所匹配到的最后一个字符的索引 &quot;&quot;&quot;        return self.match.end(*args, **kwargs) + self.start_idx    def expand(self, *args, **kwargs):        &quot;&quot;&quot; 根据模版返回相应的字符串，类似与 sub 函数里面的 repl， 可使用 \1 或者 \g 来选择分组 &quot;&quot;&quot;        return self.match.expand(*args, **kwargs)    def group(self, *args, **kwargs):        &quot;&quot;&quot; 根据提供的索引或名字返回响应分组的内容，默认返回 start() 到 end() 之间的字符串， 提供多个参数将返回一个元组 &quot;&quot;&quot;        return self.match.group(*args, **kwargs)    def groupdict(self, *args, **kwargs):        &quot;&quot;&quot; 返回一个包含所有匹配到的命名分组的字典，没有命名的分组不包含在内，key 为组名， value 为匹配到的内容，参数 default 为没有参与本次匹配的命名分组提供默认值 &quot;&quot;&quot;        return self.match.groupdict(*args, **kwargs)    def _build_key_2_index(self):        groups = self.match.groups()        group_2_index = []        for idx, group in enumerate(groups, start=1):            group_2_index.append((group, self.match.start(idx)+self.start_idx, self.match.end(idx)+self.start_idx))        return group_2_index    def groupdict_with_index(self, *args, **kwargs):        &quot;&quot;&quot; 同groupdict, 但匹配结果包装为带索引的ContentWithIndex对象 &quot;&quot;&quot;        value = self.match.groupdict(*args, **kwargs)        new_group_dict = dict()        for k, v in value.items():            for group, s, e in self._build_key_2_index():                if v == group:                    new_group_dict[k] = ContentWithIndex(content=v, start=s, end=e)        assert len(new_group_dict) == len(value)        return new_group_dict    def groups(self, *args, **kwargs):        &quot;&quot;&quot; 以元组形式返回每一个分组匹配到的字符串，包括没有参与匹配的分组，其值为 default &quot;&quot;&quot;        return self.match.groups(*args, **kwargs)    def span(self, *args, **kwargs):        &quot;&quot;&quot; 返回指定分组的起止位置组成的元组，默认返回由 start() 和 end() 组成的元组 &quot;&quot;&quot;        return tuple([(s + self.start_idx, e + self.start_idx) for s, e in self.match.span(*args, **kwargs)])    def start(self, *args, **kwargs):        &quot;&quot;&quot; 返回指定分组的开始位置，默认返回正则表达式所匹配到的第一个字符的索引 &quot;&quot;&quot;        return self.match.start(*args, **kwargs) + self.start_idx    def start_and_end(self):        return self.start(), self.end()    def __str__(self):        return self.__repr__()    def __repr__(self):        return f&quot;&lt;regex.Match object; span=({self.start()}, {self.end()}), match='{self.match.group()}'&gt;&quot;匹配结果类ContentWithIndex：主要是统一像split这样的方法返回的结果12345678910111213141516171819202122232425262728class ContentWithIndex:    def __init__(self, content: str, start: int, end: int):        &quot;&quot;&quot;        带索引的切分结果        :param content: 切分结果        :param start: 起始位置        :param end: 结束位置        &quot;&quot;&quot;        self.content = content        self.start = start        self.end = end    @property    def pos(self):        return self.start    @property    def posend(self):        return self.end    def start_and_end(self):        return self.start, self.end    def __str__(self):        return self.__repr__()    def __repr__(self):        return f&quot;{self.content} ({self.start}, {self.end})&quot;re包装的工具类ReUtils123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778class ReUtils:    def __init__(self):        &quot;&quot;&quot; 正则匹配的工具类 &quot;&quot;&quot;        pass    @staticmethod    def match(pattern, string, flags=0, start: int = 0):        match = re.match(pattern=pattern, string=string, flags=flags)        return ReMatch(match=match, start=start)    @staticmethod    def fullmatch(pattern, string, flags=0, start: int = 0):        match = re.fullmatch(pattern=pattern, string=string, flags=flags)        return ReMatch(match=match, start=start)    @staticmethod    def search(pattern, string, flags=0, start: int = 0):        match = re.search(pattern=pattern, string=string, flags=flags)        return ReMatch(match=match, start=start)    @staticmethod    def sub(pattern, repl, string, count=0, flags=0):        return re.sub(pattern=pattern, repl=repl, string=string, count=count, flags=flags)    @staticmethod    def subn(pattern, repl, string, count=0, flags=0):        return re.subn(pattern=pattern, repl=repl, string=string, count=count, flags=flags)    @staticmethod    def split(pattern, string, maxsplit=0, flags=0):        return re.split(pattern=pattern, string=string, maxsplit=maxsplit, flags=flags)    @staticmethod    def split_with_index(pattern, string, maxsplit: int = 0, flags=0, start: int = 0) -&gt; List[ContentWithIndex]:        &quot;&quot;&quot;        split带内容索引        :param pattern:        :param string:        :param maxsplit:        :param flags:        :param start:        :return:        &quot;&quot;&quot;        matches = ReUtils.finditer(pattern, string, flags, start=start)  # 使用re.finditer()获取匹配的位置和内容        content_with_indexs = []        start_idx = start        for match in matches:            end_idx = match.start()  # 当前分隔符的起始位置            content_with_index = ContentWithIndex(content=string[start_idx - start:end_idx - start], start=start_idx,                                                  end=end_idx)            content_with_indexs.append(content_with_index)  # 添加分隔符之前的文本及其位置信息            start_idx = match.end()  # 更新下一个分隔符的起始位置            if maxsplit and len(content_with_indexs) &gt;= maxsplit:                content_with_index = ContentWithIndex(content=string[start_idx - start:], start=start_idx,                                                      end=len(string))                content_with_indexs.append(content_with_index)  # 添加分隔符之前的文本及其位置信息                break        else:            # 添加最后一个分隔符之后的文本            content_with_index = ContentWithIndex(content=string[start_idx - start:], start=start_idx,                                                  end=len(string) + start)            content_with_indexs.append(content_with_index)        # if maxsplit and len(content_with_indexs) &lt; maxsplit + 1 and start_idx &lt; len(string):        #     # 添加最后一个分隔符之后的文本        #     content_with_index = ContentWithIndex(content=string[start_idx-start:], start=start_idx, end=len(string))        #     content_with_indexs.append(content_with_index)        return content_with_indexs    @staticmethod    def findall(pattern, string, flags=0):        return re.findall(pattern=pattern, string=string, flags=flags)    @staticmethod    def finditer(pattern, string, flags=0, start: int = 0) -&gt; Iterable[ReMatch]:        for match in re.finditer(pattern=pattern, string=string, flags=flags):            yield ReMatch(match=match, start=start)</content>
	</entry>
	<entry>
		<title>python装饰器</title>
		<url>/article/45122.html</url>
		<content>函数基础函数定义在 Python 中，函数是一等对象，编程语言理论家把“一等对象”定义为满足下述条件的程序实体：在运行时创建能赋值给变量或数据结构中的元素能作为参数传给函数能作为函数的返回结果有了一等函数，就可以使用函数式风格编程。函数式编程的特点之一是使用高阶函数——接受函数为参数，或者把函数作为结果返回的函数是高阶函数（higher-order function）。在函数式编程范式中，最为人熟知的高阶函数有 map、filter、reduce 和 apply。在 Python 3 中，map 和 filter 还是内置函数，但是由于引入了列表推导和生成器表达式，它们变得没那么重要了。sum 和 reduce 的通用思想是把某个操作连续应用到序列的元素上，累计之前的结果，把一系列值归约成一个值。all 和 any 也是内置的归约函数。all(iterable): 如果 iterable 的每个元素都是真值，返回 True；all([]) 返回 True。any(iterable): 只要 iterable 中有元素是真值，就返回 True；any([]) 返回 False。lambda 关键字在 Python 表达式内创建匿名函数。lambda 函数的定义体中不能赋值，也不能使用 while 和 try 等 Python 语句。lambda 句法只是语法糖：与 def 语句一样，lambda 表达式会创建函数对象。函数和方法从几个维度来介绍下python中函数和方法的区别：分类角度分析(1) 函数的分类：内置函数：python内嵌的一些函数。匿名函数：一行代码实现一个函数功能。递归函数自定义函数：根据自己的需求，来进行定义函数。(2) 方法的分类：普通方法：直接用self调用的方法。私有方法：__函数名，只能在类中被调用的方法。属性方法：@property，将方法伪装成为属性，让代码看起来更合理。特殊方法(双下划线方法)：以__init__为例，是用来封装实例化对象的属性，只要是实例化对象就一定会执行__init__方法，如果对象子类中没有则会寻找父类（超类），如果父类（超类）也没有，则直接继承object(python 3.x)类，执行类中的__init__方法。类方法：通过类名的调用去操作公共模板中的属性和方法。静态方法：不用传入类空间、对象的方法， 作用是保证代码的一致性，规范性，可以完全独立类外的一个方法，但是为了代码的一致性统一的放到某个模块(py文件)中。作用域角度分析(1) 函数作用域：从函数调用开始至函数执行完成，返回给调用者后，在执行过程中开辟的空间会自动释放，也就是说函数执行完成后，函数体内部通过赋值等方式修改变量的值不会保留，会随着返回给调用者后，开辟的空间会自动释放。(2) 方法作用域：通过实例化的对象进行方法的调用，调用后开辟的空间不会释放，也就是说调用方法中对变量的修改值会一直保留。调用方式分析(1) 函数：通过函数名()的方式进行调用。(2) 方法：通过对象.方法名()的方式进行调用。123456789101112class Foo(object):    def func(self):        pass#实例化obj = Foo()# 执行方式一:调用的func是方法obj.func() #func 方法# 执行方式二：调用的func是函数Foo.func(123) # 函数可调用对象除了用户定义的函数，调用运算符（即 ()）还可以应用到其他对象上。如果想判断对象能否调用，可以使用内置的 callable() 函数。如果类定义了 __call__ 方法，那么它的实例可以作为函数调用。判断对象能否调用，最安全的方法是使用内置的 callable() 函数任何 Python 对象都可以表现得像函数。为此，只需实现实例方法 __call__。函数内省除了 __doc__，函数对象还有很多属性。使用 dir 函数可以探知 factorial 具有下述属性：123456789&gt;&gt;&gt; dir(factorial)['__annotations__', '__call__', '__class__', '__closure__', '__code__','__defaults__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__','__format__', '__ge__', '__get__', '__getattribute__', '__globals__','__gt__', '__hash__', '__init__', '__kwdefaults__', '__le__', '__lt__','__module__', '__name__', '__ne__', '__new__', '__qualname__', '__reduce__','__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__','__subclasshook__']&gt;&gt;&gt;用户定义的函数的属性名称类型说明__annotations__dict参数和返回值的注解__call__method-wrapper实现 () 运算符；即可调用对象协议__closure__tuple函数闭包，即自由变量的绑定（通常是 None）__code__code编译成字节码的函数元数据和函数定义体__defaults__tuple形式参数的默认值__get__method-wrapper实现只读描述符协议（参见第 20 章）__globals__dict函数所在模块中的全局变量__kwdefaults__dict仅限关键字形式参数的默认值__name__str函数名称__qualname__str函数的限定名称，如 Random.choice（ 参阅PEP 3155） __defaults__、__code__ 和 __annotations__ 属性，IDE 和框架使用它们提取关于函数签名的信息。参数传递args、*kwargs用法1234567891011121314# *args是用来发送一个非键值对的可变数量的参数列表给一个函数def test_var_args(f_arg, *argv):    print(&quot;first normal arg:&quot;, f_arg)    for arg in argv:        print(&quot;another arg through *argv:&quot;, arg)test_var_args('yasoob', 'python', 'eggs', 'test')first normal arg: yasoobanother arg through *argv: pythonanother arg through *argv: eggsanother arg through *argv: test12345678910# ** kwargs允许您将keyworded可变长度的参数传递给函数。如果要在函数中处理命名参数，则应使用** kwargs def greet_me(**kwargs):    for key, value in kwargs.items():        print(&quot;{0} = {1}&quot;.format(key, value))# 必须含有keygreet_me(name=&quot;yasoob&quot;,age=&quot;19&quot;,sex=&quot;girl&quot;)name = yasoobage = 19sex = girlargs、*kwargs用法1234567891011121314151617181920212223def foo(*args, **kwargs):    print('args = ', args)    print('kwargs = ', kwargs)    print('---------------------------------------')if __name__ == '__main__':    foo(1,2,3,4)    foo(a=1,b=2,c=3)    foo(1,2,3,4, a=1,b=2,c=3)    foo('a', 1, None, a=1, b='2', c=3)    args =  (1, 2, 3, 4)kwargs =  {}---------------------------------------args =  ()kwargs =  {'a': 1, 'b': 2, 'c': 3}---------------------------------------args =  (1, 2, 3, 4)kwargs =  {'a': 1, 'b': 2, 'c': 3}---------------------------------------args =  ('a', 1, None)kwargs =  {'a': 1, 'b': '2', 'c': 3}---------------------------------------*args`表示任何多个无名参数，它是一个tuple**kwargs表示关键字参数，它是一个dict同时使用*args和**kwargs时，必须*args参数列要在**kwargs前有限个参数12345678910111213141516# -*- coding: UTF-8 -*- def log(func):    def wrapper(a,b):        print(&quot;call test(%d，%d)&quot; %(a,b))        return func(a,b)    return wrapper@logdef test(a,b):    print(&quot;sum = %d&quot; % (a+b))    test(2,4)call test(2，4)sum = 6可变参数12345678910111213141516# 当装饰器不知道业务函数到底有多少个参数时，用*args 来代替def log(func):    def wrapper(*args):        print(&quot;call test()&quot; )        return func(*args)    return wrapper@logdef test(a,b,c):    print(&quot;sum = %d&quot; % (a+b+c))test(2,4,5)call test()sum = 11函数注解函数声明中的各个参数可以在 : 之后增加注解表达式。如果参数有默认值，注解放在参数名和 = 号之间。如果想注解返回值，在 ) 和函数声明末尾的 : 之间添加 -&gt; 和一个表达式。1234567&gt;from typing import List&gt;def func(num: int = 0, type: str = 'default') -&gt; List[int]:return [num, num]&gt;func(num=2)&gt;Out[3]: [2, 2]注解不会做任何处理，只是存储在函数的 __annotations__ 属性（一个字典）Python 对注解所做的唯一的事情是，把它们存储在函数的 __annotations__ 属性里。仅此而已，Python 不做检查、不做强制、不做验证，什么操作都不做。换句话说，注解对 Python 解释器没有任何意义。注解只是元数据，可以供 IDE、框架和装饰器等工具使用。标准库中还没有什么会用到这些元数据，唯有 inspect.signature() 函数知道怎么提取注解signature 函数返回一个 Signature 对象，它有一个 return_annotation 属性和一个 parameters 属性，后者是一个字典，把参数名映射到 Parameter 对象上。每个 Parameter 对象自己也有 annotation 属性inspect.signature 函数返回一个 inspect.Signature 对象，它有一个 parameters 属性，这是一个有序映射，把参数名和 inspect.Parameter 对象对应起来。各个 Parameter 属性也有自己的属性，例如 name 、 default 和 kind 。特殊的 inspect._empty 值表示没有默认值，考虑到 None 是有效的默认值（也经常这么做），而且这么做是合理的。kind 属性的值是 _ParameterKind 类中的 5 个值之一，列举如下。12345678910POSITIONAL_OR_KEYWORD可以通过定位参数和关键字参数传入的形参（多数 Python 函数的参数属于此类）。VAR_POSITIONAL定位参数元组。VAR_KEYWORD关键字参数字典。KEYWORD_ONLY仅限关键字参数（Python 3 新增）。POSITIONAL_ONLY仅限定位参数；目前，Python 声明函数的句法不支持，但是有些使用 C 语言实现且不接受关键字参数的函数（如 divmod ）支持。除了 name 、 default 和 kind ， inspect.Parameter 对象还有一个 annotation （注解）属性，它的值通常是 inspect._emptyinspect.Signature 对象有个 bind 方法，它可以把任意个参数绑定到签名中的形参上，所用的规则与实参到形参的匹配方式一样。函数式编程包operator模块Python 的目标不是变成函数式编程语言，但是得益于 operator 和 functools 等包的支持，函数式编程风格也可以信手拈来。reduce在函数式编程中，经常需要把算术运算符当作函数使用。例如，不使用递归计算阶乘。求和可以使用 sum 函数，但是求积则没有这样的函数。我们可以使用 reduceoperator 模块为多个算术运算符提供了对应的函数，从而避免编写 lambda a, b: a*b 这种平凡的匿名函数12345&gt;from functools import reduce&gt;from operator import mul&gt;def fact(n):       return reduce(mul, range(1, n+1))operator 模块中还有一类函数，能替代从序列中取出元素或读取对象属性的 lambda 表达式：因此，itemgetter 和 attrgetter 其实会自行构建函数。itemgetteritemgetter 的常见用途：根据元组的某个字段给元组列表排序。itemgetter(1) 的作用与 lambda fields: fields[1] 一样：创建一个接受集合的函数，返回索引位 1 上的元素。123456789101112131415161718192021&gt;&gt;&gt;&gt; metro_data = [&gt;...     ('Tokyo', 'JP', 36.933, (35.689722, 139.691667)),&gt;...     ('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889)),&gt;...     ('Mexico City', 'MX', 20.142, (19.433333, -99.133333)),&gt;...     ('New York-Newark', 'US', 20.104, (40.808611, -74.020386)),&gt;...     ('Sao Paulo', 'BR', 19.649, (-23.547778, -46.635833)),&gt;... ]&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; from operator import itemgetter&gt;&gt;&gt;&gt; for city in sorted(metro_data, key=itemgetter(1)):&gt;...     print(city)&gt;...&gt;('Sao Paulo', 'BR', 19.649, (-23.547778, -46.635833))&gt;('Delhi NCR', 'IN', 21.935, (28.613889, 77.208889))&gt;('Tokyo', 'JP', 36.933, (35.689722, 139.691667))&gt;('Mexico City', 'MX', 20.142, (19.433333, -99.133333))&gt;('New York-Newark', 'US', 20.104, (40.808611, -74.020386))&gt;d = {&quot;a&quot;:1,&quot;b&quot;:2,&quot;c&quot;:3}&gt;itemgetter(&quot;a&quot;,'b')(d)&gt;Out[67]: (1, 2)attrgetterattrgetter 与 itemgetter 作用类似，它创建的函数根据名称提取对象的属性。如果把多个属性名传给 attrgetter，它也会返回提取的值构成的元组。12&gt;&gt;&gt;&gt; for city in sorted(metro_areas, key=attrgetter('coord.lat')):&gt;...     print(name_lat(city))methodcallermethodcaller。它的作用与 attrgetter 和 itemgetter 类似，它会自行创建函数。methodcaller 创建的函数会在对象上调用参数指定的方法12345678&gt;&gt;&gt;&gt; from operator import methodcaller&gt;&gt;&gt;&gt; s = 'The time has come'&gt;&gt;&gt;&gt; upcase = methodcaller('upper')&gt;&gt;&gt;&gt; upcase(s)&gt;'THE TIME HAS COME'&gt;&gt;&gt;&gt; hiphenate = methodcaller('replace', ' ', '-')&gt;&gt;&gt;&gt; hiphenate(s)&gt;'The-time-has-come'其他模块下面是 operator 模块中定义的部分函数（省略了以 _ 开头的名称，因为它们基本上是实现细节）：Python 3.5 中增加了 imatmul 和 matmul。123456789&gt;&gt;&gt;&gt; [name for name in dir(operator) if not name.startswith('_')]&gt;['abs', 'add', 'and_', 'attrgetter', 'concat', 'contains',&gt;'countOf', 'delitem', 'eq', 'floordiv', 'ge', 'getitem', 'gt',&gt;'iadd', 'iand', 'iconcat', 'ifloordiv', 'ilshift', 'imod', 'imul',&gt;'index', 'indexOf', 'inv', 'invert', 'ior', 'ipow', 'irshift',&gt;'is_', 'is_not', 'isub', 'itemgetter', 'itruediv', 'ixor', 'le',&gt;'length_hint', 'lshift', 'lt', 'methodcaller', 'mod', 'mul', 'ne',&gt;'neg', 'not_', 'or_', 'pos', 'pow', 'rshift', 'setitem', 'sub',&gt;'truediv', 'truth', 'xor']这 52 个名称中大部分的作用不言而喻。以 i 开头、后面是另一个运算符的那些名称（如 iadd、iand 等），对应的是增量赋值运算符（如 +=、&amp;= 等）。如果第一个参数是可变的，那么这些运算符函数会就地修改它；否则，作用与不带 i 的函数一样，直接返回运算结果以下表格显示了抽象运算是如何对应于 Python 语法中的运算符和 operator 模块中的函数的。运算语法函数加法a + badd(a, b)字符串拼接seq1 + seq2concat(seq1, seq2)包含测试obj in seqcontains(seq, obj)除法a / btruediv(a, b)整除法a // bfloordiv(a, b)按位与a &amp; band_(a, b)按位异或a ^ bxor(a, b)按位取反~ ainvert(a)按位或a  |  bor_(a, b)取幂a ** bpow(a, b)一致标识a is bis_(a, b)一致标识a is not bis_not(a, b)索引赋值obj[k] = vsetitem(obj, k, v)索引删除del obj[k]delitem(obj, k)索引取值obj[k]getitem(obj, k)左移a &lt;&lt; blshift(a, b)取模a % bmod(a, b)乘法a * bmul(a, b)矩阵乘法a @ bmatmul(a, b)取反（算术）- aneg(a)取反（逻辑）not anot_(a)正数+ apos(a)右移a &gt;&gt; brshift(a, b)切片赋值seq[i:j] = valuessetitem(seq, slice(i, j), values)切片删除del seq[i:j]delitem(seq, slice(i, j))切片取值seq[i:j]getitem(seq, slice(i, j))字符串格式化s % objmod(s, obj)减法a - bsub(a, b)真值测试objtruth(obj)比较a &lt; blt(a, b)比较a &lt;= ble(a, b)相等a == beq(a, b)不等a != bne(a, b)比较a &gt;= bge(a, b)比较a &gt; bgt(a, b)functools模块每周一个 Python 模块 | functoolsPython functools 模块说到高阶函数，这是函数式编程范式中很重要的一个概念，简单地说， 就是一个可以接受函数作为参数或者以函数作为返回值的函数，因为 Python 中函数是一类对象， 因此很容易支持这样的函数式特性。functools 模块中函数只有 cmp_to_key、partial、reduce、total_ordering、update_wrapper、wraps、lru_cache 这几个：partial用于创建一个偏函数，它用一些默认参数包装一个可调用对象，返回结果是可调用对象，并且可以像原始对象一样对待，这样可以简化函数调用。12345from functools import partialdef add(x, y):    return x + yadd_y = partial(add, 3)  # add_y 是一个函数add_y(4)                 # 结果是 7partialmethodpartialmethod是 Python 3.4 中新引入的装饰器，作用基本类似于 partial， 不过仅作用于方法(函数和方法的异同见上方)。123456789101112131415class Cell(object):    def __init__(self):        self._alive = False    @property    def alive(self):        return self._alive    def set_state(self, state):        self._alive = bool(state)    set_alive = partialmethod(set_state, True)    set_dead = partialmethod(set_state, False)c = Cell()c.alive         # Falsec.set_alive()c.alive         # Truetotal_orderingtotal_ordering 同样是 Python 2.7 中新增函数，用于简化比较函数的写法。如果你已经定义了 __eq__ 方法，以及 __lt__、__le__、__gt__ 或者 __ge__() 其中之一， 即可自动生成其它比较方法。官方示例：12345678910@total_orderingclass Student:    def __eq__(self, other):        return ((self.lastname.lower(), self.firstname.lower()) ==                (other.lastname.lower(), other.firstname.lower()))    def __lt__(self, other):        return ((self.lastname.lower(), self.firstname.lower()) &lt;                (other.lastname.lower(), other.firstname.lower()))dir(Student)    # ['__doc__', '__eq__', '__ge__', '__gt__', '__le__', '__lt__', '__module__']cmp_to_keycmp_to_key 是 Python 2.7 中新增的函数，用于将比较函数转换为 key 函数， 这样就可以应用在接受 key 函数为参数的函数中。比如 sorted()、min()、 max()、 heapq.nlargest()、 itertools.groupby() 等。1sorted(range(5), key=cmp_to_key(lambda x, y: y-x))      # [4, 3, 2, 1, 0]lru_cache一个装饰器是在 Python3 中新加的，在 Python2 中如果想要使用可以安装第三方库 functools32。该装饰器用于缓存函数的调用结果，对于需要多次调用的函数，而且每次调用参数都相同，则可以用该装饰器缓存调用结果，从而加快程序运行。12345678910111213141516from functools import lru_cache@lru_cache(None)def add(x, y):    print(&quot;calculating: %s + %s&quot; % (x, y))    return x + yprint(add(1, 2))print(add(1, 2))  # 直接返回缓存信息print(add(2, 3))calculating: 1 + 233calculating: 2 + 35由于该装饰器会将不同的调用结果缓存在内存中，因此需要注意内存占用问题，避免占用过多内存，从而影响系统性能。lru_cache 可以使用两个可选的参数来配置maxsize 参数指定存储多少个调用的结果。缓存满了之后，旧的结果会被扔掉，腾出空间。为了得到最佳性能，maxsize 应该设为 2 的幂。typed 参数如果设为 True，把不同参数类型得到的结果分开保存，即把通常认为相等的浮点数和整数参数（如 1 和 1.0）区分开。顺便说一下，因为 lru_cache 使用字典存储结果，而且键根据调用时传入的定位参数和关键字参数创建，所以被 lru_cache 装饰的函数，它的所有参数都必须是可散列的。singledispatch见本页-单分派泛函数singledispatchwraps使用装饰器极大地复用了代码，但是他有一个缺点就是原函数的元信息不见了，比如函数的docstring、name、参数列表functools.wraps，wraps本身也是一个装饰器，它能把原函数的元信息拷贝到装饰器里面的 func 函数中，这使得装饰器里面的log函数也有和原函数test一样的元信息了说到“接受函数为参数，以函数为返回值”，在 Python 中最常用的当属装饰器了。 functools 库中装饰器相关的函数是 update_wrapper、wraps，还搭配 WRAPPER_ASSIGNMENTS 和 WRAPPER_UPDATES 两个常量使用，作用就是消除 Python 装饰器的一些负面作用。12345678910111213141516def log(func):    def wrapper():        print(&quot;call test()&quot;)        return func()    return wrapper@logdef test():    print(&quot;this is what I want&quot;)test()print(test.__name__)call test()this is what I wantwrapper1234567891011121314151617181920import functoolsdef log(func):    @functools.wraps(func)    def wrapper():        print(&quot;call test()&quot;)        return func()    return wrapper@logdef test():    print(&quot;this is what I want&quot;)test()print(test.__name__)call test()this is what I wanttestupdate_wrapperupdate_wrapper 的作用与 wraps 类似，不过功能更加强大，换句话说，wraps 其实是 update_wrapper 的特殊化，实际上 wraps(wrapped) 相当于 partial(update_wrapper, wrapped=wrapped, **kwargs)。1234def decorator(func):    def wrapper(*args, **kwargs):        return func(*args, **kwargs)    return update_wrapper(wrapper, func)装饰器(Dercrator)必须会 Python 装饰器的五个理由Python装饰器用于有切面需求的场景，比如：插入日志、性能测试、事务处理、缓存、权限校验等场景用装饰器抽离出大量与函数功能本身无关的雷同代码到装饰器中并继续重用严格来说，装饰器只是语法糖，装饰器的两大特性是：能把被装饰的函数替换成其他函数函数装饰器在加载模块时立即执行(Python 加载模块时)，被装饰的函数只有在明确调用时运行，这突出了导入时和运行时的区别。装饰器的4种类型：函数装饰函数、函数装饰类、类装饰函数、类装饰类globals关键字返回一个字典，表示当前的全局符号表。这个符号表始终针对当前模块（对函数或方法来说，是指定义它们的模块，而不是调用它们的模块）。方式一 globals 函数帮助 best_promo 自动找到其他可用的 *_promo 函数123promos = [globals()[name] for name in globals()  ➊            if name.endswith('_promo')  ➋            and name != 'best_promo']   ➌方式二收集所有可用促销的另一种方法是，在一个单独的模块中保存所有策略函数，把 best_promo 排除在外。最大的变化是内省名为 promotions 的独立模块，构建策略函数列表。12promos = [func for name, func in                inspect.getmembers(promotions, inspect.isfunction)]闭包创建保有内部状态的函数，还有一种截然不同的方式——使用闭包。在表达式中引用变量时，Python解释器将按如下顺序遍历各作用域，以解析该引用:当前函数的作用域任何外围作用域(例如，包含当前函数的其他函数)包含当前代码的那个模块的作用域(也叫全局作用域，global scope)内置作用域(也就是包含len及str等函数的那个作用域)如果上面这些地方都没有定义过名称相符的变量，那就抛出NameError异常。nonlocal语句清楚地表明: 如果在闭包内给该变量赋值，那么修改的其实是闭包外那个作用域中的变量。这与global语句互为补充，global 用来表示对该变量的赋值操作，将会直接修改模块作用域里的那个变量。ps：按照我们正常的认知，一个函数结束的时候，会把自己的临时变量都释放还给内存，之后变量都不存在了。一般情况下，确实是这样的。但是闭包是一个特别的情况，外部函数发现，自己的临时变量会在将来的内部函数中用到，自己在结束的时候，返回内函数的同时，会把外函数的临时变量送给内函数绑定在一起。所以外函数已经结束了，调用内函数的时候仍然能够使用外函数的临时变量。 global关键字如果在函数中赋值时想让解释器把 b 当成全局变量，要使用 global 声明：123456b = 6def func(a):   global b   print(a)   print(b)   b = 9自由变量闭包：只有涉及嵌套函数时才有闭包问题闭包指延伸了作用域的函数，其中包含函数定义体中引用、但是不在定义体中定义的非全局变量。函数是不是匿名的没有关系，关键是它能访问定义体之外定义的非全局变量。在 averager 函数中，series 是自由变量（free variable），指未在本地作用域中绑定的变量averager 的闭包延伸到那个函数的作用域之外，包含自由变量 series 的绑定只有嵌套在其他函数中的函数才可能需要处理不在全局作用域中的外部变量nonlocal声明但是对数字、字符串、元组等不可变类型来说，只能读取，不能更新。如果尝试重新绑定，例如 count = count + 1，其实会隐式创建局部变量 count。这样，count 就不是自由变量了，因此不会保存在闭包中，为了解决这个问题，Python 3 引入了 nonlocal 声明。它的作用是把变量标记为自由变量，即使在函数中为变量赋予新值了，也会变成自由变量。如果为 nonlocal 声明的变量赋予新值，闭包中保存的绑定会更新。12345678910def make_averager():    count, total = 0, 0    def averager(new_value):        nonlocal count, total        count += 1        total += new_value        return total / count    return averager小结：外函数内部定义可变类型的变量可以在内函数使用，对数字、字符串、元组等不可变类型来说，需要使用 nonlocal 声明变量为自由变量，内函数才可以访问到。函数装饰函数装饰器的典型行为：把被装饰的函数替换成新函数，二者接受相同的参数，而且（通常）返回被装饰的函数本该返回的值，同时还会做些额外操作。1234567891011121314151617181920212223def decorator(func):    def inner(*args, **kwargs):        print('before...........')        res = func(*args, **kwargs)        print('after............')        return res    return inner@decoratordef run():    print('run...............')    return 0if __name__ == &quot;__main__&quot;:    run()    run.__name__    # 此时decorator叫做装饰器------------------------------------------before...........run...............after............inner--------------------- functools.wrap装饰器inner的返回值要与func的一致，并且inner与func参数相同为了不改变被装饰函数或类的性质，添加functools.wrap装饰器12345678910111213141516171819202122232425from functools import wrapsdef decorator(func):    @wraps(func)    def inner():        print('before...........')        res = func()        print('after............')        return res    return inner@decoratordef run():    print('run...............')    return 0if __name__ == &quot;__main__&quot;:    run()    print(run.__name__)------------------------------------------before...........run...............after............run--------------------- 带参数的装饰器(3层)1234567891011121314151617181920212223242526272829303132333435from functools import wrapsfrom datetime import datetimedef start():    return datetime.now()def end():    return datetime.now()def Filter(start_time, end_time):    def decorator(func):        @wraps(func)        def inner(*args, **kwargs):            s = start_time()            res = func(*args,**kwargs)            e = end_time()            print(&quot;耗时 {} s&quot;.format((e-s).total_seconds()))            return res        return inner    return decorator@Filter(start, end)def run():    for i in range(2):        for j in range(3):            print(j)    return 0if __name__ == &quot;__main__&quot;:    run()    012012耗时 0.003987 s   带有不定参数的装饰器123456789101112131415161718192021222324252627282930313233343536# 带有不定参数的装饰器# 拓展的函数好多可是有参数，有的参数还是个数不定的那种import timedef deco(func):    def wrapper(*args, **kwargs):        startTime = time.time()        func(*args, **kwargs)        endTime = time.time()        msecs = (endTime - startTime) * 1000        print(&quot;time is %d ms&quot; % msecs)    return wrapper@decodef func(a, b):    print(&quot;hello,here is a func for add:&quot;)    time.sleep(1)    print(&quot;result is %d&quot; % (a + b))@decodef func2(a, b, c):    print(&quot;hello,here is a func for add:&quot;)    time.sleep(1)    print(&quot;result is %d&quot; % (a + b + c))if __name__ == '__main__':    f = func    func2(3, 4, 5)    f(3, 4)    hello,here is a func for add:result is 12time is 1000 mshello,here is a func for add:result is 7time is 1000 ms多个装饰器1234567891011121314151617181920212223242526272829303132333435363738394041# 一个函数需要加入很多功能，一个装饰器怕是搞不定，装饰器能支持多个嘛# 多个装饰器执行的顺序就是从最后一个装饰器开始，执行到第一个装饰器，再执行函数本身。# 多个装饰器import timedef deco01(func):    def urapper(*args, **kwargs):        print(&quot;this is decoe1&quot;)        startTime = time.time()        func(*args, **kwargs)        endTime = time.time()        msecs = (endTime - startTime) * 1000        print(&quot;time is %d ms&quot; % msecs)        print(&quot;decoe1 end here&quot;)    return urapperdef deco02(func):    def wrapper(*args, **kwargs):        print(&quot;this is decoe2&quot;)        func(*args, **kwargs)        print(&quot;decoe2 end here&quot;)    return wrapper@deco01@deco02def func(a, b):    print(&quot;hello,here is a func for add:&quot;)    time.sleep(1)    print(&quot;result is %d&quot; % (a + b))if __name__ == '__main__':    f = func    f(3, 4)    this is decoe1this is decoe2hello,here is a func for add:result is 7decoe2 end heretime is 1003 msdecoe1 end here函数装饰类12345678910111213141516171819def wrapClass(cls):    def inner(a):        print('class name:', cls.__name__)        return cls(a)    return inner@wrapClassclass Foo():    def __init__(self, a):        self.a = a    def fun(self):        print('self.a =', self.a)m = Foo('xiemanR')m.fun()class name: Fooself.a = xiemanR定义装饰器不仅可以是函数，还可以是类 相比函数装饰器，类装饰器具有灵活度大、高内聚、封装性等优点像__call__这样前后都带下划线的方法在Python中被称为内置(魔法)方法。重载这些魔法方法一般会改变对象的内部行为用法让类的构造函数__init__()接受一个函数重载call()并 返回一个函数使用@类形式将装饰器附加到业务函数上类装饰函数1234567891011121314class ShowFunName():    def __init__(self, func):        self._func = func    def __call__(self, a):        print('function name:', self._func.__name__)        return self._func(a)@ShowFunNamedef Bar(a):    return aprint(Bar('xiemanR'))function name: BarxiemanR类装饰类1234567891011121314151617class ShowClassName(object):    def __init__(self, cls):        self._cls = cls    def __call__(self, a):        print('class name:', self._cls.__name__)        return self._cls(a)@ShowClassNameclass Foobar(object):    def __init__(self, a):        self.value = a    def fun(self):        print(self.value)a = Foobar('xiemanR')a.fun()class name: FoobarxiemanR内置装饰器命令行神器Click命令行神器 Click教程A篇12345678910111213141516# -*- coding: utf-8 -*import click@click.command()@click.option('--count', default=1, help='Number of greetings.')@click.option('--name', prompt='Your name', help='The person to greet.')def hello(count, name):    &quot;&quot;&quot;Simple program that greets NAME for a total of COUNT times.&quot;&quot;&quot;    for x in range(count):        click.echo('Hello %s!' % name)if __name__ == '__main__':    hello()Q:\pyCharmWS&gt;python ./tempTest.py --count=3 --name=EthanHello Ethan!Hello Ethan!Hello Ethan!@property和@classmethodpython中常用的内置装饰器123456789101112131415161718192021222324252627# @property# 使调用类中的方法像引用类中的字段属性一样。被修饰的特性方法，内部可以实现处理逻辑，但对外提供统一的调用方式。遵循了统一访问的原则。# @classmethod# 类方法的第一个参数是类，将类本身作为操作的方法。类方法被哪个类调用，就传入哪个类作为第一个参数进行操作。# 注意，静态方法和类方法是为类操作准备的。虽然通过实例也能调用，但是不建议# -*- coding: utf-8 -*-# coding: utf-8class TestClass:    name = &quot;test&quot;    def __init__(self, name):        self.name = name    @property    def sayHello(self):        print(&quot;hello&quot;, self.name)    @staticmethod    def fun(self, x, y):        return x + ycls = TestClass(&quot;felix&quot;)print(f&quot;通过实例引用属性: {cls.name}&quot;)print(f&quot;像引用属性一样调用@property修饰的方法: {cls.sayHello}&quot;)print(f&quot;类名直接引用静态方法: {TestClass.fun(None, 2, 3)}&quot;)通过实例引用属性: felixhello felix像引用属性一样调用@property修饰的方法: None类名直接引用静态方法: 5functools模块的内置装饰器更详细的见上方的functools模块lru_cache做备忘functools.lru_cache 是非常实用的装饰器，它实现了备忘（memoization）功能。这是一项优化技术，它把耗时的函数的结果保存起来，避免传入相同的参数时重复计算。LRU 三个字母是“Least Recently Used”的缩写，表明缓存不会无限制增长，一段时间不用的缓存会被扔掉。生成第 n 个斐波纳契数这种慢速递归函数适合使用 lru_cache123456789101112131415161718192021import functoolsfrom clockdeco import clock@functools.lru_cache() # ➊@clock  # ➋def fibonacci(n):    if n &lt; 2:        return n    return fibonacci(n-2) + fibonacci(n-1)if __name__=='__main__':    print(fibonacci(6))    # 这样一来，执行时间减半了，而且 n 的每个值只调用一次函数[0.00000119s] fibonacci(0) -&gt; 0[0.00000119s] fibonacci(1) -&gt; 1[0.00010800s] fibonacci(2) -&gt; 1[0.00000787s] fibonacci(3) -&gt; 2[0.00016093s] fibonacci(4) -&gt; 3[0.00001216s] fibonacci(5) -&gt; 5[0.00025296s] fibonacci(6) -&gt; 8除了优化递归算法之外，lru_cache 在从 Web 中获取信息的应用中也能发挥巨大作用。特别要注意，lru_cache 可以使用两个可选的参数来配置。它的签名是：1functools.lru_cache(maxsize=128, typed=False)maxsize 参数指定存储多少个调用的结果。缓存满了之后，旧的结果会被扔掉，腾出空间。为了得到最佳性能，maxsize 应该设为 2 的幂。typed 参数如果设为 True，把不同参数类型得到的结果分开保存，即把通常认为相等的浮点数和整数参数（如 1 和 1.0）区分开。顺便说一下，因为 lru_cache 使用字典存储结果，而且键根据调用时传入的定位参数和关键字参数创建，所以被 lru_cache 装饰的函数，它的所有参数都必须是可散列的。单分派泛函数singledispatchPEP 443 — Single-dispatch generic functionsfunctools.singledispatch 是 Python 3.4 增加的，PyPI 中的 singledispatch包可以向后兼容 Python 2.6 到 Python 3.3。假设我们在开发一个调试 Web 应用的工具，我们想生成 HTML，显示不同类型的 Python 对象。我们可能会编写这样的函数：12345import htmldef htmlize(obj):    content = html.escape(repr(obj))    return '&lt;pre&gt;{}&lt;/pre&gt;'.format(content)这个函数适用于任何 Python 类型，但是现在我们想做个扩展，让它使用特别的方式显示某些类型。str：把内部的换行符替换为 ' \n'；不使用 ，而是使用int：以十进制和十六进制显示数字list：输出一个 HTML 列表，根据各个元素的类型进行格式化因为 Python 不支持重载方法或函数，所以我们不能使用不同的签名定义 htmlize 的变体，也无法使用不同的方式处理不同的数据类型。在 Python 中，一种常见的做法是把 htmlize 变成一个分派函数，使用一串 if/elif/elif，调用专门的函数，如 htmlize_str、htmlize_int，等等。这样不便于模块的用户扩展，还显得笨拙：时间一长，分派函数 htmlize 会变得很大，而且它与各个专门函数之间的耦合也很紧密。Python 3.4 新增的 functools.singledispatch 装饰器可以把整体方案拆分成多个模块，甚至可以为你无法修改的类提供专门的函数。使用 @singledispatch 装饰的普通函数会变成泛函数（generic function）：根据第一个参数的类型，以不同方式执行相同操作的一组函数如果根据多个参数选择专门的函数，那就是多分派了。singledispatch 创建一个自定义的 htmlize.register 装饰器，把多个函数绑在一起组成一个泛函数123456789101112131415161718192021222324from functools import singledispatchfrom collections import abcimport numbersimport html@singledispatch  ➊def htmlize(obj):    content = html.escape(repr(obj))    return '&lt;pre&gt;{}&lt;/pre&gt;'.format(content)@htmlize.register(str)  ➋def _(text):            ➌    content = html.escape(text).replace('\n', '&lt;br&gt;\n')    return '&lt;p&gt;{0}&lt;/p&gt;'.format(content)@htmlize.register(numbers.Integral)  ➍def _(n):    return '&lt;pre&gt;{0} (0x{0:x})&lt;/pre&gt;'.format(n)@htmlize.register(tuple)  ➎@htmlize.register(abc.MutableSequence)def _(seq):    inner = '&lt;/li&gt;\n&lt;li&gt;'.join(htmlize(item) for item in seq)    return '&lt;ul&gt;\n&lt;li&gt;' + inner + '&lt;/li&gt;\n&lt;/ul&gt;'❶ @singledispatch 标记处理 object 类型的基函数。❷ 各个专门函数使用 @«base_function».register(«type») 装饰。❸ 专门函数的名称无关紧要；_ 是个不错的选择，简单明了。❹ 为每个需要特殊处理的类型注册一个函数。numbers.Integral 是 int 的虚拟超类。❺ 可以叠放多个 register 装饰器，让同一个函数支持不同类型。只要可能，注册的专门函数应该处理抽象基类（如 numbers.Integral 和 abc.MutableSequence），不要处理具体实现（如 int 和 list）。这样，代码支持的兼容类型更广泛。例如，Python 扩展可以子类化 numbers.Integral，使用固定的位数实现 int 类型。singledispatch 机制的一个显著特征是，你可以在系统的任何地方和任何模块中注册专门函数。如果后来在新的模块中定义了新的类型，可以轻松地添加一个新的专门函数来处理那个类型。此外，你还可以为不是自己编写的或者不能修改的类添加自定义函数。@singledispatch 不是为了把 Java 的那种方法重载带入 Python。在一个类中为同一个方法定义多个重载变体，比在一个函数中使用一长串 if/elif/elif/elif 块要更好。但是这两种方案都有缺陷，因为它们让代码单元（类或函数）承担的职责太多。@singledispath 的优点是支持模块化扩展：各个模块可以为它支持的各个类型注册一个专门函数。柯里化柯里化（Currying）将原来接受两个参数的函数变成新的接受一个参数的函数的过程。新的函数返回一个以原有第二个参数为参数的函数。123456789&gt;# pip install simplecurry -i https://pypi.tuna.tsinghua.edu.cn/simple&gt;from simplecurry import curried&gt;@curried&gt;def add2(a,b,c):return c * a + b&gt;add2(2)(5)(8)   &gt;&gt;&gt;&gt; 21curry化最大的意义在于把多个参数的function等价转化成多个单参数function的级联，这样所有的函数就都统一了，方便做lambda演算。 在scala里，curry化对类型推演也有帮助，scala的类型推演是局部的，在同一个参数列表中后面的参数不能借助前面的参数类型进行推演，curry化以后，放在两个参数列表里，后面一个参数列表里的参数可以借助前面一个参数列表里的参数类型进行推演。这就是为什么 foldLeft这种函数的定义都是curry的形式。案例时间装饰器计算函数运行时间123456789101112from functools import wraps# 作为装饰器使用，返回函数执行需要花费的时间def time_this_function(func):    @wraps(func)    def wrapper(*args, **kwargs):        start = time.time()        result = func(*args, **kwargs)        end = time.time()        print(f'{func.__name__}, 耗时{round(end - start, 4)}s')        return result    return wrapper</content>
	</entry>
	<entry>
		<title>python魔法函数</title>
		<url>/article/42715.html</url>
		<content>(译)Python魔法方法指南原文Python进阶：实例讲解Python中的魔法函数（Magic Methods）魔法函数所谓魔法函数（Magic Methods），是Python的一种高级语法，允许你在类中自定义函数（函数名格式一般为__xx__），并绑定到类的特殊方法中。比如在类A中自定义__str__()函数，则在调用str(A())时，会自动调用__str__()函数，并返回相应的结果。在我们平时的使用中，可能经常使用__init__函数和__del\函数，其实这也是魔法函数的一种。魔术方法（magic method）是特殊方法的昵称。特殊方法也叫双下方法通过实现特殊方法来利用 Python 数据模型的两个好处。作为你的类的用户，他们不必去记住标准操作的各式名称（“怎么得到元素的总数？是 .size() 还是 .length() 还是别的什么？”）。可以更加方便地利用 Python 的标准库，比如 random.choice 函数，从而不用重新发明轮子。特殊方法的存在是为了被 Python 解释器调用的，你自己并不需要调用它们。除非有大量的元编程存在，直接调用特殊方法的频率应该远远低于你去实现它们的次数。唯一的例外可能是 __init__ 方法Python 之禅”中的另外一句话：“不能让特例特殊到开始破坏既定规则。”python的魔法函数总被双下划线包围，它们可以给你的类增加特殊的方法。如果你的对象实现了这些方法中的一个，那么这个方法就会在特殊情况下被调用，你可以定义想要的行为，而这一切都是自动发生的。什么是魔法函数？ 基本分类构造方法最为熟知的基本的魔法方法就是__init__ ，我们可以用它来指明一个对象初始化的行为。然而，当我们调用 x = SomeClass() 的时候， __init__ 并不是第一个被调用的方法。事实上，第一个被调用的是 __new__ ，这个方法才真正地创建了实例。当这个对象的生命周期结束的时候，__del__ 会被调用。__new__(cls,[…])__new__ 是对象实例化时第一个调用的方法，它只取下 cls 参数，并把其他参数传给 __init__ 。 __new__ 很少使用，但是也有它适合的场景，尤其是当类继承自一个像元组或者字符串这样不经常改变的类型的时候。__init__(self,[…])类的初始化方法。它获取任何传给构造器的参数（比如我们调用 x = SomeClass(10, ‘foo’) ， __init__ 就会接到参数 10 和 ‘foo’ 。 __del__(self)__new__ 和 __init__ 是对象的构造器， __del__ 是对象的销毁器。它并非实现了语句 del x (因此该语句不等同于 x.__del__())。而是定义了当对象被垃圾回收时的行为。 当对象需要在销毁时做一些处理的时候这个方法很有用，比如 socket 对象、文件对象。但是需要注意的是，当Python解释器退出但对象仍然存活的时候， __del__ 并不会 执行。 所以养成一个手工清理的好习惯是很重要的，比如及时关闭连接。操作符使用Python魔法方法的一个巨大优势就是可以构建一个拥有Python内置类型行为的对象。这意味着你可以避免使用非标准的、丑陋的方式来表达简单的操作。 在一些语言中，这样做很常见:12if instance.equals(other_instance):    # do something运用魔法方法的魔力，我们可以定义方法 eq12if instance == other_instance:    # do something这是魔法力量的一部分，这样我们就可以创建一个像内建类型那样的对象了！比较操作符Python包含了一系列的魔法方法，用于实现对象之间直接比较，而不需要采用方法调用。同样也可以重载Python默认的比较方法，改变它们的行为。__cmp__(self, other)__cmp__ 是所有比较魔法方法中最基础的一个，它实际上定义了所有比较操作符的行为（&lt;,==,!=,等等），但是它可能不能按照你需要的方式工作（例如，判断一个实例和另一个实例是否相等采用一套标准，而与判断一个实例是否大于另一实例采用另一套）。 __cmp__ 应该在 self &lt; other 时返回一个负整数，在 self == other 时返回0，在 self &gt; other 时返回正整数。最好只定义你所需要的比较形式，而不是一次定义全部。 如果你需要实现所有的比较形式，而且它们的判断标准类似，那么 __cmp__ 是一个很好的方法，可以减少代码重复，让代码更简洁。3.x版本被移除。__eq__(self, other)定义等于操作符(==)的行为。__ne__(self, other)定义不等于操作符(!=)的行为。__lt__(self, other)定义小于操作符(&lt;)的行为。__gt__(self, other)定义大于操作符(&gt;)的行为。__le__(self, other)定义小于等于操作符(&lt;)的行为。__ge__(self, other)定义大于等于操作符(&gt;)的行为。数值操作符五类：一元操作符，常见算数操作符，反射算数操作符，增强赋值操作符，和类型转换操作符。一元操作符一元操作符只有一个操作符。__pos__(self)实现取正操作，例如 +some_object。__neg__(self)实现取负操作，例如 -some_object。__abs__(self)实现内建绝对值函数 abs() 操作。__invert__(self)实现取反操作符 ~。__round__(self， n)实现内建函数 round() ，n 是近似小数点的位数。__floor__(self)实现 math.floor() 函数，即向下取整。__ceil__(self)实现 math.ceil() 函数，即向上取整。__trunc__(self)实现 math.trunc() 函数，即距离零最近的整数。常见算数操作符__add__(self, other)实现加法操作。__sub__(self, other)实现减法操作。__mul__(self, other)实现乘法操作。__floordiv__(self, other)实现使用 // 操作符的整数除法。__div__(self, other)实现使用 / 操作符的除法。__truediv__(self, other)实现 true 除法，这个函数只有使用 from __future__ import division 时才有作用。__mod__(self, other)实现 % 取余操作。__divmod__(self, other)实现 divmod 内建函数。__pow__(self, other[, modulo])实现 ** 操作符。__lshift__(self, other)实现左移位运算符 &lt;&lt; 。__rshift__(self, other)实现右移位运算符 &gt;&gt; 。__and__(self, other)实现按位与运算符 &amp; 。__or__(self, other)实现按位或运算符 | 。__xor__(self, other)实现按位异或运算符 ^ 。反射算数运算符反射只不过是运算符交换了一下位置：1some_object + other -&gt; other + some_object所有反射运算符魔法方法和它们的常见版本做的工作相同，只不过是处理交换连个操作数之后的情况。绝大多数情况下，反射运算和正常顺序产生的结果是相同的，所以很可能你定义 __radd__ 时只是调用一下 __add__。注意一点，操作符左侧的对象（也就是上面的 other ）一定不要定义（或者产生 NotImplemented 异常） 操作符的非反射版本。例如，在上面的例子中，只有当 other 没有定义 __add__ 时 someobject.\_radd__ 才会被调用。__radd__(self, other)实现反射加法操作。__rsub__(self, other)实现反射减法操作。__rmul__(self, other)实现反射乘法操作。__rfloordiv__(self, other)实现使用 // 操作符的整数反射除法。__rdiv__(self, other)实现使用 / 操作符的反射除法。__rtruediv__(self, other)实现 true 反射除法，这个函数只有使用 from __future__ import division 时才有作用。__rmod__(self, other)实现 % 反射取余操作符。__rdivmod__(self, other)实现调用 divmod(other, self) 时 divmod 内建函数的操作。__rpow__(self, other[, modulo])实现 ** 反射操作符。__rlshift__(self, other)实现反射左移位运算符 &lt;&lt; 的作用。__rshift__(self, other)实现反射右移位运算符 &gt;&gt; 的作用。__rand__(self, other)实现反射按位与运算符 &amp; 。__ror__(self, other)实现反射按位或运算符 | 。__rxor__(self, other)实现反射按位异或运算符 ^ 。增强赋值运算符Python同样提供了大量的魔法方法，可以用来自定义增强赋值操作的行为。增强赋值融合了“常见”的操作符和赋值操作，看下面的例子:12x = 5x += 1 # 也就是 x = x + 1这些方法都应该返回左侧操作数应该被赋予的值。__iadd__(self, other)实现加法赋值操作。__isub__(self, other)实现减法赋值操作。__imul__(self, other)实现乘法赋值操作。__ifloordiv__(self, other)实现使用 //= 操作符的整数除法赋值操作。__idiv__(self, other)实现使用 /= 操作符的除法赋值操作。__itruediv__(self, other)实现 true 除法赋值操作，这个函数只有使用 from __future__ import division 时才有作用。__imod__(self, other)实现 %= 取余赋值操作。__ipow__实现 **= 操作。__ilshift__(self, other)实现左移位赋值运算符 &lt;&lt;= 。__irshift__(self, other)实现右移位赋值运算符 &gt;&gt;= 。__iand__(self, other)实现按位与运算符 &amp;= 。__ior__(self, other)实现按位或赋值运算符 |= 。__ixor__(self, other)实现按位异或赋值运算符 ^= 。类型转换操作符__int__(self)实现到int的类型转换。__long__(self)实现到long的类型转换。__float__(self)实现到float的类型转换。__complex__(self)实现到complex的类型转换。__oct__(self)实现到八进制数的类型转换。__hex__(self)实现到十六进制数的类型转换。__index__(self)实现当对象用于切片表达式时到一个整数的类型转换。如果你定义了一个可能会用于切片操作的数值类型，你应该定义 __index__。__trunc__(self)当调用 math.trunc(self) 时调用该方法， __trunc__ 应该返回 self 截取到一个整数类型（通常是long类型）的值。__coerce__(self)该方法用于实现混合模式算数运算，如果不能进行类型转换， __coerce__ 应该返回 None 。反之，它应该返回一个二元组 self 和 other ，这两者均已被转换成相同的类型。因为和其他魔法方法有功能上的重复，以及本身行为令人迷惑。在python3.x版本已被移除。类的表示使用字符串来表示类是一个相当有用的特性。在Python中有一些内建方法可以返回类的表示，相对应的，也有一系列魔法方法可以用来自定义在使用这些内建函数时类的行为。__str__(self)定义对类的实例调用 str() 时的行为。__repr__(self)定义对类的实例调用 repr() 时的行为。 str() 和 repr() 最主要的差别在于“目标用户”。 repr() 的作用是产生机器可读的输出（大部分情况下，其输出可以作为有效的Python代码），而 str() 则产生人类可读的输出。__repr__ 所返回的字符串应该准确、无歧义，并且尽可能表达出如何用代码创建出这个被打印的对象。__repr__ 和 __str__ 的区别在于，后者是在 str() 函数被使用，或是在用 print 函数打印一个对象的时候才被调用的，并且它返回的字符串对终端用户更友好。如果你只想实现这两个特殊方法中的一个，__repr__ 是更好的选择，因为如果一个对象没有 __str__ 函数，而 Python 又需要调用它的时候，解释器会用 __repr__作为替代。自动化__repr__实现首先定义一个元类来获取 __init_\_()函数的参数列表12345678910111213141516171819202122232425262728#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v0.1@author: narutohyc@file: meta_interface.py@Description: @time: 2020/6/15 20:29&quot;&quot;&quot;import collectionsfrom abc import (ABC,                 abstractmethod,                 ABCMeta)import inspectclass DicMetaClass(ABCMeta):    def __new__(cls, name, bases, attrs, **kwargs):        if name == 'DicMeta':            return super().__new__(cls, name, bases, attrs, **kwargs)        # 获取__init__函数的 默认值        argspec = inspect.getfullargspec(attrs[&quot;__init__&quot;])        init_defaults = dict(zip(argspec.args[-len(argspec.defaults):], argspec.defaults))        cls.__init_defaults = init_defaults        attrs['__init_defaults__'] = init_defaults        attrs['__init_args__'] = argspec.args[1:]        return super().__new__(cls, name, bases, attrs, **kwargs)这里定义一个抽象类来实现__repr__方法12345678910111213141516171819202122232425262728293031class DicMeta(ABC, metaclass=DicMetaClass):    def __init__(self):        super(DicMeta, self).__init__()    @abstractmethod    def to_dict(self):        '''        返回字典        '''        pass    def __repr__(self):        '''        __repr__() 生成的文本字符串标准做法是需要让 eval(repr(x)) == x 为真。        如果实在不能这样子做，应该创建一个有用的文本表示，并使用 &lt; 和 &gt; 括起来。        '''        return f&quot;{self.__class__.__name__}({', '.join(['%s=%r' % (k, type(self.__dict__[k])(self.__dict__[k])) for k in self.__init_args__ if k in self.__dict__.keys()])})&quot;    def __str__(self):        self.__repr__()    @classmethod    def load_from_mapping(cls, mapping_datas):        '''        用字典来构建实例对象        '''        assert isinstance(mapping_datas, collections.abc.Mapping)        obj = cls.__new__(cls)        [setattr(obj, k, v) for k, v in mapping_datas.items()]        return obj这样的话，继承自DicMeta的子类都有了__repr__方法的默认实现了，执行eval(repr(sample))，就可以得到一个sample了。__unicode__(self)定义对类的实例调用 unicode() 时的行为。 unicode() 和 str() 很像，只是它返回unicode字符串。注意，如果调用者试图调用 str() 而你的类只实现了 __unicode__() ，那么类将不能正常工作。所有你应该总是定义 __str__() ，以防有些人没有闲情雅致来使用unicode。Python 3中string和unicode的区别不复存在，因此 __unicode__ 被取消了， __bytes__ 加入进来（与Python 2.7 中的 __str__ 和 __unicode__ 行为类似），用于新的创建字节数组的内建方法。__format__(self)定义当类的实例用于新式字符串格式化时的行为，例如， “Hello, 0:abc!”.format(a) 会导致调用 a.__format__(“abc”) 。当定义你自己的数值类型或字符串类型时，你可能想提供某些特殊的格式化选项，这种情况下这个魔法方法会非常有用。__hash__(self)定义对类的实例调用 hash() 时的行为。它必须返回一个整数，其结果会被用于字典中键的快速比较。同时注意一点，实现这个魔法方法通常也需要实现 __eq__ ，并且遵守如下的规则： a == b 意味着 hash(a) == hash(b)。__nonzero__(self)定义对类的实例调用 bool() 时的行为，根据你自己对类的设计，针对不同的实例，这个魔法方法应该相应地返回True或False。3.x中被重命名成 __bool__ 。bool(x) 的背后是调用 x.__bool__() 的结果；如果不存在 __bool__ 方法，那么 bool(x) 会尝试调用 x.__len__()。若返回 0，则 bool 会返回 False；否则返回 True。__dir__(self)定义对类的实例调用 dir() 时的行为，这个方法应该向调用者返回一个属性列表。一般来说，没必要自己实现 __dir__ 。但是如果你重定义了 __getattr__ 或者 __getattribute__ （下个部分会介绍），乃至使用动态生成的属性，以实现类的交互式使用，那么这个魔法方法是必不可少的。访问控制很多从其他语言转向Python的人都抱怨Python的类缺少真正意义上的封装（即没办法定义私有属性然后使用公有的getter和setter）。然而事实并非如此。实际上Python不是通过显式定义的字段和方法修改器，而是通过魔法方法实现了一系列的封装。__getattr__(self, name)当用户试图访问一个根本不存在（或者暂时不存在）的属性时，你可以通过这个魔法方法来定义类的行为。这个可以用于捕捉错误的拼写并且给出指引，使用废弃属性时给出警告（如果你愿意，仍然可以计算并且返回该属性），以及灵活地处理AttributeError。只有当试图访问不存在的属性时它才会被调用，所以这不能算是一个真正的封装的办法。__setattr__(self, name, value)和 __getattr__ 不同， __setattr__ 可以用于真正意义上的封装。它允许你自定义某个属性的赋值行为，不管这个属性存在与否，也就是说你可以对任意属性的任何变化都定义自己的规则。然后，一定要小心使用 __setattr__ ，这个列表最后的例子中会有所展示。__delattr__(self, name)这个魔法方法和 __setattr__ 几乎相同，只不过它是用于处理删除属性时的行为。和 setattr\_ 一样，使用它时也需要多加小心，防止产生无限递归（在 __delattr__ 的实现中调用 del self.name 会导致无限递归）。__getattribute__(self, name)__getattribute__ 看起来和上面那些方法很合得来，但是最好不要使用它。 __getattribute__ 只能用于新式类。在最新版的Python中所有的类都是新式类，在老版Python中你可以通过继承 object 来创建新式类。 __getattribute__ 允许你自定义属性被访问时的行为，它也同样可能遇到无限递归问题（通过调用基类的 __getattribute__ 来避免）。 __getattribute__ 基本上可以替代 __getattr__ 。只有当它被实现，并且显式地被调用，或者产生 AttributeError 时它才被使用。 这个魔法方法可以被使用（毕竟，选择权在你自己），我不推荐你使用它，因为它的使用范围相对有限（通常我们想要在赋值时进行特殊操作，而不是取值时），而且实现这个方法很容易出现Bug。自定义这些控制属性访问的魔法方法很容易导致问题，考虑下面这个例子:1234567def __setattr__(self, name. value):    self.name = value# 因为每次属性幅值都要调用 __setattr__()，所以这里的实现会导致递归def __setattr__(self, name, value):    self.__dict__[name] = value # 使用 __dict__ 进行赋值    # 定义自定义行为再次重申，Python的魔法方法十分强大，能力越强责任越大，了解如何正确的使用魔法方法更加重要。到这里，我们对Python中自定义属性存取控制有了什么样的印象？它并不适合轻度的使用。实际上，它有些过分强大，而且违反直觉。然而它之所以存在，是因为一个更大的原则：Python不指望让杜绝坏事发生，而是想办法让做坏事变得困难。自由是至高无上的权利，你真的可以随心所欲。下面的例子展示了实际应用中某些特殊的属性访问方法（注意我们之所以使用 super 是因为不是所有的类都有 __dict__ 属性）:自定义序列有许多办法可以让你的Python类表现得像是内建序列类型（字典，元组，列表，字符串等）。这些魔法方式是目前为止我最喜欢的。它们给了你难以置信的控制能力，可以让你的类与一系列的全局函数完美结合。在Python中，协议完全是非正式的，也不需要显式的声明，事实上，它们更像是一种参考标准。为什么我们要讲协议？因为在Python中实现自定义容器类型需要用到一些协议。首先，不可变容器类型有如下协议：想实现一个不可变容器，你需要定义 __len__ 和 __getitem__ (后面会具体说明）。可变容器的协议除了上面提到的两个方法之外，还需要定义 __setitem__ 和 __delitem__ 。最后，如果你想让你的对象可以迭代，你需要定义 __iter__ ，这个方法返回一个迭代器。迭代器必须遵守迭代器协议，需要定义 __iter__ （返回它自己）和 next 方法。容器相关__len__(self)返回容器的长度，可变和不可变类型都需要实现。__getitem__(self, key)定义对容器中某一项使用 self[key] 的方式进行读取操作时的行为。这也是可变和不可变容器类型都需要实现的一个方法。它应该在键的类型错误式产生 TypeError 异常，同时在没有与键值相匹配的内容时产生 KeyError 异常。__setitem__(self, key)定义对容器中某一项使用 self[key] 的方式进行赋值操作时的行为。它是可变容器类型必须实现的一个方法，同样应该在合适的时候产生 KeyError 和 TypeError 异常。__iter__(self, key)它应该返回当前容器的一个迭代器。迭代器以一连串内容的形式返回，最常见的是使用 iter() 函数调用，以及在类似 for x in container: 的循环中被调用。迭代器是他们自己的对象，需要定义 __iter__ 方法并在其中返回自己。特殊方法的调用是隐式的，比如 for i in x: 这个语句，背后其实用的是 iter(x)，而这个函数的背后则是 x.__iter__() 方法。当然前提是这个方法在 x 中被实现了。__reversed__(self)定义了对容器使用 reversed() 内建函数时的行为。它应该返回一个反转之后的序列。当你的序列类是有序时，类似列表和元组，再实现这个方法，__contains__(self, item)__contains__ 定义了使用 in 和 not in 进行成员测试时类的行为。你可能好奇为什么这个方法不是序列协议的一部分，原因是，如果 __contains__ 没有定义，Python就会迭代整个序列，如果找到了需要的一项就返回 True 。迭代通常是隐式的，譬如说一个集合类型没有实现 __contains__ 方法，那么 in运算符就会按顺序做一次迭代搜索。__missing__(self ,key)__missing__ 在字典的子类中使用，它定义了当试图访问一个字典中不存在的键时的行为（目前为止是指字典的实例，例如我有一个字典 d ， “george” 不是字典中的一个键，当试图访问 d[“george’] 时就会调用 d.__missing__(“george”) ）。下面是实现了一些函数式结构的列表，展示了如何实现自己的序列：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455class FunctionalList:    '''一个列表的封装类，实现了一些额外的函数式    方法，例如head, tail, init, last, drop和take。'''    def __init__(self, values=None):        if values is None:            self.values = []        else:            self.values = values    def __len__(self):        return len(self.values)    def __getitem__(self, key):        # 如果键的类型或值不合法，列表会返回异常        return self.values[key]    def __setitem__(self, key, value):        self.values[key] = value    def __delitem__(self, key):        del self.values[key]    def __iter__(self):        return iter(self.values)    def __reversed__(self):        return reversed(self.values)    def append(self, value):        self.values.append(value)    def head(self):        # 取得第一个元素        return self.values[0]    def tail(self):        # 取得除第一个元素外的所有元素        return self.valuse[1:]    def init(self):        # 取得除最后一个元素外的所有元素        return self.values[:-1]    def last(self):        # 取得最后一个元素        return self.values[-1]    def drop(self, n):        # 取得除前n个元素外的所有元素        return self.values[n:]    def take(self, n):        # 取得前n个元素        return self.values[:n]反射你可以通过定义魔法方法来控制用于反射的内建函数 isinstance 和 issubclass 的行为。下面是对应的魔法方法：__instancecheck__(self, instance)检查一个实例是否是你定义的类的一个实例（例如 isinstance(instance, class) ）。__subclasscheck__(self, subclass)检查一个类是否是你定义的类的子类（例如 issubclass(subclass, class) ）。这几个魔法方法的适用范围看起来有些窄，事实也正是如此。bu’yong在反射魔法方法上花费太多时间，因为相比其他魔法方法它们显得不是很重要。但是它们展示了在Python中进行面向对象编程（或者总体上使用Python进行编程）时很重要的一点：不管做什么事情，都会有一个简单方法，不管它常用不常用。抽象基类ABCpython抽象基类abcpython中并没有提供抽象类与抽象方法，但是提供了内置模块abc(abstract base class)来模拟实现抽象类。 抽象基类的使用：直接继承直接继承抽象基类的子类就没有这么灵活，抽象基类中可以声明”抽象方法“和“抽象属性”，只有完全覆写（实现）了抽象基类中的“抽象”内容后，才能被实例化，而虚拟子类则不受此影响。虚拟子类将其他的类”注册“到抽象基类下当虚拟子类（调用register方法），虚拟子类的好处是你实现的第三方子类不需要直接继承自基类，可以实现抽象基类中的部分API接口，也可以根本不实现，但是issubclass(), issubinstance()进行判断时仍然返回真值。定义抽象类首先在abc_base.py中定义一个抽象基类PluginBase，这个基类用于保存和加载数据。1234567891011121314import abcclass PluginBase(object):    __metaclass__ = abc.ABCMeta        @abc.abstractmethod    def load(self, input):        &quot;&quot;&quot;Retrieve data from the input source and return an object.&quot;&quot;&quot;        return        @abc.abstractmethod    def save(self, output, data):        &quot;&quot;&quot;Save the data object to the output.&quot;&quot;&quot;        return通过@abc.abstractmethod将方法声明为抽象方法。注册具体类然后在abc_register.py中定义一个具体的类：123456789101112131415import abcfrom abc_base import PluginBaseclass RegisteredImplementation(object):    def load(self, input):        return input.read()        def save(self, output, data):        return output.write(data)PluginBase.register(RegisteredImplementation)if __name__ == '__main__':    print 'Subclass:', issubclass(RegisteredImplementation, PluginBase)    print 'Instance:', isinstance(RegisteredImplementation(), PluginBase)在上面的例子中，RegisteredImplementation并没有继承自PluginBase，而是将其注册为PluginBase的一个实现。 运行结果如下：12Subclass: TrueInstance: True通过派生实现也可以在abc_subclass.py中直接继承抽象类：12345678910111213import abcfrom abc_base import PluginBaseclass SubclassImplementation(PluginBase):    def load(self, input):        return input.read()        def save(self, output, data):        return output.write(data)if __name__ == '__main__':    print 'Subclass:', issubclass(SubclassImplementation, PluginBase)    print 'Instance:', isinstance(SubclassImplementation(), PluginBase)这样做有一个副作用，当查询基类的子类时，会输出所有继承自改类的子类。1234567import abcfrom abc_base import PluginBaseimport abc_subclassimport abc_registerfor sc in PluginBase.__subclasses__():    print sc.__name__输出结果如下：1SubclassImplementation直接从抽象基类派生子类有一个好处，除非子类完全抽象基类的抽象方法，否则子类不能实例化。12345678910import abcfrom abc_base import PluginBaseclass IncompleteImplementation(PluginBase):    def save(self, output, data):        return output.write(data)if __name__ == '__main__':    print 'Subclass:', issubclass(IncompleteImplementation, PluginBase)    print 'Instance:', isinstance(IncompleteImplementation(), PluginBase)ABC中的具体方法尽管具体子类必须实现抽象类中的所有抽象方法，但是，抽象类中也可以包含具体方法。在子类中可以通过super()来调用。12345678910111213141516171819202122232425import abcfrom cStringIO import StringIOclass ABCWithConcreteImplementation(object):    __metaclass__ = abc.ABCMeta    @abc.abstractmethod    def retrieve_values(self, input):        print 'base class reading data'        return input.read()class ConcreteOverride(ABCWithConcreteImplementation):    def retrieve_values(self, input):        base_data = super(ConcreteOverride, self).retrieve_values(input)        print 'subclass sorting data'        response = sorted(base_data.splitlines())        return responseinput = StringIO(&quot;&quot;&quot;line oneline twoline three&quot;&quot;&quot;)reader = ConcreteOverride()print reader.retrieve_values(input)print输出结果如下：123base class reading datasubclass sorting data['line one', 'line three', 'line two']抽象属性可以通过@abstractproperty定义抽象属性：123456789101112131415161718192021import abcclass Base(object):    __metaclass__ = abc.ABCMeta    @abc.abstractproperty    def value(self):        return 'Should never get here'class Implementation(Base):    @property    def value(self):        return 'concrete property'try:    b = Base()    print 'Base.value:', b.valueexcept Exception, err:    print 'ERROR:', str(err)i = Implementation()print 'Implementation.value:', i.value输出结果如下：12ERROR: Can't instantiate abstract class Base with abstract methods valueImplementation.value: concrete propertymetaclass&gt;可调用的对象Python中一个特殊的魔法方法允许你自己类的对象表现得像是函数，然后你就可以“调用”它们，把它们传递到使用函数做参数的函数中，__call__(self, [args…])允许类的一个实例像函数那样被调用。本质上这代表了 x() 和 x.__call__() 是相同的。注意 __call__ 可以有多个参数，这代表你可以像定义其他任何函数一样，定义 __call__ ，喜欢用多少参数就用多少。__call__ 在某些需要经常改变状态的类的实例中显得特别有用。“调用”这个实例来改变它的状态，是一种更加符合直觉，也更加优雅的方法。一个表示平面上实体的类是一个不错的例子:1234567891011class Entity:        '''表示一个实体的类，调用它的实例        可以更新实体的位置'''        def __init__(self, size, x, y):                self.x, self.y = x, y                self.size = size        def __call__(self, x, y):                '''改变实体的位置'''                self.x, self.y = x, y上下文管理器在Python 2.5中引入了一个全新的关键词，随之而来的是一种新的代码复用方法—— with 声明。上下文管理的概念在Python中并不是全新引入的（之前它作为标准库的一部分实现），直到PEP 343被接受，它才成为一种一级的语言结构。当对象使用 with 声明创建时，上下文管理器允许类做一些设置和清理工作。上下文管理器的行为由下面两个魔法方法所定义：__enter__(self)定义使用 with 声明创建的语句块最开始上下文管理器应该做些什么。注意 __enter__ 的返回值会赋给 with 声明的目标，也就是 as 之后的东西。__exit__(self, exception_type, exception_value, traceback)定义当 with 声明语句块执行完毕（或终止）时上下文管理器的行为。它可以用来处理异常，进行清理，或者做其他应该在语句块结束之后立刻执行的工作。如果语句块顺利执行， exceptiontype , exception_value 和 traceback 会是 None 。否则，你可以选择处理这个异常或者让用户来处理。如果你想处理异常，确保 \_exit__ 在完成工作之后返回 True 。如果你不想处理异常，那就让它发生吧。对一些具有良好定义的且通用的设置和清理行为的类，__enter__ 和 __exit__ 会显得特别有用。你也可以使用这几个方法来创建通用的上下文管理器，用来包装其他对象。下面是一个例子:123456789101112131415161718class Closer:    '''    一个上下文管理器，可以在with语句中    使用close()自动关闭对象    '''    def __init__(self, obj):    self.obj = obj    def __enter__(self, obj):    return self.obj # 绑定到目标    def __exit__(self, exception_type, exception_value, traceback):        try:        self.obj.close()        except AttributeError: # obj不是可关闭的        print 'Not closable.'        return True # 成功地处理了异常这是一个 Closer 在实际使用中的例子，使用一个FTP连接来演示（一个可关闭的socket):1234567891011121314&gt;&gt;&gt; from magicmethods import Closer&gt;&gt;&gt; from ftplib import FTP&gt;&gt;&gt; with Closer(FTP('ftp.somesite.com')) as conn:...         conn.dir()...# 为了简单，省略了某些输出&gt;&gt;&gt; conn.dir()# 很长的 AttributeError 信息，不能使用一个已关闭的连接&gt;&gt;&gt; with Closer(int(5)) as i:...         i += 1...Not closable.&gt;&gt;&gt; i6这就是上下文管理器和魔法方法的力量。Python标准库包含一个 contextlib 模块，里面有一个上下文管理器 contextlib.closing() 基本上和我们的包装器完成的是同样的事情（但是没有包含任何当对象没有close()方法时的处理）。创建描述符对象描述符是一个类，当使用取值，赋值和删除 时它可以改变其他对象。描述符不是用来单独使用的，它们需要被一个拥有者类所包含。描述符可以用来创建面向对象数据库，以及创建某些属性之间互相依赖的类。描述符在表现具有不同单位的属性，或者需要计算的属性时显得特别有用（例如表现一个坐标系中的点的类，其中的距离原点的距离这种属性）。要想成为一个描述符，一个类必须具有实现 __get__ , __set__ 和 __delete__ 三个方法中至少一个。让我们一起来看一看这些魔法方法：__get__(self, instance, owner)定义当试图取出描述符的值时的行为。 instance 是拥有者类的实例， owner 是拥有者类本身。__set__(self, instance, owner)定义当描述符的值改变时的行为。 instance 是拥有者类的实例， value 是要赋给描述符的值。__delete__(self, instance, owner)定义当描述符的值被删除时的行为。 instance 是拥有者类的实例现在，来看一个描述符的有效应用：单位转换:1234567891011121314151617181920class Meter(object):    '''米的描述符。'''    def __init__(self, value=0.0):        self.value = float(value)    def __get__(self, instance, owner):        return self.value    def __set__(self, instance, owner):            self.value = float(value)class Foot(object):    '''英尺的描述符。'''    def __get(self, instance, owner):            return instance.meter * 3.2808    def __set(self, instance, value):            instance.meter = float(value) / 3.2808class Distance(object):    '''用于描述距离的类，包含英尺和米两个描述符。'''    meter = Meter()    foot = Foot()拷贝有些时候，特别是处理可变对象时，你可能想拷贝一个对象，改变这个对象而不影响原有的对象。这时就需要用到Python的 copy 模块了。然而（幸运的是），Python模块并不具有感知能力， 因此我们不用担心某天基于Linux的机器人崛起。但是我们的确需要告诉Python如何有效率地拷贝对象。__copy__(self)定义对类的实例使用 copy.copy() 时的行为。 copy.copy() 返回一个对象的浅拷贝，这意味着拷贝出的实例是全新的，然而里面的数据全都是引用的。也就是说，对象本身是拷贝的，但是它的数据还是引用的（所以浅拷贝中的数据更改会影响原对象）。__deepcopy__(self, memodict=)定义对类的实例使用 copy.deepcopy() 时的行为。 copy.deepcopy() 返回一个对象的深拷贝，这个对象和它的数据全都被拷贝了一份。 memodict 是一个先前拷贝对象的缓存，它优化了拷贝过程，而且可以防止拷贝递归数据结构时产生无限递归。当你想深拷贝一个单独的属性时，在那个属性上调用 copy.deepcopy() ，使用 memodict 作为第一个参数。这些魔法方法有什么用武之地呢？像往常一样，当你需要比默认行为更加精确的控制时。例如，如果你想拷贝一个对象，其中存储了一个字典作为缓存（可能会很大），拷贝缓存可能是没有意义的。如果这个缓存可以在内存中被不同实例共享，那么它就应该被共享。Pickling如果你和其他的Python爱好者共事过，很可能你已经听说过Pickling了。Pickling是Python数据结构的序列化过程，当你想存储一个对象稍后再取出读取时，Pickling会显得十分有用。Pickling是如此的重要，以至于它不仅仅有自己的模块（ pickle ），还有自己的协议和魔法方法。首先，我们先来简要的介绍一下如何pickle已存在的对象类型。Pickling : 小试牛刀我们一起来pickle吧。假设你有一个字典，你想存储它，稍后再取出来。你可以把它的内容写入一个文件，小心翼翼地确保使用了正确地格式，要把它读取出来，你可以使用 exec() 或处理文件输入。但是这种方法并不可靠：如果你使用纯文本来存储重要数据，数据很容易以多种方式被破坏或者修改，导致你的程序崩溃，更糟糕的情况下，还可能在你的计算机上运行恶意代码。因此，我们要pickle它:12345678import pickledata = {'foo': [1,2,3],                'bar': ('Hello', 'world!'),                'baz': True}jar = open('data.pkl', 'wb')pickle.dump(data, jar) # 将pickle后的数据写入jar文件jar.close()过了几个小时，我们想把它取出来，我们只需要反pickle它:123456import picklepkl_file = open('data.pkl', 'rb') # 与pickle后的数据连接data = pickle.load(pkl_file) # 把它加载进一个变量print datapkl_file.close()将会发生什么？正如你期待的，它就是我们之前的 data 。现在，还需要谨慎地说一句： pickle并不完美。Pickle文件很容易因为事故或被故意的破坏掉。Pickling或许比纯文本文件安全一些，但是依然有可能被用来运行恶意代码。而且它还不支持跨Python版本，所以不要指望分发pickle对象之后所有人都能正确地读取。然而不管怎么样，它依然是一个强有力的工具，可以用于缓存和其他类型的持久化工作。Pickle你的对象Pickle不仅仅可以用于内建类型，任何遵守pickle协议的类都可以被pickle。Pickle协议有四个可选方法，可以让类自定义它们的行为（这和C语言扩展略有不同，那不在我们的讨论范围之内）。__getinitargs__(self)如果你想让你的类在反pickle时调用 __init__ ，你可以定义 __getinitargs__(self) ，它会返回一个参数元组，这个元组会传递给 __init__ 。注意，这个方法只能用于旧式类。__getnewargs__(self)对新式类来说，你可以通过这个方法改变类在反pickle时传递给 __new__ 的参数。这个方法应该返回一个参数元组。__getstate__(self)你可以自定义对象被pickle时被存储的状态，而不使用对象的 __dict__ 属性。 这个状态在对象被反pickle时会被 __setstate__ 使用。__setstate__(self)当一个对象被反pickle时，如果定义了 __setstate__ ，对象的状态会传递给这个魔法方法，而不是直接应用到对象的 __dict__ 属性。这个魔法方法和 __getstate__ 相互依存：当这两个方法都被定义时，你可以在Pickle时使用任何方法保存对象的任何状态。__reduce__(self)当定义扩展类型时（也就是使用Python的C语言API实现的类型），如果你想pickle它们，你必须告诉Python如何pickle它们。 __reduce__ 被定义之后，当对象被Pickle时就会被调用。它要么返回一个代表全局名称的字符串，Pyhton会查找它并pickle，要么返回一个元组。这个元组包含2到5个元素，其中包括：一个可调用的对象，用于重建对象时调用；一个参数元素，供那个可调用对象使用；被传递给 __setstate__ 的状态（可选）；一个产生被pickle的列表元素的迭代器（可选）；一个产生被pickle的字典元素的迭代器（可选）；__reduceex\_(self)__reduceex\_ 的存在是为了兼容性。如果它被定义，在pickle时 __reduceex\_ 会代替 __reduce__ 被调用。 __reduce__ 也可以被定义，用于不支持 __reduceex\_ 的旧版pickle的API调用。一个例子Slate 它会记住它的值曾经是什么，以及那些值是什么时候赋给它的。然而 每次被pickle时它都会变成空白，因为当前的值不会被存储:1234567891011121314151617181920212223242526272829303132import timeclass Slate:        '''存储一个字符串和一个变更日志的类        每次被pickle都会忘记它当前的值'''        def __init__(self, value):                self.value = value                self.last_change = time.asctime()                self.history = {}        def change(self, new_value):                # 改变当前值，将上一个值记录到历史                self.history[self.last_change] = self.value                self.value = new_value)                self.last_change = time.asctime()        def print_change(self):                print 'Changelog for Slate object:'                for k,v in self.history.items():                        print '%s\t %s' % (k,v)        def __getstate__(self):                # 故意不返回self.value或self.last_change                # 我们想在反pickle时得到一个空白的slate                return self.history        def __setstate__(self):                # 使self.history = slate，last_change                # 和value为未定义                self.history = state                self.value, self.last_change = None, None如何调用魔法方法一些魔法方法直接和内建函数对应，这种情况下，如何调用它们是显而易见的。然而，另外的情况下，调用魔法方法的途径并不是那么明显。这个附录旨在展示那些不那么明显的调用魔法方法的语法。魔法方法什么时候被调用解释__new__(cls [,…])instance = MyClass(arg1, arg2)__new__在实例创建时调用__init__(self [,…])instance = MyClass(arg1,arg2)__init__在实例创建时调用__cmp__(self)self == other, self &gt; other 等进行比较时调用，3.x移除__pos__(self)+self一元加法符号__neg__(self)-self一元减法符号__invert__(self)~self按位取反__index__(self)x[self]当对象用于索引时__nonzero__(self)bool(self)对象的布尔值__getattr__(self, name)self.name #name不存在访问不存在的属性__setattr__(self, name)self.name = val给属性赋值__delattr__(self, name)del self.name删除属性__getattribute__(self,name)self.name访问任意属性__getitem__(self, key)self[key]使用索引访问某个元素__setitem__(self, key)self[key] = val使用索引给某个元素赋值__delitem__(self, key)del self[key]使用索引删除某个对象__iter__(self)for x in self迭代__contains__(self, value)value in self, value not in self使用in进行成员测试__call__(self [,…])self(args)“调用”一个实例__enter__(self)with self as x:with声明的上下文管理器__exit__(self, exc, val, trace)with self as x:with声明的上下文管理器__getstate__(self)pickle.dump(pkl_file, self)Pickling__setstate__(self)data = pickle.load(pkl_file)Pickling</content>
	</entry>
	<entry>
		<title>python基础知识</title>
		<url>/article/8258.html</url>
		<content>python基础知识python解释器CPython这个解释器是用C语言开发的，所以叫 CPython，在命名行下运行python，就是启动CPython解释器CPython是使用最广的Python解释器IPythonIPython是基于CPython之上的一个交互式解释器，也就是说，IPython只是在交互方式上有所增强PyPyPyPy是另一个Python解释器，它的目标是执行速度PyPy采用JIT技术，对Python代进行动态编译，所以可以显著提高Python代码的执行速度JythonJython是运行在Java平台上的Python解释器，可以直接把Python代码编译成Java字节码执行IronPythonIronPython和Jython类似，只不过IronPython是运行在微软.Net平台上的Python解释器可以直接把Python代码编译成.Net的字节码。字符集和字符编码十分钟搞清字符集和字符编码程序员必备：彻底弄懂常见的7种中文字符编码计算机屏幕上看到实体化的文字，在计算机存储介质中存放的实际是二进制的比特流两者之间的转换规则就需要一个统一的标准字符集字符集：为每一个「字符」分配一个唯一的 ID（学名为码位 / 码点 / Code Point） 为什么那么多字符集问题实际非常容易回答。问问自己为什么我们的插头拿到英国就不能用了呢？为什么显示器同时有DVI，VGA，HDMI，DP这么多接口呢？很多规范和标准在最初制定时并不会意识到这将会是以后全球普适的准则，或者处于组织本身利益就想从本质上区别于现有标准。于是，就产生了那么多具有相同效果但又不相互兼容的标准了。说了那么多我们来看一个实际例子，下面就是屌这个字在各种编码下的十六进制和二进制编码结果，怎么样有没有一种很屌的感觉？字符集16进制编码对应的二进制数据UTF-80xE5B18C111001011011000110001100UTF-160x5C4C101110001001100GBK0x8CC51000110011000101常见字符集ASCII 字符集：ASCII编码每个字母或符号占1byte(8bits)，并且8bits的最高位是0，因此ASCII能编码的字母和符号只有128个。有一些编码把8bits最高位为1的后128个值也编码上，使得1byte可以表示256个值，但是这属于扩展的ASCII，并非标准ASCII。通常所说的标准ASCII只有前128个值！ASCII编码几乎被世界上所有编码所兼容（UTF16和UTF32是个例外），因此如果一个文本文档里面的内容全都由ASCII里面的字母或符号构成，那么不管你如何展示该文档的内容，都不可能出现乱码的情况。 GB2312 字符集：最早一版的中文编码，每个字占据2bytes。由于要和ASCII兼容，那这2bytes最高位不可以为0了（否则和ASCII会有冲突）。在GB2312中收录了6763个汉字以及682个特殊符号，已经囊括了生活中最常用的所有汉字。 GBK 字符集：由于GB2312只有6763个汉字，我汉语博大精深，只有6763个字怎么够？于是GBK中在保证不和GB2312、ASCII冲突（即兼容GB2312和ASCII）的前提下，也用每个字占据2bytes的方式又编码了许多汉字。经过GBK编码后，可以表示的汉字达到了20902个，另有984个汉语标点符号、部首等。值得注意的是这20902个汉字还包含了繁体字，但是该繁体字与台湾Big5编码不兼容，因为同一个繁体字很可能在GBK和Big5中数字编码是不一样的。 Unicode 字符集：Unicode（中文：万国码、国际码、统一码、单一码）是计算机科学领域里的一项业界标准ASCII编码是1个字节，而Unicode编码通常是2个字节 它对世界上大部分的文字系统进行了整理、编码，使得电脑可以用更为简单的方式来呈现和处理文字字符编码编码规则：将「码位」转换为字节序列的规则（编码/解码 可以理解为 加密/解密 的过程）常见编码方式：ASCII 编码：将ASCII字符集转换为计算机可以接受的数字系统的数的规则。使用7位（bits）表示一个字符，共128字符；但是7位编码的字符集只能支持128个字符，为了表示更多的欧洲常用字符对ASCII进行了扩展，ASCII扩展字符集使用8位（bits）表示一个字符，共256字符。 UTF-8 编码：UTF-8（8-bit Unicode Transformation Format）是一种针对Unicode的可变长度字符编码，也是一种前缀码。其编码中的第一个字节仍与ASCII兼容，这使得原来处理ASCII字符的软件无须或只须做少部分修改，即可继续使用。 生成器、迭代器和可迭代对象迭代器和可迭代对象可迭代对象包含迭代器如果一个对象拥有iter方法，其是可迭代对象；如果一个对象拥有next方法，其是迭代器定义可迭代对象，必须实现iter方法；定义迭代器，必须实现iter和next方法生成器是一种特殊的迭代器在使用生成器时，我们创建一个函数；在使用迭代器时，我们使用内置函数iter()和next()在生成器中，我们使用关键字‘yield’来每次生成/返回一个对象生成器中有多少‘yield’语句，你可以自定义每次‘yield’暂停循环时，生成器会保存本地变量的状态迭代器并不会使用局部变量，它只需要一个可迭代对象进行迭代使用类可以实现你自己的迭代器，但无法实现生成器生成器运行速度快，语法简洁，更简单迭代器更能节约内存可迭代对象，迭代器和生成器之间的关系如下Pipenv依赖管理工具Pipenv：新一代Python项目环境与依赖管理工具pipenv使用指南使用 pipdeptree 工具来管理依赖树1234567891011121314$ pip install pipdeptree...$ pipdeptreecertifi==2020.6.20Flask==1.1.2  - click [required: &gt;=5.1, installed: 7.1.2]  - itsdangerous [required: &gt;=0.24, installed: 1.1.0]  - Jinja2 [required: &gt;=2.10.1, installed: 2.11.3]    - MarkupSafe [required: &gt;=0.23, installed: 1.1.1]  - Werkzeug [required: &gt;=0.15, installed: 1.0.1]pipdeptree==2.0.0  - pip [required: &gt;=6.0.0, installed: 19.3.1]setuptools==44.0.0.post20200106wheel==0.36.2pip-autoremove可以删除依赖包12345678910$ pip install flask$ pip install pip-autoremove$ pip-autoremove flask -y$ pipdeptreecertifi==2020.6.20pip-autoremove==0.9.1pipdeptree==2.0.0  - pip [required: &gt;=6.0.0, installed: 19.3.1]setuptools==44.0.0.post20200106wheel==0.36.2私有仓库Python 笔记 | 建立python私有仓库安装pypiserver1pip install pypiserver安装htpasswd的相关依赖123yum install apache2pip install passlibyum -y install httpd-toolshtpasswd生成上传密码1htpasswd -sc ~/pypipasswd.txt user_name创建python包存放的目录，可以放个whl包进去1mkdir ~/packages开启服务1pypi-server -p 8282 -P ~/pypipasswd.txt ~/packages pip安装1pip install --extra-index-url http://120.120.120.120:8080/simple/ --trusted-host 120.120.120.120 workbench_toutiao或者修改pip.ini(linux下是pip.conf)，用命令pip -v config list就可以找到路径1234567891011[global]no-cache-dir = truetrusted-host = pypi.tuna.tsinghua.edu.cn               192.168.xx.xx:8282               pypi.ngc.nvidia.comindex-url = https://pypi.tuna.tsinghua.edu.cn/simpleextra-index-url = http://192.168.xx.xx:8282/simple                  https://pypi.ngc.nvidia.com上传用户目录下创建.pypirc文件， 内容如下12345678[distutils]index-servers =  local[local]repository: http://192.168.xx.xx:8282username: user_namepassword: password123twine upload dist/*# 或者python setup.py sdist bdist_wheel upload -r http://192.168.xx.xx:8282查看库中常用模块1234import mathdir(math)['__doc__', '__loader__', '__name__', '__package__', '__spec__',  'acos', 'acosh', 'asin', 'asinh', 'atan', 'atan2', ...]linux后台执行py脚本程序后台运行1nohup python test.py &gt; tt.log&gt;&amp;1 &amp;nohup.out中显示不出来python程序中print的东西，这是因为python的输出有缓冲，导致nohup.out并不能够马上看到输出python 有个-u参数，使得python不启用缓冲1nohup python -u test.py&gt;tt.log&gt;&amp;1 &amp;还有一种方式(未尝试过)12import syssys.stdout = Unbuffered(sys.stdout)代码调试技巧Python 代码调试技巧Python 3.7，则无需导入任何内容，只需在代码中要放入调试器的位置调用breakpoint()12Some complicated code with bugsbreakpoint()Python 3.6及更早版本中，你可以通过显式导入pdb来执行相同的操作，像breakpoint()一样，pdb.set_trace()会将你带入pdb调试器12import pdb; pdb.set_trace()库和常用方法变量存储工具方法什么是酸洗和去除？Pickle模块接受任何Python对象并将其转换为字符串表示形式，并使用dump函数将其转储到文件中，此过程称为pickling从存储的字符串表示中检索原始Python对象的过程称为unpickling123456789101112import dillimport pickle as pkdef dump_obj(obj, file_name):    with open(file_name, 'wb') as f:        dill.dump(obj, f)def load_obj(file_name):    with open(file_name, &quot;rb&quot;) as f:        try:            return dill.load(f)        except Exception:            return pk.load(f)序列排序Python中拥有内置函数实现排序，可以直接调用它们实现排序功能list.sort(): 直接修改列表sorted(): 从一个可迭代对象构建一个新的排序列表不管是 list.sort 方法还是 sorted 函数，都有两个可选的关键字参数list.sort(cmp=None, key=None, reverse=False)cmp — 可选参数, 如果指定了该参数会使用该参数的方法进行排序key — 主要是用来进行比较的元素，只有一个参数，函数的参数就是取自于可迭代对象中，指定可迭代对象中的一个元素来进行排序reverse — 排序规则，reverse = True 降序， reverse = False 升序(默认)sorted(iterable, cmp=None, key=None, reverse=False)erable — 可迭代对象cmp — 比较的函数，这个具有两个参数，参数的值都是从可迭代对象中取出此函数必须遵守的规则为，大于则返回1，小于则返回-1，等于则返回0。key — 主要是用来进行比较的元素，只有一个参数，函数的参数就是取自于可迭代对象中，指定可迭代对象中的一个元素来进行排序reverse — 排序规则，reverse = True 降序 ，reverse = False 升序(默认)sort ()与sorted()区别sort() 是应用在 list 上的方法，sorted() 可以对所有可迭代的对象进行排序操作list 的 sort() 方法返回的是对已经存在的列表进行操作，无返回值而内建函数 sorted() 方法返回的是一个新的 list，而不是在原来的基础上进行的操作单关键字排序12345678910111213141516# 默认情况下，sorted()已按升序对输入进行排序，而reverse关键字参数则按降序排序。&gt;&gt;&gt; sorted(['cat', 'dog', 'cheetah', 'rhino', 'bear'], reverse=True)['rhino', 'dog', 'cheetah', 'cat', 'bear]&gt;&gt;&gt; animals = [...     {'type': 'penguin', 'name': 'Stephanie', 'age': 8},...     {'type': 'elephant', 'name': 'Devon', 'age': 3},...     {'type': 'puma', 'name': 'Moe', 'age': 5},... ]&gt;&gt;&gt; sorted(animals, key=lambda animal: animal['age'])[    {'type': 'elephant', 'name': 'Devon', 'age': 3},    {'type': 'puma', 'name': 'Moe', 'age': 5},    {'type': 'penguin', 'name': 'Stephanie, 'age': 8},]# 通过传入一个返回每个元素年龄的lambda函数，可以轻松地按每个字典的单个值对字典列表进行排序。在这种情况下，字典现在按年龄按升序排序。多关键字排序1234# itemgetter() 函数也支持多个 keysrows_by_lfname = sorted(rows, key=itemgetter('lname','fname'))# itemgetter() 有时候也可以用 lambda 表达式代替rows_by_lfname = sorted(rows, key=lambda r: (r['lname'],r['fname']))使用 itemgetter() 方式会运行的稍微快点，如果你对性能要求比较高的话就使用 itemgetter() 方式import路径问题12import syssys.path.append(&quot;..&quot;)判断是否有中文1234def is_chinese(uchar):    if u'\u4e00' &lt;= uchar &lt;= u'\u9fff':        return True    return False分割字符串并转换类型123456import numpy as npline = '12,26,31,17,90,28,88,40,77'npyArray = np.fromstring(line, dtype=int, sep=',')print(npyArray)array([12, 26, 31, 17, 90, 28, 88, 40, 77])检查对象是否可调用12345def fuunction(param):    print(param)print(callable(fuunction))True随机打乱多个数组123456789import numpy as npdef shuffledata(*arrs):# 調用案例 x,y = shuffledata(X,Y)arrs = list(arrs)for i, arr in enumerate(arrs):   assert len(arrs[0]) == len(arrs[i])   arrs[i] = np.array(arr)p = np.random.permutation(len(arrs[0]))return tuple(arr[p] for arr in arrs)pandas读写csv12345678910111213141516171819202122232425262728293031#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v0.1@author: narutohyc@file: hyc_test.py@Description: @time: 2020/5/28 20:08&quot;&quot;&quot;import pandas as pddef csv_test():    csv_name = 'my_csv.csv'# 初始化一个带表头的空表    f1 = pd.DataFrame(None, columns=['name', 'price', 'marks'])    f1.to_csv(csv_name, mode='w', encoding='utf-8', index=False)    # 追加数据到已存在的表    data = {&quot;name&quot;: ['google', 'baidu', 'yahoo'], &quot;marks&quot;: [100, 200, 300], &quot;price&quot;: [1, 2, 3]}    f1 = pd.DataFrame(data, columns=['name', 'price', 'marks'])    f1.to_csv(csv_name, mode='a', encoding='utf-8', header=False, index=False)    # 读取csv,index_col=False表示不存在索引列    f2 = pd.read_csv(csv_name, encoding='utf-8', index_col=False)    print()if __name__ == '__main__':    csv_test()classmethod 和 staticmethodPython 中的 classmethod 和 staticmethod 有什么具体用途？优雅判断数字所属等级有从 A 到 F 的 5 个等级，现要判断某个数值（从 0 到 1 之间）所属的等级举例，如数值 &gt;= 0.9，则属于 A；若数值 &gt;= 0.8，则属于 B；以此类推1234567891011121314from typing import Iterablefrom bisect import bisect def cal_level(score: float, breakpoints: Iterable, name_codes: List): &quot;&quot;&quot; 二分查找 计算分数等级 示例：     obj = cal_level(score=78, breakpoints=(60, 70, 80, 90), name_codes='FDCBA') :param score: 得分 :param breakpoints: 分值区间 :param name_codes: 各区间对应结果 :return: 该分值的对应结果 &quot;&quot;&quot; return name_codes[bisect(breakpoints, score)]取整操作123456789101112131415161718192021222324252627282930313233343536def learning():    import math    # 向上取整    math.ceil(2.3)  # 3    math.ceil(2.6)  # 3    # 向下取整    math.floor(2.3)  # 2    math.floor(2.6)  # 2    # 四舍五入    round(6.6)  # 7    round(6.4)  # 6    round(6.55)  # 7    # 只要不是.5的形式，也就是小数位不为5，round基本用法就是四舍五入。除此之外，round用于圆整，保留最近偶数：    round(6.5)  # 6  是偶数    round(7.5)  # 8 是偶数    import torch    import torch.nn as nn    m = nn.Linear(20, 30)    input = torch.randn(128, 20)    output = m(input)    logger.info(output.shape)  # torch.Size([128, 30])    m = nn.Conv2d(16, 33, 3, stride=2)    input = torch.randn(20, 16, 50, 100)    output = m(input)    logger.info(output.shape)  # torch.Size([20, 33, 24, 95])     (190 + 2 * 0 - (3 - 1) - 1) / 2 + 1    embedding = nn.Embedding(10, 3)    input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])    logger.info(input.shape)  # torch.Size([2, 4])    output = embedding(input)    logger.info(output.shape)  # torch.Size([2, 4, 3])腾讯三面：40亿个QQ号码如何去重？其他库哪些 Python 库让你相见恨晚？python cookbook记录数据结构和算法最大或最小的 N 个元素怎样从一个集合中获得最大或者最小的 N 个元素列表？heapq 模块有两个函数：nlargest() 和 nsmallest() 可以完美解决这个问题。1234import heapqnums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]print(heapq.nlargest(3, nums)) # Prints [42, 37, 23]print(heapq.nsmallest(3, nums)) # Prints [-4, 1, 2]两个函数都能接受一个关键字参数，用于更复杂的数据结构中：12345678910portfolio = [{'name': 'IBM', 'shares': 100, 'price': 91.1},{'name': 'AAPL', 'shares': 50, 'price': 543.22},{'name': 'FB', 'shares': 200, 'price': 21.09},{'name': 'HPQ', 'shares': 35, 'price': 31.75},{'name': 'YHOO', 'shares': 45, 'price': 16.35},{'name': 'ACME', 'shares': 75, 'price': 115.65}]cheap = heapq.nsmallest(3, portfolio, key=lambda s: s['price'])expensive = heapq.nlargest(3, portfolio, key=lambda s: s['price'])译者注：上面代码在对每个元素进行对比的时候，会以 price 的值进行比较如果你想在一个集合中查找最小或最大的 N 个元素，并且 N 小于集合元素数量，那么这些函数提供了很好的性能。因为在底层实现里面，首先会先将集合数据进行堆排序后放入一个列表中：123456nums = [1, 8, 2, 23, 7, -4, 18, 23, 42, 37, 2]import heapqheap = list(nums)heapq.heapify(heap)heap[-4, 2, 1, 23, 7, 2, 18, 23, 42, 37, 8]堆数据结构最重要的特征是 heap[0] 永远是最小的元素。并且剩余的元素可以很容易的通过调用heapq.heappop() 方法得到，该方法会先将第一个元素弹出来，然后用下一个最小的元素来取代被弹出元素（这种操作时间复杂度仅仅是$Olog(N)$，$N$是堆大小）。小结：当要查找的元素个数相对比较小的时候，函数 nlargest() 和 nsmallest() 是很合适的。如果你仅仅想查找唯一的最小或最大（N=1）的元素的话，那么使用 min() 和max() 函数会更快些。类似的，如果 N 的大小和集合大小接近的时候，通常先排序这个集合然后再使用切片操作会更快点（sorted(items)[:N] 或者是 sorted(items)[-N:]）。需要在正确场合使用函数 nlargest() 和 nsmallest() 才能发挥它们的优势（如果N 快接近集合大小了，那么使用排序操作会更好些）。实现一个优先级队列怎样实现一个按优先级排序的队列？并且在这个队列上面每次 pop 操作总是返回优先级最高的那个元素利用 heapq 模块实现了一个简单的优先级队列：12345678910111213import heapqclass PriorityQueue:    def __init__(self):        self._queue = []        self._index = 0        def push(self, item, priority):            heapq.heappush(self._queue, (-priority, self._index, item))            self._index += 1        def pop(self):            return heapq.heappop(self._queue)[-1]1234567891011121314151617181920&gt;&gt;&gt; class Item:... def __init__(self, name):... self.name = name... def __repr__(self):... return 'Item({!r})'.format(self.name)...&gt;&gt;&gt; q = PriorityQueue()&gt;&gt;&gt; q.push(Item('foo'), 1)&gt;&gt;&gt; q.push(Item('bar'), 5)&gt;&gt;&gt; q.push(Item('spam'), 4)&gt;&gt;&gt; q.push(Item('grok'), 1)&gt;&gt;&gt; q.pop()Item('bar')&gt;&gt;&gt; q.pop()Item('spam')&gt;&gt;&gt; q.pop()Item('foo')&gt;&gt;&gt; q.pop()Item('grok')&gt;&gt;&gt;仔细观察可以发现，第一个 pop() 操作返回优先级最高的元素。另外注意到如果两个有着相同优先级的元素（foo 和 grok ），pop 操作按照它们被插入到队列的顺序返回的。在上面代码中，队列包含了一个 (-priority, index, item) 的元组。优先级为负数的目的是使得元素按照优先级从高到低排序。这个跟普通的按优先级从低到高排序的堆排序恰巧相反。index 变量的作用是保证同等优先级元素的正确排序。通过保存一个不断增加的index 下标变量，可以确保元素按照它们插入的顺序排序。而且，index 变量也在相同优先级元素比较的时候起到重要作用通过引入另外的 index 变量组成三元组 (priority, index, item) ，就能很好的避免上面的错误，因为不可能有两个元素有相同的 index 值。Python 在做元组比较时候，如果前面的比较已经可以确定结果了，后面的比较操作就不会发生了：1234567a = (1, 0, Item('foo'))b = (5, 1, Item('bar'))c = (1, 2, Item('grok'))a &lt; bTruea &lt; cTrue如果你想在多个线程中使用同一个队列，那么你需要增加适当的锁和信号量机制。字典的运算怎样在数据字典中执行一些计算操作（比如求最小值、最大值、排序等等）？考虑下面的股票名和价格映射字典：123456prices = {    'ACME': 45.23,    'AAPL': 612.78,    'IBM': 205.55,    'HPQ': 37.20,    'FB': 10.75}为了对字典值执行计算操作，通常需要使用 zip() 函数先将键和值反转过来。比如，下面是查找最小和最大股票价格和股票值的代码1234min_price = min(zip(prices.values(), prices.keys()))# min_price is (10.75, 'FB')max_price = max(zip(prices.values(), prices.keys()))# max_price is (612.78, 'AAPL')类似的，可以使用 zip() 和 sorted() 函数来排列字典数据：1234prices_sorted = sorted(zip(prices.values(), prices.keys()))# prices_sorted is [(10.75, 'FB'), (37.2, 'HPQ'),# (45.23, 'ACME'), (205.55, 'IBM'),# (612.78, 'AAPL')]如果你在一个字典上执行普通的数学运算，你会发现它们仅仅作用于键，而不是值。需要注意的是在计算操作中使用到了 (值，键) 对。当多个实体拥有相同的值的时候，键会决定返回结果。比如，在执行 min() 和 max() 操作的时候，如果恰巧最小或最大值有重复的，那么拥有最小或最大键的实体会返回：12345prices = { 'AAA' : 45.23, 'ZZZ': 45.23 }min(zip(prices.values(), prices.keys()))(45.23, 'AAA')max(zip(prices.values(), prices.keys()))(45.23, 'ZZZ')查找两字典的相同点为了寻找两个字典的相同点，可以简单的在两字典的 keys() 或者 items() 方法返回结果上执行集合操作。123456# Find keys in commona.keys() &amp; b.keys() # { 'x', 'y' }# Find keys in a that are not in ba.keys() - b.keys() # { 'z' }# Find (key,value) pairs in commona.items() &amp; b.items() # { ('y', 2) }这些操作也可以用于修改或者过滤字典元素。比如，假如你想以现有字典构造一个排除几个指定键的新字典。下面利用字典推导来实现这样的需求：123# Make a new dictionary with certain keys removedc = {key:a[key] for key in a.keys() - {'z', 'w'}}# c is {'x': 1, 'y': 2}小结：一个字典就是一个键集合与值集合的映射关系。字典的 keys() 方法返回一个展现键集合的键视图对象。键视图的一个很少被了解的特性就是它们也支持集合操作，比如集合并、交、差运算。所以，如果你想对集合的键执行一些普通的集合操作，可以直接使用键视图对象而不用先将它们转换成一个 set。字典的 items() 方法返回一个包含 (键，值) 对的元素视图对象。这个对象同样也支持集合操作，并且可以被用来查找两个字典有哪些相同的键值对。尽管字典的 values() 方法也是类似，但是它并不支持这里介绍的集合操作。某种程度上是因为值视图不能保证所有的值互不相同，这样会导致某些集合操作会出现问题。不过，如果你硬要在值上面执行这些集合操作的话，你可以先将值集合转换成 set，然后再执行集合运算就行了。删除序列相同元素并保持顺序如果你想消除元素不可哈希（比如 dict 类型）的序列中重复元素的话1234567def dedupe(items, key=None):seen = set()for item in items:val = item if key is None else key(item)if val not in seen:yield itemseen.add(val)这里的 key 参数指定了一个函数，将序列元素转换成 hashable 类型。123456&gt;&gt;&gt; a = [ {'x':1, 'y':2}, {'x':1, 'y':3}, {'x':1, 'y':2}, {'x':2, 'y':4}]&gt;&gt;&gt; list(dedupe(a, key=lambda d: (d['x'],d['y'])))[{'x': 1, 'y': 2}, {'x': 1, 'y': 3}, {'x': 2, 'y': 4}]&gt;&gt;&gt; list(dedupe(a, key=lambda d: d['x']))[{'x': 1, 'y': 2}, {'x': 2, 'y': 4}]&gt;&gt;&gt;如果你想基于单个字段、属性或者某个更大的数据结构来消除重复元素，第二种方案同样可以胜任。命名切片内置的 slice() 函数创建了一个切片对象，可以被用在任何切片允许使用的地方。如果你有一个切片对象 a，你可以分别调用它的 a.start , a.stop , a.step 属性来获取更多的信息。还能通过调用切片的 indices(size) 方法将它映射到一个确定大小的序列上，这个方法返回一个三元组 (start, stop, step) ，所有值都会被合适的缩小以满足边界限制，从而使用的时候避免出现 IndexError 异常。123&gt;SHARES = slice(20, 23)&gt;PRICE = slice(31, 37)&gt;cost = int(record[SHARES]) * float(record[PRICE])字符串和文本使用多个界定符分割字符串函数 re.split() 是非常实用的，因为它允许你为分隔符指定多个正则模式。比如，在上面的例子中，分隔符可以是逗号，分号或者是空格，并且后面紧跟着任意个的空格。只要这个模式被找到，那么匹配的分隔符两边的实体都会被当成是结果中的元素返回。返回结果为一个字段列表，这个跟 str.split() 返回值类型是一样的。当你使用 re.split() 函数时候，需要特别注意的是正则表达式中是否包含一个括号捕获分组。如果使用了捕获分组，那么被匹配的文本也将出现在结果列表中。123&gt;&gt;&gt; fields = re.split(r'(;|,|\s)\s*', line)&gt;&gt;&gt; fields['asdf', ' ', 'fjdk', ';', 'afed', ',', 'fjek', ',', 'asdf', ',', 'foo']用 Shell 通配符匹配字符串使用 Unix Shell 中常用的通配符 (比如 .py , Dat[0-9].csv 等) 去匹配文本字符串fnmatch 模块提供了两个函数——fnmatch() 和 fnmatchcase() ，可以用来实现这样的匹配。12345678910&gt;&gt;&gt; from fnmatch import fnmatch, fnmatchcase&gt;&gt;&gt; fnmatch('foo.txt', '*.txt')True&gt;&gt;&gt; fnmatch('foo.txt', '?oo.txt')True&gt;&gt;&gt; fnmatch('Dat45.csv', 'Dat[0-9]*')True&gt;&gt;&gt; names = ['Dat1.csv', 'Dat2.csv', 'config.ini', 'foo.py']&gt;&gt;&gt; [name for name in names if fnmatch(name, 'Dat*.csv')]['Dat1.csv', 'Dat2.csv']fnmatch() 函数使用底层操作系统的大小写敏感规则 (不同的系统是不一样的) 来匹配模式。123456&gt;&gt;&gt; # On OS X (Mac)&gt;&gt;&gt; fnmatch('foo.txt', '*.TXT')False&gt;&gt;&gt; # On Windows&gt;&gt;&gt; fnmatch('foo.txt', '*.TXT')True如果你对这个区别很在意，可以使用 fnmatchcase() 来代替。它完全使用你的模式大小写匹配12&gt;&gt;&gt; fnmatchcase('foo.txt', '*.TXT')Falsefnmatch() 函数匹配能力介于简单的字符串方法和强大的正则表达式之间。如果在数据处理操作中只需要简单的通配符就能完成的时候，这通常是一个比较合理的方案。如果你的代码需要做文件名的匹配，最好使用 glob 模块。字符串匹配和搜索匹配或者搜索特定模式的文本如果你想匹配的是字面字符串，那么你通常只需要调用基本字符串方法就行，比如str.find() , str.endswith() , str.startswith() 或者类似的方法对于复杂的匹配需要使用正则表达式和 re 模块。为了解释正则表达式的基本原理，假设你想匹配数字格式的日期字符串比如 11/27/2012 ，你可以这样做1234567891011121314151617&gt;&gt;&gt; text1 = '11/27/2012'&gt;&gt;&gt; text2 = 'Nov 27, 2012'&gt;&gt;&gt;&gt;&gt;&gt; import re&gt;&gt;&gt; # Simple matching: \d+ means match one or more digits&gt;&gt;&gt; if re.match(r'\d+/\d+/\d+', text1):... print('yes')... else:... print('no')...yes&gt;&gt;&gt; if re.match(r'\d+/\d+/\d+', text2):... print('yes')... else:... print('no')...no如果你想使用同一个模式去做多次匹配，你应该先将模式字符串预编译为模式对象。match() 总是从字符串开始去匹配，如果你想查找字符串任意部分的模式出现位置，使用 findall() 方法去代替在定义正则式的时候，通常会利用括号去捕获分组。1datepat = re.compile(r'(\d+)/(\d+)/(\d+)')捕获分组可以使得后面的处理更加简单，因为可以分别将每个组的内容提取出来。123456789101112131415161718192021222324252627&gt;&gt;&gt; m = datepat.match('11/27/2012')&gt;&gt;&gt; m&lt;_sre.SRE_Match object at 0x1005d2750&gt;&gt;&gt;&gt; # Extract the contents of each group&gt;&gt;&gt; m.group(0)'11/27/2012'&gt;&gt;&gt; m.group(1)'11'&gt;&gt;&gt; m.group(2)'27'&gt;&gt;&gt; m.group(3)'2012'&gt;&gt;&gt; m.groups()('11', '27', '2012')&gt;&gt;&gt; month, day, year = m.groups()&gt;&gt;&gt;&gt;&gt;&gt; # Find all matches (notice splitting into tuples)&gt;&gt;&gt; text'Today is 11/27/2012. PyCon starts 3/13/2013.'&gt;&gt;&gt; datepat.findall(text)[('11', '27', '2012'), ('3', '13', '2013')]&gt;&gt;&gt; for month, day, year in datepat.findall(text):... print('{}-{}-{}'.format(year, month, day))...2012-11-272013-3-13&gt;&gt;&gt;findall() 方法会搜索文本并以列表形式返回所有的匹配。如果你想以迭代方式返回匹配，可以使用 finditer() 方法来代替核心步骤就是先使用 re.compile() 编译正则表达式字符串，然后使用 match() , findall() 或者 finditer() 等方法。字符串搜索和替换对于简单的字面模式，直接使用 str.replace() 方法即可对于复杂的模式，请使用 re 模块中的 sub() 函数。为了说明这个，假设你想将形式为 11/27/2012 的日期字符串改成 2012-11-27 。示例如下：1234&gt;&gt;&gt; text = 'Today is 11/27/2012. PyCon starts 3/13/2013.'&gt;&gt;&gt; import re&gt;&gt;&gt; re.sub(r'(\d+)/(\d+)/(\d+)', r'\3-\1-\2', text)'Today is 2012-11-27. PyCon starts 2013-3-13.'sub() 函数中的第一个参数是被匹配的模式，第二个参数是替换模式。反斜杠数字比如 \3 指向前面模式的捕获组号。对于更加复杂的替换，可以传递一个替换回调函数来代替12345678&gt;&gt;&gt; from calendar import month_abbr&gt;&gt;&gt; def change_date(m):... mon_name = month_abbr[int(m.group(1))]... return '{} {} {}'.format(m.group(2), mon_name, m.group(3))...&gt;&gt;&gt; datepat.sub(change_date, text)'Today is 27 Nov 2012. PyCon starts 13 Mar 2013.'&gt;&gt;&gt;一个替换回调函数的参数是一个 match 对象，也就是 match() 或者 find() 返回的对象。使用 group() 方法来提取特定的匹配部分。回调函数最后返回替换字符串。如果除了替换后的结果外，你还想知道有多少替换发生了，可以使用 re.subn()来代替。将 Unicode 文本标准化删除字符串中不需要的字符去掉文本字符串开头，结尾或者中间不想要的字符，比如空白。strip() 方法能用于删除开始或结尾的字符。lstrip() 和 rstrip() 分别从左和从右执行删除操作。默认情况下，这些方法会去除空白字符，但是你也可以指定其他字符。12345678910111213141516&gt;&gt;&gt; # Whitespace stripping&gt;&gt;&gt; s = ' hello world \n'&gt;&gt;&gt; s.strip()'hello world'&gt;&gt;&gt; s.lstrip()'hello world \n'&gt;&gt;&gt; s.rstrip()' hello world'&gt;&gt;&gt;&gt;&gt;&gt; # Character stripping&gt;&gt;&gt; t = '-----hello====='&gt;&gt;&gt; t.lstrip('-')'hello====='&gt;&gt;&gt; t.strip('-=')'hello'&gt;&gt;&gt;这些 strip() 方法在读取和清理数据以备后续处理的时候是经常会被用到的。比如，你可以用它们来去掉空格，引号和完成其他任务。但是需要注意的是去除操作不会对字符串的中间的文本产生任何影响。如果你想处理中间的空格，那么你需要求助其他技术。比如使用 replace() 方法或者是用正则表达式替换。审查清理文本字符串一些无聊的幼稚黑客在你的网站页面表单中输入文本”pýtĥöñ”，然后你想将这些字符清理掉。文本清理问题会涉及到包括文本解析与数据处理等一系列问题。在非常简单的情形下，你可能会选择使用字符串函数 (比如 str.upper() 和 str.lower() ) 将文本转为标准格式。使用 str.replace() 或者 re.sub() 的简单替换操作能删除或者改变指定的字符序列。你同样还可以使用 2.9 小节的 unicodedata.normalize() 函数将 nicode文本标准化。字符串对齐对于基本的字符串对齐操作，可以使用字符串的 ljust() , rjust() 和 center()方法。1234567&gt;&gt;&gt;&gt; text = 'Hello World'&gt;&gt;&gt;&gt; text.ljust(20)&gt;'Hello World '&gt;&gt;&gt;&gt; text.rjust(20)&gt;' Hello World'&gt;&gt;&gt;&gt; text.center(20)&gt;' Hello World '所有这些方法都能接受一个可选的填充字符。1234&gt;&gt;&gt;&gt; text.rjust(20,'=')&gt;'=========Hello World'&gt;&gt;&gt;&gt; text.center(20,'*')&gt;'****Hello World*****'函数 format() 同样可以用来很容易的对齐字符串。你要做的就是使用 &lt;,&gt; 或者 ^字符后面紧跟一个指定的宽度。1234567&gt;&gt;&gt;&gt; format(text, '&gt;20')&gt;' Hello World'&gt;&gt;&gt;&gt; format(text, '&lt;20')&gt;'Hello World '&gt;&gt;&gt;&gt; format(text, '^20')&gt;' Hello World '&gt;&gt;&gt;&gt;如果你想指定一个非空格的填充字符，将它写到对齐字符的前面即可1234&gt;&gt;&gt;&gt; format(text, '=&gt;20s')&gt;'=========Hello World'&gt;&gt;&gt;&gt; format(text, '*^20s')&gt;'****Hello World*****'当格式化多个值的时候，这些格式代码也可以被用在 format() 方法中。12&gt;&gt;&gt;&gt; '{:&gt;10s} {:&gt;10s}'.format('Hello', 'World')&gt;' Hello World'format() 函数的一个好处是它不仅适用于字符串。它可以用来格式化任何值，使得它非常的通用。比如，你可以用它来格式化数字：12345&gt;&gt;&gt;&gt; x = 1.2345&gt;&gt;&gt;&gt; format(x, '&gt;10')&gt;' 1.2345'&gt;&gt;&gt;&gt; format(x, '^10.2f')&gt;' 1.23 '在新版本代码中，你应该优先选择 format() 函数或者方法。format() 要比% 操作符的功能更为强大。并且 format() 也比使用 ljust() , rjust() 或 center() 方法更通用，因为它可以用来格式化任意对象，而不仅仅是字符串。以指定列宽格式化字符串使用 textwrap 模块来格式化字符串的输出。比如，假如你有下列的长字符串：123s = &quot;Look into my eyes, look into my eyes, the eyes, the eyes, \the eyes, not around the eyes, don't look around the eyes, \look into my eyes, you're under.&quot;下面演示使用 textwrap 格式化字符串的多种方式：123456789101112131415161718192021&gt;&gt;&gt; import textwrap&gt;&gt;&gt; print(textwrap.fill(s, 70))Look into my eyes, look into my eyes, the eyes, the eyes, the eyes,not around the eyes, don't look around the eyes, look into my eyes,you're under.&gt;&gt;&gt; print(textwrap.fill(s, 40))Look into my eyes, look into my eyes,the eyes, the eyes, the eyes, not aroundthe eyes, don't look around the eyes,look into my eyes, you're under.&gt;&gt;&gt; print(textwrap.fill(s, 40, initial_indent=' '))Look into my eyes, look into myeyes, the eyes, the eyes, the eyes, notaround the eyes, don't look around theeyes, look into my eyes, you're under.&gt;&gt;&gt; print(textwrap.fill(s, 40, subsequent_indent=' '))Look into my eyes, look into my eyes,the eyes, the eyes, the eyes, notaround the eyes, don't look aroundthe eyes, look into my eyes, you'reunder.textwrap 模块对于字符串打印是非常有用的，特别是当你希望输出自动匹配终端大小的时候。你可以使用 os.get_terminal_size() 方法来获取终端的大小尺寸。123&gt;&gt;&gt; import os&gt;&gt;&gt; os.get_terminal_size().columns80fill() 方法接受一些其他可选参数来控制 tab，语句结尾等。在字符串中处理 html 和 xml字符串令牌解析迭代器与生成器顺序迭代合并后的排序迭代对象有一系列排序序列，想将它们合并后得到一个排序序列并在上面迭代遍历heapq.merge() 函数可以帮你解决这个问题1234567891011121314&gt;&gt;&gt; import heapq&gt;&gt;&gt; a = [1, 4, 7, 10]&gt;&gt;&gt; b = [2, 5, 6, 11]&gt;&gt;&gt; for c in heapq.merge(a, b):... print(c)...1245671011heapq.merge 可迭代特性意味着它不会立马读取所有序列。这就意味着你可以在非常长的序列中使用它，而不会有太大的开销。要强调的是 heapq.merge() 需要所有输入序列必须是排过序的。特别的，它并不会预先读取所有数据到堆栈中或者预先排序，也不会对输入做任何的排序检测。它仅仅是检查所有序列的开始部分并返回最小的那个，这个过程一直会持续直到所有输入序列中的元素都被遍历完。文件与 IO数据编码和处理函数给函数参数增加元信息写好了一个函数，然后想为这个函数的参数增加一些额外的信息，这样的话其他使用者就能清楚的知道这个函数应该怎么使用。使用函数参数注解是一个很好的办法，它能提示程序员应该怎样正确使用这个函数。例如，下面有一个被注解了的函数：12def add(x:int, y:int) -&gt; int:return x + ypython 解释器不会对这些注解添加任何的语义。它们不会被类型检查，运行时跟没有加注解之前的效果也没有任何差距。尽管你可以使用任意类型的对象给函数添加注解 (例如数字，字符串，对象实例等等)，不过通常来讲使用类或者字符串会比较好点。函数注解只存储在函数的 __annotations__ 属性中12&gt;&gt;&gt; add.__annotations__{'y': &lt;class 'int'&gt;, 'return': &lt;class 'int'&gt;, 'x': &lt;class 'int'&gt;}尽管注解的使用方法可能有很多种，但是它们的主要用途还是文档。因为 python并没有类型声明，通常来讲仅仅通过阅读源码很难知道应该传递什么样的参数给这个函数。这时候使用注解就能给程序员更多的提示，让他们可以正确的使用函数。ps：这里可以结合元类，为参数做类型检测（narutohyc个人想法，还未实现）将单方法的类转换为函数你有一个除 __init__() 方法外只定义了一个方法的类。为了简化代码，你想将它转换成一个函数123456def urltemplate(template):    def opener(**kwargs):    return urlopen(template.format_map(kwargs))    return opener# Example useyahoo = urltemplate('http://finance.yahoo.com/d/quotes.csv?s={names}&amp;f=,→ {fields}')大部分情况下，你拥有一个单方法类的原因是需要存储某些额外的状态来给方法使用。带额外状态信息的回调函数你的代码中需要依赖到回调函数的使用 (比如事件处理器、等待后台任务完成后的回调等)，并且你还需要让回调函数拥有额外的状态值，以便在它的内部使用到。内联回调函数当你编写使用回调函数的代码的时候，担心很多小函数的扩张可能会弄乱程序控制流。你希望找到某个方法来让代码看上去更像是一个普通的执行序列。通过使用生成器和协程可以使得回调函数内联在某个函数中。为了演示说明，假设你有如下所示的一个执行某种计算任务然后调用一个回调函数的函数12345def apply_async(func, args, *, callback):    # Compute the result    result = func(*args)    # Invoke the callback with the result    callback(result)接下来让我们看一下下面的代码，它包含了一个 Async 类和一个 inlined_async装饰器：12345678910111213141516171819202122232425from queue import Queuefrom functools import wrapsclass Async:    def __init__(self, func, args):        self.func = func        self.args = argsdef inlined_async(func):    @wraps(func)    def wrapper(*args):        f = func(*args)        result_queue = Queue()        result_queue.put(None)        while True:            result = result_queue.get()            try:                a = f.send(result)                apply_async(a.func, a.args, callback=result_queue.put)            except StopIteration:                break    return wrapper这两个代码片段允许你使用 yield 语句内联回调步骤。比如：123456789101112def add(x, y):    return x + y@inlined_asyncdef test():    r = yield Async(add, (2, 3))    print(r)    r = yield Async(add, ('hello', 'world'))    print(r)    for n in range(10):        r = yield Async(add, (n, n))        print(r)    print('Goodbye')如果你调用 test() ，你会得到类似如下的输出：123456789101112135helloworld024681012141618Goodbye你会发现，除了那个特别的装饰器和 yield 语句外，其他地方并没有出现任何的回调函数 (其实是在后台定义的)。本小节会实实在在的测试你关于回调函数、生成器和控制流的知识。首先，在需要使用到回调的代码中，关键点在于当前计算工作会挂起并在将来的某个时候重启 (比如异步执行)。当计算重启时，回调函数被调用来继续处理结果。applyasync() 函数演示了执行回调的实际逻辑，尽管实际情况中它可能会更加复杂(包括线程、进程、事件处理器等等)。计算的暂停与重启思路跟生成器函数的执行模型不谋而合。具体来讲，yield 操作会使一个生成器函数产生一个值并暂停。接下来调用生成器的 \_next__() 或 send()方法又会让它从暂停处继续执行。根据这个思路，这一小节的核心就在 inline_async() 装饰器函数中了。关键点就是，装饰器会逐步遍历生成器函数的所有 yield 语句，每一次一个。为了这样做，刚开始的时候创建了一个 result 队列并向里面放入一个 None 值。然后开始一个循环操作，从队列中取出结果值并发送给生成器，它会持续到下一个 yield 语句，在这里一个 Async 的实例被接受到。然后循环开始检查函数和参数，并开始进行异步计算apply_async() 。然而，这个计算有个最诡异部分是它并没有使用一个普通的回调函数，而是用队列的 put() 方法来回调。这时候，是时候详细解释下到底发生了什么了。主循环立即返回顶部并在队列上执行 get() 操作。如果数据存在，它一定是 put() 回调存放的结果。如果没有数据，那么先暂停操作并等待结果的到来。这个具体怎样实现是由 apply_async() 函数来决定的。如果你不相信会有这么神奇的事情，你可以使用 multiprocessing 库来试一下，在单独的进程中执行异步计算操作，如下所示：123456if __name__ == '__main__':    import multiprocessing    pool = multiprocessing.Pool()    apply_async = pool.apply_async    # Run the test function    test()实际上你会发现这个真的就是这样的，但是要解释清楚具体的控制流得需要点时间了。将复杂的控制流隐藏到生成器函数背后的例子在标准库和第三方包中都能看到。比如，在 contextlib 中的 @contextmanager 装饰器使用了一个令人费解的技巧，通过一个 yield 语句将进入和离开上下文管理器粘合在一起。另外非常流行的 Twisted 包中也包含了非常类似的内联回调。访问闭包中定义的变量类与对象python中继承多个类(多继承)的时候，大致可以分成以下两个情况12345678910111213141516171819202122232425class FirstCls:    def __init__(self, value: int):        self.value = valueclass SecondCls:    def __init__(self, value: int):        self.value = valueclass ThirdCls:    def __init__(self, value: int, name: str):        self.value = value        self.name = nameclass MyCls_0(FirstCls, SecondCls):    def __init__(self, value: int):        super(MyCls_0, self).__init__(value=value)class MyCls_1(FirstCls, ThirdCls):    def __init__(self, value: int, name: str):        FirstCls.__init__(self=self, value=value)        ThirdCls.__init__(self=self, value=value, name=name)MyCls_0 类MyCls_0 继承自 FirstCls 和 SecondCls。在 MyCls_0 的构造函数中：使用 super(MyCls_0, self).__init__(value=value) 调用父类构造函数。这里 super() 调用的是 FirstCls 的构造函数，因为根据 Python 的方法解析顺序（MRO），FirstCls 在 SecondCls 之前。由于 FirstCls 和 SecondCls 的 __init__ 方法签名相同，所以不需要显式调用 SecondCls 的构造函数MyCls_1 类MyCls_1 继承自 FirstCls 和 ThirdCls。在 MyCls_1 的构造函数中：显式调用 FirstCls.__init__(self=self, value=value)，传入 value 参数初始化 FirstCls 部分显式调用 ThirdCls.__init__(self=self, value=value, name=name)，传入 value 和 name 参数初始化 ThirdCls 部分由于 FirstCls 和 ThirdCls 的构造函数签名不相同，MyCls_1 需要显式调用两个父类的构造函数，以确保所有的属性都被正确初始化元编程pycharm基础设置如果已经有setting.zip可以直接导入即可./res/5. python基础知识/settings.zippycharm激活idea PyCharm 全家桶激活码 - lookdiv秘钥: lookdiv.comgit配置配置git路径Setting-&gt;Version Control-&gt;gitPath to Git executable: S:\Git\cmd\git.exepython模板设置打开Setting-&gt;Editor-&gt;File and Code Templates-&gt;Files-&gt;Python Script配置如下信息(修改作者名，其他无需修改)：123456789101112#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v0.1@author: narutohyc@file: ${NAME}.py@Description: @time: ${DATE} ${TIME}&quot;&quot;&quot;快捷键设置操作快捷键补充说明Reformat CodeCtrl+Alt+F格式化代码CloseCtrl+R关闭当前py文件窗口Split VerticallyCtrl+Shift+向右箭头垂直切分窗口Split HorizontallyCtrl+Shift+向左箭头水平切分窗口Git PullCtrl+Shift+Lgit pullGit commit FileCtrl+Kgit commitGit Commit and pushCtrl+Shift+Kgit pushChange font sizectrl+N-&gt;Actions-&gt;Change font size with Ctrl+Mouse WheelShow in ExplorerCrtl+Shift+S项目依赖导出依赖安装pipreqs1pip install pipreqs导出依赖, 强制覆盖原先的依赖1pipreqs ./  --encoding=utf8 --force安装依赖安装依赖库1pip install -r requirements.txt离线安装单个下载1pip download package_name -d &quot;下载的路径(windows下双引号来表示文件夹)&quot;批量下载1pip download -d /tmp/packages -r requirements.txt批量安装已经导出的包其中 —no-index 代表忽视pip 忽视默认的依赖包索引—find-links= 代表从你指定的目录寻下找离线包 1pip install --no-index --find-links=/tmp/packages -r requirements.txt环境迁移基于anaconda虚拟环境迁移新建虚拟环境切换到虚拟环境安装conda install conda-pack打包：1conda pack -n 自己的虚拟环境名 -o env_name.tar.gz --ignore-missing-files目标服务器上，在env_name.tar.gz文件同目录下创建restore.py文件12345678910111213141516171819# 获取环境的 tar.gz 文件，env_name 是要打包的环境名# conda install conda-pack# conda pack -n env_nameimport osimport tarfilename = 'bilu'file_name = '/home/faduit/huangyc/install_packages/bilu.tar.gz'new_env_path = f'/home/faduit/huangyc/anaconda3/envs/{name}'    # ananconda 存放环境的路径if not os.path.exists(new_env_path):    os.mkdir(new_env_path)def untar(file_name, dir):    t_file = tarfile.open(file_name)    t_file.extractall(new_env_path)untar(file_name, new_env_path)以上文件中需要改 3 个地方（目标服务器上需要已经安装好 Anaconda）123name = ''    # 给环境赋一个新的名字file_name = '/home/username/env_name.tar.gz'    # file_name 改成自己的 .tar.gz 文件的绝对路径new_env_path = f'/home/username/anaconda3/envs/{name}'    # 此路径中 username 改为自己的用户名改好后执行脚本文件即可激活12# Activate the environment. This adds `my_env/bin` to your pathsource my_env/bin/activateNginxNginx快速入门什么是Nginx？Nginx (engine x) 是一个高性能的HTTP和反向代理web服务器，同时也提供了IMAP/POP3/SMTP服务。Nginx是由伊戈尔·赛索耶夫为俄罗斯访问量第二的Rambler.ru站点（俄文：Рамблер）开发的，第一个公开版本0.1.0发布于2004年10月4日。2011年6月1日，nginx 1.0.4发布。其特点是占有内存少，并发能力强，事实上nginx的并发能力在同类型的网页服务器中表现较好，中国大陆使用nginx网站用户有：百度、京东、新浪、网易、腾讯、淘宝等。在全球活跃的网站中有12.18%的使用比率，大约为2220万个网站。Nginx 是一个安装非常的简单、配置文件非常简洁（还能够支持perl语法）、Bug非常少的服务。Nginx 启动特别容易，并且几乎可以做到7*24不间断运行，即使运行数个月也不需要重新启动。你还能够不间断服务的情况下进行软件版本的升级。Nginx代码完全用C语言从头写成。官方数据测试表明能够支持高达 50,000 个并发连接数的响应。Nginx作用Http代理Http代理，反向代理：作为web服务器最常用的功能之一，尤其是反向代理。正向代理反向代理负载均衡Nginx提供的负载均衡策略有2种：内置策略和扩展策略。内置策略为轮询，加权轮询，Ip hash。扩展策略，就天马行空，只有你想不到的没有他做不到的。轮询加权轮询iphash对客户端请求的ip进行hash操作，然后根据hash结果将同一个客户端ip的请求分发给同一台服务器进行处理，可以解决session不共享的问题。fair按后端服务器的响应时间来分配请求，响应时间短的优先分配。url_hash按访问url的hash结果来分配请求，使每个url定向到同一个后端服务器，后端服务器为缓存时比较有效。动静分离动静分离，在我们的软件开发中，有些请求是需要后台处理的，有些请求是不需要经过后台处理的（如：css、html、jpg、js等等文件），这些不需要经过后台处理的文件称为静态文件。让动态网站里的动态网页根据一定规则把不变的资源和经常变的资源区分开来，动静资源做好了拆分以后，我们就可以根据静态资源的特点将其做缓存操作。提高资源响应的速度。基本使用配置监听nginx的配置文件是conf目录下的nginx.conf，默认配置的nginx监听的端口为80，如果80端口被占用可以修改为未被占用的端口即可。当我们修改了nginx的配置文件nginx.conf 时，不需要关闭nginx后重新启动nginx，只需要执行命令 nginx -s reload 即可让改动生效关闭nginx如果使用cmd命令窗口启动nginx， 关闭cmd窗口是不能结束nginx进程的，可使用两种方法关闭nginx(1)输入nginx命令 nginx -s stop(快速停止nginx) 或 nginx -s quit(完整有序的停止nginx)(2)使用taskkill taskkill /f /t /im nginx.exe1234taskkill是用来终止进程的，/f是强制终止 ./t终止指定的进程和任何由此启动的子进程。/im示指定的进程名称 .Nginx常用命令指令123456cd /usr/local/nginx/sbin/./nginx  启动./nginx -s stop  停止./nginx -s quit  安全退出./nginx -s reload  重新加载配置文件ps aux|grep nginx  查看nginx进程防火墙1234567891011121314151617181920# 开启service firewalld start# 重启service firewalld restart# 关闭service firewalld stop# 查看防火墙规则firewall-cmd --list-all# 查询端口是否开放firewall-cmd --query-port=8080/tcp# 开放80端口firewall-cmd --permanent --add-port=80/tcp# 移除端口firewall-cmd --permanent --remove-port=8080/tcp#重启防火墙(修改配置后要重启防火墙)firewall-cmd --reload# 参数解释1、firwall-cmd：是Linux提供的操作firewall的一个工具；2、--permanent：表示设置为持久；3、--add-port：标识添加的端口；演示1234567upstream lb{    server 127.0.0.1:8080 weight=1;    server 127.0.0.1:8081 weight=1;}location / {    proxy_pass http://lb;}</content>
	</entry>
	<entry>
		<title>python元编程</title>
		<url>/article/16156.html</url>
		<content>动态属性在 Python 中，数据的属性和处理数据的方法统称属性（attribute）。其实，方法只是可调用的属性。属性描述符(get/set/delete)python 使用特性管理实例属性(转)Python描述符（descriptor）解密python理解描述符(descriptor)python 描述符总结Python描述符 (descriptor) 详解在不可散列的类中使用描述符-pythonpython高级编程——描述符Descriptor详解（下篇）基本概念描述符是对多个属性运用相同存取逻辑的一种方式。例如，Django ORM 和 SQL Alchemy 等 ORM 中的字段类型是描述符，把数据库记录中字段里的数据与 Python 对象的属性对应起来。为什么需要描述符：对property来说，最大的缺点就是它们不能重复使用。虽然property可以让类从外部看起来接口整洁漂亮，但是却做不到内部同样整洁漂亮。描述符是property的升级版，允许你为重复的property逻辑编写单独的类来处理。基本要求：描述符是实现了特定协议的类，这个协议包括 __get__、__set__ 和 __delete__ 方法。property 类实现了完整的描述符协议。通常，可以只实现部分协议。其实，我们在真实的代码中见到的大多数描述符只实现了 __get__ 和 __set__ 方法，还有很多只实现了其中的一个。实现了 __get__、__set__ 或 __delete__ 方法的类是描述符。用法：描述符的用法是，创建一个描述符类，它的实例对象作为另一个类的属性。为了让描述符能够正常工作，它们必须定义在类的层次上。如果你不这么做，那么Python无法自动为你调用__get__和__set__方法。大致流程：定义一个描述符类D，其内包含一个或多个__get__()、__set__()、__delete__()方法将描述符类D的实例对象d赋值给另一个要代理的类中某个属性attr，即attr=D()之后访问、赋值、删除attr属性，将会自动触发描述符类中的__get__()、__set__()、__delete__()方法实现：要定义描述符类很简单，只要某个类中包含了下面一个或多个方法，就算是满足描述符协议，就是描述符类，就可以作为属性操作的代理器。1234class Descriptor():    def __get__(self, instance, owner):...    def __set__(self, instance, value):...    def __delete__(self, instance):...需要注意的是，__get__的返回值需要是属性值或抛异常，另外两个方法要返回None。类属性描述符对象和实例属性同名时：描述符针对的是类属性，但是当一个类中，如果类属性是描述符对象，而实例属性由于这个描述符属性同名12345678910class Person:    character = CharacterDescriptor('乐观的')    weight = WeightDescriptor(150)    def __init__(self, character,weight):        self.character = character        self.weight = weight       p = Person('悲观的', 200)print(p.character)  #属性的访问print(p.weight)     #从上面的运行结果可以看出，首先是访问了描述符的__set__方法，这是因为在构建对象的时候，相当于为character和weight赋值，然后再调用__get__方法，这是因为访问了类属性character和weight，但是最终打印出来值却并不是类属性的值，这是因为，实例属性实际上是在“描述符类属性”后面访问的，所以覆盖掉了。专有名词描述符类: 实现了描述符协议的类，描述符类的一些协议(__get__、__set__或__delete__ )。实现了__get__、__set__、__delete__ 方法的类是描述符，只要实现了其中一个就是。托管类： 将描述符实例作为类属性的类，比如Fruits 类，他有 weight、price 两个类属性，且都被赋予了描述符类的实例。描述符实例: 描述符类创建出描述符实例，通常来讲，描述符类的实例会被赋给托管类的类属性。托管实例: 托管类创建出来的实例托管属性: 托管类中由描述符实例处理的公开属性存储属性: 可以粗略的理解为、托管实例的属性、在上例中使用 vars(apple) 得到的结果中 price 和 weight 实例属性就是存储属性，它们实际存储着*实例的*属性值非数据描述符：一个类，如果只定义了__get__() 或者是__delete__()方法，而没有定义__set__()方法，则认为是非数据描述符(即没有定义__set__)数据描述符：一个类，不仅定义了__get__() 方法，还定义__set__(), __delete__() 方法，则认为是数据描述符(即定义了__get__和__set__)ps： 托管属性是类(Fruits)属性、存储属性是实例(apple)的属性。&gt;Quantity 实例是描述符，因此有个放大镜，用于获取值（__get__），以及一个手抓，用于设置值（__set__）。例子要点定义位置：为了让描述符能够正常工作，它们必须定义在类的层次上。如果你不这么做，那么Python无法自动为你调用__get__和__set__方法。独立实例：类使用了一个字典来单独保存专属于实例的数据。这个一般来说是没问题的，除非你用到了不可哈希（unhashable）的对象不可哈希处理：list的子类是不可哈希的，因此它们不能为描述符类用做数据字典的key。有一些方法可以规避这个问题，但是都不完美。最好的方法可能就是给你的描述符加标签了。描述符可以安全的在这里存储数据。只是要记住，不要在别的地方也给这个描述符添加标签。这样的代码很脆弱也有很多微妙之处。但这个方法的确很普遍，可以用在不可哈希的所有者类上。123456789101112131415class Descriptor(object):    def __init__(self, label):        self.label = label    def __get__(self, instance, owner):        print('__get__', instance, owner)        return instance.__dict__.get(self.label)    def __set__(self, instance, value):        print('__set__')        instance.__dict__[self.label] = valueclass Foo(list):    x = Descriptor('x')    y = Descriptor('y')泄漏内存问题：WeakKeyDictionary可以保证描述符类不会泄漏内存：WeakKeyDictionary的特殊之处在于：如果运行期系统发现这种字典所持有的引用，是整个程序里面指向Exam实例的最后一份引用，那么，系统就会自动将该实例从字典的键中移除。模拟ORM代码-描述符类1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#!/usr/bin/env Python# -- coding: utf-8 --import weakrefimport numbersclass Field:    passclass IntField(Field):    # 数据描述符    def __init__(self, db_column, min_value=None, max_value=None):        self.min_value = min_value        self.max_value = max_value        self.db_column = db_column        if min_value is not None:            if not isinstance(min_value, numbers.Integral):                raise ValueError(&quot;min_value must be int&quot;)            elif min_value &lt; 0:                raise ValueError(&quot;min_value must be positive int&quot;)        if max_value is not None:            if not isinstance(max_value, numbers.Integral):                raise ValueError(&quot;max_value must be int&quot;)            elif max_value &lt; 0:                raise ValueError(&quot;max_value must be positive int&quot;)        if min_value is not None and max_value is not None:            if min_value &gt; max_value:                raise ValueError(&quot;min_value must be smaller than max_value&quot;)        self._value = weakref.WeakKeyDictionary()    def __get__(self, instance, owner):        if instance is None:            return self        return self._value.get(instance, 0)    def __set__(self, instance, value):        if not isinstance(value, numbers.Integral):            raise ValueError(&quot;int value need&quot;)        if value &lt; self.min_value or value &gt; self.max_value:            raise ValueError(&quot;value must between min_value and max_value&quot;)        self._value[instance] = valueclass CharField(Field):    def __init__(self, db_column, max_length=None):        # self._value = None        self.db_column = db_column        if max_length is None:            raise ValueError(&quot;you must spcify max_lenth for charfiled&quot;)        self.max_length = max_length        self._value = weakref.WeakKeyDictionary()    def __get__(self, instance, owner):        if instance is None:            return self        return self._value.get(instance, '0')    def __set__(self, instance, value):        if not isinstance(value, str):            raise ValueError(&quot;string value need&quot;)        if len(value) &gt; self.max_length:            raise ValueError(&quot;value len excess len of max_length&quot;)        self._value[instance] = value代码-元类123456789101112131415161718192021222324252627282930313233343536373839404142class ModelMetaClass(type):    def __new__(cls, name, bases, attrs, **kwargs):        if name == &quot;BaseModel&quot;:            return super().__new__(cls, name, bases, attrs, **kwargs)        fields = {}        for key, value in attrs.items():            if isinstance(value, Field):                fields[key] = value        attrs_meta = attrs.get(&quot;Meta&quot;, None)        _meta = {}        db_table = name.lower()        if attrs_meta is not None:            table = getattr(attrs_meta, &quot;db_table&quot;, None)            if table is not None:                db_table = table        _meta[&quot;db_table&quot;] = db_table        attrs[&quot;_meta&quot;] = _meta        attrs[&quot;fields&quot;] = fields        del attrs[&quot;Meta&quot;]        return super().__new__(cls, name, bases, attrs, **kwargs)class BaseModel(metaclass=ModelMetaClass):    def __init__(self, *args, **kwargs):        for key, value in kwargs.items():            setattr(self, key, value)        return super(BaseModel, self).__init__()    def save(self):        fields = []        values = []        for key, value in self.fields.items():            db_column = value.db_column            if db_column is None:                db_column = key.lower()            fields.append(db_column)            value = getattr(self, key)            values.append(str(value))        sql = &quot;insert {db_table}({fields}) value({values})&quot;.format(db_table=self._meta[&quot;db_table&quot;],                                                                   fields=&quot;,&quot;.join(fields), values=&quot;,&quot;.join(values))        print(sql)代码-测试12345678910111213141516171819class User(BaseModel):    name = CharField(db_column=&quot;name&quot;, max_length=10)    age = IntField(db_column=&quot;age&quot;, min_value=1, max_value=100)    class Meta:        db_table = &quot;user&quot;if __name__ == '__main__':    first_user = User(name=&quot;bobby&quot;, age=28)    first_user.name = &quot;bobby&quot;    first_user.age = 28    second_user = User(name=&quot;bobby&quot;, age=23)    print(first_user.name is second_user.name)    second_user.name = 'okay'    print(first_user.name is second_user.name)    second_user.name = 'sec_boddy'    print(first_user.name)    print(second_user.name)    print(first_user.name is second_user.name)输出12345TrueFalsebobbysec_boddyFalse成绩管理代码123456789101112131415161718192021222324252627282930313233#!/usr/bin/env Python# -- coding: utf-8 --import weakrefclass Grade:    def __init__(self):        self._values = weakref.WeakKeyDictionary()    def __get__(self, instance, owner):        if instance is None:            return self        return self._values.get(instance, 0)    def __set__(self, instance, value):        if not (0 &lt;= value &lt;= 100):            raise ValueError('Grade must be between 0 and 100')        self._values[instance] = valueclass Exam:    # https://lingxiankong.github.io/2014-03-28-python-descriptor.html    # 为了让描述符能够正常工作，它们必须定义在类的层次上。如果你不这么做，那么Python无法自动为你调用__get__和__set__方法。    # 确保实例的数据只属于实例本身    math_grade = Grade()    writing_grade = Grade()    science_grade = Grade()    if __name__ == '__main__':    first_exam = Exam()    first_exam.writing_grade = 82    second_exam = Exam()    second_exam.writing_grade = 75    logger.info(f'first {first_exam.writing_grade}')    logger.info(f'second {second_exam.writing_grade}')    输出122020-06-25 20:18:23 lazy_db INFO:  first 822020-06-25 20:18:23 lazy_db INFO:  second 75成绩管理2.0代码123456789101112131415161718192021222324252627282930class Grade:    def __init__(self, name):        self.name = name        self.internal_name = '_' + self.name    def __get__(self, instance, owner):        if instance is None:            return self        return getattr(instance, self.internal_name, '')    def __set__(self, instance, value):        if not (0 &lt;= value &lt;= 100):            raise ValueError('Grade must be between 0 and 100')        setattr(instance, self.internal_name, value)class Exam:    # https://lingxiankong.github.io/2014-03-28-python-descriptor.html    # 为了让描述符能够正常工作，它们必须定义在类的层次上。如果你不这么做，那么Python无法自动为你调用__get__和__set__方法。    # 确保实例的数据只属于实例本身    math_grade = Grade('math_grade')    writing_grade = Grade('writing_grade')    science_grade = Grade('science_grade')if __name__ == '__main__':    first_exam = Exam()    first_exam.writing_grade = 82    second_exam = Exam()    second_exam.writing_grade = 75    logger.info(f'first {first_exam.writing_grade}')    logger.info(f'second {second_exam.writing_grade}')输出122020-06-25 20:18:23 lazy_db INFO:  first 822020-06-25 20:18:23 lazy_db INFO:  second 75成绩管理2.1代码123456789101112131415161718192021222324252627282930313233343536373839404142class Grade:    def __init__(self):        self.name = None        self.internal_name = None    def __get__(self, instance, owner):        if instance is None:            return self        return getattr(instance, self.internal_name, '')    def __set__(self, instance, value):        if not (0 &lt;= value &lt;= 100):            raise ValueError('Grade must be between 0 and 100')        setattr(instance, self.internal_name, value)class Meta(type):    def __new__(cls, name, bases, class_dict):        for key, value in class_dict.items():            if isinstance(value, Grade):                value.name = key                value.internal_name = '_' + key        cls = type.__new__(cls, name, bases, class_dict)        return clsclass Exam(object,metaclass=Meta):    # https://lingxiankong.github.io/2014-03-28-python-descriptor.html    # 为了让描述符能够正常工作，它们必须定义在类的层次上。如果你不这么做，那么Python无法自动为你调用__get__和__set__方法。    # 确保实例的数据只属于实例本身    math_grade = Grade()    writing_grade = Grade()    science_grade = Grade()    def __init__(self, writing_grade):        self.writing_grade = writing_gradeif __name__ == '__main__':    first_exam = Exam(85)    first_exam.writing_grade = 82    second_exam = Exam(13)    second_exam.writing_grade = 75    logger.info(f'first {first_exam.writing_grade}')    logger.info(f'second {second_exam.writing_grade}')进阶添加回调描述符仅仅是类，也许你想要为它们增加一些方法。举个例子，描述符是一个用来回调property的很好的手段。比如我们想要一个类的某个部分的状态发生变化时就立刻通知我们。下面的大部分代码是用来做这个的：12345678910111213141516171819202122232425262728293031323334class CallbackProperty(object):    &quot;&quot;&quot;A property that will alert observers when upon updates&quot;&quot;&quot;    def __init__(self, default=None):        self.data = WeakKeyDictionary()        self.default = default        self.callbacks = WeakKeyDictionary()    def __get__(self, instance, owner):        return self.data.get(instance, self.default)    def __set__(self, instance, value):                for callback in self.callbacks.get(instance, []):            # alert callback function of new value            callback(value)        self.data[instance] = value    def add_callback(self, instance, callback):        &quot;&quot;&quot;Add a new function to call everytime the descriptor updates&quot;&quot;&quot;        #but how do we get here?!?!        if instance not in self.callbacks:            self.callbacks[instance] = []        self.callbacks[instance].append(callback)class BankAccount(object):    balance = CallbackProperty(0)def low_balance_warning(value):    if value &lt; 100:        print &quot;You are poor&quot;ba = BankAccount()# will not work -- try it#ba.balance.add_callback(ba, low_balance_warning)这是一个很有吸引力的模式——我们可以自定义回调函数用来响应一个类中的状态变化，而且完全无需修改这个类的代码。这样做可真是替人分忧解难呀。现在，我们所要做的就是调用ba.balance.add_callback(ba, low_balance_warning)，以使得每次balance变化时low_balance_warning都会被调用。但是我们是如何做到的呢？当我们试图访问它们时，描述符总是会调用__get__。就好像addcallback方法是无法触及的一样！其实关键在于利用了一种特殊的情况，即，当从类的层次访问时，`_get`方法的第一个参数是None。1234567891011121314151617181920212223242526272829303132333435363738394041class CallbackProperty(object):    &quot;&quot;&quot;A property that will alert observers when upon updates&quot;&quot;&quot;    def __init__(self, default=None):        self.data = WeakKeyDictionary()        self.default = default        self.callbacks = WeakKeyDictionary()    def __get__(self, instance, owner):        if instance is None:            return self                return self.data.get(instance, self.default)    def __set__(self, instance, value):        for callback in self.callbacks.get(instance, []):            # alert callback function of new value            callback(value)        self.data[instance] = value    def add_callback(self, instance, callback):        &quot;&quot;&quot;Add a new function to call everytime the descriptor within instance updates&quot;&quot;&quot;        if instance not in self.callbacks:            self.callbacks[instance] = []        self.callbacks[instance].append(callback)class BankAccount(object):    balance = CallbackProperty(0)def low_balance_warning(value):    if value &lt; 100:        print &quot;You are now poor&quot;ba = BankAccount()BankAccount.balance.add_callback(ba, low_balance_warning)ba.balance = 5000print &quot;Balance is %s&quot; % ba.balanceba.balance = 99print &quot;Balance is %s&quot; % ba.balanceBalance is 5000You are now poorBalance is 99实现底层 @classmethod123456789101112131415161718192021222324252627282930class NewDefine_classmethod:    &quot;&quot;&quot;    使用“描述符”和“装饰器”结合起来，模拟@classmethod    &quot;&quot;&quot;    def __init__(self, function):        self.function = function     def __get__(self, instance, owner):        #对传进函数进行加工,最后返回该函数        def wrapper(*args, **kwargs):   #使用不定参数是为了匹配需要修饰的函数参数            print(&quot;给函数添加额外功能&quot;)            self.function(owner, *args, **kwargs)        return wrapper class Person:    name='我有姓名'    def __init__(self):        pass     @NewDefine_classmethod    def study_1(cls):        print(f'我的名字是：{cls.name},我会搞学习！')     @NewDefine_classmethod    def study_2(cls,score):        print(f'我的名字是：{cls.name},我会搞学习！,而且这次考试考了 {score} 分')    print(Person.study_1())print(Person.study_2(99))可以分这样几步分析：第一步：@NewDefine_classmethod本质上是一个“类装饰器”，从它的定义可知，它的定义为class NewDefine_classmethod(function).我们发现，python系统定义的@classmethod其实它的定义也是一样的，如下，class classmethod(function) .怎么样？它们二者的定义是不是一样？第二步：NewDefineclassmethod本质上又是一个描述符，因为在它的内部实现了\_get__协议，由此可见，NewDefine_classmethod是“集装饰器-描述符”于一身的。第三步：运行过程分析，因为study_1=NewDefine_classmethod（study_1）,所以，study_1本质上是一个NewDefine_classmethod的对象，又因为NewDefine_classmethod本质上是实现了描述符的，所以，study_1本质上是一个定义在类中的描述符属性。第四步：因为study1本质上是一个定义在类中的描述符属性。所以在执行Person.study_1的时候，相当于是访问类的描述符属性，所以会进入到描述符的\_get__方法。现在是不是觉得原来python描述符还有这样神奇的使用呢？注意：如果修饰的函数本身是具有返回值的，在__get__里面所定义的wrapper里面一定要返回，即return self.function(owner, args, *kwargs)。还有一个地方需要注意的是，因为这是自定义的底层实现，所以一些集成IDE可能会显示有语法错误，但是这没有关系，这正是python灵活多变的地方，运行并不会出现错误。实现底层 @staticmethodstaticmethod方法与classmethod方法的区别在于classmethod方法在使用需要传进一个类的引用作为参数。而staticmethod则不用。1234567891011121314151617181920212223242526272829class NewDefine_staticmethod:    &quot;&quot;&quot;    使用“描述符”和“装饰器”结合起来，模拟@classmethod    &quot;&quot;&quot;    def __init__(self, function):        self.function = function     def __get__(self, instance, owner):        #对传进函数进行加工,最后返回该函数        def wrapper(*args, **kwargs):   #使用不定参数是为了匹配需要修饰的函数参数            print(&quot;给函数添加额外功能&quot;)            self.function(*args, **kwargs)        return wrapper class Person:    name='我有姓名'    def __init__(self):        pass     @NewDefine_staticmethod    def study_1(math,english):        print(f'我数学考了 {math} 分,英语考了 {english} 分,我会搞学习！')     @NewDefine_staticmethod    def study_2(history,science):        print(f'我历史考了 {history} 分,科学考了 {science} 分,我会搞学习！')    print(Person.study_1(99,98))print(Person.study_2(88,89))类方法classmethod必须第一个参数是cls，这个实际上就是判断所属的那个类，因此在__get__里面的function在调用的时候，第一个参数需要传递为owner，因为所属的“类cls等价于Person等价于owner”，但是因为静态方法不需要任何参数cls或者是self都不需要，因此在__get__实现的时候不能再传递owner参数，否则会显示参数错误。实现底层 @property12345678910111213141516171819202122class NewDefine_property:    &quot;&quot;&quot;    使用“描述符”和“装饰器”结合起来，模拟@classmethod    &quot;&quot;&quot;    def __init__(self, function):        self.function = function     def __get__(self, instance, owner):        print(&quot;给函数添加额外功能&quot;)        return self.function(instance) class Person:    name='我有姓名'    def __init__(self):        self.__study=100     @NewDefine_property    def study_1(self):  #使用property装饰的函数一般不要用“参数”，因为它的主要功能是对属性的封装        return self.__study p=Person()print(p.study_1)基本思想和前面分析的还是一样的，但是有几个地方有所区别，需要注意：第一：@property的目的是封装一个方法，是这个方法可以被当做属性访问第二：调用的方式与前面有所不同，__get__里面不能再定义wrapper了，否则不会调用wrapper。得不到想要的结果，为什么呢？因为调用的方式不一样，根据前面的分析，study_1的本质是描述符属性，但是前面的调用均是使用的Person.study_1()或者是p.study_1()的形式，还是当成方法去使用的。但是此处不一样了，直接就是当成属性去使用，p.study1 ，不再是方法调用，因此wrapper函数得不到调用。所以\_get__方法得到了进一步简化。按需生成属性Python 魔法方法（三） __getattr__，__setattr__， __delattr__Python高级用法之动态属性使用__getattr__、__setattr__和__getattribute__来动态生成属性Python 语言提供了一些挂钩，使得开发者很容易就能编写出通用的代码，以便将多个系统黏合起来。例如，我们要把数据库的行（row）表示为 Python 对象。由于数据库有自己的一套结构（schema），也称架构、模式、纲要、概要、大纲，所以在操作与行相对应的对象时，我们必须知道这个数据库的结构。然而，把 Python 对象与数据库相连接的这些代码，却不需要知道行的结构，所以，这部分代码应该写得通用一些。那么，如何实现这种通用的代码呢？普通的实例属性、@property 方法和描述符，都不能完成此功能，因为它们都必须预先定义好，而像这样的动态行为，则可以通过 Python 的__getattr__特殊方法来做。如果某个类定义了__getattr__，同时系统在该类对象的实例字典中又找不到待查询的属性，那么，系统就会调用这个方法。实例属性查找python属性查找（attribute lookup）首先需要明白的是实例属性查找的过程：如果obj是某个类的实例，那么obj.name（以及等价的getattr(obj,’name’)）首先调用__getattribute__。如果类定义了__getattr__方法，那么在__getattribute__抛出 AttributeError 的时候就会调用到__getattr__，而对于描述符(__get__）的调用，则是发生在__getattribute__内部的。官网文档是这么描述的  The implementation works through a precedence chain that gives data descriptors priority over instance variables, instance variables priority over non-data descriptors, and assigns lowest priority to __getattr__() if provided.  obj = Clz(), 那么obj.attr 顺序如下：（1）如果“attr”是出现在Clz或其基类的__dict__中， 且attr是data descriptor， 那么调用其__get__方法, 否则（2）如果“attr”出现在obj的__dict__中， 那么直接返回 obj.__dict__[‘attr’]， 否则（3）如果“attr”出现在Clz或其基类的__dict__中（3.1）如果attr是non-data descriptor，那么调用其__get__方法， 否则（3.2）返回 __dict__[‘attr’]（4）如果Clz有__getattr__方法，调用__getattr__方法，否则（5）抛出AttributeError程序每次访问对象的属性时，Python 系统都会调用这个特殊方法，即使属性字典里面已经有了该属性，也依然会触发 __getattribute__ 方法。这样就可以在程序每次访问属性时，检查全局事务状态。按照 Python 处理缺失属性的标准流程，如果程序动态地访问了一个不应该有的属性，那么可以在 __getattr__ 和 __getattribute__ 里面抛出 AttributeError 异常。实现通用的功能时，我们经常会在 Python 代码里使用内置的 hasattr 函数来判断对象是否已经拥有了相关的属性，并用内置的 __getattr__ 函数来获取属性值。这些函数会先在实例字典中搜索待查询的属性，然后再调用 __getattr__。四个魔法函数访问时机如果某个类定义了__getattr__ ，同时系统在该类对象的实例字典中又找不到待查询的属性，那么，系统就会调用这个方法。程序每次访问对象的属性时，Python 系统都会调用这个特殊方法，即使属性字典里面已经有了该属性，也依然会触发__getattribute__方法。这样就可以在程序每次访问属性按照Python处理缺失属性的标准流程，如果程序动态地访问了一个不应该有的属性，那么可以在__getattr__ 和__getattribute__ 里面抛出AttributeError异常。只要对实例的属性赋值，无论是直接赋值，还是通过内置的setattr函数赋值，都会触发__setattr__方法 。__getattr__当我们访问一个不存在的属性的时候，会抛出异常，提示我们不存在这个属性。而这个异常就是__getattr__方法抛出的，其原因在于他是访问一个不存在的属性的最后落脚点，作为异常抛出的地方提示出错再适合不过了。看例子，我们找一个存在的属性和不存在的属性。1234567891011121314class A(object):    def __init__(self, value):        self.value = value     def __getattr__(self, item):        print &quot;into __getattr__&quot;        return  &quot;can not find&quot; a = A(10)print a.value# 10print a.name# into __getattr__# can not find可以看出，访问存在的属性时，会正常返回值，若该值不存在，则会进入最后的兜底函数__getattr__。__setattr__在对一个属性设置值的时候，会调用到这个函数，每个设置值的方式都会进入这个方法。12345678910111213141516171819202122class A(object):    def __init__(self, value):        print &quot;into __init__&quot;        self.value = value     def __setattr__(self, name, value):        print &quot;into __setattr__&quot;        if value == 10:            print &quot;from __init__&quot;        object.__setattr__(self, name, value)  a = A(10)# into __init__# into __setattr__# from __init__print a.value# 10a.value = 100# into __setattr__print a.value# 100在实例化的时候，会进行初始化，在__init__里，对value的属性值进行了设置，这时候会调用__setattr__方法。在对a.value重新设置值100的时候，会再次进入__setattr__方法。需要注意的地方是，在重写__setattr__方法的时候千万不要重复调用造成死循环。123456class A(object):    def __init__(self, value):        self.value = value     def __setattr__(self, name, value):        self.name = value这是个死循环。当我们实例化这个类的时候，会进入__init__，然后对value进行设置值，设置值会进入__setattr__方法，而__setattr__方法里面又有一个self.name=value设置值的操作，会再次调用自身__setattr__，造成死循环。除了上面调用object类的__setattr__避开死循环，还可以如下重写__setattr__避开循环。1234567891011class A(object):    def __init__(self, value):        self.value = value     def __setattr__(self, name, value):        self.__dict__[name] = value  a = A(10)print a.value# 10__delattr____delattr__是个删除属性的方法123456789101112131415161718class A(object):    def __init__(self, value):        self.value = value     def __delattr__(self, item):        object.__delattr__(self, item)     def __getattr__(self, item):        return &quot;when can not find attribute into __getattr__&quot;   a = A(10)print a.value# 10del a.valueprint a.value# when can not find attribute into __getattr____delattr__也要避免死循环的问题，就如__setattr__一样，在重写__delattr__，避免重复调用。__getattribute__使用__getattribute__对属性的访问做额外处理假设我们需要在数据库中实现事物（transaction）处理，即每次在访问属性时，需要额外调用特殊方法检查数据库中对应的行是否有效，以及相关的事务是否依然开放。此时使用__getattr__无法实现这种功能，因为第二次访问属性时，Python会直接返回上首次调用时存储在__dict__中的属性值，而不会再次调用__getattr__插寻属性的状态。此种情况下我们需要使用__getattribute__，该方法在用户每次访问属性是都会被调用。12345678910111213141516171819202122232425class LazyDB(object):    def __init__(self):        self.exist = 1     def __getattribute__(self, item):        print('__getattribute__ (%s) called' % item)        try:            return super().__getattribute__(item)        except AttributeError:            value = ' '.join(['default value: ', item])            setattr(self, item, value)            return value data = LazyDB()print(data.foo) ##每次访问类属性时都会被调用，此处是第1次调用print(data.foo) ##每次访问类属性时都会被调用，此处是第2次调用print(data.__dict__)  ##每次访问类属性时都会被调用，此处是第3次待用 ###输出如下：__getattribute__ (foo) calleddefault value:  foo__getattribute__ (foo) calleddefault value:  foo__getattribute__ (__dict__) called{'exist': 1, 'foo': 'default value:  foo'}要点__getattr__ 和 __setatr__，我们可以用惰性的方式来加载并保存对象的属性。要理解 __getattr__ 与 __getattribute__ 的区别：前者只会在待访问的属性缺失时触发，而后者则会在每次访问属性时触发。如果要在 __getattribute__ 和 __setattr__ 方法中访问实例属性，那么应该直接通过super()（也就是object类的同名方法）来做，以避免无限递归。总结：（1）对于类装饰器属性，只要出现属性访问（不管是通过对象访问还是类名访问），都会优先调用装饰器的__get__方法;（2）对于类装饰器属性，若出现属性修改（不管是通过对象访问还是类名访问），都会优先调用装饰器的__set__方法;（3）对于类装饰器属性，若出现属性删除（不管是通过对象访问还是类名访问），都会优先调用装饰器的__delete__方法例子元类第33/34条用元类核实和登记子类,3334,验证,注册基本概念类元编程是指在运行时创建或定制类的技艺。在 Python 中，类是一等对象，因此任何时候都可以使用函数新建类，而无需使用 class 关键字。类装饰器也是函数，不过能够审查、修改，甚至把被装饰的类替换成其他类。最后，元类是类元编程最高级的工具：使用元类可以创建具有某种特质的全新类种，例如我们见过的抽象基类。元类的定义Python定义元类时，需要从 type类中继承，然后重写 __new__方法，便可以实现意想不到的功能。1234567891011121314class Meta(type):    def __new__(meta,name,bases,class_dict):        #...各种逻辑实现1        cls = type.__new__(meta,name,bases,class_dict)        print('当前类名',name)        print('父类',bases)        print('全部类属性',class_dict)        #...各种逻辑实现2        return clsclass MyClass(object,metaclass=Meta):    stuff = 33    def foo(self):        pass123当前类名 MyClass父类 (&lt;class 'object'&gt;,)全部类属性 {'__module__': '__main__', '__qualname__': 'MyClass', 'stuff': 33, 'foo': &lt;function MyClass.foo at 0x0000019E028315E8&gt;}元类可以获知那个类的名称、其所继承的父类，以及定义在class语句体中的全部类属性元类的本质在Python当中万物皆对象，我们用 class关键字定义的类本身也是一个对象， 负责产生该对象的类称之为元类 ，元类可以简称为类的类， 元类的主要目的是为了控制类的创建行为 。type是Python的一个内建元类，用来直接控制生成类，在Python中任何 class定义的类其实都是 type类实例化的结果。只有继承了 type类才能称之为一个元类，否则就是一个普通的自定义类，自定义元类可以控制类的产生过程，类的产生过程其实就是元类的调用过程。小结元类的各种操作可以实现类的验证和注册逻辑，均可以在元类的 __new__方法中实现，主要原因是当子类对象构建时，会先调用元类的 __new__方法，产生一个空对象，然后再调用子类的 __init__方法给对象属性进行赋值。验证子类元类是python比较高级的用法，简而言之，元类就是创建类的类。而type就是一个元类，是用来创建类对象的类。因此，要定义元类就要使其继承type类。通常情况下，开发者在使用OOP的方式编程时，往往会用到__init__方法，即构造函数。该方法会在类初始化时运行。但是我们可以将验证的时机提前，以至于提前到类创建之时，因此就会用到__new__方法。12345678910111213141516171819class Base(type):    def __new__(cls, name, param, dicts):        print(cls)        print(name)        print(param)        print(dicts)        return super().__new__(cls, name, param, dicts) class Meta(metaclass=Base):    name = 'yang'     def person(self):        passMeta() &lt;class '__main__.Base'&gt;Meta(){'__module__': '__main__', '__qualname__': 'Meta', 'name': 'yang', 'person': &lt;function Meta.person at 0x10c6492f0&gt;}元类中所编写的验证逻辑，针对的是该基类的子类，而不是基类本身。__new__()方法接收到的参数依次是：当前准备创建的类的对象；类的名字；类继承的父类集合；类的方法集合。案例1 ：编写一个多边形类，当边数小于 3时，其类报错，实现其验证逻辑。123456789101112131415161718192021class ValidatePolygon(type):    ## __new__当中放入验证逻辑    def __new__(meta,name,bases,class_dict):        if bases!=(object,): ##针对子类而不对基类            if class_dict['sides'] &lt; 3:                raise ValueError('Polygons need 3+ sides')        return type.__new__(meta,name,bases,class_dict)        class Polygons(object,metaclass=ValidatePolygon):    sides = None        @classmethod    def interior_angles(cls):        return (cls.sides - 2) * 180        class Triangle(Polygons):    sides = 3    ### 类设计报错。class Line(Polygons):    sides = 1注册子类元类还有一个用途，就是在程序中 自动注册类型。开发者每次从基类中继承子类时，基类的元类都可以自动运行注册代码。案例2 ：实现对象的序列化与反序列化1234567891011121314151617181920212223242526272829###建立类名与该类对象的映射关系，维护registry字典。registry = {}def register_class(target_class):    registry[target_class.__name__] = target_classdef deserialize(data):    params = json.loads(data)    name = params['class']    target_class = registry[name]    return target_classclass Meta(type):    def __new__(meta,name,bases,class_dict):        cls = type.__new__(meta,name,bases,class_dict)        register_class(cls) ##注册子类        return clsclass BetterSerializable(object):    def __init__(self,*args):        self.args = args        def serialize(self):        return json.dumps({'class':self.__class__.__name__,            'args':self.args,})    def __repr__(self):        passclass RegisterSerializabel(BetterSerializable,metaclass=Meta):    pass通过元类来实现类的注册，可以确保所有的子类都不会遗漏，从而避免后续的错误。获取__init__的默认参数获取__init__的默认参数，并在classmethod方法中为没有给定的属性赋默认值，提升代码的健壮性元类定义：1234567891011121314151617181920212223242526#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v0.1@author: narutohyc@file: meta_interface.py@Description: @time: 2020/6/15 20:29&quot;&quot;&quot;import collectionsfrom abc import (ABC,                 abstractmethod,                 ABCMeta)import inspectclass DicMetaClass(ABCMeta):    def __new__(cls, name, bases, attrs, **kwargs):        if name == 'DicMeta':            return super().__new__(cls, name, bases, attrs, **kwargs)        # 获取__init__函数的 默认值        argspec = inspect.getfullargspec(attrs[&quot;__init__&quot;])        init_defaults = dict(zip(argspec.args[-len(argspec.defaults):], argspec.defaults))        cls.__init_defaults = init_defaults        attrs['__init_defaults__'] = init_defaults        return super().__new__(cls, name, bases, attrs, **kwargs)抽象父类：1234567891011121314151617181920212223242526272829303132333435#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v0.1@author: narutohyc@file: meta_interface.py@Description: @time: 2020/6/15 20:29&quot;&quot;&quot;from abc import (ABC,                 abstractmethod,                 ABCMeta)class DicMeta(ABC, metaclass=DicMetaClass):    def __init__(self):        pass    @abstractmethod    def to_dict(self):        '''        返回字典        '''        pass    @classmethod    def load_from_mapping(cls, mapping_datas):        '''        用字典来构建实例对象        '''        assert isinstance(mapping_datas, collections.abc.Mapping)        obj = cls.__new__(cls)        [setattr(obj, k, v) for k, v in mapping_datas.items()]        return obj子类实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v0.1@author: narutohyc@file: text_meta.py@Description: @time: 2020/5/22 14:55&quot;&quot;&quot;from augmentation.meta_class.meta_interface import DicMetafrom utils.utils_func import gen_md5, str2boolimport reclass TaskMeta(DicMeta):    '''    数据包装类的bean结构    '''    def __init__(self, text, doc_id, sentence_id, reg_lst,                 has_reg=True,                 flag=None,                 dataset='train',                 text_source=&quot;primitive&quot;):        super(TaskMeta, self).__init__()        self.text = text        self.doc_id = doc_id        self.sentence_id = sentence_id        if reg_lst and isinstance(reg_lst[0], list):            reg_lst = ['%s %s %s' % (tag, start_idx, value) for tag, start_idx, value in reg_lst]        self.reg_lst = sorted(reg_lst, key=lambda reg: int(re.sub(' +', ' ', reg).split(&quot; &quot;, 2)[1])) if reg_lst else []        self.flag = list(set(i.split(' ', 2)[0] for i in self.reg_lst)) if flag is None else flag        self.has_reg = str2bool(has_reg)        self.dataset = dataset        self.text_source = text_source        self._id = gen_md5(self.text)    @classmethod    def load_from_mapping(cls, mapping_datas):        '''        用字典来构建 TaskMeta实例        '''        obj = super(TaskMeta, cls).load_from_mapping(mapping_datas)        obj._id = gen_md5(obj.text)        [setattr(obj, k, v) for k, v in obj.__init_defaults__.items() if not hasattr(obj, k)]        if obj.flag is None:            obj.flag = list(set(i.split(' ', 2)[0] for i in obj.reg_lst))        obj.has_reg = str2bool(obj.has_reg)        return obj        @property    def to_dict(self):        '''        当该类没有其他多余属性时  可以直接返回self.__dict__的副本        '''        return {&quot;text&quot;: self.text,                &quot;doc_id&quot;: self.doc_id,                &quot;sentence_id&quot;: self.sentence_id,                &quot;reg_lst&quot;: self.reg_lst,                &quot;flag&quot;: list(self.flag),                &quot;has_reg&quot;: self.has_reg,                &quot;dataset&quot;: self.dataset,                &quot;text_source&quot;: self.text_source,                &quot;_id&quot;: self._id}    测试类：12345678910task_meta_0 = TaskMeta.load_from_mapping({'text': '斯坦福大学开发的基于条件随机场的命名实体识别系统，该系统参数是基于CoNLL、MUC-6、MUC-7和ACE命名实体语料训练出来的。',                                           'doc_id': 'id1', 'sentence_id': 'id1',                                           'reg_lst': ['学校 0 斯坦福大学', '标注 33 CoNLL', '标注 39 MUC-6', '标注 45 MUC-7', '标注 51 ACE']})task_meta_1 = TaskMeta.load_from_mapping({'text': '斯坦福大学开发的基于条件随机场的命名实体识别系统，该系统参数是基于CoNLL、MUC-6、MUC-7和ACE命名实体语料训练出来的。',                                           'doc_id': 'id1', 'sentence_id': 'id1',                                           'reg_lst': ['学校 0 斯坦福大学', '标注 33 CoNLL', '标注 39 MUC-6', '标注 45 MUC-7', '标注 51 ACE'],                                           'flag': ['学校', '标注'], 'has_reg': True,                                           'dataset': 'train', 'text_source': 'primitive', '_id': '3b895befc659345be8686bd7de4d7693'})task_meta_0.to_dict == task_meta_1.to_dictOut[33]: True可以看出，taskmeta_0和task_meta_1两者的 值是完全相同的，这里就可以做到共享\_init__默认参数的效果注解类的属性元类还有一个更有用处的功能，那就是可以在某个类刚定义好但是尚未使用的时候，提前修改或注解该类的属性。这种写法通常会与描述符(descriptor) 搭配起来(参见本书第31条)，令这些属性可以更加详细地了解自己在外围类中的使用方式。例如，要定义新的类，用来表示客户数据库里的某- -行。同时，我们还希望在该类的相关属性与数据库表的每一列之间， 建立对应关系。于是，用下面这个描述符类，把属性与列名联系起来。123456789101112class Field:       def __init__(self, name):           self.name = name           self.internal_name = '_' + self.name       def __get__(self, instance, owner):           if instance is None:               return self           return getattr(instance, self.internal_name, '')       def __set__(self, instance, value):           setattr(instance, self.internal_name, value)由于列的名称已经保存到了Field描述符中，所以我们可以通过内置的setattr和getattr函数，把每个实例的所有状态都作为protected字段，存放在该实例的字典里面。在本书前面的例子中，为了避免内存泄漏，我们曾经用weakref字典来构建描述符，而刚才的那段代码，目前看来，似乎要比weakref方案便捷得多。接下来定义表示数据行的Customer类，定义该类的时候，我们要为每个类属性指定对应的列名。12345class Customer:       first_name = Field('first_name')       last_name = Field('last_name')       prefix = Field('prefix')       suffix = Field('suffix')问题在于，上面这种写法显得有些重复。在Customer类的class语句体中，我们既然要将构建好的Field对象赋给Customer.first name, 那为什么还要把这个字段名(本例中是’first name’) 再传给Field的构造器呢?之所以还要把字段名传给Field构造器，是因为定义Customer类的时候，Python 会以从右向左的顺序解读赋值语句，这与从左至右的阅读顺序恰好相反。首先，Python 会以Field(first name’) 的形式来调用Field 构造器，然后，它把调用构造器所得的返回值，赋给Customer.field name。 从这个顺序来看，Field 对象没有办法提前知道自己会赋给Customer类里的哪一个属性。    为了消除这种重复代码，我们现在用元类来改写它。使用元类，就相当于直接在class语句上面放置挂钩，只要class语句体处理完毕，这个挂钩就会立刻触发。于是，我们可以借助元类，为Field描述符自动设置其Field.name和Field.internal_ name, 而不用再像刚才那样，把列的名称手工传给Field 构造器。12345678class Meta(type):       def __new__(meta, name, bases, class_dict):           for key, value in class_dict.items():               if isinstance(value, Field):                   value.name = key                   value.internal_name = '_' + key           cls = type.__new__(meta, name, bases, class_dict)           return cls下面定义一一个基类，该基类使用刚才定义好的Meta作为其元类。凡是代表数据库里面某一行的类，都应该从这个基类中继承，以确保它们能够利用元类所提供的功能:12class DatabaseRow(object, metaclass=Meta):pass采用元类来实现这套方案时，Field 描述符类基本上是无需修改的。唯一 要调整的地方就在于:现在不需要再给构造器传人参数了，因为刚才编写的Meta.__new__ 方法会自动把相关的属性设置好。1234class Field:       def __init__(self):           self.name = None           self.internal_name = None有了元类、新的DatabaseRow基类以及新的Field描述符之后，我们在为数据行定义DatabaseRow子类时，就不用再像原来那样，编写重复的代码了。12345class BetterCustomer(DatabaseRow):       first_name = Field()       last_name = Field()       prefix = Field()       suffix = Field()ORM例子ORM 是 python编程语言后端web框架 Django的核心思想，“Object Relational Mapping”，即对象-关系映射，简称ORM。一个句话理解就是：创建一个实例对象，用创建它的类名当做数据表名，用创建它的类属性对应数据表的字段，当对这个实例对象操作时，能够对应MySQL语句 代码123456789101112131415161718192021222324252627282930313233343536373839404142class ModelMetaclass(type):    def __new__(cls, name, bases, attrs):        mappings = dict()        for k, v in attrs.items():            if isinstance(v, tuple):                print('Found mapping :%s ==&gt; %s' % (k, v))                mappings[k] = v        for k in mappings.keys():            attrs.pop(k)        attrs['__mappings__'] = mappings        attrs['__table__'] = name        return type.__new__(cls, name, bases, attrs)class Model(object, metaclass=ModelMetaclass):    def __init__(self, **kwargs):        for name, value in kwargs.items():            setattr(self, name, value)    def save(self):        fields, args = [], []        for k, v in self.__mappings__.items():            fields.append(v[0])            args.append(getattr(self, k, None))        args_temp = list()        for temp in args:            if isinstance(temp, int):                args_temp.append(str(temp))            elif isinstance(temp, str):                args_temp.append(&quot;&quot;&quot;'%s'&quot;&quot;&quot; % temp)        sql = 'insert into %s (%s) values (%s)' % (self.__table__, ','.join(fields), ','.join(args_temp))        print(sql)class User(Model):    uid = ('uid', 'int unsigned')    name = ('username', 'varchar(30)')    email = ('email', 'varchar(30)')    password = ('password', 'varchar(30)')user = User(uid=1234, name='naruto', email='1832044042@qq.mail', password='hycpass')user.save()输出12345Found mapping :uid ==&gt; ('uid', 'int unsigned')Found mapping :name ==&gt; ('username', 'varchar(30)')Found mapping :email ==&gt; ('email', 'varchar(30)')Found mapping :password ==&gt; ('password', 'varchar(30)')insert into User (uid,username,email,password) values (1234,'naruto','1832044042@qq.mail','hycpass')要点借助元类，我们可以在某个类完全定义好之前，率先修改该类的属性。描述符与元类能够有效地组合起来，以便对某种行为做出修饰，或在程序运行时探查相关信息。如果把元类与描述符相结合，那就可以在不使用weakref模块的前提下避免内存泄漏。</content>
	</entry>
	<entry>
		<title>多线程_基础篇</title>
		<url>/article/9f4ad441.html</url>
		<content>基础篇1231. Java多线程系列目录(共43篇)-转https://www.jianshu.com/p/dca27f047923https://www.cnblogs.com/skywang12345/p/java_threads_category.html线程共包括以下5种状态：新建状态(New) : 线程对象被创建后，就进入了新建状态。例如，Thread thread = new Thread()。就绪状态(Runnable): 也被称为“可执行状态”。线程对象被创建后，其它线程调用了该对象的start()方法，从而来启动该线程。例如，thread.start()。处于就绪状态的线程，随时可能被CPU调度执行。运行状态(Running): 线程获取CPU权限进行执行。需要注意的是，线程只能从就绪状态进入到运行状态。阻塞状态(Blocked) : 阻塞状态是线程因为某种原因放弃CPU使用权，暂时停止运行。直到线程进入就绪状态，才有机会转到运行状态。阻塞的情况分三种：(01) 等待阻塞 — 通过调用线程的wait()方法，让线程等待某工作的完成。(02) 同步阻塞 — 线程在获取synchronized同步锁失败(因为锁被其它线程所占用)，它会进入同步阻塞状态。(03) 其他阻塞 — 通过调用线程的sleep()或join()或发出了I/O请求时，线程会进入到阻塞状态。当sleep()状态超时、join()等待线程终止或者超时、或者I/O处理完毕时，线程重新转入就绪状态。死亡状态(Dead) : 线程执行完了或者因异常退出了run()方法，该线程结束生命周期。扩展：Object类，定义了wait(), notify(), notifyAll()等休眠/唤醒函数。Thread类，线程操作函数。例如sleep()休眠函数, interrupt()中断函数, getName()获取线程名称等。synchronized关键字，区分为synchronized代码块和synchronized方法，让线程获取对象的同步锁。实现多线程的两种方式 1231. Runnable 是一个接口，该接口中只包含了一个run()方法。   Runnable的作用，实现多线程。我们可以定义一个类A实现Runnable接口；然后，通过new Thread(new A())等方式新建线程。2. Thread 是一个类。Thread本身就实现了Runnable接口。Thread的作用，实现多线程。Thread和Runnable的异同点1234Thread 和 Runnable 的相同点：都是“多线程的实现方式”。Thread 和 Runnable 的不同点：Thread 是类，而Runnable是接口；Thread本身是实现了Runnable接口的类。我们知道“一个类只能有一个父类，但是却能实现多个接口”，因此Runnable具有更好的扩展性。此外，Runnable还可以用于“资源的共享”。即，多个线程都是基于某一个Runnable对象建立的，它们会共享Runnable对象上的资源。//通常，建议通过“Runnable”实现多线程！Runnable的多线程示例123456789101112131415161718192021222324252627282930313233343536373839class MyThread implements Runnable {    private int ticket = 10;    public void run() {        for (int i = 0; i &lt; 20; i++) {            if (this.ticket &gt; 0) {                System.out.println(Thread.currentThread().getName() + &quot; 卖票：ticket&quot; + this.ticket--);            }        }    }};public class Test {    public static void main(String[] args) {        MyThread mt = new MyThread();        // 启动3个线程t1,t2,t3(它们共用一个Runnable对象)，这3个线程一共卖10张票！        Thread t1 = new Thread(mt);        Thread t2 = new Thread(mt);        Thread t3 = new Thread(mt);        t1.start();        t2.start();        t3.start();    }}//结果说明：(01) 和下面“MyThread继承于Thread”不同；这里的MyThread实现了Thread接口。(02) 主线程main创建并启动3个子线程，而且这3个子线程都是基于“mt这个Runnable对象”而创建的。运行结果是这3个子线程一共卖出了10张票。这说明它们是共享了MyThread接口的。Thread-1 卖票：ticket10Thread-2 卖票：ticket9Thread-0 卖票：ticket9Thread-2 卖票：ticket7Thread-1 卖票：ticket8Thread-2 卖票：ticket5Thread-0 卖票：ticket6Thread-2 卖票：ticket3Thread-1 卖票：ticket4Thread-2 卖票：ticket1Thread-0 卖票：ticket2Thread的多线程示例123456789101112131415161718192021222324252627282930313233343536373839404142class MyThread extends Thread{    private int ticket=5;    public void run(){        for(int i=0;i&lt;20;i++){            if(this.ticket&gt;0){                System.out.println(this.getName()+&quot; 卖票：ticket&quot;+this.ticket--);            }        }    }}public class Test {    public static void main(String[] args) {        // 启动3个线程t1,t2,t3；每个线程各卖10张票！        MyThread t1=new MyThread();        MyThread t2=new MyThread();        MyThread t3=new MyThread();        t1.start();        t2.start();        t3.start();    }}//结果说明：(01) MyThread继承于Thread，它是自定义个线程。每个MyThread都会卖出10张票。(02) 主线程main创建并启动3个MyThread子线程。每个子线程都各自卖出了10张票。Thread-0 卖票：ticket5Thread-0 卖票：ticket4Thread-0 卖票：ticket3Thread-0 卖票：ticket2Thread-0 卖票：ticket1Thread-1 卖票：ticket5Thread-1 卖票：ticket4Thread-1 卖票：ticket3Thread-1 卖票：ticket2Thread-1 卖票：ticket1Thread-2 卖票：ticket5Thread-2 卖票：ticket4Thread-2 卖票：ticket3Thread-2 卖票：ticket2Thread-2 卖票：ticket1Thread中start()和run()的区别 12start(): 它的作用是启动一个新线程，新线程会执行相应的run()方法。start()不能被重复调用。run(): run()就和普通的成员方法一样，可以被重复调用。单独调用run()的话，会在当前线程中执行run()，而并不会启动新线程！synchronized关键字synchronized原理123在java中，每一个对象有且仅有一个同步锁。这也意味着，同步锁是依赖于对象而存在。当我们调用某对象的synchronized方法时，就获取了该对象的同步锁。例如，synchronized(obj)就获取了“obj这个对象”的同步锁。不同线程对同步锁的访问是互斥的。也就是说，某时间点，对象的同步锁只能被一个线程获取到！通过同步锁，我们就能在多线程中，实现对“对象/方法”的互斥访问。 例如，现在有两个线程A和线程B，它们都会访问“对象obj的同步锁”。假设，在某一时刻，线程A获取到“obj的同步锁”并在执行一些操作；而此时，线程B也企图获取“obj的同步锁” —— 线程B会获取失败，它必须等待，直到线程A释放了“该对象的同步锁”之后线程B才能获取到“obj的同步锁”从而才可以运行。synchronized基本规则123第一条: 当一个线程访问“某对象”的“synchronized方法”或者“synchronized代码块”时，其他线程对“该对象”的该“synchronized方法”或者“synchronized代码块”的访问将被阻塞。第二条: 当一个线程访问“某对象”的“synchronized方法”或者“synchronized代码块”时，其他线程仍然可以访问“该对象”的非同步代码块。第三条: 当一个线程访问“某对象”的“synchronized方法”或者“synchronized代码块”时，其他线程对“该对象”的其他的“synchronized方法”或者“synchronized代码块”的访问将被阻塞。123456789101112131415161718192021222324252627282930313233343536373839// 第一条class MyRunable implements Runnable {    @Override    public void run() {        synchronized(this) {            try {                  for (int i = 0; i &lt; 5; i++) {                    Thread.sleep(100); // 休眠100ms                    System.out.println(Thread.currentThread().getName() + &quot; loop &quot; + i);                  }            } catch (InterruptedException ie) {              }        }      }}public class Demo1_1 {    public static void main(String[] args) {          Runnable demo = new MyRunable();     // 新建“Runnable对象”        Thread t1 = new Thread(demo, &quot;t1&quot;);  // 新建“线程t1”, t1是基于demo这个Runnable对象        Thread t2 = new Thread(demo, &quot;t2&quot;);  // 新建“线程t2”, t2是基于demo这个Runnable对象        t1.start();                          // 启动“线程t1”        t2.start();                          // 启动“线程t2”     } }// 结果说明：run()方法中存在“synchronized(this)代码块”，而且t1和t2都是基于&quot;demo这个Runnable对象&quot;创建的线程。这就意味着，我们可以将synchronized(this)中的this看作是“demo这个Runnable对象”；因此，线程t1和t2共享“demo对象的同步锁”。所以，当一个线程运行的时候，另外一个线程必须等待“运行线程”释放“demo的同步锁”之后才能运行。t1 loop 0t1 loop 1t1 loop 2t1 loop 3t1 loop 4t2 loop 0t2 loop 1t2 loop 2t2 loop 3t2 loop 41234567891011121314151617181920212223242526272829303132333435363738394041424344454647// 第一条_续class MyThread extends Thread {    public MyThread(String name) {        super(name);    }    @Override    public void run() {        synchronized(this) {            try {                  for (int i = 0; i &lt; 5; i++) {                    Thread.sleep(100); // 休眠100ms                    System.out.println(Thread.currentThread().getName() + &quot; loop &quot; + i);                  }            } catch (InterruptedException ie) {              }        }      }}public class Demo1_2 {    public static void main(String[] args) {          Thread t1 = new MyThread(&quot;t1&quot;);  // 新建“线程t1”        Thread t2 = new MyThread(&quot;t2&quot;);  // 新建“线程t2”        t1.start();                      // 启动“线程t1”        t2.start();                      // 启动“线程t2”     } }// 代码说明：比较Demo1_2 和 Demo1_1，我们发现，Demo1_2中的MyThread类是直接继承于Thread，而且t1和t2都是MyThread子线程。幸运的是，在“Demo1_2的run()方法”也调用了synchronized(this)，正如“Demo1_1的run()方法”也调用了synchronized(this)一样！// 结果说明：如果这个结果一点也不令你感到惊讶，那么我相信你对synchronized和this的认识已经比较深刻了。否则的话，请继续阅读这里的分析。synchronized(this)中的this是指“当前的类对象”，即synchronized(this)所在的类对应的当前对象。它的作用是获取“当前对象的同步锁”。对于Demo1_2中，synchronized(this)中的this代表的是MyThread对象，而t1和t2是两个不同的MyThread对象，因此t1和t2在执行synchronized(this)时，获取的是不同对象的同步锁。对于Demo1_1对而言，synchronized(this)中的this代表的是MyRunable对象；t1和t2共同一个MyRunable对象，因此，一个线程获取了对象的同步锁，会造成另外一个线程等待。t1 loop 0t2 loop 0t1 loop 1t2 loop 1t1 loop 2t2 loop 2t1 loop 3t2 loop 3t1 loop 4t2 loop 4123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// 第二条class Count {    // 含有synchronized同步块的方法    public void synMethod() {        synchronized(this) {            try {                  for (int i = 0; i &lt; 5; i++) {                    Thread.sleep(100); // 休眠100ms                    System.out.println(Thread.currentThread().getName() + &quot; synMethod loop &quot; + i);                  }            } catch (InterruptedException ie) {              }        }      }    // 非同步的方法    public void nonSynMethod() {        try {              for (int i = 0; i &lt; 5; i++) {                Thread.sleep(100);                System.out.println(Thread.currentThread().getName() + &quot; nonSynMethod loop &quot; + i);              }        } catch (InterruptedException ie) {          }    }}public class Demo2 {    public static void main(String[] args) {          final Count count = new Count();        // 新建t1, t1会调用“count对象”的synMethod()方法        Thread t1 = new Thread(                new Runnable() {                    @Override                    public void run() {                        count.synMethod();                    }                }, &quot;t1&quot;);        // 新建t2, t2会调用“count对象”的nonSynMethod()方法        Thread t2 = new Thread(                new Runnable() {                    @Override                    public void run() {                        count.nonSynMethod();                    }                }, &quot;t2&quot;);          t1.start();  // 启动t1        t2.start();  // 启动t2    } }// 结果说明：主线程中新建了两个子线程t1和t2。t1会调用count对象的synMethod()方法，该方法内含有同步块；而t2则会调用count对象的nonSynMethod()方法，该方法不是同步方法。t1运行时，虽然调用synchronized(this)获取“count的同步锁”；但是并没有造成t2的阻塞，因为t2没有用到“count”同步锁。t1 synMethod loop 0t2 nonSynMethod loop 0t1 synMethod loop 1t2 nonSynMethod loop 1t1 synMethod loop 2t2 nonSynMethod loop 2t1 synMethod loop 3t2 nonSynMethod loop 3t1 synMethod loop 4t2 nonSynMethod loop 41234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// 第三条class Count {    // 含有synchronized同步块的方法    public void synMethod() {        synchronized(this) {            try {                  for (int i = 0; i &lt; 5; i++) {                    Thread.sleep(100); // 休眠100ms                    System.out.println(Thread.currentThread().getName() + &quot; synMethod loop &quot; + i);                  }            } catch (InterruptedException ie) {              }        }      }    // 也包含synchronized同步块的方法    public void nonSynMethod() {        synchronized(this) {            try {                  for (int i = 0; i &lt; 5; i++) {                    Thread.sleep(100);                    System.out.println(Thread.currentThread().getName() + &quot; nonSynMethod loop &quot; + i);                  }            } catch (InterruptedException ie) {              }        }    }}public class Demo3 {    public static void main(String[] args) {          final Count count = new Count();        // 新建t1, t1会调用“count对象”的synMethod()方法        Thread t1 = new Thread(                new Runnable() {                    @Override                    public void run() {                        count.synMethod();                    }                }, &quot;t1&quot;);        // 新建t2, t2会调用“count对象”的nonSynMethod()方法        Thread t2 = new Thread(                new Runnable() {                    @Override                    public void run() {                        count.nonSynMethod();                    }                }, &quot;t2&quot;);          t1.start();  // 启动t1        t2.start();  // 启动t2    } }// 结果说明：主线程中新建了两个子线程t1和t2。t1和t2运行时都调用synchronized(this)，这个this是Count对象(count)，而t1和t2共用count。因此，在t1运行时，t2会被阻塞，等待t1运行释放“count对象的同步锁”，t2才能运行。t1 synMethod loop 0t1 synMethod loop 1t1 synMethod loop 2t1 synMethod loop 3t1 synMethod loop 4t2 nonSynMethod loop 0t2 nonSynMethod loop 1t2 nonSynMethod loop 2t2 nonSynMethod loop 3t2 nonSynMethod loop 4synchronized方法 和 synchronized代码块123456789101112// synchronized方法示例public synchronized void foo1() {    System.out.println(&quot;synchronized methoed&quot;);}// synchronized代码块public void foo2() {    synchronized (this) {        System.out.println(&quot;synchronized methoed&quot;);    }}// synchronized代码块中的this是指当前对象。也可以将this替换成其他对象，例如将this替换成obj，则foo2()在执行synchronized(obj)时就获取的是obj的同步锁。// synchronized代码块可以更精确的控制冲突限制访问区域，有时候表现更高效率。实例锁和全局锁1234567891011121314151617实例锁 -- 锁在某一个实例对象上。如果该类是单例，那么该锁也具有全局锁的概念。// 实例锁对应的就是synchronized关键字。全局锁 -- 该锁针对的是类，无论实例多少个对象，那么线程都共享该锁。// 全局锁对应的就是static synchronized（或者是锁在该类的class或者classloader对象上）。               pulbic class Something {    public synchronized void isSyncA(){}    public synchronized void isSyncB(){}    public static synchronized void cSyncA(){}    public static synchronized void cSyncB(){}}假设，Something有两个实例x和y。分析下面4组表达式获取的锁的情况。(01) (不能被同时访问)x.isSyncA()与x.isSyncB() (02) (可以同时被访问)x.isSyncA()与y.isSyncA()(03) (不能被同时访问)x.cSyncA()与y.cSyncB()(04) (可以被同时访问)x.isSyncA()与Something.cSyncA()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263// (01) 不能被同时访问。因为isSyncA()和isSyncB()都是访问同一个对象(对象x)的同步锁！// LockTest1.java的源码class Something {    public synchronized void isSyncA(){        try {              for (int i = 0; i &lt; 5; i++) {                Thread.sleep(100); // 休眠100ms                System.out.println(Thread.currentThread().getName()+&quot; : isSyncA&quot;);            }        }catch (InterruptedException ie) {          }      }    public synchronized void isSyncB(){        try {              for (int i = 0; i &lt; 5; i++) {                Thread.sleep(100); // 休眠100ms                System.out.println(Thread.currentThread().getName()+&quot; : isSyncB&quot;);            }        }catch (InterruptedException ie) {          }      }}public class LockTest1 {    Something x = new Something();    Something y = new Something();    // 比较(01) x.isSyncA()与x.isSyncB()     private void test1() {        // 新建t11, t11会调用 x.isSyncA()        Thread t11 = new Thread(                new Runnable() {                    @Override                    public void run() {                        x.isSyncA();                    }                }, &quot;t11&quot;);        // 新建t12, t12会调用 x.isSyncB()        Thread t12 = new Thread(                new Runnable() {                    @Override                    public void run() {                        x.isSyncB();                    }                }, &quot;t12&quot;);          t11.start();  // 启动t11        t12.start();  // 启动t12    }    public static void main(String[] args) {        LockTest1 demo = new LockTest1();        demo.test1();    }}t11 : isSyncAt11 : isSyncAt11 : isSyncAt11 : isSyncAt11 : isSyncAt12 : isSyncBt12 : isSyncBt12 : isSyncBt12 : isSyncBt12 : isSyncB12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// (02) 可以同时被访问。因为访问的不是同一个对象的同步锁，x.isSyncA()访问的是x的同步锁，而y.isSyncA()访问的是y的同步锁。// LockTest2.java的源码class Something {    public synchronized void isSyncA(){        try {              for (int i = 0; i &lt; 5; i++) {                Thread.sleep(100); // 休眠100ms                System.out.println(Thread.currentThread().getName()+&quot; : isSyncA&quot;);            }        }catch (InterruptedException ie) {          }      }    public synchronized void isSyncB(){        try {              for (int i = 0; i &lt; 5; i++) {                Thread.sleep(100); // 休眠100ms                System.out.println(Thread.currentThread().getName()+&quot; : isSyncB&quot;);            }        }catch (InterruptedException ie) {          }      }    public static synchronized void cSyncA(){        try {              for (int i = 0; i &lt; 5; i++) {                Thread.sleep(100); // 休眠100ms                System.out.println(Thread.currentThread().getName()+&quot; : cSyncA&quot;);            }         }catch (InterruptedException ie) {          }      }    public static synchronized void cSyncB(){        try {              for (int i = 0; i &lt; 5; i++) {                Thread.sleep(100); // 休眠100ms                System.out.println(Thread.currentThread().getName()+&quot; : cSyncB&quot;);            }         }catch (InterruptedException ie) {          }      }}public class LockTest2 {    Something x = new Something();    Something y = new Something();    // 比较(02) x.isSyncA()与y.isSyncA()    private void test2() {        // 新建t21, t21会调用 x.isSyncA()        Thread t21 = new Thread(                new Runnable() {                    @Override                    public void run() {                        x.isSyncA();                    }                }, &quot;t21&quot;);        // 新建t22, t22会调用 x.isSyncB()        Thread t22 = new Thread(                new Runnable() {                    @Override                    public void run() {                        y.isSyncA();                    }                }, &quot;t22&quot;);          t21.start();  // 启动t21        t22.start();  // 启动t22    }    public static void main(String[] args) {        LockTest2 demo = new LockTest2();        demo.test2();    }}t21 : isSyncAt22 : isSyncAt21 : isSyncAt22 : isSyncAt21 : isSyncAt22 : isSyncAt21 : isSyncAt22 : isSyncAt21 : isSyncAt22 : isSyncA12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182// (03) 不能被同时访问。因为cSyncA()和cSyncB()都是static类型，x.cSyncA()相当于Something.isSyncA()，y.cSyncB()相当于Something.isSyncB()，因此它们共用一个同步锁，不能被同时访问。// LockTest3.java的源码class Something {    public synchronized void isSyncA(){        try {              for (int i = 0; i &lt; 5; i++) {                Thread.sleep(100); // 休眠100ms                System.out.println(Thread.currentThread().getName()+&quot; : isSyncA&quot;);            }        }catch (InterruptedException ie) {          }      }    public synchronized void isSyncB(){        try {              for (int i = 0; i &lt; 5; i++) {                Thread.sleep(100); // 休眠100ms                System.out.println(Thread.currentThread().getName()+&quot; : isSyncB&quot;);            }        }catch (InterruptedException ie) {          }      }    public static synchronized void cSyncA(){        try {              for (int i = 0; i &lt; 5; i++) {                Thread.sleep(100); // 休眠100ms                System.out.println(Thread.currentThread().getName()+&quot; : cSyncA&quot;);            }         }catch (InterruptedException ie) {          }      }    public static synchronized void cSyncB(){        try {              for (int i = 0; i &lt; 5; i++) {                Thread.sleep(100); // 休眠100ms                System.out.println(Thread.currentThread().getName()+&quot; : cSyncB&quot;);            }         }catch (InterruptedException ie) {          }      }}public class LockTest3 {    Something x = new Something();    Something y = new Something();    // 比较(03) x.cSyncA()与y.cSyncB()    private void test3() {        // 新建t31, t31会调用 x.isSyncA()        Thread t31 = new Thread(                new Runnable() {                    @Override                    public void run() {                        x.cSyncA();                    }                }, &quot;t31&quot;);        // 新建t32, t32会调用 x.isSyncB()        Thread t32 = new Thread(                new Runnable() {                    @Override                    public void run() {                        y.cSyncB();                    }                }, &quot;t32&quot;);          t31.start();  // 启动t31        t32.start();  // 启动t32    }    public static void main(String[] args) {        LockTest3 demo = new LockTest3();        demo.test3();    }}t31 : cSyncAt31 : cSyncAt31 : cSyncAt31 : cSyncAt31 : cSyncAt32 : cSyncBt32 : cSyncBt32 : cSyncBt32 : cSyncBt32 : cSyncB123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081// (04) 可以被同时访问。因为isSyncA()是实例方法，x.isSyncA()使用的是对象x的锁；而cSyncA()是静态方法，Something.cSyncA()可以理解对使用的是“类的锁”。因此，它们是可以被同时访问的。// LockTest4.java的源码class Something {    public synchronized void isSyncA(){        try {              for (int i = 0; i &lt; 5; i++) {                Thread.sleep(100); // 休眠100ms                System.out.println(Thread.currentThread().getName()+&quot; : isSyncA&quot;);            }        }catch (InterruptedException ie) {          }      }    public synchronized void isSyncB(){        try {              for (int i = 0; i &lt; 5; i++) {                Thread.sleep(100); // 休眠100ms                System.out.println(Thread.currentThread().getName()+&quot; : isSyncB&quot;);            }        }catch (InterruptedException ie) {          }      }    public static synchronized void cSyncA(){        try {              for (int i = 0; i &lt; 5; i++) {                Thread.sleep(100); // 休眠100ms                System.out.println(Thread.currentThread().getName()+&quot; : cSyncA&quot;);            }         }catch (InterruptedException ie) {          }      }    public static synchronized void cSyncB(){        try {              for (int i = 0; i &lt; 5; i++) {                Thread.sleep(100); // 休眠100ms                System.out.println(Thread.currentThread().getName()+&quot; : cSyncB&quot;);            }         }catch (InterruptedException ie) {          }      }}public class LockTest4 {    Something x = new Something();    Something y = new Something();    // 比较(04) x.isSyncA()与Something.cSyncA()    private void test4() {        // 新建t41, t41会调用 x.isSyncA()        Thread t41 = new Thread(                new Runnable() {                    @Override                    public void run() {                        x.isSyncA();                    }                }, &quot;t41&quot;);        // 新建t42, t42会调用 x.isSyncB()        Thread t42 = new Thread(                new Runnable() {                    @Override                    public void run() {                        Something.cSyncA();                    }                }, &quot;t42&quot;);          t41.start();  // 启动t41        t42.start();  // 启动t42    }    public static void main(String[] args) {        LockTest4 demo = new LockTest4();        demo.test4();    }}t41 : isSyncAt42 : cSyncAt41 : isSyncAt42 : cSyncAt41 : isSyncAt42 : cSyncAt41 : isSyncAt42 : cSyncAt41 : isSyncAt42 : cSyncA线程等待与唤醒wait(), notify(), notifyAll()等方法介绍Object类中关于等待/唤醒的API详细信息如下：notify()  — 唤醒在此对象监视器上等待的单个线程。notifyAll()  — 唤醒在此对象监视器上等待的所有线程。wait() — 当前线程处于“等待(阻塞)状态”，“直到其他线程调用此对象的 notify() 方法或 notifyAll() 方法”，当前线程被唤醒(进入“就绪状态”)。wait(long timeout) — 让当前线程处于“等待(阻塞)状态”，“直到其他线程调用此对象的 notify() 方法或 notifyAll() 方法，或者超过指定的时间量”，当前线程被唤醒(进入“就绪状态”)。wait(long timeout, int nanos)  — 让当前线程处于“等待(阻塞)状态”，“直到其他线程调用此对象的 notify() 方法或 notifyAll() 方法，或者其他某个线程中断当前线程，或者已超过某个实际时间量”，当前线程被唤醒(进入“就绪状态”)。wait()和notify()示例123456789101112131415161718192021222324252627282930313233343536373839404142434445// WaitTest.java的源码class ThreadA extends Thread{    public ThreadA(String name) {        super(name);    }    public void run() {        synchronized (this) {            System.out.println(Thread.currentThread().getName()+&quot; call notify()&quot;);            // 唤醒当前的wait线程            notify();        }    }}public class WaitTest {    public static void main(String[] args) {        ThreadA t1 = new ThreadA(&quot;t1&quot;);        synchronized(t1) {            try {                // 启动“线程t1”                System.out.println(Thread.currentThread().getName()+&quot; start t1&quot;);                t1.start();                // 主线程等待t1通过notify()唤醒。                System.out.println(Thread.currentThread().getName()+&quot; wait()&quot;);                t1.wait();                System.out.println(Thread.currentThread().getName()+&quot; continue&quot;);            } catch (InterruptedException e) {                e.printStackTrace();            }        }    }}main start t1main wait()t1 call notify()main continue// 结果说明：如下图，说明了“主线程”和“线程t1”的流程。(01) 注意，图中&quot;主线程&quot; 代表“主线程main”。&quot;线程t1&quot; 代表WaitTest中启动的“线程t1”。 而“锁” 代表“t1这个对象的同步锁”。(02) “主线程”通过 new ThreadA(&quot;t1&quot;) 新建“线程t1”。随后通过synchronized(t1)获取“t1对象的同步锁”。然后调用t1.start()启动“线程t1”。(03) “主线程”执行t1.wait() 释放“t1对象的锁”并且进入“等待(阻塞)状态”。等待t1对象上的线程通过notify() 或 notifyAll()将其唤醒。(04) “线程t1”运行之后，通过synchronized(this)获取“当前对象的锁”；接着调用notify()唤醒“当前对象上的等待线程”，也就是唤醒“主线程”。(05) “线程t1”运行完毕之后，释放“当前对象的锁”。紧接着，“主线程”获取“t1对象的锁”，然后接着运行。jdk文档中关于wait的一段介绍：1234567Causes the current thread to wait until another thread invokes the notify() method or the notifyAll() method for this object. In other words, this method behaves exactly as if it simply performs the call wait(0).The current thread must own this object's monitor. The thread releases ownership of this monitor and waits until another thread notifies threads waiting on this object's monitor to wake up either through a call to the notify method or the notifyAll method. The thread then waits until it can re-obtain ownership of the monitor and resumes execution.引起“当前线程”等待，直到另外一个线程调用notify()或notifyAll()唤醒该线程。换句话说，这个方法和wait(0)的效果一样！(补充，对于wait(long millis)方法，当millis为0时，表示无限等待，直到被notify()或notifyAll()唤醒)。“当前线程”在调用wait()时，必须拥有该对象的同步锁。该线程调用wait()之后，会释放该锁；然后一直等待直到“其它线程”调用对象的同步锁的notify()或notifyAll()方法。然后，该线程继续等待直到它重新获取“该对象的同步锁”，然后就可以接着运行。注意：jdk的解释中，说wait()的作用是让“当前线程”等待，而“当前线程”是指正在cpu上运行的线程！这也意味着，虽然t1.wait()是通过“线程t1”调用的wait()方法，但是调用t1.wait()的地方是在“主线程main”中。而主线程必须是“当前线程”，也就是运行状态，才可以执行t1.wait()。所以，此时的“当前线程”是“主线程main”！因此，t1.wait()是让“主线程”等待，而不是“线程t1”！wait(long timeout)和notify()1234567891011121314151617181920212223242526272829303132333435363738394041424344wait(long timeout)会让当前线程处于“等待(阻塞)状态”，“直到其他线程调用此对象的 notify() 方法或 notifyAll() 方法，或者超过指定的时间量”，当前线程被唤醒(进入“就绪状态”)。// WaitTimeoutTest.java的源码class ThreadA extends Thread{    public ThreadA(String name) {        super(name);    }    public void run() {        System.out.println(Thread.currentThread().getName() + &quot; run &quot;);        // 死循环，不断运行。        while(true)            ;    }}public class WaitTimeoutTest {    public static void main(String[] args) {        ThreadA t1 = new ThreadA(&quot;t1&quot;);        synchronized(t1) {            try {                // 启动“线程t1”                System.out.println(Thread.currentThread().getName() + &quot; start t1&quot;);                t1.start();                // 主线程等待t1通过notify()唤醒 或 notifyAll()唤醒，或超过3000ms延时；然后才被唤醒。                System.out.println(Thread.currentThread().getName() + &quot; call wait &quot;);                t1.wait(3000);                System.out.println(Thread.currentThread().getName() + &quot; continue&quot;);            } catch (InterruptedException e) {                e.printStackTrace();            }        }    }}main start t1main call wait t1 run                  // 大约3秒之后...输出“main continue”main continue    // 结果说明：如下图，说明了“主线程”和“线程t1”的流程。(01) 注意，图中&quot;主线程&quot; 代表WaitTimeoutTest主线程(即，线程main)。&quot;线程t1&quot; 代表WaitTest中启动的线程t1。 而“锁” 代表“t1这个对象的同步锁”。(02) 主线程main执行t1.start()启动“线程t1”。(03) 主线程main执行t1.wait(3000)，此时，主线程进入“阻塞状态”。需要“用于t1对象锁的线程通过notify() 或者 notifyAll()将其唤醒” 或者 “超时3000ms之后”，主线程main才进入到“就绪状态”，然后才可以运行。(04) “线程t1”运行之后，进入了死循环，一直不断的运行。(05) 超时3000ms之后，主线程main会进入到“就绪状态”，然后接着进入“运行状态”。wait() 和 notifyAll()1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class NotifyAllTest {    private static Object obj = new Object();    public static void main(String[] args) {        ThreadA t1 = new ThreadA(&quot;t1&quot;);        ThreadA t2 = new ThreadA(&quot;t2&quot;);        ThreadA t3 = new ThreadA(&quot;t3&quot;);        t1.start();        t2.start();        t3.start();        try {            System.out.println(Thread.currentThread().getName()+&quot; sleep(3000)&quot;);            Thread.sleep(3000);        } catch (InterruptedException e) {            e.printStackTrace();        }        synchronized(obj) {            // 主线程等待唤醒。            System.out.println(Thread.currentThread().getName()+&quot; notifyAll()&quot;);            obj.notifyAll();        }    }    static class ThreadA extends Thread{        public ThreadA(String name){            super(name);        }        public void run() {            synchronized (obj) {                try {                    // 打印输出结果                    System.out.println(Thread.currentThread().getName() + &quot; wait&quot;);                    // 唤醒当前的wait线程                    obj.wait();                    // 打印输出结果                    System.out.println(Thread.currentThread().getName() + &quot; continue&quot;);                } catch (InterruptedException e) {                    e.printStackTrace();                }            }        }    }}t1 waitmain sleep(3000)t3 waitt2 waitmain notifyAll()t2 continuet3 continuet1 continue// 结果说明：参考下面的流程图。 (01) 主线程中新建并且启动了3个线程&quot;t1&quot;, &quot;t2&quot;和&quot;t3&quot;。(02) 主线程通过sleep(3000)休眠3秒。在主线程休眠3秒的过程中，我们假设&quot;t1&quot;, &quot;t2&quot;和&quot;t3&quot;这3个线程都运行了。以&quot;t1&quot;为例，当它运行的时候，它会执行obj.wait()等待其它线程通过notify()或额nofityAll()来唤醒它；相同的道理，&quot;t2&quot;和&quot;t3&quot;也会等待其它线程通过nofity()或nofityAll()来唤醒它们。(03) 主线程休眠3秒之后，接着运行。执行 obj.notifyAll() 唤醒obj上的等待线程，即唤醒&quot;t1&quot;, &quot;t2&quot;和&quot;t3&quot;这3个线程。 紧接着，主线程的synchronized(obj)运行完毕之后，主线程释放“obj锁”。这样，&quot;t1&quot;, &quot;t2&quot;和&quot;t3&quot;就可以获取“obj锁”而继续运行了！Object中的wait(), notify()等函数，和synchronized一样，会对“对象的同步锁”进行操作。wait()会使“当前线程”等待，因为线程进入等待状态，所以线程应该释放它锁持有的“同步锁”，否则其它线程获取不到该“同步锁”而无法运行！OK，线程调用wait()之后，会释放它锁持有的“同步锁”；而且，根据前面的介绍，我们知道：等待线程可以被notify()或notifyAll()唤醒。现在，请思考一个问题：notify()是依据什么唤醒等待线程的？或者说，wait()等待线程和notify()之间是通过什么关联起来的？答案是：依据“对象的同步锁”。负责唤醒等待线程的那个线程(我们称为“唤醒线程”)，它只有在获取“该对象的同步锁”(这里的同步锁必须和等待线程的同步锁是同一个)，并且调用notify()或notifyAll()方法之后，才能唤醒等待线程。虽然，等待线程被唤醒；但是，它不能立刻执行，因为唤醒线程还持有“该对象的同步锁”。必须等到唤醒线程释放了“对象的同步锁”之后，等待线程才能获取到“对象的同步锁”进而继续运行。总之，notify(), wait()依赖于“同步锁”，而“同步锁”是对象锁持有，并且每个对象有且仅有一个！这就是为什么notify(), wait()等函数定义在Object类，而不是Thread类中的原因。线程让步yield()yield()的作用是让步。它能让当前线程由“运行状态”进入到“就绪状态”，从而让其它具有相同优先级的等待线程获取执行权；但是，并不能保证在当前线程调用yield()之后，其它具有相同优先级的线程就一定能获得执行权；也有可能是当前线程又进入到“运行状态”继续运行！12345678910111213141516171819202122232425262728293031323334353637383940414243444546// YieldTest.java的源码class ThreadA extends Thread{    public ThreadA(String name){         super(name);     }     public synchronized void run(){         for(int i=0; i &lt;10; i++){             System.out.printf(&quot;%s [%d]:%d\n&quot;, this.getName(), this.getPriority(), i);             // i整除4时，调用yield            if (i%4 == 0)                Thread.yield();        }     } } public class YieldTest{     public static void main(String[] args){         ThreadA t1 = new ThreadA(&quot;t1&quot;);         ThreadA t2 = new ThreadA(&quot;t2&quot;);         t1.start();         t2.start();    } }// 结果说明：“线程t1”在能被4整数的时候，并没有切换到“线程t2”。这表明，yield()虽然可以让线程由“运行状态”进入到“就绪状态”；但是，它不一定会让其它线程获取CPU执行权(即，其它线程进入到“运行状态”)，即使这个“其它线程”与当前调用yield()的线程具有相同的优先级。t1 [5]:0t2 [5]:0t1 [5]:1t1 [5]:2t1 [5]:3t1 [5]:4t1 [5]:5t1 [5]:6t1 [5]:7t1 [5]:8t1 [5]:9t2 [5]:1t2 [5]:2t2 [5]:3t2 [5]:4t2 [5]:5t2 [5]:6t2 [5]:7t2 [5]:8t2 [5]:9yield() 与 wait()的比较wait()的作用是让当前线程由“运行状态”进入“等待(阻塞)状态”的同时，也会释放同步锁。而yield()的作用是让步，它也会让当前线程离开“运行状态”。它们的区别是：(01) wait()是让线程由“运行状态”进入到“等待(阻塞)状态”，而不yield()是让线程由“运行状态”进入到“就绪状态”。(02) wait()是会线程释放它所持有对象的同步锁，而yield()方法不会释放锁。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 下面通过示例演示yield()是不会释放锁的。// YieldLockTest.java 的源码public class YieldLockTest{     private static Object obj = new Object();    public static void main(String[] args){         ThreadA t1 = new ThreadA(&quot;t1&quot;);         ThreadA t2 = new ThreadA(&quot;t2&quot;);         t1.start();         t2.start();    }     static class ThreadA extends Thread{        public ThreadA(String name){             super(name);         }         public void run(){             // 获取obj对象的同步锁            synchronized (obj) {                for(int i=0; i &lt;10; i++){                     System.out.printf(&quot;%s [%d]:%d\n&quot;, this.getName(), this.getPriority(), i);                     // i整除4时，调用yield                    if (i%4 == 0)                        Thread.yield();                }            }        }     } }// 结果说明：主线程main中启动了两个线程t1和t2。t1和t2在run()会引用同一个对象的同步锁，即synchronized(obj)。在t1运行过程中，虽然它会调用Thread.yield()；但是，t2是不会获取cpu执行权的。因为，t1并没有释放“obj所持有的同步锁”！t1 [5]:0t1 [5]:1t1 [5]:2t1 [5]:3t1 [5]:4t1 [5]:5t1 [5]:6t1 [5]:7t1 [5]:8t1 [5]:9t2 [5]:0t2 [5]:1t2 [5]:2t2 [5]:3t2 [5]:4t2 [5]:5t2 [5]:6t2 [5]:7t2 [5]:8t2 [5]:9线程休眠sleep()sleep() 定义在Thread.java中。sleep() 的作用是让当前线程休眠，即当前线程会从“运行状态”进入到“休眠(阻塞)状态”。sleep()会指定休眠时间，线程休眠的时间会大于/等于该休眠时间；在线程重新被唤醒时，它会由“阻塞状态”变成“就绪状态”，从而等待cpu的调度执行。1234567891011121314151617181920212223242526272829303132333435363738// SleepTest.java的源码class ThreadA extends Thread{    public ThreadA(String name){         super(name);     }     public synchronized void run() {         try {            for(int i=0; i &lt;10; i++){                 System.out.printf(&quot;%s: %d\n&quot;, this.getName(), i);                 // i能被4整除时，休眠100毫秒                if (i%4 == 0)                    Thread.sleep(100);            }         } catch (InterruptedException e) {            e.printStackTrace();        }    } } public class SleepTest{     public static void main(String[] args){         ThreadA t1 = new ThreadA(&quot;t1&quot;);         t1.start();     } }// 结果说明：程序比较简单，在主线程main中启动线程t1。t1启动之后，当t1中的计算i能被4整除时，t1会通过Thread.sleep(100)休眠100毫秒。t1: 0t1: 1t1: 2t1: 3t1: 4t1: 5t1: 6t1: 7t1: 8t1: 9sleep() 与 wait()的比较wait()的作用是让当前线程由“运行状态”进入“等待(阻塞)状态”的同时，也会释放同步锁。而sleep()的作用是也是让当前线程由“运行状态”进入到“休眠(阻塞)状态”。但是，wait()会释放对象的同步锁，而sleep()则不会释放锁。12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455// SleepLockTest.java的源码public class SleepLockTest{     private static Object obj = new Object();    public static void main(String[] args){         ThreadA t1 = new ThreadA(&quot;t1&quot;);         ThreadA t2 = new ThreadA(&quot;t2&quot;);         t1.start();         t2.start();    }     static class ThreadA extends Thread{        public ThreadA(String name){             super(name);         }         public void run(){             // 获取obj对象的同步锁            synchronized (obj) {                try {                    for(int i=0; i &lt;10; i++){                         System.out.printf(&quot;%s: %d\n&quot;, this.getName(), i);                         // i能被4整除时，休眠100毫秒                        if (i%4 == 0)                            Thread.sleep(100);                    }                } catch (InterruptedException e) {                    e.printStackTrace();                }            }        }     } }// 结果说明：主线程main中启动了两个线程t1和t2。t1和t2在run()会引用同一个对象的同步锁，即synchronized(obj)。在t1运行过程中，虽然它会调用Thread.sleep(100)；但是，t2是不会获取cpu执行权的。因为，t1并没有释放“obj所持有的同步锁”！注意，若我们注释掉synchronized (obj)后再次执行该程序，t1和t2是可以相互切换的。t1: 0t1: 1t1: 2t1: 3t1: 4t1: 5t1: 6t1: 7t1: 8t1: 9t2: 0t2: 1t2: 2t2: 3t2: 4t2: 5t2: 6t2: 7t2: 8t2: 9join() 方法join() 的作用：让“主线程”等待“子线程”结束之后才能继续运行。1234567891011121314151617181920212223// 主线程public class Father extends Thread {    public void run() {        Son s = new Son();        s.start();        s.join();        ...    }}// 子线程public class Son extends Thread {    public void run() {        ...    }}// 说明：上面的有两个类Father(主线程类)和Son(子线程类)。因为Son是在Father中创建并启动的，所以，Father是主线程类，Son是子线程类。在Father主线程中，通过new Son()新建“子线程s”。接着通过s.start()启动“子线程s”，并且调用s.join()。在调用s.join()之后，Father主线程会一直等待，直到“子线程s”运行完毕；在“子线程s”运行完毕之后，Father主线程才能接着运行。 这也就是我们所说的“join()的作用，是让主线程会等待子线程结束之后才能继续运行”！// 问题：虽然s.join()被调用的地方是发生在“Father主线程”中，但是s.join()是通过“子线程s”去调用的join()。那么，join()方法中的isAlive()应该是判断“子线程s”是不是Alive状态；对应的wait(0)也应该是“让子线程s”等待才对。但如果是这样的话，s.join()的作用怎么可能是“让主线程等待，直到子线程s完成为止”呢，应该是让&quot;子线程等待才对(因为调用子线程对象s的wait方法嘛)&quot;？答案：wait()的作用是让“当前线程”等待，而这里的“当前线程”是指当前在CPU上运行的线程。所以，虽然是调用子线程的wait()方法，但是它是通过“主线程”去调用的；所以，休眠的是主线程，而不是“子线程”！12345678910111213141516171819202122232425262728293031323334// JoinTest.java的源码public class JoinTest{     public static void main(String[] args){         try {            ThreadA t1 = new ThreadA(&quot;t1&quot;); // 新建“线程t1”            t1.start();                     // 启动“线程t1”            t1.join();                        // 将“线程t1”加入到“主线程main”中，并且“主线程main()会等待它的完成”            System.out.printf(&quot;%s finish\n&quot;, Thread.currentThread().getName());         } catch (InterruptedException e) {            e.printStackTrace();        }    }     static class ThreadA extends Thread{        public ThreadA(String name){             super(name);         }         public void run(){             System.out.printf(&quot;%s start\n&quot;, this.getName());             // 延时操作            for(int i=0; i &lt;1000000; i++) ;            System.out.printf(&quot;%s finish\n&quot;, this.getName());         }     } }// 结果说明：运行流程如图 (01) 在“主线程main”中通过 new ThreadA(&quot;t1&quot;) 新建“线程t1”。 接着，通过 t1.start() 启动“线程t1”，并执行t1.join()。(02) 执行t1.join()之后，“主线程main”会进入“阻塞状态”等待t1运行结束。“子线程t1”结束之后，会唤醒“主线程main”，“主线程”重新获取cpu执行权，继续运行。t1 startt1 finishmain finishinterrupt()和线程终止方式123456789101112131415// 关于interrupt()，java的djk文档描述如下：http://docs.oracle.com/javase/7/docs/api/Interrupts this thread.Unless the current thread is interrupting itself, which is always permitted, the checkAccess method of this thread is invoked, which may cause a SecurityException to be thrown.If this thread is blocked in an invocation of the wait(), wait(long), or wait(long, int) methods of the Object class, or of the join(), join(long), join(long, int), sleep(long), or sleep(long, int), methods of this class, then its interrupt status will be cleared and it will receive an InterruptedException.If this thread is blocked in an I/O operation upon an interruptible channel then the channel will be closed, the thread's interrupt status will be set, and the thread will receive a ClosedByInterruptException.If this thread is blocked in a Selector then the thread's interrupt status will be set and it will return immediately from the selection operation, possibly with a non-zero value, just as if the selector's wakeup method were invoked.If none of the previous conditions hold then this thread's interrupt status will be set.Interrupting a thread that is not alive need not have any effect.interrupt()的作用是中断本线程。本线程中断自己是被允许的；其它线程调用本线程的interrupt()方法时，会通过checkAccess()检查权限。这有可能抛出SecurityException异常。如果本线程是处于阻塞状态：调用线程的wait(), wait(long)或wait(long, int)会让它进入等待(阻塞)状态，或者调用线程的join(), join(long), join(long, int), sleep(long), sleep(long, int)也会让它进入阻塞状态。若线程在阻塞状态时，调用了它的interrupt()方法，那么它的“中断状态”会被清除并且会收到一个InterruptedException异常。例如，线程通过wait()进入阻塞状态，此时通过interrupt()中断该线程；调用interrupt()会立即将线程的中断标记设为“true”，但是由于线程处于阻塞状态，所以该“中断标记”会立即被清除为“false”，同时，会产生一个InterruptedException的异常。如果线程被阻塞在一个Selector选择器中，那么通过interrupt()中断它时；线程的中断标记会被设置为true，并且它会立即从选择操作中返回。如果不属于前面所说的情况，那么通过interrupt()中断线程时，它的中断标记会被设置为“true”。中断一个“已终止的线程”不会产生任何操作。终止线程的方式Thread中的stop()和suspend()方法，由于固有的不安全性，已经建议不再使用！终止处于“阻塞状态”的线程12345678910111213141516通常，我们通过“中断”方式终止处于“阻塞状态”的线程。当线程由于被调用了sleep(), wait(), join()等方法而进入阻塞状态；若此时调用线程的interrupt()将线程的中断标记设为true。由于处于阻塞状态，中断标记会被清除，同时产生一个InterruptedException异常。将InterruptedException放在适当的为止就能终止线程，形式如下：@Overridepublic void run() {    try {        while (true) {            // 执行任务...        }    } catch (InterruptedException ie) {          // 由于产生InterruptedException异常，退出while(true)循环，线程终止！    }}说明：在while(true)中不断的执行任务，当线程处于阻塞状态时，调用线程的interrupt()产生InterruptedException中断。中断的捕获在while(true)之外，这样就退出了while(true)循环！注意：对InterruptedException的捕获务一般放在while(true)循环体的外面，这样，在产生异常时就退出了while(true)循环。否则，InterruptedException在while(true)循环体之内，就需要额外的添加退出处理。终止处于“运行状态”的线程1234567891011121314151617181920212223242526272829303132333435363738通常，我们通过“标记”方式终止处于“运行状态”的线程。其中，包括“中断标记”和“额外添加标记”。// (01) 通过“中断标记”终止线程。@Overridepublic void run() {    while (!isInterrupted()) {        // 执行任务...    }}说明：isInterrupted()是判断线程的中断标记是不是为true。当线程处于运行状态，并且我们需要终止它时；可以调用线程的interrupt()方法，使用线程的中断标记为true，即isInterrupted()会返回true。此时，就会退出while循环。注意：interrupt()并不会终止处于“运行状态”的线程！它会将线程的中断标记设为true。// (02) 通过“额外添加标记”。private volatile boolean flag= true;protected void stopTask() {    flag = false;}@Overridepublic void run() {    while (flag) {        // 执行任务...    }}说明：线程中有一个flag标记，它的默认值是true；并且我们提供stopTask()来设置flag标记。当我们需要终止该线程时，调用该线程的stopTask()方法就可以让线程退出while循环。注意：将flag定义为volatile类型，是为了保证flag的可见性。即其它线程通过stopTask()修改了flag之后，本线程能看到修改后的flag的值。// 综合线程处于“阻塞状态”和“运行状态”的终止方式，比较通用的终止线程的形式如下：@Overridepublic void run() {    try {        // 1. isInterrupted()保证，只要中断标记为true就终止线程。        while (!isInterrupted()) {            // 执行任务...        }    } catch (InterruptedException ie) {          // 2. InterruptedException异常保证，当InterruptedException异常产生时，线程被终止。    }}终止线程的示例1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 终止线程的示例// interrupt()常常被用来终止“阻塞状态”线程。// Demo1.java的源码class MyThread extends Thread {    public MyThread(String name) {        super(name);    }    @Override    public void run() {        try {              int i=0;            while (!isInterrupted()) {                Thread.sleep(100); // 休眠100ms                i++;                System.out.println(Thread.currentThread().getName()+&quot; (&quot;+this.getState()+&quot;) loop &quot; + i);              }        } catch (InterruptedException e) {              System.out.println(Thread.currentThread().getName() +&quot; (&quot;+this.getState()+&quot;) catch InterruptedException.&quot;);          }    }}public class Demo1 {    public static void main(String[] args) {          try {              Thread t1 = new MyThread(&quot;t1&quot;);  // 新建“线程t1”            System.out.println(t1.getName() +&quot; (&quot;+t1.getState()+&quot;) is new.&quot;);              t1.start();                      // 启动“线程t1”            System.out.println(t1.getName() +&quot; (&quot;+t1.getState()+&quot;) is started.&quot;);              // 主线程休眠300ms，然后主线程给t1发“中断”指令。            Thread.sleep(300);            t1.interrupt();            System.out.println(t1.getName() +&quot; (&quot;+t1.getState()+&quot;) is interrupted.&quot;);            // 主线程休眠300ms，然后查看t1的状态。            Thread.sleep(300);            System.out.println(t1.getName() +&quot; (&quot;+t1.getState()+&quot;) is interrupted now.&quot;);        } catch (InterruptedException e) {              e.printStackTrace();        }    } }// 结果说明：(01) 主线程main中通过new MyThread(&quot;t1&quot;)创建线程t1，之后通过t1.start()启动线程t1。(02) t1启动之后，会不断的检查它的中断标记，如果中断标记为“false”；则休眠100ms。(03) t1休眠之后，会切换到主线程main；主线程再次运行时，会执行t1.interrupt()中断线程t1。t1收到中断指令之后，会将t1的中断标记设置“false”，而且会抛出InterruptedException异常。在t1的run()方法中，是在循环体while之外捕获的异常；因此循环被终止。t1 (NEW) is new.t1 (RUNNABLE) is started.t1 (RUNNABLE) loop 1t1 (RUNNABLE) loop 2t1 (TIMED_WAITING) is interrupted.t1 (RUNNABLE) catch InterruptedException.t1 (TERMINATED) is interrupted now.1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253// 通过“额外添加标记”的方式终止“状态状态”的线程的示例// Demo3.java的源码class MyThread extends Thread {    private volatile boolean flag= true;    public void stopTask() {        flag = false;    }    public MyThread(String name) {        super(name);    }    @Override    public void run() {        synchronized(this) {            try {                int i=0;                while (flag) {                    Thread.sleep(100); // 休眠100ms                    i++;                    System.out.println(Thread.currentThread().getName()+&quot; (&quot;+this.getState()+&quot;) loop &quot; + i);                  }            } catch (InterruptedException ie) {                  System.out.println(Thread.currentThread().getName() +&quot; (&quot;+this.getState()+&quot;) catch InterruptedException.&quot;);              }        }      }}public class Demo3 {    public static void main(String[] args) {          try {              MyThread t1 = new MyThread(&quot;t1&quot;);  // 新建“线程t1”            System.out.println(t1.getName() +&quot; (&quot;+t1.getState()+&quot;) is new.&quot;);              t1.start();                      // 启动“线程t1”            System.out.println(t1.getName() +&quot; (&quot;+t1.getState()+&quot;) is started.&quot;);              // 主线程休眠300ms，然后主线程给t1发“中断”指令。            Thread.sleep(300);            t1.stopTask();            System.out.println(t1.getName() +&quot; (&quot;+t1.getState()+&quot;) is interrupted.&quot;);            // 主线程休眠300ms，然后查看t1的状态。            Thread.sleep(300);            System.out.println(t1.getName() +&quot; (&quot;+t1.getState()+&quot;) is interrupted now.&quot;);        } catch (InterruptedException e) {              e.printStackTrace();        }    } }t1 (NEW) is new.t1 (RUNNABLE) is started.t1 (RUNNABLE) loop 1t1 (RUNNABLE) loop 2t1 (TIMED_WAITING) is interrupted.t1 (RUNNABLE) loop 3t1 (TERMINATED) is interrupted now.interrupted() 和 isInterrupted()都能够用于检测对象的“中断标记”。区别是，interrupted()除了返回中断标记之外，它还会清除中断标记(即将中断标记设为false)；而isInterrupted()仅仅返回中断标记。线程优先级和守护线程java 中的线程优先级的范围是1～10，默认的优先级是5。“高优先级线程”会优先于“低优先级线程”执行。java 中有两种线程：用户线程和守护线程。可以通过isDaemon()方法来区别它们：如果返回false，则说明该线程是“用户线程”；否则就是“守护线程”。用户线程一般用户执行用户级任务，而守护线程也就是“后台线程”，一般用来执行后台任务。需要注意的是：Java虚拟机在“用户线程”都结束后会后退出。JDK 中关于线程优先级和守护线程的介绍如下：123456789101112131415Every thread has a priority. Threads with higher priority are executed in preference to threads with lower priority. Each thread may or may not also be marked as a daemon. When code running in some thread creates a new Thread object, the new thread has its priority initially set equal to the priority of the creating thread, and is a daemon thread if and only if the creating thread is a daemon.When a Java Virtual Machine starts up, there is usually a single non-daemon thread (which typically calls the method named main of some designated class). The Java Virtual Machine continues to execute threads until either of the following occurs:The exit method of class Runtime has been called and the security manager has permitted the exit operation to take place.All threads that are not daemon threads have died, either by returning from the call to the run method or by throwing an exception that propagates beyond the run method. Marks this thread as either a daemon thread or a user thread. The Java Virtual Machine exits when the only threads running are all daemon threads.每个线程都有一个优先级。“高优先级线程”会优先于“低优先级线程”执行。每个线程都可以被标记为一个守护进程或非守护进程。在一些运行的主线程中创建新的子线程时，子线程的优先级被设置为等于“创建它的主线程的优先级”，当且仅当“创建它的主线程是守护线程”时“子线程才会是守护线程”。当Java虚拟机启动时，通常有一个单一的非守护线程（该线程通过是通过main()方法启动）。JVM会一直运行直到下面的任意一个条件发生，JVM就会终止运行：(01) 调用了exit()方法，并且exit()有权限被正常执行。(02) 所有的“非守护线程”都死了(即JVM中仅仅只有“守护线程”)。每一个线程都被标记为“守护线程”或“用户线程”。当只有守护线程运行时，JVM会自动退出。12345678910111213141516171819202122232425262728293031323334353637383940class MyThread extends Thread{      public MyThread(String name) {        super(name);    }    public void run(){        for (int i=0; i&lt;5; i++) {            System.out.println(Thread.currentThread().getName()                    +&quot;(&quot;+Thread.currentThread().getPriority()+ &quot;)&quot;                    +&quot;, loop &quot;+i);        }    } }public class Demo {      public static void main(String[] args) {          System.out.println(Thread.currentThread().getName()                +&quot;(&quot;+Thread.currentThread().getPriority()+ &quot;)&quot;);        Thread t1=new MyThread(&quot;t1&quot;);    // 新建t1        Thread t2=new MyThread(&quot;t2&quot;);    // 新建t2        t1.setPriority(1);                // 设置t1的优先级为1        t2.setPriority(10);                // 设置t2的优先级为10        t1.start();                        // 启动t1        t2.start();                        // 启动t2    }  }// 结果说明：(01) 主线程main的优先级是5。(02) t1的优先级被设为1，而t2的优先级被设为10。cpu在执行t1和t2的时候，根据时间片轮循调度，所以能够并发执行。main(5)t1(1), loop 0t2(10), loop 0t1(1), loop 1t2(10), loop 1t1(1), loop 2t2(10), loop 2t1(1), loop 3t2(10), loop 3t1(1), loop 4t2(10), loop 41234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465// 守护线程的示例// Demo.javaclass MyThread extends Thread{      public MyThread(String name) {        super(name);    }    public void run(){        try {            for (int i=0; i&lt;5; i++) {                Thread.sleep(3);                System.out.println(this.getName() +&quot;(isDaemon=&quot;+this.isDaemon()+ &quot;)&quot; +&quot;, loop &quot;+i);            }        } catch (InterruptedException e) {        }    } }class MyDaemon extends Thread{      public MyDaemon(String name) {        super(name);    }    public void run(){        try {            for (int i=0; i&lt;10000; i++) {                Thread.sleep(1);                System.out.println(this.getName() +&quot;(isDaemon=&quot;+this.isDaemon()+ &quot;)&quot; +&quot;, loop &quot;+i);            }        } catch (InterruptedException e) {        }    } }public class Demo {      public static void main(String[] args) {          System.out.println(Thread.currentThread().getName()                +&quot;(isDaemon=&quot;+Thread.currentThread().isDaemon()+ &quot;)&quot;);        Thread t1=new MyThread(&quot;t1&quot;);    // 新建t1        Thread t2=new MyDaemon(&quot;t2&quot;);    // 新建t2        t2.setDaemon(true);                // 设置t2为守护线程        t1.start();                        // 启动t1        t2.start();                        // 启动t2    }  }// 结果说明：(01) 主线程main是用户线程，它创建的子线程t1也是用户线程。(02) t2是守护线程。在“主线程main”和“子线程t1”(它们都是用户线程)执行完毕，只剩t2这个守护线程的时候，JVM自动退出。main(isDaemon=false)t2(isDaemon=true), loop 0t2(isDaemon=true), loop 1t1(isDaemon=false), loop 0t2(isDaemon=true), loop 2t2(isDaemon=true), loop 3t1(isDaemon=false), loop 1t2(isDaemon=true), loop 4t2(isDaemon=true), loop 5t2(isDaemon=true), loop 6t1(isDaemon=false), loop 2t2(isDaemon=true), loop 7t2(isDaemon=true), loop 8t2(isDaemon=true), loop 9t1(isDaemon=false), loop 3t2(isDaemon=true), loop 10t2(isDaemon=true), loop 11t1(isDaemon=false), loop 4t2(isDaemon=true), loop 12生产消费者问题生产/消费者问题是个非常典型的多线程问题，涉及到的对象包括“生产者”、“消费者”、“仓库”和“产品”。他们之间的关系如下：(01) 生产者仅仅在仓储未满时候生产，仓满则停止生产。(02) 消费者仅仅在仓储有产品时候才能消费，仓空则等待。(03) 当消费者发现仓储没产品可消费时候会通知生产者生产。(04) 生产者在生产出可消费产品时候，应该通知等待的消费者去消费。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120// 通过wait()/notify()方式实现该模型(后面在学习了线程池相关内容之后，再通过其它方式实现生产/消费者模型)。// Demo1.java// 仓库class Depot {    private int capacity;    // 仓库的容量    private int size;        // 仓库的实际数量    public Depot(int capacity) {        this.capacity = capacity;        this.size = 0;    }    public synchronized void produce(int val) {        try {             // left 表示“想要生产的数量”(有可能生产量太多，需多此生产)            int left = val;            while (left &gt; 0) {                // 库存已满时，等待“消费者”消费产品。                while (size &gt;= capacity)                    wait();                // 获取“实际生产的数量”(即库存中新增的数量)                // 如果“库存”+“想要生产的数量”&gt;“总的容量”，则“实际增量”=“总的容量”-“当前容量”。(此时填满仓库)                // 否则“实际增量”=“想要生产的数量”                int inc = (size+left)&gt;capacity ? (capacity-size) : left;                size += inc;                left -= inc;                System.out.printf(&quot;%s produce(=) --&gt; left==, inc==, size==\n&quot;,                         Thread.currentThread().getName(), val, left, inc, size);                // 通知“消费者”可以消费了。                notifyAll();            }        } catch (InterruptedException e) {        }    }     public synchronized void consume(int val) {        try {            // left 表示“客户要消费数量”(有可能消费量太大，库存不够，需多此消费)            int left = val;            while (left &gt; 0) {                // 库存为0时，等待“生产者”生产产品。                while (size &lt;= 0)                    wait();                // 获取“实际消费的数量”(即库存中实际减少的数量)                // 如果“库存”&lt;“客户要消费的数量”，则“实际消费量”=“库存”；                // 否则，“实际消费量”=“客户要消费的数量”。                int dec = (size&lt;left) ? size : left;                size -= dec;                left -= dec;                System.out.printf(&quot;%s consume(=) &lt;-- left==, dec==, size==\n&quot;,                         Thread.currentThread().getName(), val, left, dec, size);                notifyAll();            }        } catch (InterruptedException e) {        }    }    public String toString() {        return &quot;capacity:&quot;+capacity+&quot;, actual size:&quot;+size;    }} // 生产者class Producer {    private Depot depot;    public Producer(Depot depot) {        this.depot = depot;    }    // 消费产品：新建一个线程向仓库中生产产品。    public void produce(final int val) {        new Thread() {            public void run() {                depot.produce(val);            }        }.start();    }}// 消费者class Customer {    private Depot depot;    public Customer(Depot depot) {        this.depot = depot;    }    // 消费产品：新建一个线程从仓库中消费产品。    public void consume(final int val) {        new Thread() {            public void run() {                depot.consume(val);            }        }.start();    }}public class Demo1 {      public static void main(String[] args) {          Depot mDepot = new Depot(100);        Producer mPro = new Producer(mDepot);        Customer mCus = new Customer(mDepot);        mPro.produce(60);        mPro.produce(120);        mCus.consume(90);        mCus.consume(150);        mPro.produce(110);    }}// 说明：(01) Producer是“生产者”类，它与“仓库(depot)”关联。当调用“生产者”的produce()方法时，它会新建一个线程并向“仓库”中生产产品。(02) Customer是“消费者”类，它与“仓库(depot)”关联。当调用“消费者”的consume()方法时，它会新建一个线程并消费“仓库”中的产品。(03) Depot是“仓库”类，仓库中记录“仓库的容量(capacity)”以及“仓库中当前产品数目(size)”。“仓库”类的生产方法produce()和消费方法consume()方法都是synchronized方法，进入synchronized方法体，意味着这个线程获取到了该“仓库”对象的同步锁。这也就是说，同一时间，生产者和消费者线程只能有一个能运行。通过同步锁，实现了对“残酷”的互斥访问。对于生产方法produce()而言：当仓库满时，生产者线程等待，需要等待消费者消费产品之后，生产线程才能生产；生产者线程生产完产品之后，会通过notifyAll()唤醒同步锁上的所有线程，包括“消费者线程”，即我们所说的“通知消费者进行消费”。对于消费方法consume()而言：当仓库为空时，消费者线程等待，需要等待生产者生产产品之后，消费者线程才能消费；消费者线程消费完产品之后，会通过notifyAll()唤醒同步锁上的所有线程，包括“生产者线程”，即我们所说的“通知生产者进行生产”。      Thread-0 produce( 60) --&gt; left=  0, inc= 60, size= 60Thread-4 produce(110) --&gt; left= 70, inc= 40, size=100Thread-2 consume( 90) &lt;-- left=  0, dec= 90, size= 10Thread-3 consume(150) &lt;-- left=140, dec= 10, size=  0Thread-1 produce(120) --&gt; left= 20, inc=100, size=100Thread-3 consume(150) &lt;-- left= 40, dec=100, size=  0Thread-4 produce(110) --&gt; left=  0, inc= 70, size= 70Thread-3 consume(150) &lt;-- left=  0, dec= 40, size= 30Thread-1 produce(120) --&gt; left=  0, inc= 20, size= 50</content>
	</entry>
	<entry>
		<title>设计模式</title>
		<url>/article/364ea8cc.html</url>
		<content>设计模式的概念设计模式(Design pattern)代表了最佳的实践，通常被有经验的面向对象的软件开发人员所采用设计模式是软件开发人员在软件开发过程中面临的一般问题的解决方案。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的设计模式（Design Pattern）是一套被反复使用、多数人知晓的、经过分类的、代码设计经验的总结使用设计模式的目的：为了代码可重用性、让代码更容易被他人理解、保证代码可靠性 设计模式使代码编写真正工程化；设计模式是软件工程的基石脉络，如同大厦的结构一样设计模式的组成和分类根据其目的（模式是用来做什么的）可分为创建型(Creational)，结构型(Structural)和行为型(Behavioral)三种：• 创建型模式主要用于创建对象• 结构型模式主要用于处理类或对象的组合• 行为型模式主要用于描述对类或对象怎样交互和怎样分配职责根据范围（模式主要是用于处理类之间关系还是处理对象之间的关系）可分为类模式和对象模式两种：• 类模式处理类和子类之间的关系，这些关系通过继承建立，在编译时刻就被确定下来，是属于静态的• 对象模式处理对象间的关系，这些关系在运行时刻变化，更具动态性UML图示说明TypeSymbolDrawingExtension（扩展）&lt;|—Composition（组合）*—Aggregation（聚合）o—使用.. 来代替 -- 可以得到点线在这些规则下，也可以绘制下列图形1234567@startuml Class01 &lt;|-- Class02 Class03 *-- Class04 Class05 o-- Class06 Class07 .. Class08 Class09 -- Class10 @enduml 1234567@startuml Class11 &lt;|.. Class12 Class13 --&gt; Class14 Class15 ..&gt; Class16 Class17 ..|&gt; Class18 Class19 &lt;--* Class20 @enduml 1234567@startuml Class21 #-- Class22 Class23 x-- Class24 Class25 }-- Class26 Class27 +-- Class28 Class29 ^-- Class30 @enduml 定义可访问性一旦你定义了域或者方法，你可以定义相应条目的可访问性质。CharacterIcon for fieldIcon for methodVisibility-private#protected~package private+public六大设计原则学习6大设计原则、23种设计模式总结下软件设计的六大原则6大设计原则总结JAVA设计模式总结之23种设计模式图说设计模式单一职责原则(SRP)开放封闭原则(OCP)里式替换原则(LSP)迪米特法则(LKP)接口隔离原则(ISP)依赖倒置原则(DIP)There should never be more than one reason for a class to change.理解：一个类应该只有一个发生变化的原因，即一个类只负责一项职责。如果一个类有多个职责，这些职责就耦合在了一起。当一个职责发生变化时，可能会影响其它的职责。另外，多个职责耦合在一起会影响复用性。此原则的核心是解耦和增强内聚性总结：一个类只承担一个职责Software entities like classes,modules and functions should be open for extension but closed for modifications.理解：对扩展开放，对更改封闭。类模块应该是可扩展的，但是不可修改总结：对软件实体的改动，最好用扩展而非修改的方式Functions that use pointers or references to base classes must be able to use objects of derived classes without knowing it.理解：父类可被子类替换，但反之不一定成立。也就是说，代码中可以将父类全部替换为子类，程序不会出现异常，但反过来就不一定了总结：在继承类是，务必重写（override）父类中所有的方法，尤其需要注意父类的protected方法（它们往往是让你重写的），子类尽量不要暴露自己的public方法供外界调用Only talk to you immediate friends.理解：如果两个类不必彼此直接通信，那么这两个类就不应当发生直接的相互作用，如果其中一个类需要调用另一个类的某一个方法的话，可以通过第三者转发这个调用总结：一定要做到：低耦合、高内聚The dependency of one class to another one should depend on the smallest possible interface.理解：不要对外暴露没有实际意义的接口。也就是说，尽量保证接口的实用性。当需要对外暴露接口时，需要再三斟酌，若没必要对外提供就删了吧，因为一旦提供了就意味着，将来要多做一件事情，何苦给自己找事做呢总结：不要对外暴露没有实际意义的接口High level modules should not depends upon low level modules. Both should depend upon abstractions. Abstractions should not depend upon details. Details should depend upon abstractions.理解：高层模块不应该依赖于底层模块，而应该依赖于抽象。抽象不应依赖于细节，细节应依赖于抽象。应该面向接口编程，不该面向实现类编程。面向实现类编程相当于就事论事，那是正向依赖；面向接口编程，相当于透过现象看本质，抓住事务的共性，那就是反向依赖，即依赖倒置总结：面向接口编程，提取出事务的本质和共性创建型模式在线plantuml工具plantuml语法Draw Diagrams With Markdown抽象工厂模式(Abstract Factory)123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778@startumlabstract AbsBmwCar{  +drive()}note left: 抽象产品，代表了某一类的产品class BmwSportCar{  +drive()}class BmwBusinessCar{  +drive()}note left: 具体某类产品abstract AbsAudiCar{  +drive()}class AudiSportCar{  +drive()}class AudiBusinessCar{  +drive()}abstract AbsBenzCar{  +drive()}class BenzSportCar{  +drive()}class BenzBusinessCar{  +drive()}abstract AbsDriver{  +createBenzCar()  +createBmwCar()  +createAudiCar()}note left: 抽象工厂，包含了所有产品创建的抽象方法class SportDriver{  +createBenzCar()  +createBmwCar()  +createAudiCar()}class BusinessDriver{  +createBenzCar()  +createBmwCar()  +createAudiCar()}class ClientAbsBmwCar &lt;|-- BmwSportCarAbsBmwCar &lt;|-- BmwBusinessCarAbsAudiCar &lt;|-- AudiSportCarAbsAudiCar &lt;|-- AudiBusinessCarAbsBenzCar &lt;|-- BenzSportCarAbsBenzCar &lt;|-- BenzBusinessCarAbsDriver &lt;|-- SportDriverAbsDriver &lt;|-- BusinessDriverClient --&gt; AbsBmwCarClient --&gt; AbsAudiCar Client --&gt; AbsBenzCar Client --&gt; AbsDriverSportDriver ..&gt; BmwSportCarSportDriver ..&gt; BmwBusinessCarSportDriver ..&gt; AudiSportCarSportDriver ..&gt; AudiBusinessCarSportDriver ..&gt; BenzSportCarSportDriver ..&gt; BenzBusinessCarBusinessDriver ..&gt; BmwSportCarBusinessDriver ..&gt; BmwBusinessCarBusinessDriver ..&gt; AudiBusinessCarBusinessDriver ..&gt; AudiBusinessCarBusinessDriver ..&gt; BenzSportCarBusinessDriver ..&gt; BenzBusinessCar@enduml《JAVA设计模式》之抽象工厂模式(Abstract Factory)每一个模式都是针对一定问题的解决方案。抽象工厂模式与工厂方法模式的最大区别就在于，工厂方法模式针对的是一个产品等级结构；而抽象工厂模式则需要面对多个产品等级结构。优点抽象工厂模式正如其名字一样，理解起来非常抽象，正是因为这种抽象，使得抽象工厂模式非常强大和灵活，比其他两种工厂设计模式要强大很多。抽象工厂模式可以创建多个系列，并且每个系列抽象子类一一对应，这种强大的功能是其他两种工厂模式都不能实现的。通过抽象工厂模式统一控制多个系列的抽象子类，可以用多个系列的抽象子类完成一些复杂的需求。例如我们文中说到的本地持久化方案的切换，最后通过我们的不断优化，做到只需要修改一个预编译指令的参数即可切换整个数据持久化方案，这是其他工厂模式所不能完成的。抽象工厂模式延续了工厂模式的优点，外界接触不到任何类型的抽象子类，而只需要知道不同类型的抽象类即可，抽象子类的创建过程都在工厂子类中。这种设计方式充分的利用了面向对象语言中的多态特性，使灵活性大大提升。而且抽象工厂模式是非常符合开放封闭原则的，对扩展的开放以及对修改的封闭都完美支持。缺点抽象工厂模式带来的缺点也是显而易见的，最明显的缺点就是模式比较庞大，所以需要在适合的业务场景使用这种模式，否则会适得其反。上面说到抽象工厂和工厂方法模式的功能添加都非常复杂，那么我们有没有什么办法可以简化呢？ 答案是肯定有的： 那就是工厂模式 +  配置文件 + 反射。具体怎么实现，请看下回分解。建造者模式(Builder)建造者模式1234567891011121314151617181920212223242526272829303132@startumlclass Director{  -builder:Builder  +construct():void}note top: 指挥者,构建一个使用Builder接口的对象interface Builder{  #buildPart1():void  #buildPart2():void  #retrieveResult():Product}note top: Builder是为创建一个Product对象\n的各个部件指定的抽象接口class Product {  -part1:String  -part2:String}note bottom: 具体产品class ConcreteBuilder{  -product:Product  #buildPart1():void  #buildPart2():void  #retrieveResult():Product}note bottom: 具体创建者,实现Builder接口,\n构建和装配各个部件Builder &lt;|-- ConcreteBuilderConcreteBuilder .left.&gt; ProductDirector o-right-&gt; Builder@enduml工厂方法模式(Factory Method)123456789101112131415161718192021222324252627282930313233@startumlinterface Icar{  +run()}class Audi{  +run()}class Byd{  +run()}interface ICarFactory{  +createCar()}class AudiFactory{  +createCar()}class BydFactory{  +createCar()}Icar .left.&gt; ICarFactoryIcar &lt;|-- AudiIcar &lt;|-- BydICarFactory &lt;|-- AudiFactoryICarFactory &lt;|-- BydFactoryAudi &lt;.. AudiFactoryByd &lt;.. BydFactory@enduml工厂方法模式角色：​           抽象工厂(Creator)角色：是工厂方法模式的核心，与应用程序无关。任何在模式中创建的对象的工厂类必须实现这个接口。​           具体工厂(Concrete Creator)角色：这是实现抽象工厂接口的具体工厂类，包含与应用程序密切相关的逻辑，并且受到应用程序调用以创建产品对象。​           抽象产品(Product)角色：工厂方法模式所创建的对象的超类型，也就是产品对象的共同父类或共同拥有的接口。​           具体产品(Concrete Product)角色：这个角色实现了抽象产品角色所定义的接口。某具体产品有专门的具体工厂创建，它们之间往往一一对应。在工厂方法模式中，核心工厂类不在负责产品的创建，而是将具体的创建工作交给子类去完成。也就是后所这个核心工厂仅仅只是提供创建的接口，具体实现方法交给继承它的子类去完成。原型模式(Prototype)123456789101112131415161718192021222324252627282930313233343536373839@startumlclass Client{}note right:让一个原型克隆自身,\n从而新建一个对象abstract Shape{  -id:String  #type:String  +draw():void  +clone():Object}note top: 原型类,定义实现一个克隆自身的方法class Rectangle{  +draw():void}note bottom: 具体原型类,实现自己的操作class Square{  +draw():void}class Circle{  +draw():void}class ShapeCache {  -shapeMap:Hashtable&lt;String, Shape&gt;  +getShape(shapeId:String):Shape  +loadCache():void}note bottom: 从数据库获取实体类，并把它们存储在一个Hashtable中Shape &lt;|-- RectangleShape &lt;|-- SquareShape &lt;|-- CircleShape &lt;-left- ShapeCacheClient --&gt; ShapeCache@enduml单例模式(Singleton)12345678@startumlclass Singleton{  -instance : Singleton  -Singleton()  +GetInstance()}note right:Singleton类，定义一个GetInstance操作，允许客户访问它的唯一实例。\n GetInstance是一个静态方法，主要创建自己的唯一实例@enduml单例模式只允许创建一个对象，因此节省内存，加快对象访问速度，因此对象需要被公用的场合适合使用，如多个模块使用同一个数据源连接对象等等。如： （1）需要频繁实例化然后销毁的对象。 （2）创建对象时耗时过多或者耗资源过多，但又经常用到的对象。 （3）有状态的工具类对象。 （4）频繁访问数据库或文件的对象。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254package per.hyc.designPattern.Singleton;/** * 1、懒汉式，线程不安全 * 是否 Lazy 初始化：是 * 是否多线程安全：否 * 实现难度：易 * 描述：这种方式是最基本的实现方式，这种实现最大的问题就是不支持多线程。因为没有加锁 synchronized，所以严格意义上它并不算单例模式。 * 这种方式 lazy loading 很明显，不要求线程安全，在多线程不能正常工作。 */class SingleObject_1 {    private static SingleObject_1 instance;    private SingleObject_1() {    }    public static SingleObject_1 getInstance() {        if (instance == null) {            instance = new SingleObject_1();        }        return instance;    }    public void showMessage() {        System.out.println(&quot;1、懒汉式，线程不安全&quot;);    }}/** * 2、懒汉式，线程安全 * 是否 Lazy 初始化：是 * 是否多线程安全：是 * 实现难度：易 * 描述：这种方式具备很好的 lazy loading，能够在多线程中很好的工作，但是，效率很低，99% 情况下不需要同步。 * 优点：第一次调用才初始化，避免内存浪费。 * 缺点：必须加锁 synchronized 才能保证单例，但加锁会影响效率。 * getInstance() 的性能对应用程序不是很关键（该方法使用不太频繁）。 */class SingleObject_2 {    private static SingleObject_2 instance;    private SingleObject_2() {    }    public static synchronized SingleObject_2 getInstance() {        if (instance == null) {            instance = new SingleObject_2();        }        return instance;    }    public void showMessage() {        System.out.println(&quot;2、懒汉式，线程安全&quot;);    }}/** * 3、饿汉式 * 是否 Lazy 初始化：否 * 是否多线程安全：是 * 实现难度：易 * 描述：这种方式比较常用，但容易产生垃圾对象。 * 优点：没有加锁，执行效率会提高。 * 缺点：类加载时就初始化，浪费内存。 * 它基于 classloader 机制避免了多线程的同步问题，不过，instance 在类装载时就实例化， * 虽然导致类装载的原因有很多种，在单例模式中大多数都是调用 getInstance 方法， * 但是也不能确定有其他的方式（或者其他的静态方法）导致类装载，这时候初始化 instance * 显然没有达到 lazy loading 的效果。 */class SingleObject_3 {    private static SingleObject_3 instance = new SingleObject_3();    private SingleObject_3() {    }    public static SingleObject_3 getInstance() {        return instance;    }    public void showMessage() {        System.out.println(&quot;3、饿汉式&quot;);    }}/** * 4、双检锁/双重校验锁（DCL，即 double-checked locking） * JDK 版本：JDK1.5 起 * 是否 Lazy 初始化：是 * 是否多线程安全：是 * 实现难度：较复杂 * 描述：这种方式采用双锁机制，安全且在多线程情况下能保持高性能。 * getInstance() 的性能对应用程序很关键。 */class SingleObject_4 {    private volatile static SingleObject_4 singleton;    private SingleObject_4() {    }    public static SingleObject_4 getInstance() {        if (singleton == null) {            synchronized (SingleObject_4.class) {                if (singleton == null) {                    singleton = new SingleObject_4();                }            }        }        return singleton;    }    public void showMessage() {        System.out.println(&quot;4、双检锁/双重校验锁（DCL，即 double-checked locking）&quot;);    }}/** * 5、登记式/静态内部类 * 是否 Lazy 初始化：是 * 是否多线程安全：是 * 实现难度：一般 * 描述：这种方式能达到双检锁方式一样的功效，但实现更简单。对静态域使用延迟初始化， * 应使用这种方式而不是双检锁方式。这种方式只适用于静态域的情况， * 双检锁方式可在实例域需要延迟初始化时使用。 * 这种方式同样利用了 classloader 机制来保证初始化 instance 时只有一个线程， * 它跟第 3 种方式不同的是：第 3 种方式只要 Singleton 类被装载了， * 那么 instance 就会被实例化（没有达到 lazy loading 效果）， * 而这种方式是 Singleton 类被装载了，instance 不一定被初始化。 * 因为 SingletonHolder 类没有被主动使用，只有通过显式调用 getInstance 方法时， * 才会显式装载 SingletonHolder 类，从而实例化 instance。 * 想象一下，如果实例化 instance 很消耗资源，所以想让它延迟加载， * 另外一方面，又不希望在 Singleton 类加载时就实例化， * 因为不能确保 Singleton 类还可能在其他的地方被主动使用从而被加载， * 那么这个时候实例化 instance 显然是不合适的。 * 这个时候，这种方式相比第 3 种方式就显得很合理。 */class SingleObject_5 {    private static class SingletonHolder {        private static final SingleObject_5 INSTANCE = new SingleObject_5();    }    private SingleObject_5() {    }    public static final SingleObject_5 getInstance() {        return SingletonHolder.INSTANCE;    }    public void showMessage() {        System.out.println(&quot;5、登记式/静态内部类&quot;);    }}/** * 6、枚举 * JDK 版本：JDK1.5 起 * 是否 Lazy 初始化：否 * 是否多线程安全：是 * 实现难度：易 * 描述：这种实现方式还没有被广泛采用，但这是实现单例模式的最佳方法。 * 它更简洁，自动支持序列化机制，绝对防止多次实例化。 * 这种方式是 Effective Java 作者 Josh Bloch 提倡的方式，它不仅能避免多线程同步问题， * 而且还自动支持序列化机制，防止反序列化重新创建新的对象，绝对防止多次实例化。 * 不过，由于 JDK1.5 之后才加入 enum 特性，用这种方式写不免让人感觉生疏， * 在实际工作中，也很少用。 * 不能通过 reflection attack 来调用私有构造方法。 */enum SingleObject_6 {    INSTANCE;    public void showMessage() {        System.out.println(&quot;6、枚举&quot;);    }}/** * 经验之谈：一般情况下，不建议使用第 1 种和第 2 种懒汉方式，建议使用第 3 种饿汉方式。 * 只有在要明确实现 lazy loading 效果时，才会使用第 5 种登记方式。 * 如果涉及到反序列化创建对象时，可以尝试使用第 6 种枚举方式。 * 如果有其他特殊的需求，可以考虑使用第 4 种双检锁方式。 */public class Singleton {    static void TestSingleObject_1() {        //不合法的构造函数        //编译时错误：构造函数 SingleObject() 是不可见的        //SingleObject object = new SingleObject();        //获取唯一可用的对象        SingleObject_1 object = SingleObject_1.getInstance();        //显示消息        object.showMessage();    }    static void TestSingleObject_2() {        //获取唯一可用的对象        SingleObject_2 object = SingleObject_2.getInstance();        //显示消息        object.showMessage();    }    static void TestSingleObject_3() {        //获取唯一可用的对象        SingleObject_3 object = SingleObject_3.getInstance();        //显示消息        object.showMessage();    }    static void TestSingleObject_4() {        //获取唯一可用的对象        SingleObject_4 object = SingleObject_4.getInstance();        //显示消息        object.showMessage();    }    static void TestSingleObject_5() {        //获取唯一可用的对象        SingleObject_5 object = SingleObject_5.getInstance();        //显示消息        object.showMessage();    }    static void TestSingleObject_6() {        //获取唯一可用的对象        SingleObject_6 object = SingleObject_6.INSTANCE;        //显示消息        object.showMessage();    }    public static void main(String[] args) {        TestSingleObject_1();        TestSingleObject_2();        TestSingleObject_3();        TestSingleObject_4();        TestSingleObject_5();        TestSingleObject_6();    }}1、懒汉式，线程不安全2、懒汉式，线程安全3、饿汉式4、双检锁/双重校验锁（DCL，即 double-checked locking）5、登记式/静态内部类6、枚举行为型模式职责链模式(Chain of Responsibility)123456789101112131415161718192021222324252627282930@startumlabstract AbstractLogger{  #level:int  +logMessage(level: int, message:string)  +setNextLogger(nextLogger: AbstractLogger)  +write(message:string)}note right: 定义一个处理请示的接口class Clientclass ConsoleLogger{  +write(message:string)}class ErrorLogger{  +write(message:string)}class FileLogger{  +write(message:string)}note bottom: 具体处理者类，处理它所负责的请求，可访问它的后继者，\n 如果可以处理该请求就处理，不然将请求转发给它的后继者Client -right-&gt; AbstractLoggerAbstractLogger&lt;|-- ConsoleLoggerAbstractLogger &lt;|-- ErrorLoggerAbstractLogger &lt;|-- FileLoggerAbstractLogger &lt;--o AbstractLogger@enduml优点责任链模式将请求和处理分开，请求者不知道是谁处理的，处理者可以不用知道请求的全貌。提高系统的灵活性。缺点降低程序的性能。每个请求都是从链头遍历到链尾，当链比较长的时候，性能会大幅下降。不易于调试。由于该模式采用了类似递归的方式，调试的时候逻辑比较复杂。应用场景责任链模式是一种常见的模式，Struts2的核心控件FilterDispatcher是一个Servlet过滤器，该控件就是采用责任链模式，可以对用户请求进行层层过滤处理。责任链模式在实际项目中的使用比较多，其典型的应用场景如下：一个请求需要一系列的处理工作。业务流的处理，例如文件审批。对系统进行扩展补充。在责任链模式中，客户只需要将请求发送到责任链上即可，无须关心请求的处理细节和请求的传递过程，所以责任链将请求的发送者和请求的处理者解耦了。责任链模式是一种对象行为型模式，其主要优点如下。不能保证每个请求一定被处理。由于一个请求没有明确的接收者，所以不能保证它一定会被处理，该请求可能一直传到链的末端都得不到处理。对比较长的职责链，请求的处理可能涉及多个处理对象，系统性能将受到一定影响。职责链建立的合理性要靠客户端来保证，增加了客户端的复杂性，可能会由于职责链的错误设置而导致系统出错，如可能会造成循环调用。命令模式(Command)12345678910111213141516171819202122232425262728293031323334353637@startumlclass Invoker{  -concreteCommandA:ICommand  -concreteCommandB:ICommand  +orderA():void  +orderB():void}note top: 要求该命令执行这个请求interface ICommand {  +execute():void}note top: 用来声明执行操作的接口class ConcreteCommandA{  -receiver:Receiver  +execute():void}class ConcreteCommandB{  -receiver:Receiver  +execute():void}note bottom: 将一个接收者对象绑定于一个动作,\n调用接收者相应的操作,以实现Executeclass Receiver{  +execute():void}note bottom: 知道如何实施与执行一个请求相关\n的操作,任何类都可能作为一个接收者Client .right.&gt; InvokerClient .right.&gt; ReceiverICommand &lt;|-- ConcreteCommandAICommand &lt;|-- ConcreteCommandBInvoker o-right-&gt; ICommandReceiver &lt;-- ConcreteCommandA@enduml命令模式的几个角色：抽象命令接口ICommand：定义命令的接口，声明执行的方法。具体的命令对象ConcreteCommand：持有具体的接受者对象，完成具体的具体的命令。接受者对象Receiver：接受者对象，真正执行命令的对象。传递命令对象Invoker：持有命令对象，要求命令对象执行请求。客户端对象Client：创建具体命令的对象并且设置命令对象的接受者。Java设计模式（18）之命令模式解释器模式(Interpreter)1234567891011121314151617181920212223242526272829303132333435363738394041@startumlabstract ReadXmlExpression{  +interpret(c:Context):String[]}note left: 抽象表达式,声明一个抽象的解释操作,\n这个接口为抽象语法树中所有的节点共享class ElementExpression{  -eles:Collection&lt;ReadXmlExpression&gt;  -eleName:String  +addEle(ele:ReadXmlExpression):boolean  +removeEle(ele:ReadXmlExpression):boolean  +interpret(c:Context):String[]}note bottom: 非终结表达式,为文法中的非终结\n符实现解释操作。对文法中每一条\n规则R1..Rn都需要一个具体的非终结符\n表达式类class ElementTerminalExpression{  -eleName:String  +interpret(c:Context):String[]}note bottom: 终结表达式,实现与文法中的终结符\n相关联的解释操作class PropertyTerminalExpression{  -eleName:String  +interpret(c:Context):String[]}class Context{  -preEle:Element  -document:Document  +reInit():void  +getNowEle(pEle:Element,eleName:String):Element}note right: 包含解释器之外的一些全局信息ReadXmlExpression &lt;|-- ElementExpressionReadXmlExpression &lt;|-- ElementTerminalExpressionReadXmlExpression &lt;|-- PropertyTerminalExpressionClinet ..&gt; ContextClinet ..&gt; ReadXmlExpressionElementExpression o--&gt; ReadXmlExpression@enduml迭代器模式(Iterator)1234567891011121314151617181920212223242526272829303132@startumlinterface IAggregate{  +CreateIterator(): int}note top: 聚集接口class Clientinterface AbsIterator{  +First()  +Next()  +IsDone()  +CurrentItem()}note top: 迭代接口类，用于得到开始对象、 \n 得到下一个对象、判断是否到结尾、 \n 当前对象等抽象方法，统一接口class ConcreteIteratornote bottom: 具体迭代器类class ConcreteAggregate{  +CreateIterator(): int}note bottom: 具体聚集类，继承自IAggregateClient -left-&gt; AbsIteratorClient -right-&gt; IAggregateIAggregate &lt;|-- ConcreteAggregateAbsIterator &lt;|-- ConcreteIteratorConcreteAggregate &lt;-left- ConcreteIteratorConcreteAggregate .left.&gt; ConcreteIterator@enduml中介者模式(Mediator)1234567891011121314151617181920@startumlinterface Mediatornote left: 接口终中介者，定义了Colleague \n 对象到Mediator对象的接口abstract AbsColleagueclass ConcreteMediatornote left: 具体中介者对象，实现抽象类的方法，\n 它需要知道所有具体的Colleague类，并从具体的 \n Colleague接收消息，向具体Colleague对象发出命令class ColleagueAnote top: 具体Colleague类，每个具体Colleague只知道自己 \n 的行为，而不了解其他Colleague类的情况， \n 但它们却都认识Mediator对象class ColleagueBMediator &lt;|-- ConcreteMediatorAbsColleague &lt;|-- ColleagueAAbsColleague &lt;|-- ColleagueBColleagueA &lt;-- ConcreteMediatorColleagueB &lt;-- ConcreteMediator@enduml中介者模式的定义用一个中介者对象封装一系列的对象交互，中介者使各对象不需要显示地相互作用，从而使耦合松散，而且可以独立地改变它们之间的交互。中介者模式的优点适当地使用中介者模式可以避免同事类之间的过度耦合，使得各同事类之间可以相对独立地使用。使用中介者模式可以将对象间一对多的关联转变为一对一的关联，使对象间的关系易于理解和维护。使用中介者模式可以将对象的行为和协作进行抽象，能够比较灵活的处理对象间的相互作用。适用场景 在面向对象编程中，一个类必然会与其他的类发生依赖关系，完全独立的类是没有意义的。一个类同时依赖多个类的情况也相当普遍，既然存在这样的情况，说明，一对多的依赖关系有它的合理性，适当的使用中介者模式可以使原本凌乱的对象关系清晰，但是如果滥用，则可能会带来反的效果。一般来说，只有对于那种同事类之间是网状结构的关系，才会考虑使用中介者模式。可以将网状结构变为星状结构，使同事类之间的关系变的清晰一些。 中介者模式是一种比较常用的模式，也是一种比较容易被滥用的模式。对于大多数的情况，同事类之间的关系不会复杂到混乱不堪的网状结构，因此，大多数情况下，将对象间的依赖关系封装的同事类内部就可以的，没有必要非引入中介者模式。滥用中介者模式，只会让事情变的更复杂。优点：1、降低了类的复杂度，将一对多转化成了一对一。2、各个类之间的解耦。3、符合迪米特原则。缺点：1.中介者会庞大，变得复杂难以维护。2.如果中介者类出现问题，将会导致所有相关类出问题。Where:多个类相互耦合，形成了网状结构。设计模式之中介者模式（Mediator Pattern）备忘录模式(Memento)Java设计模式学习记录-备忘录模式https://www.cnblogs.com/jimoer/p/9537997.html12345678910111213141516171819202122@startumlclass Game{  +playerStep:int  +restore(gameMemento: GameMemento):void  +createGameMemento():GameMemento}note Bottom: 负责创建一个备忘录GameMemento， \n 用以记录当前时刻的内部状态， \n 并可使用备忘录恢复内部状态class GameMemento{  -playerSteps:int}note top: 负责存储Game对象的内部状态，\n 并可防止Game以外的对象访问 \n 备忘录GameMementoclass Caretaker{  -gameMemento: GameMemento  +saveMemento(gameMemento:GameMemento):void}note right: 负责保存好备忘录GameMementoGame .right.&gt; GameMementoGameMemento &lt;--o Caretaker@enduml观察者模式(Observer)123456789101112131415161718192021222324252627282930313233343536373839404142434445@startumlclass ISubject{  +attach(observer: Observer)  +detach(observer: Observer)  +notifyAllObservers()}note top: ISubject类,它把所有对观察者对象的引用保存在一个\n聚集里,每个主题都可以有任何数量的观察者.  主题提供\n一个接口,可以增加和删除观察者对象 class Subject{  #observers:List&lt;Observer&gt;  +state: int  +getState()  +setState()  +attach(observer: Observer)  +detach(observer: Observer)  +notifyAllObservers()}note bottom: Subject类,具体主题,将有关状态存入\n具体观察者对象;在具体主题的内部状态改变\n的时候，给所有登记过的观察者发出通知 abstract Observer{  #subject:Subject  +update()}note top: 抽象观察者,为所有的具体观察者定义\n一个接口,在得到主题的通知时更新自己class BinaryObserver{  +update()}class OctalObserver{  +update()}note bottom: 具体观察者,实现抽象观察者角色所\n要求的更新接口,以便使自身状态与主题状态相协调class HexaObserver{  +update()}ISubject &lt;|-- SubjectObserver &lt;|-- BinaryObserverObserver &lt;|-- OctalObserverObserver &lt;|-- HexaObserverSubject &lt;-left- HexaObserverISubject -right-&gt; Observer@enduml状态模式(State)123456789101112131415161718192021222324252627@startumlinterface Statenote left: 状态类接口，定义一个接口以封装与 \n Context的一个特定状态相关的行为class Context{  +Request()}note left: 维护一个ConcreteState子类的实例，\n 这个实例定义当前的状态class ConcreteStateA{  +Handle()}note right: 具体状态，每一个子类实现一个 \n 与Context的一个状态相关的行为class ConcreteStateB{  +Handle()}class ConcreteStateC{  +Handle()}Context o--&gt; StateState &lt;|-- ConcreteStateAState &lt;|-- ConcreteStateBState &lt;|-- ConcreteStateC@enduml策略模式(Strategy)12345678910111213141516171819202122232425262728@startumlclass OperationAdd{  +doOperation()}class OperationSubstract{  +doOperation()}class OperationMultiply{  +doOperation()}note bottom: 具体策略类，封装了具体的算法或行为，继承于IStrategyclass Context{  +executeStrategy()}note bottom: Context上下文，用一个 \n ConcreateStategy来配置， \n 维护一个对IStrategy对象的引用interface IStrategy{  +doOperation()}note right:策略类，定义所有支持的算法的公共接口Context &lt;-left-o IStrategyIStrategy&lt;|-- OperationAddIStrategy&lt;|-- OperationSubstractIStrategy&lt;|-- OperationMultiply@enduml二十三种设计模式 - 策略模式模式定义策略模式：属于对象的行为模式。其用意是针对一组算法，将每一个算法封装到具有共同接口的独立的类中，从而使得它们可以相互替换。策略模式：使得算法可以在不影响到客户端的情况下发生变化。策略模式：是对算法的包装，是把使用算法的责任和算法本身分割开来，委派给不同的对象管理。策略模式通常把一个系列的算法包装到一系列的策略类里面，作为一个抽象策略类的子类。用一句话来说，就是：“准备一组算法，并将每一个算法封装起来，使得它们可以互换”。参与者策略模式参与者：Context：环境角色，持有一个Strategy的引用。Strategy：抽象策略角色，这是一个抽象角色，通常由一个接口或抽象类实现。此角色给出所有的具体策略类所需的接口。ConcreteStrategy：具体策略角色，包装了相关的算法或行为优点：业务逻辑和前端展示相互分离开了。业务逻辑的修改，不影响前端代码展示。每一个业务逻辑单独一个类，修改或者添加一个类，不会影响到其他的类。使用工厂类封装了业务逻辑类，前端不需要知道到底每种业务怎么实现，只需要知道他的父类即可。缺点：如果活动很频繁，经常会搞各种各样的活动，那么业务逻辑就会有很多种，每一次都要增加一个类。每增加一个类都要修改工厂类，修改会很频繁。策略模式的优点（1）策略模式提供了管理相关的算法族的办法。策略类的等级结构定义了一个算法或行为族。恰当使用继承可以把公共的代码移到父类里面，从而避免代码重复。（2）使用策略模式可以避免使用多重条件(if-else)语句。多重条件语句不易维护，它把采取哪一种算法或采取哪一种行为的逻辑与算法或行为的逻辑混合在一起，统统列在一个多重条件语句里面，比使用继承的办法还要原始和落后。策略模式的缺点（1）客户端必须知道所有的策略类，并自行决定使用哪一个策略类。这就意味着客户端必须理解这些算法的区别，以便适时选择恰当的算法类。换言之，策略模式只适用于客户端知道算法或行为的情况。（2）由于策略模式把每个具体的策略实现都单独封装成为类，如果备选的策略很多的话，那么对象的数目就会很可观用途不一样工厂是创建型模式,它的作用就是创建对象；策略是行为型模式,它的作用是让一个对象在许多行为中选择一种行为;关注点不一样一个关注对象创建一个关注行为的封装解决不同的问题工厂模式是创建型的设计模式，它接受指令，创建出符合要求的实例；它主要解决的是资源的统一分发，将对象的创建完全独立出来，让对象的创建和具体的使用客户无关。主要应用在多数据库选择，类库文件加载等。策略模式是为了解决的是策略的切换与扩展，更简洁的说是定义策略族，分别封装起来，让他们之间可以相互替换，策略模式让策略的变化独立于使用策略的客户。工厂相当于黑盒子，策略相当于白盒子；模板方法模式(Template Method)12345678910111213141516171819202122232425@startumlabstract AbsTestPaper{  +TestQuestion1()  +TestQuestion2()  +TestQuestion3()  +Answer1()  +Answer2()  +Answer3()}class TestPaperA{  +Answer1()  +Answer2()  +Answer3()}class TestPaperB{  +Answer1()  +Answer2()  +Answer3()}AbsTestPaper &lt;|-- TestPaperAAbsTestPaper &lt;|-- TestPaperB@enduml访问者模式(Visitor)设计模式：访问者（Visitor）模式访问者模式一篇就够了12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@startumlclass Clinet{  +visitor()}abstract Staff{  +accept(visitor: Visitor)}note top: 定义一个accept操作,它以一个访问为参数class Engineer{  +accept(visitor: Visitor)}class Manager{  +accept(visitor: Visitor)}note bottom: 具体元素,实现accept操作class BusinessReport{  -mStaffs: List&lt;Staff&gt;  +showReport(visitor: Visitor)}note bottom: 能枚举它的元素,可以提供一个高层的接口\n以允许访问者访问它的元素interface Visitor{  +visit(engineer: Engineer)  +visit(manager: Manager)}note top:为该对象结构中每个实现类声明一个visit操作class CEOVisitor{  +visit(engineer: Engineer)  +visit(manager: Manager) }class CTOVisitor{  +visit(engineer: Engineer)  +visit(manager: Manager) }note bottom: 具体访问者,实现每个由Visitor声明的操作,\n每个操作实现算法的一部分,而该算法\n片段仍是对应于结构中对象的类Staff &lt;|-- EngineerStaff &lt;|-- ManagerVisitor &lt;|-- CEOVisitorVisitor &lt;|-- CTOVisitorClinet --&gt; BusinessReportClinet -right-&gt; VisitorBusinessReport -left-&gt; Staff@enduml角色介绍Visitor：接口或者抽象类，定义了对每个 Element 访问的行为，它的参数就是被访问的元素，它的方法个数理论上与元素的个数是一样的，因此，访问者模式要求元素的类型要稳定，如果经常添加、移除元素类，必然会导致频繁地修改 Visitor 接口，如果出现这种情况，则说明不适合使用访问者模式。ConcreteVisitor：具体的访问者，它需要给出对每一个元素类访问时所产生的具体行为。Element：元素接口或者抽象类，它定义了一个接受访问者（accept）的方法，其意义是指每一个元素都要可以被访问者访问。ElementA、ElementB：具体的元素类，它提供接受访问的具体实现，而这个具体的实现，通常情况下是使用访问者提供的访问该元素类的方法。ObjectStructure：定义当中所提到的对象结构，对象结构是一个抽象表述，它内部管理了元素集合，并且可以迭代这些元素提供访问者访问。结构型模式适配器模式(Adapter)123456789101112131415161718192021222324252627282930313233343536373839@startumlabstract AbsPlayer{  +attack()  +defense()}note left: 球员抽象类class Forwards{  +attack()  +defense()}class Center{  +attack()  +defense()}class Guards{  +attack()  +defense()}class Translator{  +attack()  +defense()}class ChinaCenter{  +jingong()  +fangshou()}note left: 中国中锋AbsPlayer &lt;|-- ForwardsAbsPlayer &lt;|-- CenterAbsPlayer &lt;|-- GuardsAbsPlayer &lt;|-- TranslatorChinaCenter &lt;-- Translator@enduml123456789优点：1：将目标类和适配者类解耦，通过引入一个适配器类来重用现有的适配者类，无需修改原有结构。2：增加了类的透明性和复用性，将具体的业务实现过程封装在适配者类中，对于客户端类而言是透明的，而且提高了适配者的复用性，同一适配者类可以在多个不同的系统中复用。3：灵活性和扩展性都非常好，通过使用配置文件，可以很方便的更换适配器，也可以在不修改原有代码的基础上 增加新的适配器，完全复合开闭原则。缺点：1：一次最多只能适配一个适配者类，不能同时适配多个适配者。2：适配者类不能为最终类，在C#中不能为sealed类3：目标抽象类只能为接口，不能为类，其使用有一定的局限性。桥接模式(Bridge)1234567891011121314151617181920212223242526272829303132@startumlinterface Color{  +bepaint(penType:String,name:String):void}class Blue{  +bepaint(penType:String,name:String):void}class Green{  +bepaint(penType:String,name:String):void}class Pen{  #color:Color  +draw(name:String):void}class SmallPen{  +draw(name:String):void}class MiddlePen{  +draw(name:String):void}Pen &lt;|-- SmallPenPen &lt;|-- MiddlePenColor &lt;|-- BlueColor &lt;|-- GreenPen o-left-&gt; Color@enduml概述在软件系统中，某些类型由于自身的逻辑，它具有两个或多个维度的变化，那么如何应对这种“多维度的变化”？如何利用面向对象的技术来使得该类型能够轻松的沿着多个方向进行变化，而又不引入额外的复杂度？这就要使用Bridge模式。意图【GOF95】在提出桥梁模式的时候指出，桥梁模式的用意是”将抽象化(Abstraction)与实现化(Implementation)脱耦，使得二者可以独立地变化”。这句话有三个关键词，也就是抽象化、实现化和脱耦。抽象化存在于多个实体中的共同的概念性联系，就是抽象化。作为一个过程，抽象化就是忽略一些信息，从而把不同的实体当做同样的实体对待【LISKOV94】。实现化抽象化给出的具体实现，就是实现化。脱耦所谓耦合，就是两个实体的行为的某种强关联。而将它们的强关联去掉，就是耦合的解脱，或称脱耦。在这里，脱耦是指将抽象化和实现化之间的耦合解脱开，或者说是将它们之间的强关联改换成弱关联。将两个角色之间的继承关系改为聚合关系，就是将它们之间的强关联改换成为弱关联。因此，桥梁模式中的所谓脱耦，就是指在一个软件系统的抽象化和实现化之间使用组合/聚合关系而不是继承关系，从而使两者可以相对独立地变化。这就是桥梁模式的用意。组合模式(Composite)12345678910111213141516171819202122232425262728293031323334353637@startumlabstract AbsCompany{  +add(company: AbsCompany)  +remove(company: AbsCompany)  +display(depth: int)  +lineOfDuty()}note right: 组合中的对象声明接口，在适当情况下，\n 实现所有类共有接口的默认行为。声明一个\n 接口用于访问和管理AbsCompany的子部件class ConcreteCompany{  +add(company: AbsCompany)  +remove(company: AbsCompany)  +display(depth: int)  +lineOfDuty()}note bottom: 定义有枝节点行为，用来存储子部件，\n 在AbsCompany接口中实现与子部件有关 \n 的操作，比如增加Add和删除Removeclass HRDepartment{  +add(company: AbsCompany)  +remove(company: AbsCompany)  +display(depth: int)  +lineOfDuty()}note bottom: 在组合中表示叶节点对象， \n 叶节点没有子节点class FinanceDepartment{  +add(company: AbsCompany)  +remove(company: AbsCompany)  +display(depth: int)  +lineOfDuty()}AbsCompany &lt;|-- ConcreteCompanyAbsCompany &lt;|-- HRDepartmentAbsCompany &lt;|-- FinanceDepartmentAbsCompany &lt;--o ConcreteCompany@enduml组合模式(Composite Pattern)：组合多个对象形成树形结构以表示具有 “整体—部分” 关系的层次结构。组合模式对单个对象（即叶子对象）和组合对象（即容器对象）的使用具有一致性，组合模式又可以称为 “整体—部分”(Part-Whole) 模式，它是一种对象结构型模式。由于在软件开发中存在大量的树形结构，因此组合模式是一种使用频率较高的结构型设计模式，Java SE中的AWT和Swing包的设计就基于组合模式。除此以外，在XML解析、组织结构树处理、文件系统设计等领域，组合模式都得到了广泛应用。角色Component（抽象构件）：它可以是接口或抽象类，为叶子构件和容器构件对象声明接口，在该角色中可以包含所有子类共有行为的声明和实现。在抽象构件中定义了访问及管理它的子构件的方法，如增加子构件、删除子构件、获取子构件等。Leaf（叶子构件）：它在组合结构中表示叶子节点对象，叶子节点没有子节点，它实现了在抽象构件中定义的行为。对于那些访问及管理子构件的方法，可以通过异常等方式进行处理。Composite（容器构件）：它在组合结构中表示容器节点对象，容器节点包含子节点，其子节点可以是叶子节点，也可以是容器节点，它提供一个集合用于存储子节点，实现了在抽象构件中定义的行为，包括那些访问及管理子构件的方法，在其业务方法中可以递归调用其子节点的业务方法。组合模式的关键是定义了一个抽象构件类，它既可以代表叶子，又可以代表容器，而客户端针对该抽象构件类进行编程，无须知道它到底表示的是叶子还是容器，可以对其进行统一处理。同时容器对象与抽象构件类之间还建立一个聚合关联关系，在容器对象中既可以包含叶子，也可以包含容器，以此实现递归组合，形成一个树形结构。透明与安全在使用组合模式时，根据抽象构件类的定义形式，我们可将组合模式分为透明组合模式和安全组合模式两种形式。透明组合模式透明组合模式中，抽象构件角色中声明了所有用于管理成员对象的方法，譬如在示例中 Component 声明了 add、remove 方法，这样做的好处是确保所有的构件类都有相同的接口。透明组合模式也是组合模式的标准形式。透明组合模式的缺点是不够安全，因为叶子对象和容器对象在本质上是有区别的，叶子对象不可能有下一个层次的对象，即不可能包含成员对象，因此为其提供 add()、remove() 等方法是没有意义的，这在编译阶段不会出错，但在运行阶段如果调用这些方法可能会出错（如果没有提供相应的错误处理代码）安全组合模式在安全组合模式中，在抽象构件角色中没有声明任何用于管理成员对象的方法，而是在容器构件 Composite 类中声明并实现这些方法。组合模式总结组合模式的主要优点如下：组合模式可以清楚地定义分层次的复杂对象，表示对象的全部或部分层次，它让客户端忽略了层次的差异，方便对整个层次结构进行控制。客户端可以一致地使用一个组合结构或其中单个对象，不必关心处理的是单个对象还是整个组合结构，简化了客户端代码。在组合模式中增加新的容器构件和叶子构件都很方便，无须对现有类库进行任何修改，符合“开闭原则”。组合模式为树形结构的面向对象实现提供了一种灵活的解决方案，通过叶子对象和容器对象的递归组合，可以形成复杂的树形结构，但对树形结构的控制却非常简单。组合模式的主要缺点如下：使得设计更加复杂，客户端需要花更多时间理清类之间的层次关系。在增加新构件时很难对容器中的构件类型进行限制。适用场景：在具有整体和部分的层次结构中，希望通过一种方式忽略整体与部分的差异，客户端可以一致地对待它们。在一个使用面向对象语言开发的系统中需要处理一个树形结构。在一个系统中能够分离出叶子对象和容器对象，而且它们的类型不固定，需要增加一些新的类型。装饰模式(Decorator)12345678910111213141516171819202122232425262728293031323334@startumlinterface IComponent{  +Operation()}note left: IComponent是定义一个对象接口，\n 可以给这些对象动态地添加职责class ConcreteComponent{  +Operation()}note bottom: ConcreteComponent是定义了一个具体的对象，\n 也可以给这个对象添加一些职责abstract AbsDecorator{  -component:IComponent  +Operation()}note left: AbsDecorator，装饰抽象类，\n 继承了IComponent，从外类来扩展\n IComponent类的功能，\n 但对于IComponent来说，是无需知道\n AbsDecorator的存在的class ConcreteDecoratorA{  +Operation()  -AddedBehaviorA()}class ConcreteDecoratorB{  +Operation()  -AddedBehaviorB()}note bottom: ConcreteDecorator就是具体的装饰对象，\n 起到给IComponent添加职责的功能IComponent &lt;|-- ConcreteComponentIComponent &lt;|-- AbsDecoratorAbsDecorator  &lt;|-- ConcreteDecoratorAAbsDecorator  &lt;|-- ConcreteDecoratorBIComponent &lt;--o AbsDecorator@enduml装饰器模式：在不改变原有结构的情况下，允许动态的给一个现有对象添加一些额外的职责（额外的结构）装饰器模式主要解决：动态对一个类进行扩展，保证其原有结构不变的基础上获得新的结构和功能。这种扩展不以继承的方式实现，有效的防止子类过分膨胀的问题。优点：装饰类和被装饰类可以独立发展，不会相互耦合。装饰模式是继承的一个替代的方法，装饰模式可以动态扩展一个实现类的功能。另外，装饰模式可以实现未知顺序的扩展，将原有的父子结构转换为兄弟结构，降低系统的耦合度。装饰器模式的优点装饰模式与继承关系的目的都是要扩展对象的功能，但是装饰模式可以提供比继承更多的灵活性。通过使用不同的具体装饰类以及这些装饰类的排列组合，可以创造出很多不同行为的组合。可以使用多个具体装饰类来装饰同一对象，得到功能更为强大的对象。装饰器模式的缺点这种比继承更加灵活机动的特性，也同时意味着装饰模式比继承更加易于出错，排错也很困难，对于多次装饰的对象，调试时寻找错误可能需要逐级排查，较为烦琐。外观模式(Facade)12345678910111213141516171819202122232425262728293031@startumlclass ControllerFacade{  +on():void  +off():void}note right: Facadew外观类\n 知道哪些子系统负责处理请求，\n 将客户的请求代理给适当的子系统对象 class Light{  +on():void  +off():void}class Television{  +on():void  +off():void}class Aircondition{  +on():void  +off():void}class ClientClient -right-&gt; ControllerFacadeControllerFacade--&gt; LightControllerFacade --&gt; TelevisionControllerFacade--&gt; Aircondition@enduml门面模式要求一个子系统的外部与其内部的通信必须通过一个统一的门面(Facade)对象进行。门面模式提供一个高层次的接口，使得子系统更易于使用。就如同医院的接待员一样，门面模式的门面类将客户端与子系统的内部复杂性分隔开，使得客户端只需要与门面对象打交道，而不需要与子系统内部的很多对象打交道。初学者往往以为通过继承一个门面类便可在子系统中加入新的行为，这是错误的。门面模式的用意是为子系统提供一个集中化和简化的沟通管道，而不能向子系统加入新的行为。享元模式(Flyweight)享元模式享元模式（Flyweight）又称为 轻量级模式，它是一种对象结构型模式。面向对象技术可以很好地解决一些灵活性或可扩展性问题，但在很多情况下需要在系统中增加类和对象的个数。当对象数量太多时，将导致运行代价过高，带来性能下降等问题。享元模式 正是为解决这一类问题而诞生的。享元模式 是对象池的一种实现。类似于线程池，线程池可以避免不停的创建和销毁多个对象，消耗性能。享元模式 也是为了减少内存的使用，避免出现大量重复的创建销毁对象的场景。享元模式 的宗旨是共享细粒度对象，将多个对同一对象的访问集中起来，不必为每个访问者创建一个单独的对象，以此来降低内存的消耗。享元模式 把一个对象的状态分成内部状态和外部状态，内部状态即是不变的，外部状态是变化的；然后通过共享不变的部分，达到减少对象数量并节约内存的目的。享元模式 本质：缓存共享对象，降低内存消耗12345678910111213141516171819202122@startumlinterface IFlyweight{  +operation(extrinsicState:String):void}class ConcreteFlyweight{  -intrinsicState:String  +operation(extrinsicState:String):void}class FlyweightFactory{  -pool:Map&lt;String, IFlyweight&gt;  +getFlyweight(intrinsicState:String):IFlyweight}class ClientClient ..&gt; FlyweightFactoryConcreteFlyweight --|&gt; IFlyweightFlyweightFactory o-right- IFlyweight ConcreteFlyweight &lt;.right. Client@enduml享元模式（Flyweight）又称为 轻量级模式，它是一种对象结构型模式。面向对象技术可以很好地解决一些灵活性或可扩展性问题，但在很多情况下需要在系统中增加类和对象的个数。当对象数量太多时，将导致运行代价过高，带来性能下降等问题。享元模式 正是为解决这一类问题而诞生的。享元模式 是对象池的一种实现。类似于线程池，线程池可以避免不停的创建和销毁多个对象，消耗性能。享元模式 也是为了减少内存的使用，避免出现大量重复的创建销毁对象的场景。享元模式 的宗旨是共享细粒度对象，将多个对同一对象的访问集中起来，不必为每个访问者创建一个单独的对象，以此来降低内存的消耗。享元模式 把一个对象的状态分成内部状态和外部状态，内部状态即是不变的，外部状态是变化的；然后通过共享不变的部分，达到减少对象数量并节约内存的目的。享元模式 本质：缓存共享对象，降低内存消耗优点享元模式 可以极大减少内存中对象的数量，使得相同对象或相似对象在内存中只保存一份，降低内存占用，增强程序的性能；享元模式 的外部状态相对独立，而且不会影响其内部状态，从而使得享元对象可以在不同的环境中被共享；缺点享元模式 使得系统更加复杂，需要分离出内部状态和外部状态，这使得程序的逻辑复杂化；为了使对象可以共享，享元模式 需要将享元对象的状态外部化，而且外部状态必须具备固化特性，不应该随内部状态改变而改变，否则会导致系统的逻辑混乱；使用场景系统中存在大量的相似对象；细粒度的对象都具备较接近的外部状态，而且内部状态与环境无关，也就是说对象没有特定身份；需要缓冲池的场景；java中，从享元模式角度再看String123456String a = &quot;将导致运行代价过高&quot;;String b = &quot;将导致运行代价过高&quot;;String c = new String(&quot;将导致运行代价过高&quot;);String d = new String(&quot;将导致运行代价过高&quot;);a==b  truec==d  false代理模式(Proxy)12345678910111213141516171819202122@startumlinterface IGiveGift{  +giveDolls()  +giveFlowers()}note right: IGiveGift类，定义了Pursuit和GiftProxy的公用接口，\n 这样就在任何使用Pursuit的地方都可以使用GiftProxyclass Pursuit{  +giveDolls()  +giveFlowers()}class GiftProxy{  +giveDolls()  +giveFlowers()}Client -right-&gt; IGiveGiftIGiveGift&lt;|-- PursuitIGiveGift&lt;|-- GiftProxyPursuit &lt;-left- GiftProxy: -Pursuit@enduml代理模式：为其他对象提供一种代理以控制对这个对象的访问。代理模式使用一个代理类来代表另一个类的功能，代理类持有现有对象，并利用现有对象对外界提供功能。代理模式可以看作是对原有对象的一个再包装，现在其他对象对原有对象的访问都应该通过代理的模式来做，起到控制访问的效果。代理模式主要解决：直接访问对象的时候需要很多额外的操作（比如权限控制，安全性检查等），这个对象被访问的时候希望能无一例外的对这些请求做控制，此时则需要使用到代理模式。优点：1、职责清晰，满足单一职责原则。 被代理者和代理者各自负责自己的职责即可，将被代理者的访问控制转移到代理者身上。2、高扩展性，代理者可以作为一个被代理者被再次包装，在不改变原有结构的基础上扩展新功能。代理模式可以看作是一种特殊目的的装饰器模式。装饰器是在不改变原有类的结构下，为其增加新的功能。代理模式则是在不改变原有类的结构下，控制其他类对其的访问，这种控制功能可以看作是为原有类增加的特殊的新功能。代理模式为了实现对原有类的访问的控制，则要求持有原有对象。装饰器模式没有必要的要求，装饰器模式可以和被装饰类（原有类）互相独立，只需要在需要装饰的时候可以获取到被装饰类即可。依据第二点，可以看出代理模式和装饰器模式之间最大的区别在于：代理模式中，代理者和被代理者的关系需要在编译时就确定下来，实现对被代理者的有效控制；装饰器模式中，装饰者和被装饰者之间是相互独立的，当被装饰者有新功能需要添加的时候，使用合适的装饰者将其装饰即可。设计模式总结设计模式总结设计模式总结常见设计模式总结设计模式用法单例保证类的实例只有一个简单工厂根据参数创建对应具体子类策略算法、规则的封装、传入具体调用，调用具体算法装饰者动态对一个对象进行增属性、调用方法等操作，链式操作随意组合。梳头、画眉、只梳头不画眉、只画眉不梳头工厂方法创建类，一个实现类要有一个工厂类总是通过对应的工厂类创建实现类，判断在客户端进行。工厂类太多。代理对象中保存能执行另一种操作的对象，通过这个保存的对象去操作。原型用克隆(Clone)，代替new对象。克隆的方式能够保留一些同样的信息。模板方法提炼出相同的公共代码，封装为一个方法模板。外观通过改造内部实现，让外部看起来调用的方式很简单(如，实际需要调用3个方法，但增加一个方法来调用这3个方法，让外部只调用一个方法即可)。建造者将一个复制对象的构建与它的表示分离，使得同样的构建过程可以创建不同的表示。观察者观察者类里面放了一个集合，包含了多个被通知类，被通知类实现一个抽象方法，有一个方法接受通知。抽象工厂增加一个接口，让外部依赖接口而不是具体类状态多个大致相同的类，只是状态不同，随着不停的调用一个状态会转为另一个状态(一个类转为另一个类)适配器将一个类的接口转换成客户希望的另外一个接口可以理解为，仅仅改为一个方法名，在方法内部调用真正的方法(名字不同)。备忘录增加一个类用于存储状态，一个类负责备份和恢复状态组合部分-整体关系，解决无限递归问题。迭代器分离了集合对象的遍历行为，抽象出一个迭代器类来负责。解决遍历问题桥接将继承关系分离出来改为聚合关系命令对请求的封装，请求-》真正执行(队列)职责链类似于Asp.net管道，一条链操作，每个类都有处理的机会没权限就往上级抛，直到有权限的类能够处理中介者通过一个中介类来处理两个类之间的信息交换享元对象的大多数状态为外部状态，如果删除对象的外部状态那么可以用相对较少的共享对象取代很多组对象。访问者它把数据结构和作用于结构上的操作之间的耦合解脱开（用类封装变化的数据）解释器用类去封装一条规则graph LRA[Hard edge] --&gt;B(Round edge)    B --&gt; C{Decision}    C --&gt;|One| D[Result one]    C --&gt;|Two| E[Result two]    C --&gt;|Two| F[Result two]    F --&gt;|Two| G[Result two]</content>
	</entry>
	<entry>
		<title>python协程</title>
		<url>/article/56863.html</url>
		<content>python迭代对象Coroutines Are Faster To Start Than Threads in Python扫描内存中放不下的数据集时，我们要找到一种惰性获取数据项的方式，即按需一次获取一个数据项这就是迭代器模式(Iterator pattern)，迭代是数据处理的基石Python 2.2(2001 年)加入了 yield 关键字，这个关键字用于构建生成器(generator)，其作用与迭代器一样所有生成器都是迭代器，因为生成器完全实现了迭代器接口迭代器用于从集合中取出元素；而生成器用于”凭空”生成元素Python 社区中，大多数时候都把迭代器和生成器视作同一概念内置的 range() 函数也返回一个类似生成器的对象，而以前则返回完整的列表如果一定要让 range() 函数返回列表，那么必须明确指明(例如，list(range(100)))序列可以迭代的原因：iter函数解释器需要迭代对象 x 时，会自动调用 iter(x)内置的 iter 函数有以下作用检查对象是否实现了 __iter__ 方法，如果实现了就调用它，获取一个迭代器如果没有实现 __iter__ 方法，但是实现了 __getitem__ 方法，Python 会创建一个迭代器，尝试按顺序(从索引 0 开始)获取元素如果尝试失败，Python 抛出 TypeError 异常，通常会提示”C object is not iterable”(C 对象不可迭代)，其中 C 是目标对象所属的类任何 Python 序列都可迭代的原因是，它们都实现了 __getitem__ 方法鸭子类型（duck typing）的极端形式不仅要实现特殊的 __iter__ 方法，还要实现 __getitem__ 方法而且 __getitem__ 方法的参数是从 0 开始的整数（int），这样才认为对象是可迭代的白鹅类型（goose-typing）理论中，可迭代对象的定义简单一些，不过没那么灵活如果实现了 __iter__ 方法，那么就认为对象是可迭代的检查对象 x 能否迭代，最准确的方法是：调用 iter(x)函数，如果不可迭代，再处理 TypeError 异常这比使用 isinstance(x, abc.Iterable) 更准确因为 iter(x) 函数会考虑到遗留的 __getitem__ 方法，而 abc.Iterable 类则不考虑可迭代的对象使用 iter 内置函数可以获取迭代器的对象如果对象实现了能返回迭代器的 __iter__ 方法，那么对象就是可迭代的。序列都可以迭代是实现了 __getitem__方法，而且其参数是从零开始的索引，这种对象也可以迭代遍历方式下面是一个简单的 for 循环，迭代一个字符串。这里，字符串 'ABC' 是可迭代的对象。背后是有迭代器的，只不过我们看不到：1234567&gt;&gt;&gt; s = 'ABC'&gt;&gt;&gt; for char in s:...     print(char)...ABC如果没有 for 语句，不得不使用 while 循环模拟，要像下面这样写：123456789101112131415161718&gt;&gt;&gt; s = 'ABC'&gt;&gt;&gt; it = iter(s)  # ➊&gt;&gt;&gt; while True:...     try:...         print(next(it))  # ➋...     except StopIteration:  # ➌...         del it  # ➍...         break  # ➎...ABC❶ 使用可迭代的对象构建迭代器 it。❷ 不断在迭代器上调用 next 函数，获取下一个字符。❸ 如果没有字符了，迭代器会抛出 StopIteration 异常。❹ 释放对 it 的引用，即废弃迭代器对象。❺ 退出循环。StopIteration 异常表明迭代器到头了Python 语言内部会处理 for 循环和其他迭代上下文(如列表推导、元组拆包等等)中的 StopIteration 异常明确可迭代的对象和迭代器之间的关系：Python 从可迭代的对象中获取迭代器迭代器标准的迭代器接口有两个方法__next__: 返回下一个可用的元素，如果没有元素了，抛出 StopIteration 异常__iter__: 返回 self，以便在应该使用可迭代对象的地方使用迭代器，例如在 for 循环中Iterator 抽象基类实现 __iter__ 方法的方式是返回实例本身(return self)如果想再次迭代，要重新构建迭代器，在需要可迭代对象的地方可以使用迭代器检查迭代器遗留对象因为迭代器只需 __next__ 和 __iter__ 两个方法，所以除了调用 next() 方法，以及捕获 StopIteration 异常之外，没有办法检查是否还有遗留的元素Python 中的迭代器还实现了 __iter__ 方法，因此迭代器也可以迭代使用迭代器模式实现 Sentence 类12345678910111213141516171819202122232425262728293031import reimport reprlibRE_WORD = re.compile('\w+')class Sentence:    def __init__(self, text):       self.text = text       self.words = RE_WORD.findall(text)    def __repr__(self):       return 'Sentence(%s)' % reprlib.repr(self.text)    def __iter__(self):  ➊       return SentenceIterator(self.words)  ➋class SentenceIterator:    def __init__(self, words):       self.words = words  ➌       self.index = 0  ➍    def __next__(self):       try:           word = self.words[self.index]  ➎       except IndexError:           raise StopIteration()  ➏       self.index += 1  ➐       return word  ➑    def __iter__(self):  ➒       return self对这个示例来说，其实没必要在 SentenceIterator 类中实现 __iter__方法，不过这么做是对的，因为迭代器应该实现 __next__ 和 __iter__ 两个方法，而且这么做能让迭代器通过 issubclass(SentenceIterator, abc.Iterator) 测试可迭代的对象有个 __iter__ 方法，每次都实例化一个新的迭代器而迭代器要实现 __next__ 方法，返回单个元素，此外还要实现 __iter__ 方法，返回迭代器本身为了”支持多种遍历”，必须能从同一个可迭代的实例中获取多个独立的迭代器，而且各个迭代器要能维护自身的内部状态，因此这一模式正确的实现方式是，每次调用 iter(my_iterable) 都新建一个独立的迭代器。这就是为什么这个示例需要定义 SentenceIterator 类规约函数接受一个可迭代的对象，然后返回单个结果，这些函数叫归约函数对 all 和 any 函数来说，有一项重要的优化措施是 reduce 函数做不到的，这两个函数会短路(即一旦确定了结果就立即停止使用迭代器)模块函数说明内置all(it)it 中的所有元素都为真值时返回 True，否则返回 Falseall([]) 返回 True内置any(it)只要 it 中有元素为真值就返回 True，否则返回 Falseany([]) 返回 False内置max(it, [key=,] [default=])返回 it 中值最大的元素；*key 是排序函数，与 sorted 函数中的一样如果可迭代的对象为空，返回 default内置min(it, [key=,] [default=])返回 it 中值最小的元素；#key 是排序函数，与 sorted 函数中的一样如果可迭代的对象为空，返回 defaultfunctoolsreduce(func, it, [initial])把前两个元素传给 func，然后把计算结果和第三个元素传给 func，以此类推，返回最后的结果如果提供了 initial，把它当作第一个元素传入内置sum(it, start=0)it 中所有元素的总和，如果提供可选的 start，会把它加上（计算浮点数的加法时，可以使用 math.fsum 函数提高精度）在 Python 中迭代对象 x 时会调用 iter(x)iter 函数还有一个鲜为人知的用法：传入两个参数，使用常规的函数或任何可调用的对象创建迭代器这样使用时，第一个参数必须是可调用的对象，用于不断调用(没有参数)，产出各个值；第二个值是哨符，这是个标记值，当可调用的对象返回这个值时，触发迭代器抛出 StopIteration 异常，而不产出哨符生成器只要 Python 函数的定义体中有 yield 关键字，该函数就是生成器函数生成器函数会创建一个生成器对象，包装生成器函数的定义体把生成器传给 next(...) 函数时，生成器函数会向前，执行函数定义体中的下一个 yield 语句返回产出的值，并在函数定义体的当前位置暂停最终，函数的定义体返回时，外层的生成器对象会抛出 StopIteration 异常——这一点与迭代器协议一致如果一个类只是为了构建生成器而去实现 __iter__ 方法，那还不如使用生成器函数惰性是好的特质，至少在编程语言和 API 中是如此，惰性实现是指尽可能延后生成值这样做能节省内存，而且或许还可以避免做无用的处理12345678910111213141516171819import reimport reprlibRE_WORD = re.compile('\w+')class Sentence: def __init__(self, text):     self.text = text  ➊ def __repr__(self):     return 'Sentence(%s)' % reprlib.repr(self.text) def __iter__(self):     for match in RE_WORD.finditer(self.text):  ➋         yield match.group()  ➌❶ 不再需要 words 列表。❷ finditer 函数构建一个迭代器，包含 self.text 中匹配 RE_WORD 的单词，产出 MatchObject 实例。❸ match.group() 方法从 MatchObject 实例中提取匹配正则表达式的具体文本生成器表达式可以理解为列表推导的惰性版本：不会迫切地构建列表，而是返回一个生成器，按需惰性生成元素也就是说，如果列表推导是制造列表的工厂，那么生成器表达式就是制造生成器的工厂123456789101112131415import reimport reprlibRE_WORD = re.compile('\w+')class Sentence:    def __init__(self, text):        self.text = text    def __repr__(self):        return 'Sentence(%s)' % reprlib.repr(self.text)    def __iter__(self):        return (match.group() for match in RE_WORD.finditer(self.text))最终的效果一样：调用 __iter__ 方法会得到一个生成器对象生成器表达式是语法糖：完全可以替换成生成器函数，不过有时使用生成器表达式更便利生成器表达式是创建生成器的简洁句法，这样无需先定义函数再调用生成器函数灵活得多，可以使用多个语句实现复杂的逻辑，也可以作为协程使用遇到简单的情况时，可以使用生成器表达式如果生成器表达式要分成多行写，我倾向于定义生成器函数，以便提高可读性如果函数或构造方法只有一个参数，传入生成器表达式时不用写一对调用函数的括号再写一对括号围住生成器表达式，只写一对括号就行了迭代器有两个特点接口Python 的迭代器协议定义了两个方法：__next__ 和 __iter__生成器对象实现了这两个方法，因此从这方面来看，所有生成器都是迭代器实现方式生成器这种 Python 语言结构可以使用两种方式编写：含有 yield 关键字的函数，或者生成器表达式调用生成器函数或者执行生成器表达式得到的生成器对象属于语言内部的 GeneratorType 类型迭代器可以节约大量的空间当需要生成的数列数量非常大时，由于代码中将数字存储在 list 中，会导致巨大内存占用12345678910111213141516171819# 生成器class Fab(object):   def __init__(self, max):       self.max = max       self.n, self.a, self.b = 0, 0, 1    def __iter__(self):       return self    def next(self):       if self.n &lt; self.max:           r = self.b           self.a, self.b = self.b, self.a + self.b           self.n = self.n + 1           return r       raise StopIteration()Fab(6)Out[56]: [1, 1, 2, 3, 5, 8]上述代码通过类的形式将函数封装为一个可迭代对象通过next方法在循环的时候每次去取一个数，只有在需要使用的时候才会生成，内存占用很小但是，上述代码较为繁琐，在Python中，有一种语法糖能简化，那就是 yield 如果这个元素可以通过某种方式推算出来切可以进行循环操作，就避免了大的内存占用只需要函数在循环时计算得下一个数字并返回，这样就不必创建完整的 list ，从而节省大量空间在Python中，这种一边循环一边计算的机制，称为生成器：generator1234567891011# yield 语法糖def fab(max):    n, a, b = 0, 0, 1    while n &lt; max:        yield b        # print b        a, b = b, a + b        n = n + 1fab(6)Out[56]: [1, 1, 2, 3, 5, 8]调用和 Fab 版的完全一致，也可以使用 next 方法等。简单的说， yield 的作用就是把一个函数变为一个 generator ，带有 yield 的函数不再是一个普通函数， Python 解释器会将器视为 generator 在 for 循环执行时，每次循环都会执行 fab 函数内部的代码，执行到 yield 时函数就返回一个迭代值，下次迭代时，就从 yield 的下一句继续执行，调用next也是同理当函数执行结束时，会抛出 StopIteration 异常，表示迭代完成itertools模块itertools —- 为高效循环而创建迭代器的函数Python-进阶-itertools模块小结itertools迭代器的特点是：惰性求值（Lazy evaluation），即只有当迭代至某个值时，它才会被计算，这个特点使得迭代器特别适合于遍历大文件或无限集合等，因为我们不用一次性将它们存储在内存中Python 内置的 itertools 模块包含了一系列用来产生不同类型迭代器的函数或类，这些函数的返回都是一个迭代器，我们可以通过 for 循环来遍历取值，也可以使用 next() 来取值itertools 模块提供的迭代器函数有以下几种类型：无限迭代器：生成一个无限序列，比如自然数序列 1, 2, 3, 4, ...有限迭代器：接收一个或多个序列（sequence）作为参数，进行组合、分组和过滤等组合生成器：序列的排列、组合，求序列的笛卡儿积等无限迭代器itertools 模块提供了三个函数（事实上，它们是类）用于生成一个无限序列迭代器count(firstval=0, step=1)创建一个从 firstval (默认值为 0) 开始，以 step (默认值为 1) 为步长的的无限整数迭代器cycle(iterable)对 iterable 中的元素反复执行循环，返回迭代器repeat(object [,times]反复生成 object，如果给定 times，则重复次数为 times，否则为无限IteratorArgumentsResultsExamplecount()start, [step]start, start+step, start+2*step, …count(10) --&gt; 10 11 12 13 14 ...cycle()pp0, p1, … plast, p0, p1, …cycle('ABCD') --&gt; A B C D A B C D ...repeat()elem [,n]elem, elem, elem, … endlessly or up to n timesrepeat(10, 3) --&gt; 10 10 1012345678910111213import itertoolsnums = itertools.count(start=2,step=3)for i in nums:    if i&gt;15:        break    print(i, end=' ')Out[17]: 2 5 8 11 14 [i for i in itertools.repeat(['a','b'],3)]Out[18]: [['a', 'b'], ['a', 'b'], ['a', 'b']]有限迭代器itertools 模块提供了多个函数（类），接收一个或多个迭代对象作为参数，对它们进行组合、分组和过滤等chain()compress()dropwhile()groupby()ifilter()ifilterfalse()islice()imap()starmap()tee()takewhile()izip()izip_longest()IteratorArgumentsResultsExamplechain()p, q, …p0, p1, … plast, q0, q1, …chain('ABC', 'DEF') --&gt; A B C D E Fcompress()data, selectors(d[0] if s[0]), (d[1] if s[1]), …compress('ABCDEF', [1,0,1,0,1,1]) --&gt; A C E Fdropwhile()pred, seqseq[n], seq[n+1], starting when pred failsdropwhile(lambda x: x&lt;5, [1,4,6,4,1]) --&gt; 6 4 1groupby()iterable[, keyfunc]sub-iterators grouped by value of keyfunc(v)ifilter()pred, seqelements of seq where pred(elem) is trueifilter(lambda x: x%2, range(10)) --&gt; 1 3 5 7 9ifilterfalse()pred, seqelements of seq where pred(elem) is falseifilterfalse(lambda x: x%2, range(10)) --&gt; 0 2 4 6 8islice()seq, [start,] stop [, step]elements from seq[start:stop:step]islice('ABCDEFG', 2, None) --&gt; C D E F Gstarmap()func, seqfunc(seq[0]), func(seq[1]), …starmap(pow, [(2,5), (3,2), (10,3)]) --&gt; 32 9 1000tee()it, nit1, it2, … itn splits one iterator into ntakewhile()pred, seqseq[0], seq[1], until pred failstakewhile(lambda x: x&lt;5, [1,4,6,4,1]) --&gt; 1 4zip_longest()p, q, …(p[0], q[0]), (p[1], q[1]), …zip_longest('ABCD', 'xy', fillvalue='-') --&gt; Ax By C- D-1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374[item for item in itertools.chain([1, 2, 3], ['a', 'b', 'c'])]Out[19]: [1, 2, 3, 'a', 'b', 'c']# 接收一个可迭代对象作为参数，返回一个迭代器string = itertools.chain.from_iterable('ABCD')# compress 可用于对数据进行筛选，当selectors的某个元素为true时，则保留data对应位置的元素，否则去除：# compress(data, selectors)list(compress('ABCDEF', [1, 1, 0, 1, 0, 1]))Out[23]:['A', 'B', 'D', 'F']# dropwhile(predicate, iterable)# 其中，predicate 是函数，iterable 是可迭代对象。对于 iterable 中的元素，如果 predicate(item) 为 true，则丢弃该元素，否则返回该项及所有后续项。&gt;&gt;&gt; list(dropwhile(lambda x: x &lt; 5, [1, 3, 6, 2, 1]))[6, 2, 1]&gt;&gt;&gt;&gt;&gt;&gt; list(dropwhile(lambda x: x &gt; 3, [2, 1, 6, 5, 4]))[2, 1, 6, 5, 4]# groupby(iterable[, keyfunc])# 相邻相同元素分组# 其中，iterable 是一个可迭代对象，keyfunc 是分组函数，用于对 iterable 的连续项进行分组，如果不指定，则默认对 iterable 中的连续相同项进行分组，返回一个 (key, sub-iterator) 的迭代器。&gt;&gt;&gt; for key, value_iter in groupby('aaabbbaaccd'):...     print key, ':', list(value_iter)...a : ['a', 'a', 'a']b : ['b', 'b', 'b']a : ['a', 'a']c : ['c', 'c']d : ['d']# ifilter(function or None, sequence)# 将 iterable 中 function(item) 为 True 的元素组成一个迭代器返回，如果 function 是 None，则返回 iterable 中所有计算为 True 的项。&gt;&gt;&gt; list(ifilter(lambda x: x &lt; 6, range(10)))[0, 1, 2, 3, 4, 5]&gt;&gt;&gt;&gt;&gt;&gt; list(ifilter(None, [0, 1, 2, 0, 3, 4]))[1, 2, 3, 4]# ifilterfalse 的使用形式和 ifilter 类似，它将 iterable 中 function(item) 为 False 的元素组成一个迭代器返回，如果 function 是 None，则返回 iterable 中所有计算为 False 的项。&gt;&gt;&gt; list(ifilterfalse(lambda x: x &lt; 6, range(10)))[6, 7, 8, 9]&gt;&gt;&gt;&gt;&gt;&gt; list(ifilter(None, [0, 1, 2, 0, 3, 4]))[0, 0]# islice(iterable, [start,] stop [, step]) # 其中，iterable 是可迭代对象，start 是开始索引，stop 是结束索引，step 是步长，start 和 step 可选。&gt;&gt;&gt; list(islice([10, 6, 2, 8, 1, 3, 9], 5))[10, 6, 2, 8, 1]&gt;&gt;&gt;&gt;&gt;&gt; list(islice(count(), 6))[0, 1, 2, 3, 4, 5]&gt;&gt;&gt;&gt;&gt;&gt; list(islice(count(), 3, 10))[3, 4, 5, 6, 7, 8, 9]&gt;&gt;&gt; list(islice(count(), 3, 10 ,2))[3, 5, 7, 9]# tee 用于从 iterable 创建 n 个独立的迭代器，以元组的形式返回，n 的默认值是 2。iter1, iter2 = tee('abcde')# n 默认为 2，创建两个独立的迭代器# takewhile(predicate, iterable)# 其中，predicate 是函数，iterable 是可迭代对象。对于 iterable 中的元素，如果 predicate(item) 为 true，则保留该元素，只要 predicate(item) 为 false，则立即停止迭代。&gt;&gt;&gt; list(takewhile(lambda x: x &lt; 5, [1, 3, 6, 2, 1]))[1, 3]&gt;&gt;&gt; list(takewhile(lambda x: x &gt; 3, [2, 1, 6, 5, 4]))[]from itertools import zip_longest[item for item in zip_longest('ABCD', 'xy')]Out[41]: [('A', 'x'), ('B', 'y'), ('C', None), ('D', None)][item for item in zip_longest('ABCD', 'xy',fillvalue='-')]Out[42]: [('A', 'x'), ('B', 'y'), ('C', '-'), ('D', '-')]itertools.groupby用法1234567891011121314# 分组统计，并获取每组的具体元素import refrom itertools import groupbysentence = '我是 哈哈  一直在  我 三连击 陈飒飒  阿豆腐干   阿苏打水丢阿萨德'res= itertools.groupby(sorted(re.sub(' +',' ',sentence).split(' '),key=len,reverse=True),len)for k,v in res:print('%s-&gt;%s'%(k,list(v)))8-&gt;['阿苏打水丢阿萨德']4-&gt;['阿豆腐干']3-&gt;['一直在', '三连击', '陈飒飒']2-&gt;['我是', '哈哈']1-&gt;['我']itertools.tee用法itertools.tee分裂出来的多个生成器不是线程安全的，不能在多线程里面运行，否则会导致报错，这里给出一个报错的例子：1234567891011121314import itertoolsimport threadingdef generator():    for i in range(1000000):        yield ig = generator()g_1, g_2 = itertools.tee(g, 2)for x in [g_1, g_2]:    threading.Thread(target=sum, args=(x,)).start()多线程安全版本12345678910111213141516171819class ThreadingTee:    def __init__(self, tee_obj, lock):        self.tee_obj = tee_obj        self.lock = lock    def __iter__(self):        return self    def __next__(self):        with self.lock:            return next(self.tee_obj)    def __copy__(self):        return ThreadingTee(self.tee_obj.__copy__(), self.lock)    def threading_tee(iterable, n=2):        &quot;&quot;&quot;tuple of n independent thread-safe iterators&quot;&quot;&quot;        lock = Lock()        return tuple(ThreadingTee(tee_obj, lock) for tee_obj in itertools.tee(iterable, n))组合生成器itertools 模块还提供了多个组合生成器函数，用于求序列的排列、组合等productpermutationscombinationscombinations_with_replacementIteratorArgumentsResultsproduct()p, q, … [repeat=1]cartesian product, equivalent to a nested for-looppermutations()p[, r]r-length tuples, all possible orderings, no repeated elementscombinations()p, rr-length tuples, in sorted order, no repeated elementscombinations_with_replacement()p, rr-length tuples, in sorted order, with repeated elementsproduct('ABCD', repeat=2)AA AB AC AD BA BB BC BD CA CB CC CD DA DB DC DDpermutations('ABCD', 2)AB AC AD BA BC BD CA CB CD DA DB DCcombinations('ABCD', 2)AB AC AD BC BD CDcombinations_with_replacement('ABCD', 2)AA AB AC AD BB BC BD CC CD DD1234567891011121314151617181920212223242526272829303132333435# product 用于求多个可迭代对象的笛卡尔积，它跟嵌套的 for 循环等价。它的一般使用形式如下：# product(iter1, iter2, ... iterN, [repeat=1])# 其中，repeat 是一个关键字参数，用于指定重复生成序列的次数，&gt;&gt;&gt; for item in product('ABCD', 'xy'):...     print(item)('A', 'x')('A', 'y')('B', 'x')('B', 'y')('C', 'x')('C', 'y')('D', 'x')('D', 'y')&gt;&gt;&gt; list(product('ABC', repeat=2))[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'B'), ('B', 'C'), ('C', 'A'), ('C', 'B'), ('C', 'C')]# permutations 用于生成一个排列，它的一般使用形式如下：# permutations(iterable[, r])# 其中，r 指定生成排列的元素的长度，如果不指定，则默认为可迭代对象的元素长度。&gt;&gt;&gt; list(permutations('ABC', 2))[('A', 'B'), ('A', 'C'), ('B', 'A'), ('B', 'C'), ('C', 'A'), ('C', 'B')]&gt;&gt;&gt; list(permutations('ABC'))[('A', 'B', 'C'), ('A', 'C', 'B'), ('B', 'A', 'C'), ('B', 'C', 'A'), ('C', 'A', 'B'), ('C', 'B', 'A')]# combinations 用于求序列的组合，它的使用形式如下：# combinations(iterable, r)# 其中，r 指定生成组合的元素的长度。&gt;&gt;&gt; from itertools import combinations&gt;&gt;&gt; list(combinations('ABC', 2))[('A', 'B'), ('A', 'C'), ('B', 'C')]# combinations_with_replacement 和 combinations 类似，但它生成的组合包含自身元素。&gt;&gt;&gt; from itertools import combinations_with_replacement&gt;&gt;&gt; list(combinations_with_replacement('ABC', 2))[('A', 'A'), ('A', 'B'), ('A', 'C'), ('B', 'B'), ('B', 'C'), ('C', 'C')]其他方法accumulate对iterable对象逐个进行func操作(默认是累加)1itertools.accumulate(iterable[, func, *, initial=None])123456&gt;&gt;&gt; from itertools import accumulate&gt;&gt;&gt; import operator  # operator --- 标准运算符替代函数&gt;&gt;&gt; a = [1,2,3,4,5]&gt;&gt;&gt; b = accumulate(a)  # 默认是累加 这里返回的是一个可迭代对象&gt;&gt;&gt; list(b)   # 强制转化[1, 3, 6, 10, 15]使用现有扩展功能使用内置的itertools可以组合扩展出更多更强大功能的方法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161def take(n, iterable):    &quot;Return first n items of the iterable as a list&quot;    return list(islice(iterable, n))def tabulate(function, start=0):    &quot;Return function(0), function(1), ...&quot;    return imap(function, count(start))def consume(iterator, n):    &quot;Advance the iterator n-steps ahead. If n is none, consume entirely.&quot;    # Use functions that consume iterators at C speed.    if n is None:        # feed the entire iterator into a zero-length deque        collections.deque(iterator, maxlen=0)    else:        # advance to the empty slice starting at position n        next(islice(iterator, n, n), None)def nth(iterable, n, default=None):    &quot;Returns the nth item or a default value&quot;    return next(islice(iterable, n, None), default)def quantify(iterable, pred=bool):    &quot;Count how many times the predicate is true&quot;    return sum(imap(pred, iterable))def padnone(iterable):    &quot;&quot;&quot;Returns the sequence elements and then returns None indefinitely.    Useful for emulating the behavior of the built-in map() function.    &quot;&quot;&quot;    return chain(iterable, repeat(None))def ncycles(iterable, n):    &quot;Returns the sequence elements n times&quot;    return chain.from_iterable(repeat(tuple(iterable), n))def dotproduct(vec1, vec2):    return sum(imap(operator.mul, vec1, vec2))def flatten(listOfLists):    &quot;Flatten one level of nesting&quot;    return chain.from_iterable(listOfLists)def repeatfunc(func, times=None, *args):    &quot;&quot;&quot;Repeat calls to func with specified arguments.    Example:  repeatfunc(random.random)    &quot;&quot;&quot;    if times is None:        return starmap(func, repeat(args))    return starmap(func, repeat(args, times))def pairwise(iterable):    &quot;s -&gt; (s0,s1), (s1,s2), (s2, s3), ...&quot;    a, b = tee(iterable)    next(b, None)    return izip(a, b)def grouper(iterable, n, fillvalue=None):    &quot;Collect data into fixed-length chunks or blocks&quot;    # grouper('ABCDEFG', 3, 'x') --&gt; ABC DEF Gxx    args = [iter(iterable)] * n    return izip_longest(fillvalue=fillvalue, *args)def roundrobin(*iterables):    &quot;roundrobin('ABC', 'D', 'EF') --&gt; A D E B F C&quot;    # Recipe credited to George Sakkis    pending = len(iterables)    nexts = cycle(iter(it).next for it in iterables)    while pending:        try:            for next in nexts:                yield next()        except StopIteration:            pending -= 1            nexts = cycle(islice(nexts, pending))def powerset(iterable):    &quot;powerset([1,2,3]) --&gt; () (1,) (2,) (3,) (1,2) (1,3) (2,3) (1,2,3)&quot;    s = list(iterable)    return chain.from_iterable(combinations(s, r) for r in range(len(s)+1))def unique_everseen(iterable, key=None):    &quot;List unique elements, preserving order. Remember all elements ever seen.&quot;    # unique_everseen('AAAABBBCCDAABBB') --&gt; A B C D    # unique_everseen('ABBCcAD', str.lower) --&gt; A B C D    seen = set()    seen_add = seen.add    if key is None:        for element in ifilterfalse(seen.__contains__, iterable):            seen_add(element)            yield element    else:        for element in iterable:            k = key(element)            if k not in seen:                seen_add(k)                yield elementdef unique_justseen(iterable, key=None):    &quot;List unique elements, preserving order. Remember only the element just seen.&quot;    # unique_justseen('AAAABBBCCDAABBB') --&gt; A B C D A B    # unique_justseen('ABBCcAD', str.lower) --&gt; A B C A D    return imap(next, imap(itemgetter(1), groupby(iterable, key)))def iter_except(func, exception, first=None):    &quot;&quot;&quot; Call a function repeatedly until an exception is raised.    Converts a call-until-exception interface to an iterator interface.    Like __builtin__.iter(func, sentinel) but uses an exception instead    of a sentinel to end the loop.    Examples:        bsddbiter = iter_except(db.next, bsddb.error, db.first)        heapiter = iter_except(functools.partial(heappop, h), IndexError)        dictiter = iter_except(d.popitem, KeyError)        dequeiter = iter_except(d.popleft, IndexError)        queueiter = iter_except(q.get_nowait, Queue.Empty)        setiter = iter_except(s.pop, KeyError)    &quot;&quot;&quot;    try:        if first is not None:            yield first()        while 1:            yield func()    except exception:        passdef random_product(*args, **kwds):    &quot;Random selection from itertools.product(*args, **kwds)&quot;    pools = map(tuple, args) * kwds.get('repeat', 1)    return tuple(random.choice(pool) for pool in pools)def random_permutation(iterable, r=None):    &quot;Random selection from itertools.permutations(iterable, r)&quot;    pool = tuple(iterable)    r = len(pool) if r is None else r    return tuple(random.sample(pool, r))def random_combination(iterable, r):    &quot;Random selection from itertools.combinations(iterable, r)&quot;    pool = tuple(iterable)    n = len(pool)    indices = sorted(random.sample(xrange(n), r))    return tuple(pool[i] for i in indices)def random_combination_with_replacement(iterable, r):    &quot;Random selection from itertools.combinations_with_replacement(iterable, r)&quot;    pool = tuple(iterable)    n = len(pool)    indices = sorted(random.randrange(n) for i in xrange(r))    return tuple(pool[i] for i in indices)def tee_lookahead(t, i):    &quot;&quot;&quot;Inspect the i-th upcomping value from a tee object    while leaving the tee object at its current position.    Raise an IndexError if the underlying iterator doesn't    have enough values.    &quot;&quot;&quot;    for value in islice(t.__copy__(), i, None):        return value    raise IndexError(i)自定义扩展1234567891011121314# 将序列按大小切分,更好的性能from itertools import chain, islicedef chunks(iterable, size, format=iter):    it = iter(iterable)    while True:        yield format(chain((it.next(),), islice(it, size - 1)))&gt;&gt;&gt; l = [&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;, &quot;e&quot;, &quot;f&quot;, &quot;g&quot;]&gt;&gt;&gt; for chunk in chunks(l, 3, tuple):        print(chunk)(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)(&quot;d&quot;, &quot;e&quot;, &quot;f&quot;)(&quot;g&quot;,)python协程谈谈Python协程技术的演进谈谈Python协程技术的演进Python协程从入门到放弃到死亡到重生Python协程的演化-从yield/send到async/await协程的实现方式有很多，这里我们来列举三种基本方式利用 yield 来实现协程利用 greenlet 模块实现利用 gevent 模块实现并行与并发并行是指两个或者多个事件在同一时刻发生，通常是当系统有一个以上CPU或CPU核心时，才有可能并行两个或者多个事件、线程并步抢占CPU资源并发是指两个或者多个事件在同一时间间隔内发生在一定时间间隔内，有多个程序在执行，但在同一时刻，只有一个程序在执行生成器和协程协程的概念协程通过允许多个入口点在某些位置暂停和恢复执行来概括用于非抢占式多任务的子程序yield 关键字已经可以暂停执行，如果在暂停后有方法把一些值发送回到暂停窒息的函数中，那么便就可以理解为协程添加了把东西发回已经暂停的生成器中的方法，这个方法就是 send()生成器和协程从语法上讲，生成器是一个带yield语句的函数。协程，又称微线程，纤程，英文Coroutine从某些角度来理解，协程其实就是一个可以暂停执行的函数，并且可以恢复继续执行生成器和协程都是通过 python 中 yield 的关键字实现的next()和send()生成器只调用 next 来不断的生成数据，而协程会调用 next 和 send 来返回结果和接收参数与.__next__() 方法一样，.send() 方法致使生成器前进到下一个 yield 语句不过.send() 方法还允许使用生成器的客户把数据发给自己，即不管传给 .send() 方法什么参数，那个参数都会成为生成器函数定义体中对应的 yield 表达式的值也就是说，.send() 方法允许在客户代码和生成器之间双向交换数据而 .__next__() 方法只允许客户从生成器中获取数据Python 新引入的 yield from 句法允许生成器或协程把工作委托给第三方完成，这样就无需嵌套 for 循环作为变通了123456789101112131415161718192021222324252627282930313233343536import random,timedef stupid_fib(n):    index = 0    a = 0    b = 1    while index &lt; n:        sleep_cnt = yield b        print('let me think {0} secs'.format(sleep_cnt))        time.sleep(sleep_cnt)        a, b = b, a + b        index += 1        print('-'*10 + 'test yield send' + '-'*10)N = 5sfib = stupid_fib(N)fib_res = next(sfib)while True:    print(fib_res)    try:        fib_res = sfib.send(random.uniform(0, 0.5))    except StopIteration:        break    ----------test yield send----------1let me think 0.2438615286011866 secs1let me think 0.027476256830278822 secs2let me think 0.09717699872403579 secs3let me think 0.017161862262742633 secs5let me think 0.3313821890336833 secs其中 next(sfib) 相当于 sfib.send(None)，可以使得 sfib 运行至第一个 yield 处返回后续将一个随机的秒数发送给 sfib ，作为当前中断的 yield 表达式的返回值yield 表达式的作用包含了三个步骤：向函数外抛出值暂停，等待 next() 或 send() 恢复赋值，接受 send 发送过来的数据需要注意的是，在使用 send(None) 或者 next() 预激生成器函数，并执行到第一个 yield 语句结束的位置时，实际程序只执行了1、2两步，程序返回了值，并暂停，并没有执行第三步去赋值。在后续的循环中，才会进行赋值。生成器对象有 send 、throw 和 close 方法，这三个方法的作用分别是：发送数据给生成器并赋值给 yield 语句向生成器中抛入异常由生成器内部处理终止生成器这三个方法使得生成器进化成协程协程状态协程有四种存在状态：GEN_CREATED 创建完成，等待执行GEN_RUNNING 解释器正在执行（这个状态在下面的示例程序中无法看到）GEN_SUSPENDED 在 yield 表达式处暂停GEN_CLOSE 执行结束，生成器停止可以使用 inspect.getgeneratorstate 方法查看协程的当前状态12345678910111213141516171819202122232425262728293031323334In [202]: import inspectIn [203]: def generator():     ...:     i = '激活生成器'     ...:     while True:     ...:         try:     ...:             value = yield i     ...:         except ValueError:     ...:             print('OVER')     ...:         i = value     ...:In [204]: g = generator()  # 1In [205]: inspect.getgeneratorstate(g)  # 2Out[205]: 'GEN_CREATED'In [206]: next(g)  # 3Out[206]: '激活生成器'In [207]: inspect.getgeneratorstate(g)Out[207]: 'GEN_SUSPENDED'In [208]: g.send('Hello Shiyanlou')  # 4Out[208]: 'Hello Shiyanlou'In [209]: g.throw(ValueError)  # 5OVEROut[209]: 'Hello Shiyanlou'In [210]: g.close()  # 6In [211]: inspect.getgeneratorstate(g)Out[211]: 'GEN_CLOSED'代码说明创建生成器查看生成器状态这步操作叫做预激生成器（或协程），这是必须做的。在生成器创建完成后，需要将其第一次运行到 yield 语句处暂停暂停状态的生成器可以使用 send 方法发送数据，此方法的参数就是 yield 表达式的值也就是 yield 表达式等号前面的 value 变量的值变成 ‘Hello Shiyanlou’，继续向下执行完一次 while 循环，变量 i 被赋值，继续运行下一次循环，yield 表达式弹出变量 i向生成器抛入异常，异常会被 try except 捕获，作进一步处理close 方法终止生成器，异常不会被抛出因为生成器的调用方也就是程序员自己可以控制生成器的启动、暂停、终止，而且可以向生成器内部传入数据所以这种生成器又叫做协程，generator 函数既可以叫做生成器函数，也可以叫协程函数，这是生成器向协程的过渡阶段yield from在 Python3.3 出现了 yield from 语法, yield from item 表达式从 item 中获得迭代器yield from 可以代替 for 循环，使得代码更为精炼，yield from 后面需要加的是可迭代对象yield from i 完全代替了内层的 for 循环。而且代码读起来更顺畅，不过感觉更像是语法糖除了代替循环之外，yield from 还会创建通道，把内层生成器直接与外层生成器的客户端联系起来把生成器当成协程使用时，这个通道特别重要，不仅能为客户端代码生成值，还能使用客户端代码提供的值123456789101112131415# yield fromdef first_gen():    for c in &quot;AB&quot;:        yield c    for i in range(0, 3):        yield iprint(list(first_gen()))def second_gen():    yield from &quot;AB&quot;    yield from range(0, 3)print(list(second_gen()))['A', 'B', 0, 1, 2]['A', 'B', 0, 1, 2]当 yiled from 后面加上一个生成器之后，就实现了生成的嵌套。实现生成器的嵌套，不一定要使用 yield from但它可以让我们避免让自己处理各种料想不到的异常，如果自己去实现，会加大编码的难度。yield from 的主要功能是打开双向通道，把最外层的调用与最内层的子生成器连接起来这样二者就可以直接发送和产出值，还可以直接穿入异常委派生成器在 yied from 表达式处暂停时，调用方可以直接把数据发给子生成器子生成器再把产出值发给调用方，子生成器返回之后，解释器会抛出 StopIteration 异常委托生成器的作用就是：在调用方与子生成器之间建立一个双向通道为什么一定要使用 yield from 语句呢：在使用 yiled from 语句时，语句为我们已经处理了很多的异常yield from 获取返回值python协程系列（三）——yield from原理详解在使用yield生成器的时候，如果使用for语句去迭代生成器，则不会显式的出发StopIteration异常，而是自动捕获StopIteration异常所以如果遇到return，只是会终止迭代，而不会触发异常，故而也就没办法获取return的值123456789101112131415161718192021def my_generator():    for i in range(5):        if i==2:            return '我被迫中断了'        else:            yield idef main(generator):    try:        for i in generator:  #不会显式触发异常，故而无法获取到return的值            print(i)    except StopIteration as exc:        print(exc.value)g=my_generator()  #调用main(g)'''运行结果为：01'''从上面的例子可以看出，for迭代语句不会显式触发异常，故而无法获取到return的值迭代到2的时候遇到return语句，隐式的触发了StopIteration异常，就终止迭代了，但是在程序中不会显示出来12345678910111213141516171819202122232425def my_generator():    for i in range(5):        if i==2:            return '我被迫中断了'        else:            yield idef main(generator):    try:        print(next(generator))   #每次迭代一个值，则会显式出发StopIteration        print(next(generator))        print(next(generator))        print(next(generator))        print(next(generator))    except StopIteration as exc:        print(exc.value)     #获取返回的值g=my_generator()main(g)'''运行结果为：01我被迫中断了'''现在我们使用yield from来完成上面的同样的功能123456789101112131415161718192021222324def my_generator():    for i in range(5):        if i==2:            return '我被迫中断了'        else:            yield idef wrap_my_generator(generator):  #定义一个包装“生成器”的生成器，它的本质还是生成器    result = yield from generator    #自动触发StopIteration异常，并且将return的返回值赋值给yield from表达式的结果    print(result)def main(generator):    for j in generator:        print(j)g = my_generator()wrap_g = wrap_my_generator(g)main(wrap_g)  #调用'''运行结果为：01我被迫中断了'''从上面的比较可以看出，yield from具有以下几个特点：调用方—&gt;生成器函数(协程函数)调用方—&gt;生成器包装函数—&gt;生成器函数(协程函数)return返回的值或者是StopIteration的value 属性的值变成 yield from 表达式的值，即上面的resultgreenlet 和 geventgreenlet可以实现协程，不过每一次都要人为去指向下一个该执行的协程greenlet 可以从一个协程切换到任意其他协程，但必须保证 greenlet 的正常结束，在协程之间的任意切换很容易出现问题greelet 是 Stackless 发展来的 Cpython 扩展包， greelet 是底层实现了原生协程的C扩展库迭代器(即可指子生成器)产生的值直接返还给调用者任何使用send()方法发给委派生产器(即外部生产器)的值被直接传递给迭代器。如果send值是None，则调用迭代器next()方法；如果不为None，则调用迭代器的send()方法。如果对迭代器的调用产生Stoplteration异常，委派生产器恢复继续执行yield from后面的语句；若迭代器产生其他任何异常，则都传递给委派生产器子生成器可能只是一个迭代器，并不是一个作为协程的生成器，所以它不支持.throw()和.close()方法，即可能会产生AttributeError异常除了GeneratorExit异常外的其他抛给委派生产器的异常，将会被传递到迭代器的throw()方法，如果迭代器throw()调用产生了Stoplteration异常，委派生产器恢复并继续执行，其他异常则传递给委派生产器如果GeneratorExit异常被抛给委派生产器，或者委派生产器的close()方法被调用，如果迭代器有close()的话也将被调用。如果close()调用产生异常，异常将传递给委派生产器。否则，委派生产器将抛出GeneratorExit 异常当迭代器结束并抛出异常时，yield from表达式的值是其Stoplteration异常中的第一个参数一个生成器中的return expr语句将会从生成器退出并抛出 Stoplteration(expr)异常greenlet12345678910111213141516171819202122232425262728293031# 使用 greenlet 实现的 生产者-消费者 模型：# greenlet 的价值在于高性能的原生协程，且语义更加明确、显式切换，执行到 switch 时就切换程序# 直接将函数包装成协程，可以保留原代码的风格# 基于greenlet的生产者消费者协程from greenlet import greenletimport randomimport timedef Producer():    while True:        item = random.randint(1, 10)        print(&quot;生产&lt;{}&gt;中...&quot;.format(item))        time.sleep(1)        c.switch(item)  # 切换到消费者，并将item传入。def Consumer():    while True:        item = p.switch()  # 切换到生产者。等待生产者传递参数item        print(&quot;消费&lt;{}&gt;中..&quot;.format(item))c = greenlet(Consumer)  # 将普通函数编程协程p = greenlet(Producer)  # 同理c.switch()  # 启动协程，Consumer先执行&quot;&quot;&quot;从consumer开始执行，执行到item=p.switch()时，程序切换到producer，并等待传参producer得到执行权后，生成一个item,并往下执行代码当producer执行到c.switch(item)时，程序携带传递的item切换到consumer,consumer继续往下执行，直到下一次运行到p.switch时，交出执行权，切换到producer，重复以上过程&quot;&quot;&quot;geventgevent 是实现协程的第三方库，通过封装 greenlet，epoll 回调编程模式，生成器协程实现当遇到 IO 操作时，就自动切换到其他协程，等到 IO 操作完成，再在适当的时候切换回来继续执行gevent 会自动切换协程，就保证总有协程在执行，而不是等待 IO由于切换实在 IO 操作时自动完成，所以 gevent 需要修改 Python 的自带的一些保准库，这一过程在启动时通过 monkey patch 完成gevent的价值在于它的使用基于epoll的libev来避开阻塞；使用基于gevent的高效协程，来切换执行只在遇到阻塞的时候切换，没有轮询和线程开销1234567891011121314151617181920212223242526272829303132333435&quot;&quot;&quot; gevent: 通过greenlet实现协程，核心就是遇到IO操作，会自动切换到其他协程 &quot;&quot;&quot;# 将python标准库中的一些阻塞操作变为非阻塞from gevent import monkey;monkey.patch_all()# 使用猴子补丁要写在第一行import geventdef test1():    print(&quot;test1&quot;)    gevent.sleep(0)  # 模拟耗时操作    print(&quot;test11&quot;)def test2():    print(&quot;test2&quot;)    gevent.sleep(0)  # 模拟耗时操作    print(&quot;test22&quot;)# g1 = gevent.spawn(test1)  # 将函数封装成协程，并启动# g2 = gevent.spawn(test2)# gevent.joinall([g1, g2])&quot;&quot;&quot;# joinall() 阻塞当前流程，执行给定的greenlet(列表中的对象),等待程序执行完# spawn是启动协程，参数为函数名及其参数test1test2test11test22代码执行test1,打印test1，遇到gevent.sleep(0)时切换程序，执行test2test()执行，打印test2，执行到gevent.sleep(0)时切换程序执行test1在gevent.sleep(0)后面的代码，直到再次遇到gevent时，切换程序然后在test2中，继续执行gevent后的代码，直到遇到gevent时，再次切换直到程序执行完毕&quot;&quot;&quot;asyncio.coroutine在 Python3.4 中加入了 asyncio 库，使得 Python 获得了事件循环的特性，但这个还是以生成器对象为基础yield from 在 asyncio 模块中很常用，通过 asnyncio+生成器 ，我们可以实现这样一个异步的模型：12345678910111213141516171819202122232425262728293031323334import asyncio@asyncio.coroutinedef counttdown(number, n):    while n &gt; 0:        print(&quot;T-minus&quot;, n, &quot;({})&quot;.format(number))        yield from asyncio.sleep(1)        n -= 1loop = asyncio.get_event_loop()tasks = [    asyncio.ensure_future(counttdown(&quot;A&quot;, 2)),    asyncio.ensure_future(counttdown(&quot;B&quot;, 5)),]loop.run_until_complete(asyncio.wait(tasks))loop.close()【out】Traceback (most recent call last):  File &quot;&lt;ipython-input-2-cff1e6420e75&gt;&quot;, line 15, in &lt;module&gt;    loop.run_until_complete(asyncio.wait(tasks))  File &quot;S:\Anaconda3\lib\asyncio\base_events.py&quot;, line 555, in run_until_complete    self.run_forever()  File &quot;S:\Anaconda3\lib\asyncio\base_events.py&quot;, line 510, in run_forever    raise RuntimeError('This event loop is already running')RuntimeError: This event loop is already runningT-minus 2 (A)T-minus 5 (B)T-minus 1 (A)T-minus 4 (B)T-minus 3 (B)T-minus 2 (B)T-minus 1 (B)这里 asyncio.coroutine 装饰器是用来标记这个函数是一个协程，因为 asyncio 要求所有用作协程的生成器必须由 asyncio.coroutine 装饰这段代码中，事件循环会启动两个 countdown() 协程，它们会一直执行，知道遇到 yield from asyncio.sleep() ，暂停执行，并将一个 async.Future 对象返回给事件循环事件循环会监控这个 asyncio.Future 对象，一旦执行完成后，会将这个 Future 的执行结果返回给刚刚因为这个 Futur e暂停的协程，并且继续执行原协程event_loop 事件循环：程序开启一个无限的循环，程序员会把一些函数注册到事件循环上。当满足事件发生的时候，调用相应的协程函数coroutine 协程：协程对象，指一个使用async关键字定义的函数，它的调用不会立即执行函数，而是会返回一个协程对象协程对象需要注册到事件循环，由事件循环调用task 任务：一个协程对象就是一个原生可以挂起的函数，任务则是对协程进一步封装，其中包含任务的各种状态协程对象不能直接运行，在注册事件循环的时候，其实是 run_until_complete 方法将协程包装成一个任务（ task ）对象task 对象是 Future 的子类，保存了协程运行后的状态，用于未来获取协程的结果1234567891011121314151617181920212223242526272829在上面的代码中， asyncio.sleep 中，创建了一个 Futrure 对象，作为更内层的协程对象，通过 yield from 交给了事件循环，而 Future 是一个实现了 __iter__ 对象的生成器。@coroutinedef sleep(delay, result=None, *, loop=None):    &quot;&quot;&quot;Coroutine that completes after a given time (in seconds).&quot;&quot;&quot;    future = futures.Future(loop=loop)    h = future._loop.call_later(delay,                                future._set_result_unless_cancelled, result)    try:        return (yield from future)    finally:        h.cancel() class Future:#blabla...    def __iter__(self):        if not self.done():            self._blocking = True            yield self  # This tells Task to wait for completion.        assert self.done(), &quot;yield from wasn't used with future&quot;        return self.result()  # May raise too.# 当协程 yield from asyncio.sleep 时，事件循环其实是与 Future 对象建立了联系。程序运行结果如下：T-minus 2 (A)T-minus 5 (B)T-minus 1 (A)T-minus 4 (B)T-minus 3 (B)T-minus 2 (B)T-minus 1 (B)async 和 awaithello world123456789101112131415161718192021222324252627282930313233343536373839import asyncioimport timeasync def main():    print(f&quot;started at {time.strftime('%X')}&quot;)    await say_after(1, 'hello')    await say_after(2, 'world')    print(f&quot;finished at {time.strftime('%X')}&quot;)asyncio.run(main())# 预期输出started at 17:13:52helloworldfinished at 17:13:55async def main():    task1 = asyncio.create_task(        say_after(1, 'hello'))    task2 = asyncio.create_task(        say_after(2, 'world'))    print(f&quot;started at {time.strftime('%X')}&quot;)    # Wait until both tasks are completed (should take around 2 seconds.)    await task1    await task2    print(f&quot;finished at {time.strftime('%X')}&quot;)# 预期的输出显示代码段的运行时间比之前快了 1 秒started at 17:14:32helloworldfinished at 17:14:34asyncio模块历史演进asyncio是python3.4引入的库，翻译过来就是异步I/O用await代替yield from，功能一模一样，程序调度装饰器@asyncio.coroutine和关键字async1234567@asyncio.coroutinedef func1():    yield from asyncio.sleep(2)  # 遇到IO耗时操作，自动化切换到tasks中的其他任务# 等价于async def func1():    yield from asyncio.sleep(2)  # 遇到IO耗时操作，自动化切换到tasks中的其他任务Event loop123456# 版本3.7以前loop = asyncio.get_event_loop() # 创建一个事件循环loop.run_until_complete(result) # 将协程当做任务提交到事件循环的任务列表中，协程执行完成之后终止# 版本3.7引入asyncio.run(result)在 Python3.5 中引入了 async 和 await ，可以将它们理解为 asyncio.coroutine / yield from 的完美替身， async/await 让协程表面上独立于生成器而存在，将细节隐藏于 asyncio 模块之下。使用 await 可以针对耗时的操作进行挂起，类似于生成器里的 yield 一样，使函数让出控制权协程遇到 await ，事件循环挂起该协程，直到其他协程也挂起或者执行完毕，再进行下一个协程的执行。耗时的操作一般是一些 IO 操作，如网络请求，文件读取等如果一个对象可以在 await 语句中使用，那么它就是 可等待 对象可等待对象有三种主要类型: 协程, 任务 和 Future协程: Python 协程属于 可等待 对象，因此可以在其他协程中被等待1234async def nested():    return 42await nested()任务: 被用来“并行的”调度协程，当一个协程通过 asyncio.create_task() 等函数被封装为一个 任务，该协程会被自动调度执行:123456# apiasyncio.create_task(coro, *, name=None, context=None)# 调用task = asyncio.create_task(nested())await taskFutures: Future 是一种特殊的 低层级 可等待对象，表示一个异步操作的 最终结果当一个 Future 对象 被等待，这意味着协程将保持等待直到该 Future 对象在其他地方操作完毕在 asyncio 中需要 Future 对象以便允许通过 async/await 使用基于回调的代码通常情况下 没有必要 在应用层级的代码中创建 Future 对象Future 对象有时会由库和某些 asyncio API 暴露给用户，用作可等待对象12345678async def main():    await function_that_returns_a_future_object()    # this is also valid:    await asyncio.gather(        function_that_returns_a_future_object(),        some_python_coroutine()    )一个很好的返回对象的低层级函数的示例是 loop.run_in_executor()这里可以使用 asyncio.sleep 来进行模拟举例：1234567891011121314151617181920212223242526272829303132333435363738import asyncioimport timenow = lambda: time.time()async def do_some_work(x):    print('Waiting: ', x)    await asyncio.sleep(x)    return 'Done after {}s'.format(x)start = now()coroutine1 = do_some_work(1)coroutine2 = do_some_work(2)coroutine3 = do_some_work(4)tasks = [    asyncio.ensure_future(coroutine1),    asyncio.ensure_future(coroutine2),    asyncio.ensure_future(coroutine3)]loop = asyncio.get_event_loop()loop.run_until_complete(asyncio.wait(tasks))for task in tasks:    print('Task ret: ', task.result())print('TIME: ', now() - start)Waiting:  1Waiting:  2Waiting:  4Task ret:  Done after 1sTask ret:  Done after 2sTask ret:  Done after 4sTIME:  4.003541946411133在 sleep 的时候，使用 await 让出控制权。当遇到阻塞调用的函数的时候，使用 await 方法将协程的控制权让出，以便 loop 调用其他的协程注意的区别是： await 接受的对象必须是一个 awaitable 的对象，所谓 awaitable 的对象，就是一个实现了 await() 方法的对象，而且这个方法必须返回一个不是协程的迭代器在 Python3.6 中， yield 和 await 也可以在同一个函数中使用，初次之外，也可以在列表推导等地方使用 async for 或 await 语法1234567result = [i async for i in aiter() if i % 2]result = [await func() for fun in funcs if await condition()]async def test(x, y):    for i in range(y):        yield i        await asyncio.sleep(x)协程与异步与多线程编程不同的是，多个协程总是运行在同一个线程中，一旦其中的一个协程发生阻塞行为，进而所有的协程都无法继续运行例如在我们进行爬虫编写时，习惯使用 requests 库，而这个库就是阻塞的尝试使用协程的方式进行编写：123456789101112131415161718192021222324252627282930313233import asyncioimport requestsimport time start = time.time() async def get(url):    return requests.get(url) async def request():    url = 'http://127.0.0.1:5000'    print('Waiting for', url)    response = await get(url)    print('Get response from', url, 'Result:', response.text) tasks = [asyncio.ensure_future(request()) for _ in range(5)]loop = asyncio.get_event_loop()loop.run_until_complete(asyncio.wait(tasks)) end = time.time()print('Cost time:', end - start)Waiting for http://127.0.0.1:5000Get response from http://127.0.0.1:5000 Result: Hello!Waiting for http://127.0.0.1:5000Get response from http://127.0.0.1:5000 Result: Hello!Waiting for http://127.0.0.1:5000Get response from http://127.0.0.1:5000 Result: Hello!Waiting for http://127.0.0.1:5000Get response from http://127.0.0.1:5000 Result: Hello!Waiting for http://127.0.0.1:5000Get response from http://127.0.0.1:5000 Result: Hello!Cost time: 15.134317874908447而不使用协程，使用普通方式，也是这个时间。为什么会这样呢，究其原因是 requests 并不支持异步操作。在运行时阻塞并未挂起另外 await 后面所跟的对象必须是：一个原生 coroutine 对象，一个由 types.coroutine 装饰的生成器，这个生成器可以返回 coroutine 对象而 requests 返回的对象不符合上述条件。为了程序运行不报错，上面代码在 await 时对 requsts 进行了一次 async 函数的包装，但是它并不是“原生的coroutine对象”，因此也就不能真正异步了可以通过使用实现了异步的 aiohttp 或者 Trip 库改写上述爬虫12345678910111213141516171819202122232425262728293031import asyncioimport timeimport aiohttpfrom spider_normal import targets, show_resultsfinal_results = {}async def get_content(url):    async with aiohttp.ClientSession() as session:        async with session.get(url) as resp:            content = await resp.read()            return len(content)async def spider(url):    length = await get_content(url)    final_results[url] = length    return Truedef main():    loop = asyncio.get_event_loop()    cor = [spider(url) for url in targets]    start_time = time.time()    result = loop.run_until_complete(asyncio.gather(*cor))    print(&quot;Use time: {:.2f}s&quot;.format(time.time() - start_time))    show_results(final_results)    print(&quot;loop result: &quot;, result)if __name__ == '__main__':    main()12345678910111213141516171819202122232425262728293031323334353637383940414243444546# lightless @ LL-DESKTOP in C:\Users\lightless\Desktop [22:49:11] $ python .\spider_asyncio.py Use time: 2.23s Length:   227  URL:https://www.baidu.com/ Length:  11759 URL:https://www.zhihu.com/ Length:  40740 URL:https://lightless.me/archives/python-coroutine-from-start-to-boom.html Length: 150227 URL:https://github.com/aio-libs -engtn: LO1 Length:  59391 URL:https://vww.python.org/dev/peps/pep-0380/ loop result: [True,True,True,True,True] # lightless @ LL-DESKTOP in C:\Users\lightless\Desktop [22:49:15] $ python .\spider_asyncio.py Use time: 1.62s Length:  11759 URL:https://www.zhihu.com/ Length:   227  URL:https://www.baidu.com/ Length:  40830 URL:https://lightless.me/archives/python-coroutine-from-start-to-boom.html Length:  59391 URL:https://ww.python.org/dev/peps/pep-0380/ Length: 150227 URL:https://github.com/aio-libs loop result: [True,True,True,True,True]# lightless @ LL-DESKTOP in C:\Users\lightless\Desktop [22:49:20] $ python .\spider_asyncio.py Use time: 1.59s Length:  11759 URL:https://ww.zhihu.com/ Length:   227  URL:https://www.baidu.com/ Length:  40629 URL:https://lightless.me/archives/python-coroutine-from-start-to-boom.html Length:  59391 URL:https://ww.python.org/dev/peps/pep-0380/ Length: 150219 URL:https://github.com/aio-libs loop result: [True,True,True,True.True] # lightless @ LL-DESKToP in C:\Users\lightless\Desktop [22:49:23] $ python .\spider_asyncio.py Use time: 4 70s Length:   227 URL:https://www.baidu.com Length: 11759 URL:https://www.zhihu.com/ Lengtn: 59391 URL:https://www.python.org/dev/peps/pep-0380/ Length:150218 URL:https://github.com/aio-libs Length: 40740 URL:https://lightless.me/archives/python-coroutine-from-start-to-boom.html loop result: [True.True,True,True.True] # lightless @ LL-DESKTOP in C:\Users\lightless\Desktop [22:49:29] $ python .\spider_asyncio.py Use time: 1.85s Length:   227 URL:https://www.baidu.com/ Length: 11759 URL:https://www.zhihu.com/ Length: 40777 URL:https://lightless.me/archives/python-coroutine-from-start-to-boom.html Length: 59391 URL:https://www.python.org/dev/peps/pep-0380/ Length:150227 URL:httos://github.com/aio-libsloop result:[True,True,True,True,True] # lightless @ LL-DESKTOP in C:\Users\lightless\Desktop [22:49:32]应用生产者和消费者1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v1.0@author: huangyc@file: async_test.py@Description: @time: 2023/4/6 10:31&quot;&quot;&quot;import asyncio, randomimport threadingrandom.seed(5)async def rnd_sleep(t):    # sleep for T seconds on average    await asyncio.sleep(t * random.random() * 2)async def producer(queue):    lst = list(range(10))    for token in lst:        # produce a token and send it to a consumer        print(f'produced {token}')        await queue.put(token)        await rnd_sleep(.1)async def consumer(queue):    while True:        token = await queue.get()        # process the token received from a producer        await rnd_sleep(.1)        queue.task_done()        print(f'consumed {token}')async def main():    queue = asyncio.Queue()    # fire up the both producers and consumers    producers = [asyncio.create_task(producer(queue)) for _ in range(3)]    consumers = [asyncio.create_task(consumer(queue)) for _ in range(10)]    # with both producers and consumers running, wait for    # the producers to finish    await asyncio.gather(*producers)    print('---- done producing')    # wait for the remaining tasks to be processed    await queue.join()    # cancel the consumers, which are now idle    for c in consumers:        c.cancel()if __name__ == '__main__':    print(&quot;hello&quot;)    # 多线程+协程方式    t1 = threading.Thread(target=asyncio.run, args=(main(),))    t1.start()    t1.join()    # 协程调用    # asyncio.run(main())    print(&quot;end&quot;)多进程、多线程与协程计算密集型：多进程，可以最大限度发挥CPU运算能力IO 密集型：推荐优先使用协程，内存开销少，执行效率高；其次是多线程，虽然不如协程高效，但同样能极大提升程序运行效率CPU 密集和 IO 密集：多进程+协程，既充分利用多核，又充分发挥协程的高效率，可获得极高的性能</content>
	</entry>
	<entry>
		<title>多线程_JUC锁集合</title>
		<url>/article/10fd680e.html</url>
		<content>JUC锁集合同步锁: 即通过synchronized关键字来进行同步，实现对竞争资源的互斥访问的锁。Java 1.0版本中就已经支持同步锁了。对于每一个对象，有且仅有一个同步锁；不同的线程能共同访问该同步锁。但是，在同一个时间点，该同步锁能且只能被一个线程获取到。这样，获取到同步锁的线程就能进行CPU调度，从而在CPU上执行；而没有获取到同步锁的线程，必须进行等待，直到获取到同步锁之后才能继续运行。这就是，多线程通过同步锁进行同步的原理！ JUC包中的锁 : 相比同步锁，JUC包中的锁的功能更加强大，它为锁提供了一个框架，该框架允许更灵活地使用锁，只是它的用法更难罢了。　　JUC包中的锁，包括：Lock接口，ReadWriteLock接口，LockSupport阻塞原语，Condition条件，AbstractOwnableSynchronizer/AbstractQueuedSynchronizer/AbstractQueuedLongSynchronizer三个抽象类，ReentrantLock独占锁，ReentrantReadWriteLock读写锁。由于CountDownLatch，CyclicBarrier和Semaphore也是通过AQS来实现的；Lock接口: JUC包中的 Lock 接口支持那些语义不同(重入、公平等)的锁规则。所谓语义不同，是指锁可是有”公平机制的锁”、”非公平机制的锁”、”可重入的锁”等等。”公平机制”是指”不同线程获取锁的机制是公平的”，而”非公平机制”则是指”不同线程获取锁的机制是非公平的”，”可重入的锁”是指同一个锁能够被一个线程多次获取。 ReadWriteLock: ReadWriteLock 接口以和Lock类似的方式定义了一些读取者可以共享而写入者独占的锁。JUC包只有一个类实现了该接口，即 ReentrantReadWriteLock，因为它适用于大部分的标准用法上下文。但程序员可以创建自己的、适用于非标准要求的实现。 AQS: AbstractOwnableSynchronizer/AbstractQueuedSynchronizer/AbstractQueuedLongSynchronizer 　　AbstractQueuedSynchronizer就是被称之为AQS的类，它是一个非常有用的超类，可用来定义锁以及依赖于排队阻塞线程的其他同步器；ReentrantLock，ReentrantReadWriteLock，CountDownLatch，CyclicBarrier和Semaphore等这些类都是基于AQS类实现的。AbstractQueuedLongSynchronizer 类提供相同的功能但扩展了对同步状态的 64 位的支持。两者都扩展了类 AbstractOwnableSynchronizer（一个帮助记录当前保持独占同步的线程的简单类）。LockSupport: LockSupport提供“创建锁”和“其他同步类的基本线程阻塞原语”。　　LockSupport的功能和”Thread中的Thread.suspend()和Thread.resume()有点类似”，LockSupport中的park() 和 unpark() 的作用分别是阻塞线程和解除阻塞线程。但是park()和unpark()不会遇到“Thread.suspend 和 Thread.resume所可能引发的死锁”问题。 Condition: Condition需要和Lock联合使用，它的作用是代替Object监视器方法，可以通过await(),signal()来休眠/唤醒线程。        Condition 接口描述了可能会与锁有关联的条件变量。这些变量在用法上与使用 Object.wait 访问的隐式监视器类似，但提供了更强大的功能。需要特别指出的是，单个 Lock 可能与多个 Condition 对象关联。为了避免兼容性问题，Condition 方法的名称与对应的 Object 版本中的不同。 ReentrantLock: ReentrantLock是独占锁。所谓独占锁，是指只能被独自占领，即同一个时间点只能被一个线程锁获取到的锁。ReentrantLock锁包括”公平的ReentrantLock”和”非公平的ReentrantLock”。”公平的ReentrantLock”是指”不同线程获取锁的机制是公平的”，而”非公平的　　ReentrantLock”则是指”不同线程获取锁的机制是非公平的”，ReentrantLock是”可重入的锁”。ReentrantReadWriteLock: ReentrantReadWriteLock是读写锁接口ReadWriteLock的实现类，它包括子类ReadLock和WriteLock。ReentrantLock是共享锁，而WriteLock是独占锁。CountDownLatch: CountDownLatch是一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待。CountDownLatch包含了sync对象，sync是Sync类型。CountDownLatch的Sync是实例类，它继承于AQS。CyclicBarrier: CyclicBarrier是一个同步辅助类，允许一组线程互相等待，直到到达某个公共屏障点 (common barrier point)。因为该 barrier 在释放等待线程后可以重用，所以称它为循环 的 barrier。1234CyclicBarrier是包含了&quot;ReentrantLock对象lock&quot;和&quot;Condition对象trip&quot;，它是通过独占锁实现的。CyclicBarrier和CountDownLatch的区别**是：(01) CountDownLatch的作用是允许1或N个线程等待其他线程完成执行；而CyclicBarrier则是允许N个线程相互等待。(02) CountDownLatch的计数器无法被重置；CyclicBarrier的计数器可以被重置后使用，因此它被称为是循环的barrier。Semaphore: Semaphore是一个计数信号量，它的本质是一个”共享锁”。    　信号量维护了一个信号量许可集。线程可以通过调用acquire()来获取信号量的许可；当信号量中有可用的许可时，线程能获取该许可；否则线程必须等待，直到有可用的许可为止。 线程可以通过release()来释放它所持有的信号量许可。和”ReentrantLock”一样，Semaphore包含了sync对象，sync是Sync类型；而且，Sync也是一个继承于AQS的抽象类。Sync也包括”公平信号量”FairSync和”非公平信号量”NonfairSync。互斥锁ReentrantLockReentrantLock是一个可重入的互斥锁，又被称为“独占锁”。12顾名思义，ReentrantLock锁在同一个时间点只能被一个线程锁持有；而可重入的意思是，ReentrantLock锁，可以被单个线程多次获取。ReentrantLock分为“公平锁”和“非公平锁”。它们的区别体现在获取锁的机制上是否公平。“锁”是为了保护竞争资源，防止多个线程同时操作线程而出错，ReentrantLock在同一个时间点只能被一个线程获取(当某线程获取到“锁”时，其它线程就必须等待)；ReentraantLock是通过一个FIFO的等待队列来管理获取该锁所有线程的。在“公平锁”的机制下，线程依次排队获取锁；而“非公平锁”在锁是可获取状态时，不管自己是不是在队列的开头都会获取锁。1234567891011121314151617181920212223242526272829303132333435363738394041// ReentrantLock函数列表// 创建一个 ReentrantLock ，默认是“非公平锁”。ReentrantLock()// 创建策略是fair的 ReentrantLock。fair为true表示是公平锁，fair为false表示是非公平锁。ReentrantLock(boolean fair)// 查询当前线程保持此锁的次数。int getHoldCount()// 返回目前拥有此锁的线程，如果此锁不被任何线程拥有，则返回 null。protected Thread getOwner()// 返回一个 collection，它包含可能正等待获取此锁的线程。protected Collection&lt;Thread&gt; getQueuedThreads()// 返回正等待获取此锁的线程估计数。int getQueueLength()// 返回一个 collection，它包含可能正在等待与此锁相关给定条件的那些线程。protected Collection&lt;Thread&gt; getWaitingThreads(Condition condition)// 返回等待与此锁相关的给定条件的线程估计数。int getWaitQueueLength(Condition condition)// 查询给定线程是否正在等待获取此锁。boolean hasQueuedThread(Thread thread)// 查询是否有些线程正在等待获取此锁。boolean hasQueuedThreads()// 查询是否有些线程正在等待与此锁有关的给定条件。boolean hasWaiters(Condition condition)// 如果是“公平锁”返回true，否则返回false。boolean isFair()// 查询当前线程是否保持此锁。boolean isHeldByCurrentThread()// 查询此锁是否由任意线程保持。boolean isLocked()// 获取锁。void lock()// 如果当前线程未被中断，则获取锁。void lockInterruptibly()// 返回用来与此 Lock 实例一起使用的 Condition 实例。Condition newCondition()// 仅在调用时锁未被另一个线程保持的情况下，才获取该锁。boolean tryLock()// 如果锁在给定等待时间内没有被另一个线程保持，且当前线程未被中断，则获取该锁。boolean tryLock(long timeout, TimeUnit unit)// 试图释放此锁。void unlock()123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108// ReentrantLock示例import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;// LockTest1.java// 仓库class Depot {     private int size;        // 仓库的实际数量    private Lock lock;        // 独占锁    public Depot() {        this.size = 0;        this.lock = new ReentrantLock();    }    public void produce(int val) {        lock.lock();        try {            size += val;            System.out.printf(&quot;%s produce(%d) --&gt; size=%d\n&quot;,                     Thread.currentThread().getName(), val, size);        } finally {            lock.unlock();        }    }    public void consume(int val) {        lock.lock();        try {            size -= val;            System.out.printf(&quot;%s consume(%d) &lt;-- size=%d\n&quot;,                     Thread.currentThread().getName(), val, size);        } finally {            lock.unlock();        }    }}; // 生产者class Producer {    private Depot depot;        public Producer(Depot depot) {        this.depot = depot;    }    // 消费产品：新建一个线程向仓库中生产产品。    public void produce(final int val) {        new Thread() {            public void run() {                depot.produce(val);            }        }.start();    }}// 消费者class Customer {    private Depot depot;        public Customer(Depot depot) {        this.depot = depot;    }    // 消费产品：新建一个线程从仓库中消费产品。    public void consume(final int val) {        new Thread() {            public void run() {                depot.consume(val);            }        }.start();    }}public class LockTest1 {      public static void main(String[] args) {          Depot mDepot = new Depot();        Producer mPro = new Producer(mDepot);        Customer mCus = new Customer(mDepot);        mPro.produce(60);        mPro.produce(120);        mCus.consume(90);        mCus.consume(150);        mPro.produce(110);    }}Thread-0 produce(60) --&gt; size=60Thread-1 produce(120) --&gt; size=180Thread-3 consume(150) &lt;-- size=30Thread-2 consume(90) &lt;-- size=-60Thread-4 produce(110) --&gt; size=50结果分析：(01) Depot 是个仓库。通过produce()能往仓库中生产货物，通过consume()能消费仓库中的货物。通过独占锁lock实现对仓库的互斥访问：在操作(生产/消费)仓库中货品前，会先通过lock()锁住仓库，操作完之后再通过unlock()解锁。(02) Producer是生产者类。调用Producer中的produce()函数可以新建一个线程往仓库中生产产品。(03) Customer是消费者类。调用Customer中的consume()函数可以新建一个线程消费仓库中的产品。(04) 在主线程main中，我们会新建1个生产者mPro，同时新建1个消费者mCus。它们分别向仓库中生产/消费产品。根据main中的生产/消费数量，仓库最终剩余的产品应该是50。运行结果是符合我们预期的！这个模型存在两个问题：(01) 现实中，仓库的容量不可能为负数。但是，此模型中的仓库容量可以为负数，这与现实相矛盾！(02) 现实中，仓库的容量是有限制的。但是，此模型中的容量确实没有限制的！这两个问题，我们稍微会讲到如何解决。现在，先看个简单的示例2；通过对比“示例1”和“示例2”,我们能更清晰的认识lock(),unlock()的用途。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141// 在“示例2”中，我们通过Condition去解决“示例1”中的两个问题：// “仓库的容量不可能为负数”// “仓库的容量是有限制的”。// 解决该问题是通过Condition。Condition是需要和Lock联合使用的：通过Condition中的await()方法，能让线程阻塞[类似于wait()]；通过Condition的signal()方法，能让唤醒线程[类似于notify()]。import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.ReentrantLock;import java.util.concurrent.locks.Condition;// LockTest3.java// 仓库class Depot {    private int capacity;    // 仓库的容量    private int size;        // 仓库的实际数量    private Lock lock;        // 独占锁    private Condition fullCondtion;            // 生产条件    private Condition emptyCondtion;        // 消费条件    public Depot(int capacity) {        this.capacity = capacity;        this.size = 0;        this.lock = new ReentrantLock();        this.fullCondtion = lock.newCondition();        this.emptyCondtion = lock.newCondition();    }    public void produce(int val) {        lock.lock();        try {             // left 表示“想要生产的数量”(有可能生产量太多，需多此生产)            int left = val;            while (left &gt; 0) {                // 库存已满时，等待“消费者”消费产品。                while (size &gt;= capacity)                    fullCondtion.await();                // 获取“实际生产的数量”(即库存中新增的数量)                // 如果“库存”+“想要生产的数量”&gt;“总的容量”，则“实际增量”=“总的容量”-“当前容量”。(此时填满仓库)                // 否则“实际增量”=“想要生产的数量”                int inc = (size+left)&gt;capacity ? (capacity-size) : left;                size += inc;                left -= inc;                System.out.printf(&quot;%s produce(=) --&gt; left==, inc==, size==\n&quot;,                         Thread.currentThread().getName(), val, left, inc, size);                // 通知“消费者”可以消费了。                emptyCondtion.signal();            }        } catch (InterruptedException e) {        } finally {            lock.unlock();        }    }    public void consume(int val) {        lock.lock();        try {            // left 表示“客户要消费数量”(有可能消费量太大，库存不够，需多此消费)            int left = val;            while (left &gt; 0) {                // 库存为0时，等待“生产者”生产产品。                while (size &lt;= 0)                    emptyCondtion.await();                // 获取“实际消费的数量”(即库存中实际减少的数量)                // 如果“库存”&lt;“客户要消费的数量”，则“实际消费量”=“库存”；                // 否则，“实际消费量”=“客户要消费的数量”。                int dec = (size&lt;left) ? size : left;                size -= dec;                left -= dec;                System.out.printf(&quot;%s consume(=) &lt;-- left==, dec==, size==\n&quot;,                         Thread.currentThread().getName(), val, left, dec, size);                fullCondtion.signal();            }        } catch (InterruptedException e) {        } finally {            lock.unlock();        }    }    public String toString() {        return &quot;capacity:&quot;+capacity+&quot;, actual size:&quot;+size;    }}; // 生产者class Producer {    private Depot depot;        public Producer(Depot depot) {        this.depot = depot;    }    // 消费产品：新建一个线程向仓库中生产产品。    public void produce(final int val) {        new Thread() {            public void run() {                depot.produce(val);            }        }.start();    }}// 消费者class Customer {    private Depot depot;        public Customer(Depot depot) {        this.depot = depot;    }    // 消费产品：新建一个线程从仓库中消费产品。    public void consume(final int val) {        new Thread() {            public void run() {                depot.consume(val);            }        }.start();    }}public class LockTest3 {      public static void main(String[] args) {          Depot mDepot = new Depot(100);        Producer mPro = new Producer(mDepot);        Customer mCus = new Customer(mDepot);        mPro.produce(60);        mPro.produce(120);        mCus.consume(90);        mCus.consume(150);        mPro.produce(110);    }}Thread-0 produce( 60) --&gt; left=  0, inc= 60, size= 60Thread-1 produce(120) --&gt; left= 80, inc= 40, size=100Thread-2 consume( 90) &lt;-- left=  0, dec= 90, size= 10Thread-3 consume(150) &lt;-- left=140, dec= 10, size=  0Thread-4 produce(110) --&gt; left= 10, inc=100, size=100Thread-3 consume(150) &lt;-- left= 40, dec=100, size=  0Thread-4 produce(110) --&gt; left=  0, inc= 10, size= 10Thread-3 consume(150) &lt;-- left= 30, dec= 10, size=  0Thread-1 produce(120) --&gt; left=  0, inc= 80, size= 80Thread-3 consume(150) &lt;-- left=  0, dec= 30, size= 50公平锁基本概念:AQS  — 指AbstractQueuedSynchronizer类。AQS是java中管理“锁”的抽象类，锁的许多公共方法都是在这个类中实现。AQS是独占锁(例如，ReentrantLock)和共享锁(例如，Semaphore)的公共父类。AQS锁的类别 — 分为“独占锁”和“共享锁”两种。(01) 独占锁 — 锁在一个时间点只能被一个线程锁占有。根据锁的获取机制，它又划分为“公平锁”和“非公平锁”。公平锁，是按照通过CLH等待线程按照先来先得的规则，公平的获取锁；而非公平锁，则当线程要获取锁时，它会无视CLH等待队列而直接获取锁。独占锁的典型实例子是ReentrantLock，此外，ReentrantReadWriteLock.WriteLock也是独占锁。(02) 共享锁 — 能被多个线程同时拥有，能被共享的锁。JUC包中的ReentrantReadWriteLock.ReadLock，CyclicBarrier， CountDownLatch和Semaphore都是共享锁。这些锁的用途和原理，在以后的章节再详细介绍。CLH队列 — Craig, Landin, and Hagersten lock queueCLH队列是AQS中“等待锁”的线程队列。在多线程中，为了保护竞争资源不被多个线程同时操作而起来错误，我们常常需要通过锁来保护这些资源。在独占锁中，竞争资源在一个时间点只能被一个线程锁访问；而其它线程则需要等待。CLH就是管理这些“等待锁”的线程的队列。CLH是一个非阻塞的 FIFO 队列。也就是说往里面插入或移除一个节点的时候，在并发条件下不会阻塞，而是通过自旋锁和 CAS 保证节点插入和移除的原子性。CAS函数 — Compare And SwapCAS函数，是比较并交换函数，它是原子操作函数；即，通过CAS操作的数据都是以原子方式进行的。例如，compareAndSetHead(), compareAndSetTail(), compareAndSetNext()等函数。它们共同的特点是，这些函数所执行的动作是以原子的方式进行的。获取公平锁123456789101112131. lock()lock()在ReentrantLock.java的FairSync类中实现说明：“当前线程”实际上是通过acquire(1)获取锁的。这里说明一下“1”的含义，它是设置“锁的状态”的参数。对于“独占锁”而言，锁处于可获取状态时，它的状态值是0；锁被线程初次获取到了，它的状态值就变成了1。由于ReentrantLock(公平锁/非公平锁)是可重入锁，所以“独占锁”可以被单个线程多此获取，每获取1次就将锁的状态+1。也就是说，初次获取锁时，通过acquire(1)将锁的状态值设为1；再次获取锁时，将锁的状态值设为2；依次类推...这就是为什么获取锁时，传入的参数是1的原因了。可重入就是指锁可以被单个线程多次获取。        2. acquire()acquire()在AQS中实现的(01) “当前线程”首先通过tryAcquire()尝试获取锁。获取成功的话，直接返回；尝试失败的话，进入到等待队列排序等待(前面还有可能有需要线程在等待该锁)。(02) “当前线程”尝试失败的情况下，先通过addWaiter(Node.EXCLUSIVE)来将“当前线程”加入到&quot;CLH队列(非阻塞的FIFO队列)&quot;末尾。CLH队列就是线程等待队列。(03) 再执行完addWaiter(Node.EXCLUSIVE)之后，会调用acquireQueued()来获取锁。由于此时ReentrantLock是公平锁，它会根据公平性原则来获取锁。(04) “当前线程”在执行acquireQueued()时，会进入到CLH队列中休眠等待，直到获取锁了才返回！如果“当前线程”在休眠等待过程中被中断过，acquireQueued会返回true，此时&quot;当前线程&quot;会调用selfInterrupt()来自己给自己产生一个中断。tryAcquire()123456789101112131415161718192021221. tryAcquire()说明：根据代码，我们可以分析出，tryAcquire()的作用就是尝试去获取锁。注意，这里只是尝试！尝试成功的话，返回true；尝试失败的话，返回false，后续再通过其它办法来获取该锁。后面我们会说明，在尝试失败的情况下，是如何一步步获取锁的。2. hasQueuedPredecessors()说明：通过代码，能分析出，hasQueuedPredecessors() 是通过判断&quot;当前线程&quot;是不是在CLH队列的队首，来返回AQS中是不是有比“当前线程”等待更久的线程。下面对head、tail和Node进行说明。3. Node的源码说明：Node是CLH队列的节点，代表“等待锁的线程队列”。(01) 每个Node都会一个线程对应。(02) 每个Node会通过prev和next分别指向上一个节点和下一个节点，这分别代表上一个等待线程和下一个等待线程。(03) Node通过waitStatus保存线程的等待状态。(04) Node通过nextWaiter来区分线程是“独占锁”线程还是“共享锁”线程。如果是“独占锁”线程，则nextWaiter的值为EXCLUSIVE；如果是“共享锁”线程，则nextWaiter的值是SHARED。4. compareAndSetState()说明： compareAndSwapInt() 是sun.misc.Unsafe类中的一个本地方法。对此，我们需要了解的是 compareAndSetState(expect, update) 是以原子的方式操作当前线程；若当前线程的状态为expect，则设置它的状态为update。5. setExclusiveOwnerThread()说明：setExclusiveOwnerThread()的作用就是，设置线程t为当前拥有“独占锁”的线程6. getState(), setState()说明：state表示锁的状态，对于“独占锁”而已，state=0表示锁是可获取状态(即，锁没有被任何线程锁持有)。由于java中的独占锁是可重入的，state的值可以&gt;1。addWaiter(Node.EXCLUSIVE)123456789101112addWaiter(Node.EXCLUSIVE)的作用是，创建“当前线程”的Node节点，且Node中记录“当前线程”对应的锁是“独占锁”类型，并且将该节点添加到CLH队列的末尾。1.addWaiter()说明：对于“公平锁”而言，addWaiter(Node.EXCLUSIVE)会首先创建一个Node节点，节点的类型是“独占锁”(Node.EXCLUSIVE)类型。然后，再将该节点添加到CLH队列的末尾。2. compareAndSetTail()说明：compareAndSetTail也属于CAS函数，也是通过“本地方法”实现的。compareAndSetTail(expect, update)会以原子的方式进行操作，它的作用是判断CLH队列的队尾是不是为expect，是的话，就将队尾设为update。3. enq()说明： enq()的作用很简单。如果CLH队列为空，则新建一个CLH表头；然后将node添加到CLH末尾。否则，直接将node添加到CLH末尾。小结：addWaiter()的作用，就是将当前线程添加到CLH队列中。这就意味着将当前线程添加到等待获取“锁”的等待线程队列中了。acquireQueued()12345678910111213141516171819202122232425262728293031323334353637383940414243acquireQueued()的作用就是逐步的去执行CLH队列的线程，如果当前线程获取到了锁，则返回；否则，当前线程进行休眠，直到唤醒并重新获取锁了才返回。1. acquireQueued()说明：acquireQueued()的目的是从队列中获取锁。2. shouldParkAfterFailedAcquire()说明：(01) 关于waitStatus请参考下表(中扩号内为waitStatus的值)，更多关于waitStatus的内容，可以参考前面的Node类的介绍。    CANCELLED[1]  -- 当前线程已被取消    SIGNAL[-1]    -- “当前线程的后继线程需要被unpark(唤醒)”。一般发生情况是：当前线程的后继线程处于阻塞状态，而当前线程被release或cancel掉，因此需要唤醒当前线程的后继线程。    CONDITION[-2] -- 当前线程(处在Condition休眠状态)在等待Condition唤醒    PROPAGATE[-3] -- (共享锁)其它线程获取到“共享锁”    [0]           -- 当前线程不属于上面的任何一种状态。(02) shouldParkAfterFailedAcquire()通过以下规则，判断“当前线程”是否需要被阻塞。    规则1：如果前继节点状态为SIGNAL，表明当前节点需要被unpark(唤醒)，此时则返回true。    规则2：如果前继节点状态为CANCELLED(ws&gt;0)，说明前继节点已经被取消，则通过先前回溯找到一个有效(非CANCELLED状态)的节点，并返回false。    规则3：如果前继节点状态为非SIGNAL、非CANCELLED，则设置前继的状态为SIGNAL，并返回false。如果“规则1”发生，即“前继节点是SIGNAL”状态，则意味着“当前线程”需要被阻塞。接下来会调用parkAndCheckInterrupt()阻塞当前线程，直到当前先被唤醒才从parkAndCheckInterrupt()中返回。3. parkAndCheckInterrupt())说明：parkAndCheckInterrupt()的作用是阻塞当前线程，并且返回“线程被唤醒之后”的中断状态。它会先通过LockSupport.park()阻塞“当前线程”，然后通过Thread.interrupted()返回线程的中断状态。这里介绍一下线程被阻塞之后如何唤醒。一般有2种情况：第1种情况：unpark()唤醒。“前继节点对应的线程”使用完锁之后，通过unpark()方式唤醒当前线程。第2种情况：中断唤醒。其它线程通过interrupt()中断当前线程。补充：LockSupport()中的park(),unpark()的作用 和 Object中的wait(),notify()作用类似，是阻塞/唤醒。它们的用法不同，park(),unpark()是轻量级的，而wait(),notify()是必须先通过Synchronized获取同步锁。4. 再次tryAcquire()说明：(01) 通过node.predecessor()获取前继节点。predecessor()就是返回node的前继节点，若对此有疑惑可以查看下面关于Node类的介绍。(02) p == head &amp;&amp; tryAcquire(arg)首先，判断“前继节点”是不是CHL表头。如果是的话，则通过tryAcquire()尝试获取锁。其实，这样做的目的是为了“让当前线程获取锁”，但是为什么需要先判断p==head呢？理解这个对理解“公平锁”的机制很重要，因为这么做的原因就是为了保证公平性！(a) 前面，我们在shouldParkAfterFailedAcquire()我们判断“当前线程”是否需要阻塞；(b) 接着，“当前线程”阻塞的话，会调用parkAndCheckInterrupt()来阻塞线程。当线程被解除阻塞的时候，我们会返回线程的中断状态。而线程被解决阻塞，可能是由于“线程被中断”，也可能是由于“其它线程调用了该线程的unpark()函数”。(c) 再回到p==head这里。如果当前线程是因为其它线程调用了unpark()函数而被唤醒，那么唤醒它的线程，应该是它的前继节点所对应的线程(关于这一点，后面在“释放锁”的过程中会看到)。 OK，是前继节点调用unpark()唤醒了当前线程！此时，再来理解p==head就很简单了：当前继节点是CLH队列的头节点，并且它释放锁之后；就轮到当前节点获取锁了。然后，当前节点通过tryAcquire()获取锁；获取成功的话，通过setHead(node)设置当前节点为头节点，并返回。总之，如果“前继节点调用unpark()唤醒了当前线程”并且“前继节点是CLH表头”，此时就是满足p==head，也就是符合公平性原则的。否则，如果当前线程是因为“线程被中断”而唤醒，那么显然就不是公平了。这就是为什么说p==head就是保证公平性！小结：acquireQueued()的作用就是“当前线程”会根据公平性原则进行阻塞等待，直到获取锁为止；并且返回当前线程在等待过程中有没有并中断过。selfInterrupt()1234567891011121314151617181920private static void selfInterrupt() {    Thread.currentThread().interrupt();}说明：selfInterrupt()的代码很简单，就是“当前线程”自己产生一个中断。但是，为什么需要这么做呢？这必须结合acquireQueued()进行分析。如果在acquireQueued()中，当前线程被中断过，则执行selfInterrupt()；否则不会执行。在acquireQueued()中，即使是线程在阻塞状态被中断唤醒而获取到cpu执行权利；但是，如果该线程的前面还有其它等待锁的线程，根据公平性原则，该线程依然无法获取到锁。它会再次阻塞！ 该线程再次阻塞，直到该线程被它的前面等待锁的线程锁唤醒；线程才会获取锁，然后“真正执行起来”！也就是说，在该线程“成功获取锁并真正执行起来”之前，它的中断会被忽略并且中断标记会被清除！ 因为在parkAndCheckInterrupt()中，我们线程的中断状态时调用了Thread.interrupted()。该函数不同于Thread的isInterrupted()函数，isInterrupted()仅仅返回中断状态，而interrupted()在返回当前中断状态之后，还会清除中断状态。 正因为之前的中断状态被清除了，所以这里需要调用selfInterrupt()重新产生一个中断！小结：selfInterrupt()的作用就是当前线程自己产生一个中断。总结:再回过头看看acquire()函数，它最终的目的是获取锁！public final void acquire(int arg) {    if (!tryAcquire(arg) &amp;&amp;        acquireQueued(addWaiter(Node.EXCLUSIVE), arg))        selfInterrupt();}(01) 先是通过tryAcquire()尝试获取锁。获取成功的话，直接返回；尝试失败的话，再通过acquireQueued()获取锁。(02) 尝试失败的情况下，会先通过addWaiter()来将“当前线程”加入到&quot;CLH队列&quot;末尾；然后调用acquireQueued()，在CLH队列中排序等待获取锁，在此过程中，线程处于休眠状态。直到获取锁了才返回。 如果在休眠等待过程中被中断过，则调用selfInterrupt()来自己产生一个中断。释放公平锁123456789101112131415161718191. unlock()说明：unlock()是解锁函数，它是通过AQS的release()函数来实现的。在这里，“1”的含义和“获取锁的函数acquire(1)的含义”一样，它是设置“释放锁的状态”的参数。由于“公平锁”是可重入的，所以对于同一个线程，每释放锁一次，锁的状态-1。2. release()说明：release()会先调用tryRelease()来尝试释放当前线程锁持有的锁。成功的话，则唤醒后继等待线程，并返回true。否则，直接返回false。3. tryRelease()说明：tryRelease()的作用是尝试释放锁。(01) 如果“当前线程”不是“锁的持有者”，则抛出异常。(02) 如果“当前线程”在本次释放锁操作之后，对锁的拥有状态是0(即，当前线程彻底释放该“锁”)，则设置“锁”的持有者为null，即锁是可获取状态。同时，更新当前线程的锁的状态为0。4. unparkSuccessor()在release()中“当前线程”释放锁成功的话，会唤醒当前线程的后继线程。根据CLH队列的FIFO规则，“当前线程”(即已经获取锁的线程)肯定是head；如果CLH队列非空的话，则唤醒锁的下一个等待线程。说明：unparkSuccessor()的作用是“唤醒当前线程的后继线程”。后继线程被唤醒之后，就可以获取该锁并恢复运行了。关总结: “释放锁”的过程相对“获取锁”的过程比较简单。释放锁时，主要进行的操作，是更新当前线程对应的锁的状态。如果当前线程对锁已经彻底释放，则设置“锁”的持有线程为null，设置当前线程的状态为空，然后唤醒后继线程。非公平锁获取非公平锁1非公平锁和公平锁在获取锁的方法上，流程是一样的；它们的区别主要表现在“尝试获取锁的机制不同”。简单点说，“公平锁”在每次尝试获取锁时，都是采用公平策略(根据等待队列依次排序等待)；而“非公平锁”在每次尝试获取锁时，都是采用的非公平策略(无视等待队列，直接尝试获取锁，如果锁是空闲的，即可获取状态，则获取锁)。12345678910111213141516171819202122232425262728291. lock()说明：lock()会先通过compareAndSet(0, 1)来判断“锁”是不是空闲状态。是的话，“当前线程”直接获取“锁”；否则的话，调用acquire(1)获取锁。(01) compareAndSetState()是CAS函数，它的作用是比较并设置当前锁的状态。若锁的状态值为0，则设置锁的状态值为1。(02) setExclusiveOwnerThread(Thread.currentThread())的作用是，设置“当前线程”为“锁”的持有者。“公平锁”和“非公平锁”关于lock()的对比公平锁   -- 公平锁的lock()函数，会直接调用acquire(1)。非公平锁 -- 非公平锁会先判断当前锁的状态是不是空闲，是的话，就不排队，而是直接获取锁。2. acquire()(01) “当前线程”首先通过tryAcquire()尝试获取锁。获取成功的话，直接返回；尝试失败的话，进入到等待队列依次排序，然后获取锁。(02) “当前线程”尝试失败的情况下，会先通过addWaiter(Node.EXCLUSIVE)来将“当前线程”加入到&quot;CLH队列(非阻塞的FIFO队列)&quot;末尾。(03) 然后，调用acquireQueued()获取锁。在acquireQueued()中，当前线程会等待它在“CLH队列”中前面的所有线程执行并释放锁之后，才能获取锁并返回。如果“当前线程”在休眠等待过程中被中断过，则调用selfInterrupt()来自己产生一个中断。“公平锁”和“非公平锁”关于acquire()的对比公平锁和非公平锁，只有tryAcquire()函数的实现不同；即它们尝试获取锁的机制不同。这就是我们所说的“它们获取锁策略的不同所在之处”！在“Java多线程系列--“JUC锁”03之 公平锁(一)”中，已经详细介绍了acquire()涉及到的各个函数。这里仅对它们有差异的函数tryAcquire()进行说明。    说明：根据代码，我们可以分析出，tryAcquire()的作用就是尝试去获取锁。(01) 如果“锁”没有被任何线程拥有，则通过CAS函数设置“锁”的状态为acquires，同时，设置“当前线程”为锁的持有者，然后返回true。(02) 如果“锁”的持有者已经是当前线程，则将更新锁的状态即可。(03) 如果不术语上面的两种情况，则认为尝试失败。“公平锁”和“非公平锁”关于tryAcquire()的对比公平锁和非公平锁，它们尝试获取锁的方式不同。公平锁在尝试获取锁时，即使“锁”没有被任何线程锁持有，它也会判断自己是不是CLH等待队列的表头；是的话，才获取锁。而非公平锁在尝试获取锁时，如果“锁”没有被任何线程持有，则不管它在CLH队列的何处，它都直接获取锁。 释放非公平锁1非公平锁和公平锁在释放锁的方法和策略上是一样的。Condition条件Condition的作用是对锁进行更精确的控制。Condition中的await()方法相当于Object的wait()方法，Condition中的signal()方法相当于Object的notify()方法，Condition中的signalAll()相当于Object的notifyAll()方法。不同的是，Object中的wait(),notify(),notifyAll()方法是和”同步锁”(synchronized关键字)捆绑使用的；而Condition是需要与”互斥锁”/“共享锁”捆绑使用的。123456789101112131415// Condition函数列表// 造成当前线程在接到信号或被中断之前一直处于等待状态。void await()// 造成当前线程在接到信号、被中断或到达指定等待时间之前一直处于等待状态。boolean await(long time, TimeUnit unit)// 造成当前线程在接到信号、被中断或到达指定等待时间之前一直处于等待状态。long awaitNanos(long nanosTimeout)// 造成当前线程在接到信号之前一直处于等待状态。void awaitUninterruptibly()// 造成当前线程在接到信号、被中断或到达指定最后期限之前一直处于等待状态。boolean awaitUntil(Date deadline)// 唤醒一个等待线程。void signal()// 唤醒所有等待线程。void signalAll()Condition示例1234567891011121314151617181920212223242526272829// 示例1是通过Object的wait(), notify()来演示线程的休眠/唤醒功能。public class WaitTest1 {    public static void main(String[] args) {        ThreadA ta = new ThreadA(&quot;ta&quot;);        synchronized(ta) { // 通过synchronized(ta)获取“对象ta的同步锁”            try {       System.out.println(Thread.currentThread().getName()+&quot; start ta&quot;);                ta.start();                System.out.println(Thread.currentThread().getName()+&quot; block&quot;);                ta.wait();    // 等待                System.out.println(Thread.currentThread().getName()+&quot; continue&quot;);            } catch (InterruptedException e) {                e.printStackTrace();            }        }    }    static class ThreadA extends Thread{        public ThreadA(String name) {            super(name);        }        public void run() {            synchronized (this) { // 通过synchronized(this)获取“当前对象的同步锁”                System.out.println(Thread.currentThread().getName()+&quot; wakup others&quot;);                notify();    // 唤醒“当前对象上的等待线程”            }        }    }}123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354// 示例2是通过Condition的await(), signal()来演示线程的休眠/唤醒功能。import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.ReentrantLock;public class ConditionTest1 {    private static Lock lock = new ReentrantLock();    private static Condition condition = lock.newCondition();    public static void main(String[] args) {        ThreadA ta = new ThreadA(&quot;ta&quot;);        lock.lock(); // 获取锁        try {            System.out.println(Thread.currentThread().getName()+&quot; start ta&quot;);            ta.start();            System.out.println(Thread.currentThread().getName()+&quot; block&quot;);            condition.await();    // 等待            System.out.println(Thread.currentThread().getName()+&quot; continue&quot;);        } catch (InterruptedException e) {            e.printStackTrace();        } finally {            lock.unlock();    // 释放锁        }    }    static class ThreadA extends Thread{        public ThreadA(String name) {            super(name);        }        public void run() {            lock.lock();    // 获取锁            try {                System.out.println(Thread.currentThread().getName()+&quot; wakup others&quot;);                condition.signal();    // 唤醒“condition所在锁上的其它线程”            } finally {                lock.unlock();    // 释放锁            }        }    }}运行结果：main start tamain blockta wakup othersmain continue通过“示例1”和“示例2”，我们知道Condition和Object的方法有一下对应关系：              Object      Condition  休眠          wait        await唤醒个线程     notify      signal唤醒所有线程   notifyAll   signalAllCondition除了支持上面的功能之外，它更强大的地方在于：能够更加精细的控制多线程的休眠与唤醒。对于同一个锁，我们可以创建多个Condition，在不同的情况下使用不同的Condition。例如，假如多线程读/写同一个缓冲区：当向缓冲区中写入数据之后，唤醒&quot;读线程&quot;；当从缓冲区读出数据之后，唤醒&quot;写线程&quot;；并且当缓冲区满的时候，&quot;写线程&quot;需要等待；当缓冲区为空时，&quot;读线程&quot;需要等待。如果采用Object类中的wait(), notify(), notifyAll()实现该缓冲区，当向缓冲区写入数据之后需要唤醒&quot;读线程&quot;时，不可能通过notify()或notifyAll()明确的指定唤醒&quot;读线程&quot;，而只能通过notifyAll唤醒所有线程(但是notifyAll无法区分唤醒的线程是读线程，还是写线程)。  但是，通过Condition，就能明确的指定唤醒读线程。看看下面的示例3，可能对这个概念有更深刻的理解。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130// 示例3是通过Condition的高级功能。import java.util.concurrent.locks.Lock;import java.util.concurrent.locks.Condition;import java.util.concurrent.locks.ReentrantLock;class BoundedBuffer {    final Lock lock = new ReentrantLock();    final Condition notFull  = lock.newCondition();     final Condition notEmpty = lock.newCondition();     final Object[] items = new Object[5];    int putptr, takeptr, count;    public void put(Object x) throws InterruptedException {        lock.lock();    //获取锁        try {            // 如果“缓冲已满”，则等待；直到“缓冲”不是满的，才将x添加到缓冲中。            while (count == items.length)                notFull.await();            // 将x添加到缓冲中            items[putptr] = x;             // 将“put统计数putptr+1”；如果“缓冲已满”，则设putptr为0。            if (++putptr == items.length) putptr = 0;            // 将“缓冲”数量+1            ++count;            // 唤醒take线程，因为take线程通过notEmpty.await()等待            notEmpty.signal();            // 打印写入的数据            System.out.println(Thread.currentThread().getName() + &quot; put  &quot;+ (Integer)x);        } finally {            lock.unlock();    // 释放锁        }    }    public Object take() throws InterruptedException {        lock.lock();    //获取锁        try {            // 如果“缓冲为空”，则等待；直到“缓冲”不为空，才将x从缓冲中取出。            while (count == 0)                 notEmpty.await();            // 将x从缓冲中取出            Object x = items[takeptr];             // 将“take统计数takeptr+1”；如果“缓冲为空”，则设takeptr为0。            if (++takeptr == items.length) takeptr = 0;            // 将“缓冲”数量-1            --count;            // 唤醒put线程，因为put线程通过notFull.await()等待            notFull.signal();            // 打印取出的数据            System.out.println(Thread.currentThread().getName() + &quot; take &quot;+ (Integer)x);            return x;        } finally {            lock.unlock();    // 释放锁        }    } }public class ConditionTest2 {    private static BoundedBuffer bb = new BoundedBuffer();    public static void main(String[] args) {        // 启动10个“写线程”，向BoundedBuffer中不断的写数据(写入0-9)；        // 启动10个“读线程”，从BoundedBuffer中不断的读数据。        for (int i=0; i&lt;10; i++) {            new PutThread(&quot;p&quot;+i, i).start();            new TakeThread(&quot;t&quot;+i).start();        }    }    static class PutThread extends Thread {        private int num;        public PutThread(String name, int num) {            super(name);            this.num = num;        }        public void run() {            try {                Thread.sleep(1);    // 线程休眠1ms                bb.put(num);        // 向BoundedBuffer中写入数据            } catch (InterruptedException e) {            }        }    }    static class TakeThread extends Thread {        public TakeThread(String name) {            super(name);        }        public void run() {            try {                Thread.sleep(10);                    // 线程休眠1ms                Integer num = (Integer)bb.take();    // 从BoundedBuffer中取出数据            } catch (InterruptedException e) {            }        }    }}某一次结果:p1 put  1p4 put  4p5 put  5p0 put  0p2 put  2t0 take 1p3 put  3t1 take 4p6 put  6t2 take 5p7 put  7t3 take 0p8 put  8t4 take 2p9 put  9t5 take 3t6 take 6t7 take 7t8 take 8t9 take 9    结果说明：(01) BoundedBuffer 是容量为5的缓冲，缓冲中存储的是Object对象，支持多线程的读/写缓冲。多个线程操作“一个BoundedBuffer对象”时，它们通过互斥锁lock对缓冲区items进行互斥访问；而且同一个BoundedBuffer对象下的全部线程共用“notFull”和“notEmpty”这两个Condition。    notFull用于控制写缓冲，notEmpty用于控制读缓冲。当缓冲已满的时候，调用put的线程会执行notFull.await()进行等待；当缓冲区不是满的状态时，就将对象添加到缓冲区并将缓冲区的容量count+1，最后，调用notEmpty.signal()缓冲notEmpty上的等待线程(调用notEmpty.await的线程)。 简言之，notFull控制“缓冲区的写入”，当往缓冲区写入数据之后会唤醒notEmpty上的等待线程。同理，notEmpty控制“缓冲区的读取”，当读取了缓冲区数据之后会唤醒notFull上的等待线程。(02) 在ConditionTest2的main函数中，启动10个“写线程”，向BoundedBuffer中不断的写数据(写入0-9)；同时，也启动10个“读线程”，从BoundedBuffer中不断的读数据。(03) 简单分析一下运行结果。1, p1线程向缓冲中写入1。    此时，缓冲区数据:   | 1 |   |   |   |   |     2, p4线程向缓冲中写入4。    此时，缓冲区数据:   | 1 | 4 |   |   |   |     3, p5线程向缓冲中写入5。    此时，缓冲区数据:   | 1 | 4 | 5 |   |   |     4, p0线程向缓冲中写入0。    此时，缓冲区数据:   | 1 | 4 | 5 | 0 |   |     5, p2线程向缓冲中写入2。    此时，缓冲区数据:   | 1 | 4 | 5 | 0 | 2 |     此时，缓冲区容量为5；缓冲区已满！如果此时，还有“写线程”想往缓冲中写入数据，会调用put中的notFull.await()等待，直接缓冲区非满状态，才能继续运行。     6, t0线程从缓冲中取出数据1。此时，缓冲区数据:   |   | 4 | 5 | 0 | 2 |     7, p3线程向缓冲中写入3。    此时，缓冲区数据:   | 3 | 4 | 5 | 0 | 2 |     8, t1线程从缓冲中取出数据4。此时，缓冲区数据:   | 3 |   | 5 | 0 | 2 |     9, p6线程向缓冲中写入6。    此时，缓冲区数据:   | 3 | 6 | 5 | 0 | 2 |     ...LockSupport123LockSupport是用来创建锁和其他同步类的基本线程阻塞原语。 LockSupport中的park() 和 unpark() 的作用分别是阻塞线程和解除阻塞线程，而且park()和unpark()不会遇到“Thread.suspend 和 Thread.resume所可能引发的死锁”问题。因为park() 和 unpark()有许可的存在；调用 park() 的线程和另一个试图将其 unpark() 的线程之间的竞争将保持活性。1234567891011121314151617// LockSupport函数列表// 返回提供给最近一次尚未解除阻塞的 park 方法调用的 blocker 对象，如果该调用不受阻塞，则返回 null。static Object getBlocker(Thread t)// 为了线程调度，禁用当前线程，除非许可可用。static void park()// 为了线程调度，在许可可用之前禁用当前线程。static void park(Object blocker)// 为了线程调度禁用当前线程，最多等待指定的等待时间，除非许可可用。static void parkNanos(long nanos)// 为了线程调度，在许可可用前禁用当前线程，并最多等待指定的等待时间。static void parkNanos(Object blocker, long nanos)// 为了线程调度，在指定的时限前禁用当前线程，除非许可可用。static void parkUntil(long deadline)// 为了线程调度，在指定的时限前禁用当前线程，除非许可可用。static void parkUntil(Object blocker, long deadline)// 如果给定线程的许可尚不可用，则使其可用。static void unpark(Thread thread)LockSupport示例12345678910111213141516171819202122232425262728public class WaitTest1 {    public static void main(String[] args) {        ThreadA ta = new ThreadA(&quot;ta&quot;);        synchronized(ta) { // 通过synchronized(ta)获取“对象ta的同步锁”            try {                System.out.println(Thread.currentThread().getName()+&quot; start ta&quot;);                ta.start();                System.out.println(Thread.currentThread().getName()+&quot; block&quot;);                // 主线程等待                ta.wait();                System.out.println(Thread.currentThread().getName()+&quot; continue&quot;);            } catch (InterruptedException e) {                e.printStackTrace();            }        }    }    static class ThreadA extends Thread{        public ThreadA(String name) {            super(name);        }        public void run() {            synchronized (this) { // 通过synchronized(this)获取“当前对象的同步锁”                System.out.println(Thread.currentThread().getName()+&quot; wakup others&quot;);                notify();    // 唤醒“当前对象上的等待线程”            }        }    }}123456789101112131415161718192021222324252627282930313233import java.util.concurrent.locks.LockSupport;public class LockSupportTest1 {    private static Thread mainThread;    public static void main(String[] args) {        ThreadA ta = new ThreadA(&quot;ta&quot;);        // 获取主线程        mainThread = Thread.currentThread();        System.out.println(Thread.currentThread().getName()+&quot; start ta&quot;);        ta.start();        System.out.println(Thread.currentThread().getName()+&quot; block&quot;);        // 主线程阻塞        LockSupport.park(mainThread);        System.out.println(Thread.currentThread().getName()+&quot; continue&quot;);    }    static class ThreadA extends Thread{        public ThreadA(String name) {            super(name);        }        public void run() {            System.out.println(Thread.currentThread().getName()+&quot; wakup others&quot;);            // 唤醒“主线程”            LockSupport.unpark(mainThread);        }    }}运行结果：main start tamain blockta wakup othersmain continue说明：park和wait的区别。wait让线程阻塞前，必须通过synchronized获取同步锁。共享锁和ReentrantReadWriteLock12345ReadWriteLock，顾名思义，是读写锁。它维护了一对相关的锁 — — “读取锁”和“写入锁”，一个用于读取操作，另一个用于写入操作。“读取锁”用于只读操作，它是“共享锁”，能同时被多个线程获取。“写入锁”用于写入操作，它是“独占锁”，写入锁只能被一个线程锁获取。注意：不能同时存在读取锁和写入锁！ReadWriteLock是一个接口。ReentrantReadWriteLock是它的实现类，ReentrantReadWriteLock包括子类ReadLock和WriteLock。12345// ReadWriteLock函数列表// 返回用于读取操作的锁。Lock readLock()// 返回用于写入操作的锁。Lock writeLock()1234567891011121314151617181920212223242526272829303132333435363738394041// ReentrantReadWriteLock函数列表// 创建一个新的 ReentrantReadWriteLock，默认是采用“非公平策略”。ReentrantReadWriteLock()// 创建一个新的 ReentrantReadWriteLock，fair是“公平策略”。fair为true，意味着公平策略；否则，意味着非公平策略。ReentrantReadWriteLock(boolean fair)// 返回当前拥有写入锁的线程，如果没有这样的线程，则返回 null。protected Thread getOwner()// 返回一个 collection，它包含可能正在等待获取读取锁的线程。protected Collection&lt;Thread&gt; getQueuedReaderThreads()// 返回一个 collection，它包含可能正在等待获取读取或写入锁的线程。protected Collection&lt;Thread&gt; getQueuedThreads()// 返回一个 collection，它包含可能正在等待获取写入锁的线程。protected Collection&lt;Thread&gt; getQueuedWriterThreads()// 返回等待获取读取或写入锁的线程估计数目。int getQueueLength()// 查询当前线程在此锁上保持的重入读取锁数量。int getReadHoldCount()// 查询为此锁保持的读取锁数量。int getReadLockCount()// 返回一个 collection，它包含可能正在等待与写入锁相关的给定条件的那些线程。protected Collection&lt;Thread&gt; getWaitingThreads(Condition condition)// 返回正等待与写入锁相关的给定条件的线程估计数目。int getWaitQueueLength(Condition condition)// 查询当前线程在此锁上保持的重入写入锁数量。int getWriteHoldCount()// 查询是否给定线程正在等待获取读取或写入锁。boolean hasQueuedThread(Thread thread)// 查询是否所有的线程正在等待获取读取或写入锁。boolean hasQueuedThreads()// 查询是否有些线程正在等待与写入锁有关的给定条件。boolean hasWaiters(Condition condition)// 如果此锁将公平性设置为 ture，则返回 true。boolean isFair()// 查询是否某个线程保持了写入锁。boolean isWriteLocked()// 查询当前线程是否保持了写入锁。boolean isWriteLockedByCurrentThread()// 返回用于读取操作的锁。ReentrantReadWriteLock.ReadLock readLock()// 返回用于写入操作的锁。ReentrantReadWriteLock.WriteLock writeLock()获取共享锁12345678910111213141516171819202122获取共享锁的思想(即lock函数的步骤)，是先通过tryAcquireShared()尝试获取共享锁。尝试成功的话，则直接返回；尝试失败的话，则通过doAcquireShared()不断的循环并尝试获取锁，若有需要，则阻塞等待。doAcquireShared()在循环中每次尝试获取锁时，都是通过tryAcquireShared()来进行尝试的。1. lock()2. acquireShared()说明：acquireShared()首先会通过tryAcquireShared()来尝试获取锁。尝试成功的话，则不再做任何动作(因为已经成功获取到锁了)。尝试失败的话，则通过doAcquireShared()来获取锁。doAcquireShared()会获取到锁了才返回。3. tryAcquireShared()说明：tryAcquireShared()的作用是尝试获取“共享锁”。如果在尝试获取锁时，“不需要阻塞等待”并且“读取锁的共享计数小于MAX_COUNT”，则直接通过CAS函数更新“读取锁的共享计数”，以及将“当前线程获取读取锁的次数+1”。否则，通过fullTryAcquireShared()获取读取锁。4. fullTryAcquireShared()说明：fullTryAcquireShared()会根据“是否需要阻塞等待”，“读取锁的共享计数是否超过限制”等等进行处理。如果不需要阻塞等待，并且锁的共享计数没有超过限制，则通过CAS尝试获取锁，并返回1。5. doAcquireShared()说明：doAcquireShared()的作用是获取共享锁。它会首先创建线程对应的CLH队列的节点，然后将该节点添加到CLH队列中。CLH队列是管理获取锁的等待线程的队列。如果“当前线程”是CLH队列的表头，则尝试获取共享锁；否则，则需要通过shouldParkAfterFailedAcquire()判断是否阻塞等待，需要的话，则通过parkAndCheckInterrupt()进行阻塞等待。doAcquireShared()会通过for循环，不断的进行上面的操作；目的就是获取共享锁。需要注意的是：doAcquireShared()在每一次尝试获取锁时，是通过tryAcquireShared()来执行的！释放共享锁1234567891011121314释放共享锁的思想，是先通过tryReleaseShared()尝试释放共享锁。尝试成功的话，则通过doReleaseShared()唤醒“其他等待获取共享锁的线程”，并返回true；否则的话，返回flase。1. unlock()说明：该函数实际上调用releaseShared(1)释放共享锁。2. releaseShared()说明：releaseShared()的目的是让当前线程释放它所持有的共享锁。它首先会通过tryReleaseShared()去尝试释放共享锁。尝试成功，则直接返回；尝试失败，则通过doReleaseShared()去释放共享锁。3. tryReleaseShared()说明：tryReleaseShared()的作用是尝试释放共享锁。4. doReleaseShared()说明：doReleaseShared()会释放“共享锁”。它会从前往后的遍历CLH队列，依次“唤醒”然后“执行”队列中每个节点对应的线程；最终的目的是让这些线程释放它们所持有的锁。公平共享锁和非公平共享锁12345和互斥锁ReentrantLock一样，ReadLock也分为公平锁和非公平锁。公平锁和非公平锁的区别，体现在判断是否需要阻塞的函数readerShouldBlock()是不同的。在公平共享锁中，如果在当前线程的前面有其他线程在等待获取共享锁，则返回true；否则，返回false。在非公平共享锁中，它会无视当前线程的前面是否有其他线程在等待获取共享锁。只要该非公平共享锁对应的线程不为null，则返回true。ReentrantReadWriteLock示例123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105import java.util.concurrent.locks.ReadWriteLock; import java.util.concurrent.locks.ReentrantReadWriteLock; public class ReadWriteLockTest1 {     public static void main(String[] args) {         // 创建账户        MyCount myCount = new MyCount(&quot;4238920615242830&quot;, 10000);         // 创建用户，并指定账户        User user = new User(&quot;Tommy&quot;, myCount);         // 分别启动3个“读取账户金钱”的线程 和 3个“设置账户金钱”的线程        for (int i=0; i&lt;3; i++) {            user.getCash();            user.setCash((i+1)*1000);        }    } } class User {    private String name;            //用户名     private MyCount myCount;        //所要操作的账户     private ReadWriteLock myLock;   //执行操作所需的锁对象     User(String name, MyCount myCount) {        this.name = name;         this.myCount = myCount;         this.myLock = new ReentrantReadWriteLock();    }    public void getCash() {        new Thread() {            public void run() {                myLock.readLock().lock();                 try {                    System.out.println(Thread.currentThread().getName() +&quot; getCash start&quot;);                     myCount.getCash();                    Thread.sleep(1);                    System.out.println(Thread.currentThread().getName() +&quot; getCash end&quot;);                 } catch (InterruptedException e) {                } finally {                    myLock.readLock().unlock();                 }            }        }.start();    }    public void setCash(final int cash) {        new Thread() {            public void run() {                myLock.writeLock().lock();                 try {                    System.out.println(Thread.currentThread().getName() +&quot; setCash start&quot;);                     myCount.setCash(cash);                    Thread.sleep(1);                    System.out.println(Thread.currentThread().getName() +&quot; setCash end&quot;);                 } catch (InterruptedException e) {                } finally {                    myLock.writeLock().unlock();                 }            }        }.start();    }}class MyCount {    private String id;         //账号     private int    cash;       //账户余额     MyCount(String id, int cash) {         this.id = id;         this.cash = cash;     }     public String getId() {         return id;     }     public void setId(String id) {         this.id = id;     }     public int getCash() {         System.out.println(Thread.currentThread().getName() +&quot; getCash cash=&quot;+ cash);         return cash;     }     public void setCash(int cash) {         System.out.println(Thread.currentThread().getName() +&quot; setCash cash=&quot;+ cash);         this.cash = cash;     } }Thread-0 getCash startThread-2 getCash startThread-0 getCash cash=10000Thread-2 getCash cash=10000Thread-0 getCash endThread-2 getCash endThread-1 setCash startThread-1 setCash cash=1000Thread-1 setCash endThread-3 setCash startThread-3 setCash cash=2000Thread-3 setCash endThread-4 getCash startThread-4 getCash cash=2000Thread-4 getCash endThread-5 setCash startThread-5 setCash cash=3000Thread-5 setCash end结果说明：(01) 观察Thread0和Thread-2的运行结果，我们发现，Thread-0启动并获取到“读取锁”，在它还没运行完毕的时候，Thread-2也启动了并且也成功获取到“读取锁”。因此，“读取锁”支持被多个线程同时获取。(02) 观察Thread-1,Thread-3,Thread-5这三个“写入锁”的线程。只要“写入锁”被某线程获取，则该线程运行完毕了，才释放该锁。因此，“写入锁”不支持被多个线程同时获取。CountDownLatch原理和示例12345CountDownLatch是一个同步辅助类，在完成一组正在其他线程中执行的操作之前，它允许一个或多个线程一直等待。CountDownLatch和CyclicBarrier的区别(01) CountDownLatch的作用是允许1或N个线程等待其他线程完成执行；而CyclicBarrier则是允许N个线程相互等待。(02) CountDownLatch的计数器无法被重置；CyclicBarrier的计数器可以被重置后使用，因此它被称为是循环的barrier。关于CyclicBarrier的原理，后面一章再来学习。12345678910111213// CountDownLatch函数列表CountDownLatch(int count)// 构造一个用给定计数初始化的 CountDownLatch。// 使当前线程在锁存器倒计数至零之前一直等待，除非线程被中断。void await()// 使当前线程在锁存器倒计数至零之前一直等待，除非线程被中断或超出了指定的等待时间。boolean await(long timeout, TimeUnit unit)// 递减锁存器的计数，如果计数到达零，则释放所有等待的线程。void countDown()// 返回当前计数。long getCount()// 返回标识此锁存器及其状态的字符串。String toString()12345678910111213141516171819202122231. CountDownLatch(int count)说明：该函数是创建一个Sync对象，而Sync是继承于AQS类。2. await()说明：该函数实际上是调用的AQS的acquireSharedInterruptibly(1);说明：acquireSharedInterruptibly()的作用是获取共享锁。如果当前线程是中断状态，则抛出异常InterruptedException。否则，调用tryAcquireShared(arg)尝试获取共享锁；尝试成功则返回，否则就调用doAcquireSharedInterruptibly()。doAcquireSharedInterruptibly()会使当前线程一直等待，直到当前线程获取到共享锁(或被中断)才返回。说明：tryAcquireShared()的作用是尝试获取共享锁。如果&quot;锁计数器=0&quot;，即锁是可获取状态，则返回1；否则，锁是不可获取状态，则返回-1。说明：(01) addWaiter(Node.SHARED)的作用是，创建”当前线程“的Node节点，且Node中记录的锁的类型是”共享锁“(Node.SHARED)；并将该节点添加到CLH队列末尾。关于Node和CLH在&quot;Java多线程系列--“JUC锁”03之 公平锁(一)&quot;已经详细介绍过，这里就不再重复说明了。(02) node.predecessor()的作用是，获取上一个节点。如果上一节点是CLH队列的表头，则”尝试获取共享锁“。(03) shouldParkAfterFailedAcquire()的作用和它的名称一样，如果在尝试获取锁失败之后，线程应该等待，则返回true；否则，返回false。(04) 当shouldParkAfterFailedAcquire()返回ture时，则调用parkAndCheckInterrupt()，当前线程会进入等待状态，直到获取到共享锁才继续运行。3. countDown()说明：该函数实际上调用releaseShared(1)释放共享锁。说明：releaseShared()的目的是让当前线程释放它所持有的共享锁。它首先会通过tryReleaseShared()去尝试释放共享锁。尝试成功，则直接返回；尝试失败，则通过doReleaseShared()去释放共享锁。说明：tryReleaseShared()的作用是释放共享锁，将“锁计数器”的值-1。总结：CountDownLatch是通过“共享锁”实现的。在创建CountDownLatch中时，会传递一个int类型参数count，该参数是“锁计数器”的初始状态，表示该“共享锁”最多能被count给线程同时获取。当某线程调用该CountDownLatch对象的await()方法时，该线程会等待“共享锁”可用时，才能获取“共享锁”进而继续运行。而“共享锁”可用的条件，就是“锁计数器”的值为0！而“锁计数器”的初始值为count，每当一个线程调用该CountDownLatch对象的countDown()方法时，才将“锁计数器”-1；通过这种方式，必须有count个线程调用countDown()之后，“锁计数器”才为0，而前面提到的等待线程才能继续运行！CountDownLatch的使用示例12345678910111213141516171819202122232425262728293031323334353637383940414243// 通过CountDownLatch实现：&quot;主线程&quot;等待&quot;5个子线程&quot;全部都完成&quot;指定的工作(休眠1000ms)&quot;之后，再继续运行。import java.util.concurrent.CountDownLatch;import java.util.concurrent.CyclicBarrier;public class CountDownLatchTest1 {    private static int LATCH_SIZE = 5;    private static CountDownLatch doneSignal;    public static void main(String[] args) {        try {            doneSignal = new CountDownLatch(LATCH_SIZE);            // 新建5个任务            for(int i=0; i&lt;LATCH_SIZE; i++)                new InnerThread().start();            System.out.println(&quot;main await begin.&quot;);            // &quot;主线程&quot;等待线程池中5个任务的完成            doneSignal.await();            System.out.println(&quot;main await finished.&quot;);        } catch (InterruptedException e) {            e.printStackTrace();        }    }    static class InnerThread extends Thread{        public void run() {            try {                Thread.sleep(1000);                System.out.println(Thread.currentThread().getName() + &quot; sleep 1000ms.&quot;);                // 将CountDownLatch的数值减1                doneSignal.countDown();            } catch (InterruptedException e) {                e.printStackTrace();            }        }    }}main await begin.Thread-0 sleep 1000ms.Thread-2 sleep 1000ms.Thread-1 sleep 1000ms.Thread-4 sleep 1000ms.Thread-3 sleep 1000ms.main await finished.    结果说明：主线程通过doneSignal.await()等待其它线程将doneSignal递减至0。其它的5个InnerThread线程，每一个都通过doneSignal.countDown()将doneSignal的值减1；当doneSignal为0时，main被唤醒后继续执行。CyclicBarrier原理和示例1234CyclicBarrier是一个同步辅助类，允许一组线程互相等待，直到到达某个公共屏障点 (common barrier point)。因为该 barrier 在释放等待线程后可以重用，所以称它为循环 的 barrier。注意比较CountDownLatch和CyclicBarrier：(01) CountDownLatch的作用是允许1或N个线程等待其他线程完成执行；而CyclicBarrier则是允许N个线程相互等待。(02) CountDownLatch的计数器无法被重置；CyclicBarrier的计数器可以被重置后使用，因此它被称为是循环的barrier。1234567891011121314151617// CyclicBarrier函数列表CyclicBarrier(int parties)// 创建一个新的 CyclicBarrier，它将在给定数量的参与者（线程）处于等待状态时启动，但它不会在启动 barrier 时执行预定义的操作。CyclicBarrier(int parties, Runnable barrierAction)// 创建一个新的 CyclicBarrier，它将在给定数量的参与者（线程）处于等待状态时启动，并在启动 barrier 时执行给定的屏障操作，该操作由最后一个进入 barrier 的线程执行。int await()// 在所有参与者都已经在此 barrier 上调用 await 方法之前，将一直等待。int await(long timeout, TimeUnit unit)// 在所有参与者都已经在此屏障上调用 await 方法之前将一直等待,或者超出了指定的等待时间。int getNumberWaiting()// 返回当前在屏障处等待的参与者数目。int getParties()// 返回要求启动此 barrier 的参与者数目。boolean isBroken()// 查询此屏障是否处于损坏状态。void reset()// 将屏障重置为其初始状态。12345678910111213141516171819202122232425262728291. 构造函数CyclicBarrier的构造函数共2个：CyclicBarrier 和 CyclicBarrier(int parties, Runnable barrierAction)。第1个构造函数是调用第2个构造函数来实现的，2. 等待函数说明：await()是通过dowait()实现的。说明：dowait()的作用就是让当前线程阻塞，直到“有parties个线程到达barrier” 或 “当前线程被中断” 或 “超时”这3者之一发生，当前线程才继续执行。(01) generation是CyclicBarrier的一个成员遍历，它的定义如下：private Generation generation = new Generation();private static class Generation {    boolean broken = false;}在CyclicBarrier中，同一批的线程属于同一代，即同一个Generation；CyclicBarrier中通过generation对象，记录属于哪一代。当有parties个线程到达barrier，generation就会被更新换代。(02) 如果当前线程被中断，即Thread.interrupted()为true；则通过breakBarrier()终止CyclicBarrier。breakBarrier()的源码如下：private void breakBarrier() {    generation.broken = true;    count = parties;    trip.signalAll();}breakBarrier()会设置当前中断标记broken为true，意味着“将该Generation中断”；同时，设置count=parties，即重新初始化count；最后，通过signalAll()唤醒CyclicBarrier上所有的等待线程。(03) 将“count计数器”-1，即--count；然后判断是不是“有parties个线程到达barrier”，即index是不是为0。当index=0时，如果barrierCommand不为null，则执行该barrierCommand，barrierCommand就是我们创建CyclicBarrier时，传入的Runnable对象。然后，调用nextGeneration()进行换代工作，nextGeneration()的源码如下：private void nextGeneration() {    trip.signalAll();    count = parties;    generation = new Generation();}首先，它会调用signalAll()唤醒CyclicBarrier上所有的等待线程；接着，重新初始化count；最后，更新generation的值。(04) 在for(;;)循环中。timed是用来表示当前是不是“超时等待”线程。如果不是，则通过trip.await()进行等待；否则，调用awaitNanos()进行超时等待。CyclicBarrier的使用示例123456789101112131415161718192021222324252627282930313233343536373839404142// 新建5个线程，这5个线程达到一定的条件时，它们才继续往后运行。import java.util.concurrent.CyclicBarrier;import java.util.concurrent.BrokenBarrierException;public class CyclicBarrierTest1 {    private static int SIZE = 5;    private static CyclicBarrier cb;    public static void main(String[] args) {        cb = new CyclicBarrier(SIZE);        // 新建5个任务        for(int i=0; i&lt;SIZE; i++)            new InnerThread().start();    }    static class InnerThread extends Thread{        public void run() {            try {                System.out.println(Thread.currentThread().getName() + &quot; wait for CyclicBarrier.&quot;);                // 将cb的参与者数量加1                cb.await();                // cb的参与者数量等于5时，才继续往后执行                System.out.println(Thread.currentThread().getName() + &quot; continued.&quot;);            } catch (BrokenBarrierException e) {                e.printStackTrace();            } catch (InterruptedException e) {                e.printStackTrace();            }        }    }}Thread-1 wait for CyclicBarrier.Thread-2 wait for CyclicBarrier.Thread-3 wait for CyclicBarrier.Thread-4 wait for CyclicBarrier.Thread-0 wait for CyclicBarrier.Thread-0 continued.Thread-4 continued.Thread-2 continued.Thread-3 continued.Thread-1 continued.结果说明：主线程中新建了5个线程，所有的这些线程都调用cb.await()等待。所有这些线程一直等待，直到cb中所有线程都达到barrier时，这些线程才继续运行！12345678910111213141516171819202122232425262728293031323334353637383940414243444546// 新建5个线程，当这5个线程达到一定的条件时，执行某项任务。import java.util.concurrent.CyclicBarrier;import java.util.concurrent.BrokenBarrierException;public class CyclicBarrierTest2 {    private static int SIZE = 5;    private static CyclicBarrier cb;    public static void main(String[] args) {        cb = new CyclicBarrier(SIZE, new Runnable () {            public void run() {                System.out.println(&quot;CyclicBarrier's parties is: &quot;+ cb.getParties());            }        });        // 新建5个任务        for(int i=0; i&lt;SIZE; i++)            new InnerThread().start();    }    static class InnerThread extends Thread{        public void run() {            try {                System.out.println(Thread.currentThread().getName() + &quot; wait for CyclicBarrier.&quot;);                // 将cb的参与者数量加1                cb.await();                // cb的参与者数量等于5时，才继续往后执行                System.out.println(Thread.currentThread().getName() + &quot; continued.&quot;);            } catch (BrokenBarrierException e) {                e.printStackTrace();            } catch (InterruptedException e) {                e.printStackTrace();            }        }    }}Thread-1 wait for CyclicBarrier.Thread-2 wait for CyclicBarrier.Thread-3 wait for CyclicBarrier.Thread-4 wait for CyclicBarrier.Thread-0 wait for CyclicBarrier.CyclicBarrier's parties is: 5Thread-0 continued.Thread-4 continued.Thread-2 continued.Thread-3 continued.Thread-1 continued.    结果说明：主线程中新建了5个线程，所有的这些线程都调用cb.await()等待。所有这些线程一直等待，直到cb中所有线程都达到barrier时，执行新建cb时注册的Runnable任务。Semaphore信号量的原理和示例123Semaphore是一个计数信号量，它的本质是一个&quot;共享锁&quot;。信号量维护了一个信号量许可集。线程可以通过调用acquire()来获取信号量的许可；当信号量中有可用的许可时，线程能获取该许可；否则线程必须等待，直到有可用的许可为止。 线程可以通过release()来释放它所持有的信号量许可。1234567891011121314151617181920212223242526272829303132333435363738394041// Semaphore的函数列表// 创建具有给定的许可数和非公平的公平设置的 Semaphore。Semaphore(int permits)// 创建具有给定的许可数和给定的公平设置的 Semaphore。Semaphore(int permits, boolean fair)// 从此信号量获取一个许可，在提供一个许可前一直将线程阻塞，否则线程被中断。void acquire()// 从此信号量获取给定数目的许可，在提供这些许可前一直将线程阻塞，或者线程已被中断。void acquire(int permits)// 从此信号量中获取许可，在有可用的许可前将其阻塞。void acquireUninterruptibly()// 从此信号量获取给定数目的许可，在提供这些许可前一直将线程阻塞。void acquireUninterruptibly(int permits)// 返回此信号量中当前可用的许可数。int availablePermits()// 获取并返回立即可用的所有许可。int drainPermits()// 返回一个 collection，包含可能等待获取的线程。protected Collection&lt;Thread&gt; getQueuedThreads()// 返回正在等待获取的线程的估计数目。int getQueueLength()// 查询是否有线程正在等待获取。boolean hasQueuedThreads()// 如果此信号量的公平设置为 true，则返回 true。boolean isFair()// 根据指定的缩减量减小可用许可的数目。protected void reducePermits(int reduction)// 释放一个许可，将其返回给信号量。void release()// 释放给定数目的许可，将其返回到信号量。void release(int permits)// 返回标识此信号量的字符串，以及信号量的状态。String toString()// 仅在调用时此信号量存在一个可用许可，才从信号量获取许可。boolean tryAcquire()// 仅在调用时此信号量中有给定数目的许可时，才从此信号量中获取这些许可。boolean tryAcquire(int permits)// 如果在给定的等待时间内此信号量有可用的所有许可，并且当前线程未被中断，则从此信号量获取给定数目的许可。boolean tryAcquire(int permits, long timeout, TimeUnit unit)// 如果在给定的等待时间内，此信号量有可用的许可并且当前线程未被中断，则从此信号量获取一个许可。boolean tryAcquire(long timeout, TimeUnit unit)1234Semaphore是通过共享锁实现的。根据共享锁的获取原则，Semaphore分为&quot;公平信号量&quot;和&quot;非公平信号量&quot;。&quot;公平信号量&quot;和&quot;非公平信号量&quot;的区别&quot;公平信号量&quot;和&quot;非公平信号量&quot;的释放信号量的机制是一样的！不同的是它们获取信号量的机制：线程在尝试获取信号量许可时，对于公平信号量而言，如果当前线程不在CLH队列的头部，则排队等候；而对于非公平信号量而言，无论当前线程是不是在CLH队列的头部，它都会直接获取信号量。该差异具体的体现在，它们的tryAcquireShared()函数的实现不同。公平信号量获取和释放1234567891011121314151617181. 公平信号量的获取Semaphore中的公平信号量是FairSync。信号量中的acquire()获取函数，实际上是调用的AQS中的acquireSharedInterruptibly()。说明：tryAcquireShared()的作用是尝试获取acquires个信号量许可数。对于Semaphore而言，state表示的是“当前可获得的信号量许可数”。说明：doAcquireSharedInterruptibly()会使当前线程一直等待，直到当前线程获取到共享锁(或被中断)才返回。(01) addWaiter(Node.SHARED)的作用是，创建”当前线程“的Node节点，且Node中记录的锁的类型是”共享锁“(Node.SHARED)；并将该节点添加到CLH队列末尾。关于Node和CLH在&quot;Java多线程系列--“JUC锁”03之 公平锁(一)&quot;已经详细介绍过，这里就不再重复说明了。(02) node.predecessor()的作用是，获取上一个节点。如果上一节点是CLH队列的表头，则”尝试获取共享锁“。(03) shouldParkAfterFailedAcquire()的作用和它的名称一样，如果在尝试获取锁失败之后，线程应该等待，则返回true；否则，返回false。(04) 当shouldParkAfterFailedAcquire()返回ture时，则调用parkAndCheckInterrupt()，当前线程会进入等待状态，直到获取到共享锁才继续运行。2. 公平信号量的释放信号量的releases()释放函数，实际上是调用的AQS中的releaseShared()。说明：releaseShared()的目的是让当前线程释放它所持有的共享锁。它首先会通过tryReleaseShared()去尝试释放共享锁。尝试成功，则直接返回；尝试失败，则通过doReleaseShared()去释放共享锁。Semaphore重写了tryReleaseShared()，如果tryReleaseShared()尝试释放共享锁失败，则会调用doReleaseShared()去释放共享锁。说明：doReleaseShared()会释放“共享锁”。它会从前往后的遍历CLH队列，依次“唤醒”然后“执行”队列中每个节点对应的线程；最终的目的是让这些线程释放它们所持有的信号量。非公平信号量获取和释放1234Semaphore中的非公平信号量是NonFairSync。在Semaphore中，“非公平信号量许可的释放(release)”与“公平信号量许可的释放(release)”是一样的。不同的是它们获取“信号量许可”的机制不同，说明：非公平信号量的tryAcquireShared()调用AQS中的nonfairTryAcquireShared()。而在nonfairTryAcquireShared()的for循环中，它都会直接判断“当前剩余的信号量许可数”是否足够；足够的话，则直接“设置可以获得的信号量许可数”，进而再获取信号量。而公平信号量的tryAcquireShared()中，在获取信号量之前会通过if (hasQueuedPredecessors())来判断“当前线程是不是在CLH队列的头部”，是的话，则返回-1。 Semaphore示例12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.Semaphore; public class SemaphoreTest1 {     private static final int SEM_MAX = 10;    public static void main(String[] args) {         Semaphore sem = new Semaphore(SEM_MAX);        //创建线程池        ExecutorService threadPool = Executors.newFixedThreadPool(3);        //在线程池中执行任务        threadPool.execute(new MyThread(sem, 5));        threadPool.execute(new MyThread(sem, 4));        threadPool.execute(new MyThread(sem, 7));        //关闭池        threadPool.shutdown();    }}class MyThread extends Thread {    private volatile Semaphore sem;    // 信号量    private int count;        // 申请信号量的大小     MyThread(Semaphore sem, int count) {        this.sem = sem;        this.count = count;    }    public void run() {        try {            // 从信号量中获取count个许可            sem.acquire(count);            Thread.sleep(2000);            System.out.println(Thread.currentThread().getName() + &quot; acquire count=&quot;+count);        } catch (InterruptedException e) {            e.printStackTrace();        } finally {            // 释放给定数目的许可，将其返回到信号量。            sem.release(count);            System.out.println(Thread.currentThread().getName() + &quot; release &quot; + count + &quot;&quot;);        }    }}(某一次)运行结果：pool-1-thread-1 acquire count=5pool-1-thread-2 acquire count=4pool-1-thread-1 release 5pool-1-thread-2 release 4pool-1-thread-3 acquire count=7pool-1-thread-3 release 7结果说明：信号量sem的许可总数是10个；共3个线程，分别需要获取的信号量许可数是5,4,7。前面两个线程获取到信号量的许可后，sem中剩余的可用的许可数是1；因此，最后一个线程必须等前两个线程释放了它们所持有的信号量许可之后，才能获取到7个信号量许可。</content>
	</entry>
	<entry>
		<title>python异步编程</title>
		<url>/article/49959.html</url>
		<content>基础概念python3 asyncio官方文档中文版asyncio —- 异步 I/O2小时学会python asyncio【花39大洋买的课程】Python进程、线程和协程实战指归异步编程基本概念python—-异步IO(asyncio)协程Python黑魔法 —- 异步IO(asyncio)协程python协程系列（六）——asyncio的EventLoop以及Future详解python协程系列（七）——asyncio结合多线程解决阻塞问题以及timer模拟asyncio —— 异步I/O、事件循环、协程和任务python协程与异步协程[进阶]-Python3 异步编程详解（史上最全篇）python多线程、多进程、协程的使用Python异步IO之协程(一): 从yield from到async的使用python异步编程模块asyncio学习(一)python异步编程模块asyncio学习(二)Python中协程异步IO（asyncio）详解理解 Python 中的异步编程Python异步编程模块asyncio学习 !此模块非常之重要!带你简单了解python协程和异步进程、线程和协程线程是并发，进程是并行；进程之间相互独立，是系统分配资源的最小单位，同一个进程中的所有线程共享资源进程拥有自己的内存空间，所以进程间数据不共享，开销大。线程：调度执行的最小单位，也叫执行路径，不能独立存在，依赖进程的存在而存在一个进程至少有一个线程，叫主线程，多个线程共享内存（数据共享和全局变量），因此提升程序的运行效率。协程：用户态的轻量级线程，调度有用户控制，拥有自己的寄存器上下文和栈，切换基本没有内核切换的开销，切换灵活。进程进程最大优势是可以充分例用计算资源使用进程处理计算密集型任务：因为不同的进程可以运行的不同CPU的不同的核上假如一台计算机的CPU共有16核，则可以启动16个或更多个进程来并行处理任务使用单个线程或两个线程的时候，耗时大约30+秒，改用两个进程后，耗时17.786秒，差不多快了一倍如果使用4个进程（前提是运行的代码的计算机至少有4个CPU核）的话，速度还能提高一倍对于计算密集型的任务，使用多进程并行处理是有效的提速手段，通常，进程数量选择CPU核数的整倍数进程通信线程间通信可以使用队列、互斥锁、信号量、事件和条件等多种同步方式，同样的，这些手段也可以应用在进程间此外，multiprocessing 模块还提供了管道和共享内存等进程间通信的手段。进程池使用多进程并行处理任务时，处理效率和进程数量并不总是成正比，当进程数量超过一定限度后，完成任务所需时间反而会延长进程池提供了一个保持合理进程数量的方案，但合理进程数量需要根据硬件状况及运行状况来确定，通常设置为 CPU 的核数multiprocessing.Pool(n) 可创建 n 个进程的进程池供用户调用如果进程池任务不满，则新的进程请求会被立即执行；如果进程池任务已满，则新的请求将等待至有可用进程时才被执行向进程池提交任务有以下两种方式apply_async(func[, args[, kwds[, callback]]]) ：非阻塞式提交。即使进程池已满，也会接受新的任务，不会阻塞主进程新任务将处于等待状态apply(func[, args[, kwds]]) ：阻塞式提交。若进程池已满，则主进程阻塞，直至有空闲进程可以使用线程线程的最大意义在于并行使用线程处理IO密集型任务：对于IO密集型（本例仅测试网络IO，没有磁盘IO）的任务，适量的线程可以在一定程度上提高处理速度随着线程数量的增加，速度的提升不再明显使用线程处理**计算密集型**任务: 对一张千万级像素的照片做低端增强，借助于NumPy的广播和矢量化计算，耗时0.38秒钟；单线程逐像素处理的话，耗时相当于NumPy的100倍；启用多线程的话，速度不仅没有加快，反倒是比单线程更慢这说明，对于计算密集型的任务来说，多线程并不能提高处理速度，相反，因为要创建和管理线程，处理速度会更慢一些线程池尽管多线程可以并行处理多个任务，但开启线程不仅花费时间，也需要占用系统资源因此，线程数量不是越多越快，而是要保持在合理的水平上线程池可以让我们用固定数量的线程完成比线程数量多得多的任务协程谈谈Python协程技术的演进概念线程常用于多任务并行。对于可以切分的IO密集型任务，将切分的每一小块任务分配给一个线程，可以显著提高处理速度而协程，无论有多少个，都被**限定在一个线程内执行**，因此，协程又被称为**微线程**从宏观上看，线程任务和协程任务都是并行的从微观上看，线程任务是分时切片轮流执行的，这种切换是系统自动完成的，无需程序员干预而协程则是根据任务特点，在任务阻塞时将控制权交给其他协程，这个权力交接的时机和位置，由程序员指定参与协程管理的每一个任务，必须存在阻塞的可能，且阻塞条件会被其它任务破坏，从而得以在阻塞解除后继续执行尽管协程难以驾驭，但是由于是在一个线程内运行，免除了线程或进程的切换开销，因而协程的运行效率高，在特定场合下仍然被广泛使用。协程演进史Py2时代，Python并不支持协程，仅可通过yield实现部分的协程功能另外可以通过gevent等第三方库实现协程，gevent最好玩的，莫过于monkey_patch(猴子补丁)Py3.4开始，Python内置asyncio标准库，正式原生支持协程asyncio的异步操作，需要在协程中通过yield from完成，协程函数则需要使用@asyncio.coroutine装饰器为了更贴近人类思维，Py3.5引入了新的语法async和await，可以让协程的代码稍微易懂一点点本质上，async就是@asyncio.coroutine，替换为await就是yield from，换个马甲，看起来就顺眼多了三者关系使用场景计算密集型(CPU操作指令比较多，如科学计算，位数多的浮点运算)* 考虑可以使用多核 CPU，使用多进程I/O密集型(读写数据操作较多的，比如爬虫)* I/O请求比较耗时的话，使用协程* I/O请求比较快的话，使用多线程协程和多线程异同点比较共同点：都是并发操作，多线程同一时间点只能有一个线程在执行，协程同一时间点只能有一个任务在执行不同点：多线程，是在I/O阻塞时通过切换线程来达到并发的效果在什么情况下做线程切换是由操作系统来决定的，开发者不用操心，但会造成竞争条件 (race condition) 协程，只有一个线程，在I/O阻塞时通过在线程内切换任务来达到并发的效果在什么情况下做任务切换是开发者决定的，不会有竞争条件 (race condition) 的情况多线程的线程切换比协程的任务切换开销更大对于开发者而言，多线程并发的代码比协程并发的更容易书写，一般情况下协程并发的处理效率比多线程并发更高ps：对第一点和第二点补充对于io阻塞的操作，协程相较于线程，能更精确的获取（或者释放）对资源的控制权这是因为用户层相较于语言层，用户层能更好的感知特定操作的时机对于非io阻塞的操作，线程相较于协程，能更公平的分配对资源的控制权这是因为语言层相较于用户层，语言层能更好的感知到多个线程的运行状态并在掌握更多信息的前提下（线程运行的字节码和时长），进行更加合理的GIL的获取和释放阻塞和非阻塞阻塞程序未得到所需计算资源时被挂起的状态程序在等待某个操作完成期间，自身无法继续干别的事情，称程序在该操作上是阻塞的常见的阻塞形式有：网络I/O阻塞、磁盘I/O阻塞、用户输入阻塞等阻塞是无处不在的，包括CPU切换上下文时，所有的进程都无法真正干事情，它们也会被阻塞（如果是多核CPU则正在执行上下文切换操作的核不可被利用）非阻塞程序在等待某操作过程中，自身不被阻塞，可以继续运行干别的事情，则称该程序在该操作上是非阻塞的非阻塞并不是在任何程序级别、任何情况下都可以存在的仅当程序封装的级别可以囊括独立的子程序单元时，它才可能存在非阻塞状态非阻塞的存在是因为阻塞存在，正因为某个操作阻塞导致的耗时与效率低下，我们才要把它变成非阻塞的异步和同步同步不同程序单元为了完成某个任务，在执行过程中需靠某种通信方式以协调一致，称这些程序单元是同步执行的例如购物系统中更新商品库存，需要用“行锁”作为通信信号，让不同的更新请求强制排队顺序执行，那更新库存的操作是同步的简言之，同步意味着有序异步为完成某个任务，不同程序单元之间过程中无需通信协调，也能完成任务的方式不相关的程序单元之间可以是异步的例如，爬虫下载网页。调度程序调用下载程序后，即可调度其他任务，而无需与该下载任务保持通信以协调行为不同网页的下载、保存等操作都是无关的，也无需相互通知协调。这些异步操作的完成时刻并不确定简言之，异步意味着无序当一个异步过程调用发出后，调用者在没有得到结果之前，就可以继续执行后续操作当这个调用完成后，一般通过状态、通知和回调来通知调用者对于异步调用，调用的返回并不受调用者控制对于通知调用者的三种方式状态：即监听被调用者的状态(轮询)，调用者需要每隔一定时间检查一次，效率会很低通知：当被调用者执行完成后，发出通知告知调用者，无需消耗太多性能回调：与通知类似，当被调用者执行完成后，会调用调用者提供的回调函数并行和并发并发(多线程)任务特点：IO密集型任务：任务包含频繁的、持续的网络IO和磁盘IO单个CPU（也可以多个CPU）将多个线程中的每个线程（多个进程中的每个进程）按时间分为一个一个的时间片每一个时刻只执行某个线程（进程）的时间片，时间片过期后转而执行下一个线程（进程）的时间片并发描述的是程序的组织结构。指程序要被设计成多个可独立执行的子任务以利用有限的计算机资源使多个任务可以被实时或近实时执行为目的并发提供了一种程序组织结构方式，让问题的解决方案可以并行执行注：并发宏观上看起来像是并行但是微观上并不能做到并行并行(多进程)任务特点：计算密集型任务：任务包含大量计算，CPU占用率高当有多个CPU或者是多核CPU时才有可能实现并行，并行就是多个线程或者多个进程同时运行并行描述的是程序的执行状态。指多个任务同时被执行以利用富余计算资源（多核CPU）加速完成多个任务为目的组合分析同步阻塞、同步非阻塞，异步阻塞、异步非阻塞举个简单的例子来描述这四种情况，老张要做两件事，用水壶烧开水，看电视，两件事情即两个任务，两个函数同步阻塞：老张把水壶放到火上，就坐在那里等水开，开了之后我再去看电视同步非阻塞：老张把水壶放到火上，去客厅看电视，时不时去厨房看看水开没有老张还是觉得自己有点傻，于是变高端了，买了把会响笛的那种水壶。水开之后，能大声发出嘀~~~~的噪音异步阻塞：老张把响水壶放到火上，然后就坐在旁边等着听那个烧开的提示音异步非阻塞：老张把响水壶放到火上，去客厅看电视，水壶响之前不再去看它了，响了再去拿壶乍一看，这同步阻塞、意不阻塞似乎没有什么区别，但实际上是有区别的，所谓同步异步，指的是消息通知的机制区别在哪里呢在这个例子中同步异步只是对于水壶而言。在使用普通水壶的时候，我要自己主动去观察水是不是烧开了，自己主动去获取烧开的这个结果，即所谓的同步但是在响水壶的时候，我不需要再管水烧到什么程度了，因为只要水烧开了，那个滴滴的噪声就会通知我的，即所谓的异步他们的相同点是，在烧水的过程中，老王啥也没干，即“阻塞”四种总结——同步/异步与阻塞/非阻塞同步阻塞形式：效率是最低的。拿上面的例子来说，在烧水的过程中，什么别的事都不做同步非阻塞形式：实际上是效率低下的。因为老王需要不断的在看电视与烧水之间来回跑动，看一下电视，又要去看一下水烧开               没有，这样来回跑很多次，在程序中，程序需要在这两种不同的行为之间来回的切换，效率可想而知是低下的异步阻塞形式：异步操作是可以被阻塞住的，只不过它不是在处理消息时阻塞，而是在等待消息通知时被阻塞这个效率其实跟同步阻塞差不多的异步非阻塞形式：效率更高。因为老王把水烧好之后就不用管了，可以安安心心去看电视，不用来回奔波看水烧开了没因为水烧开了会有提示告诉他水烧好了，这样效率岂不是更高那有没有更好的办法？当然有，如果老王还有一个帮手老张，让老王自己看电视、同时老张去烧开水，这样岂不是更好？这就是所谓的并行并发/并行、同步/异步、阻塞/非阻塞并发/并行：即能够开启多个任务，多个任务交替执行为并发，多个任务同时执行为并行同步/异步：关注的是消息通知的机制，主动等候消息则为同步、被动听消息则为异步阻塞/非阻塞：关注的是等候消息的过程中有没有干其他事总结：上面的几组概念，时刻穿插的，并没有完全的等价关系所以经常有人说，异步就是非阻塞，同步就是阻塞，并发就是非阻塞、并行就是非阻塞，这些说法都是不完全准确地。概念总结要支持并发，必须拆分为多任务，不同任务相对而言才有阻塞/非阻塞、同步/异步并行是为了利用多核加速多任务完成的进度并发是为了让独立的子任务都有机会被尽快执行，但不一定能加速整体进度非阻塞是为了提高程序整体执行效率异步是高效地组织非阻塞任务的方式所以，并发、异步、非阻塞三个词总是如影随形asyncio 标准库协程核心概念python协程的多种实现方式在Python中有多种方式可以实现协程，例如：greenlet，是一个第三方模块，用于实现协程代码（Gevent协程就是基于greenlet实现）yield，生成器，借助生成器的特点也可以实现协程代码asyncio，在Python3.4中引入的模块用于编写协程代码async &amp; awiat，Python3.5中引入的关键字，结合asyncio模块可以更方便的编写协程代码目前主流使用是Python官方推荐的 asyncio 模块和 async&amp;await 关键字的方式例如：在tonado、sanic、fastapi、django3 中均已支持任务类型计算密集型任务：任务包含大量计算，CPU占用率高IO密集型任务：任务包含频繁的、持续的网络IO和磁盘IO混合型任务：既有计算也有IO协程状态协程函数相比于一般的函数来说，我们可以将协程包装成任务Task，任务Task就在于可以跟踪它的状态，我就知道它具体执行到哪一步了一般来说，协程函数具有4种状态Pending：创建future的时候，task为pendingRunning：事件循环调用执行的时候当然就是runningDone：调用完毕自然就是doneCacelled：停止事件循环，就需要先把task取消，即为cancelledgreenlet库 greentlet是一个第三方模块，需要提前安装 pip3 install greenlet才能使用12345678910111213from greenlet import greenletdef func1():    print(1)        # 第1步：输出 1    gr2.switch()    # 第3步：切换到 func2 函数    print(2)        # 第6步：输出 2    gr2.switch()    # 第7步：切换到 func2 函数，从上一次执行的位置继续向后执行def func2():    print(3)        # 第4步：输出 3    gr1.switch()    # 第5步：切换到 func1 函数，从上一次执行的位置继续向后执行    print(4)        # 第8步：输出 4gr1 = greenlet(func1)gr2 = greenlet(func2)gr1.switch() # 第1步：去执行 func1 函数 注意：switch中也可以传递参数用于在切换执行时相互传递值yield关键字 基于Python的生成器的yield和yield form关键字实现协程代码12345678910111213def func1():    yield 1    yield from func2()    yield 2    def func2():    yield 3    yield 4    f1 = func1()for item in f1:    print(item) 注意：yield form关键字是在Python3.3中引入的入门示例asyncio模块在Python3.4之前官方未提供协程的类库，一般大家都是使用greenlet等其他来实现在Python3.4发布后官方正式支持协程，即：asyncio模块123456789101112131415161718import asyncio@asyncio.coroutinedef func1():    print(1)    yield from asyncio.sleep(2)  # 遇到IO耗时操作，自动化切换到tasks中的其他任务    print(2)    @asyncio.coroutinedef func2():    print(3)    yield from asyncio.sleep(2) # 遇到IO耗时操作，自动化切换到tasks中的其他任务    print(4)tasks = [asyncio.ensure_future( func1() ),         asyncio.ensure_future( func2() )]loop = asyncio.get_event_loop()loop.run_until_complete(asyncio.wait(tasks))注意：基于asyncio模块实现的协程比之前的要厉害，内部集成了遇到IO耗时操作自动切换的功能 async &amp; awit 关键字async &amp; awit 关键字在Python3.5版本中正式引入，让代码可以更加简便Python3.8之后 @asyncio.coroutine 装饰器就会被移除，推荐使用async &amp; awit 关键字实现协程代码12345678910111213141516import asyncioasync def func1():    print(1)    await asyncio.sleep(2)    print(2)    async def func2():    print(3)    await asyncio.sleep(2)    print(4)    tasks = [asyncio.ensure_future( func1() ),         asyncio.ensure_future( func2() )]loop = asyncio.get_event_loop()loop.run_until_complete(asyncio.wait(tasks))进阶示例用代码实现下载 url_list 中的图片 同步编程实现 12345678910111213141516171819202122232425&quot;&quot;&quot;下载图片使用第三方模块requests，请提前安装：pip3 install requests&quot;&quot;&quot;import requestsdef download_image(url):    print(&quot;开始下载:&quot;,url)        # 发送网络请求，下载图片    response = requests.get(url)    print(&quot;下载完成&quot;)        # 图片保存到本地文件    file_name = url.rsplit('_')[-1]    with open(file_name, mode='wb') as file_object:        file_object.write(response.content)        if __name__ == '__main__':    url_list = [        'https://www3.autoimg.cn/newsdfs/g26/M02/35/A9/120x90_0_autohomecar__ChsEe12AXQ6AOOH_AAFocMs8nzU621.jpg',        'https://www2.autoimg.cn/newsdfs/g30/M01/3C/E2/120x90_0_autohomecar__ChcCSV2BBICAUntfAADjJFd6800429.jpg',        'https://www3.autoimg.cn/newsdfs/g26/M0B/3C/65/120x90_0_autohomecar__ChcCP12BFCmAIO83AAGq7vK0sGY193.jpg'    ]    for item in url_list:        download_image(item)基于协程的异步编程实现1234567891011121314151617181920212223242526272829303132333435363738#!/usr/bin/env Python# -- coding: utf-8 --&quot;&quot;&quot;@version: v1.0@author: huangyc@file: asyncio_test_http.py@Description: 下载图片使用第三方模块aiohttp，请提前安装：pip3 install aiohttp@time: 2021/2/19 9:23&quot;&quot;&quot;import aiohttpimport asyncioasync def fetch(session, url):    print(&quot;发送请求：&quot;, url)    async with session.get(url, verify_ssl=False) as response:        content = await response.content.read()        file_name = url.rsplit('_')[-1]        with open(file_name, mode='wb') as file_object:            file_object.write(content)async def main():    async with aiohttp.ClientSession() as session:        url_list = [            'https://www3.autoimg.cn/newsdfs/g26/M02/35/A9/120x90_0_autohomecar__ChsEe12AXQ6AOOH_AAFocMs8nzU621.jpg',            'https://www2.autoimg.cn/newsdfs/g30/M01/3C/E2/120x90_0_autohomecar__ChcCSV2BBICAUntfAADjJFd6800429.jpg',            'https://www3.autoimg.cn/newsdfs/g26/M0B/3C/65/120x90_0_autohomecar__ChcCP12BFCmAIO83AAGq7vK0sGY193.jpg'        ]        tasks = [asyncio.create_task(fetch(session, url)) for url in url_list]        await asyncio.wait(tasks)if __name__ == '__main__':    asyncio.run(main())上述两种的执行对比之后会发现，基于协程的异步编程 要比 同步编程的效率高了很多同步编程，按照顺序逐一排队执行，如果图片下载时间为2分钟，那么全部执行完则需要6分钟。异步编程，几乎同时发出了3个下载任务的请求（遇到IO请求自动切换去发送其他任务请求），如果图片下载时间为2分钟，那么全部执行完毕也大概需要2分钟左右就可以了。第三方模块不支持协程方式异步编程12345678910111213141516171819202122232425import asyncioimport requestsasync def download_image(url):    # 发送网络请求，下载图片（遇到网络下载图片的IO请求，自动化切换到其他任务）    print(&quot;开始下载:&quot;, url)    loop = asyncio.get_event_loop()    # requests模块默认不支持异步操作，所以就使用线程池来配合实现了。    future = loop.run_in_executor(None, requests.get, url)    response = await future    print('下载完成')    # 图片保存到本地文件    file_name = url.rsplit('_')[-1]    with open(file_name, mode='wb') as file_object:        file_object.write(response.content)if __name__ == '__main__':    url_list = [        'https://www3.autoimg.cn/newsdfs/g26/M02/35/A9/120x90_0_autohomecar__ChsEe12AXQ6AOOH_AAFocMs8nzU621.jpg',        'https://www2.autoimg.cn/newsdfs/g30/M01/3C/E2/120x90_0_autohomecar__ChcCSV2BBICAUntfAADjJFd6800429.jpg',        'https://www3.autoimg.cn/newsdfs/g26/M0B/3C/65/120x90_0_autohomecar__ChcCP12BFCmAIO83AAGq7vK0sGY193.jpg'    ]    tasks = [download_image(url) for url in url_list]    loop = asyncio.get_event_loop()    loop.run_until_complete(asyncio.wait(tasks))核心概念python协程系列（五）——asyncio的核心概念与基本架构协程函数和对象协程函数：定义形式为 async def fun() 的函数在 Python 3.4 中，asyncio 模块出现，此时创建协程函数须使用 asyncio.coroutine 装饰器标记此前的包含 yield from 语句的函数既可以称作生成器函数也可以称作协程函数为了突出协程的重要性，现在使用 asyncio.coroutine 装饰器的函数就是真正的协程函数了协程对象：调用 协程函数 所返回的对象即协程函数的运行结果为协程对象，协程对象需要包装成任务注入到事件循环，由事件循环调用怎么判断一个函数是不是协程？通过asyncio.iscoroutine（obj）和 asyncio.iscoroutinefunction(func)加以判断，返回true，则是12345# 定义一个协程函数async def func():    pass# 调用协程函数，返回一个协程对象result = func()注意：调用协程函数时，函数内部代码不会执行，只是会返回一个协程对象要执行协程函数的内部代码，需要 事件循环 和 协程对象 配合才能实现123456789101112131415import asyncioasync def func():    print(&quot;协程内部代码&quot;)    # 调用协程函数，返回一个协程对象。result = func()# 方式一# loop = asyncio.get_event_loop() # 创建一个事件循环# loop.run_until_complete(result) # 将协程当做任务提交到事件循环的任务列表中，协程执行完成之后终止。# 方式二# 本质上方式一是一样的，内部先 创建事件循环 然后执行 run_until_complete，一个简便的写法。# asyncio.run 函数在 Python 3.7 中加入 asyncio 模块，asyncio.run(result)这个过程可以简单理解为：将协程当做任务添加到 事件循环 的任务列表然后事件循环检测列表中的协程是否 已准备就绪（默认可理解为就绪状态）如果准备就绪则执行其内部代码Task 对象Tasks are used to schedule coroutines concurrently.When a coroutine is wrapped into a Task with functions like asyncio.create_task() the coroutine is automatically scheduled to run soon。将协程对象作为参数创建任务，任务是对协程对象的封装，其中包含任务的各种状态，是一个可以挂起的函数协程不是线程安全，这样可以让协程加入事件循环中等待被调度执行异步编程最重要的就是对异步操作状态的把控(1) 创建任务（两种方法）：task = asyncio.create_task(coro())  # 这是3.7版本新添加的(建议)task = asyncio.ensure_future(coro()) #  Python 3.7 之前，可以使用低层级的函数也可以使用：loop.create_future()或loop.create_task(coro) (不建议手动实例化 Task 对象)本质上是将协程对象封装成task对象，并将协程立即加入事件循环，同时追踪协程的状态12345678910111213141516171819202122import asyncioasync def func():    print(1)    await asyncio.sleep(2)    print(2)    return &quot;返回值&quot;async def main():    print(&quot;main开始&quot;)    # 创建协程，将协程封装到Task对象中并添加到事件循环的任务列表中，等待事件循环去执行（默认是就绪状态）。    # 在调用    task_list = [asyncio.create_task(func(), name=&quot;n1&quot;),                 asyncio.create_task(func(), name=&quot;n2&quot;)]    print(&quot;main结束&quot;)    # 当执行某协程遇到IO操作时，会自动化切换执行其他任务。    # 此处的await是等待所有协程执行完毕，并将所有协程的返回值保存到done    # 如果设置了timeout值，则意味着此处最多等待的秒，完成的协程返回值写入到done中，未完成则写到pending中。    done, pending = await asyncio.wait(task_list, timeout=None)    print(done, pending)    asyncio.run(main())注意：asyncio.wait 源码内部会对列表中的每个协程执行ensure_future从而封装为Task对象所以在和wait配合使用时task_list的值为[func(),func()] 也是可以的。 或者：123456789101112131415import asyncioasync def func():    print(&quot;执行协程函数内部代码&quot;)    # 遇到IO操作挂起当前协程（任务），等IO操作完成之后再继续往下执行。当前协程挂起时，事件循环可以去执行其他协程（任务）。    response = await asyncio.sleep(2)    print(&quot;IO请求结束，结果为：&quot;, response)    coroutine_list = [func(), func()]# 错误：coroutine_list = [ asyncio.create_task(func()), asyncio.create_task(func()) ]  # 此处不能直接 asyncio.create_task，因为将Task立即加入到事件循环的任务列表，# 但此时事件循环还未创建，所以会报错。# 使用asyncio.wait将列表封装为一个协程，并调用asyncio.run实现执行两个协程# asyncio.wait内部会对列表中的每个协程执行ensure_future，封装为Task对象。done,pending = asyncio.run( asyncio.wait(coroutine_list) )(2) 获取某一个任务的方法：返回在某一个指定的loop中，当前正在运行的任务，如果没有任务正在运行，则返回None，如果loop为None，则默认为在当前的事件循环中获取task=asyncio.current_task(loop=None)返回某一个loop中还没有结束的任务asyncio.all_tasks(loop=None)(3) Task类常见的一些使用函数cancel()： 最好是使用他会出发CancelledError异常，所以需要取消的协程函数里面的代码最好在try-except语句块中进行，这样方便触发异常，打印相关信息，但是Task.cancel()没有办法保证任务一定会取消，而Future.cancel()是可以保证任务一定取消的 done()：当一个被包装得协程既没有触发异常、也没有被取消的时候，意味着它是done的，返回trueresult()：返回任务的执行结果，当任务被正常执行完毕，则返回结果；当任务被取消了，调用这个方法，会触发CancelledError异常；当任务返回的结果是无用的时候，则调用这个方法会触发InvalidStateError；当任务出发了一个异常而中断，调用这个方法还会再次触发这个使程序中断的异常。exception()：返回任务的异常信息，触发了什么异常，就返回什么异常如果任务是正常执行的无异常，则返回None当任务被取消了，调用这个方法会触发CancelledError异常当任务没有做完，调用这个方法会触发InvalidStateError异常 还有一些不常用的方法：add_done_callback(callback, **, context=None*) remove_done_callback(callback) get_stack(**, limit=None*) print_stack(**, limit=None, file=None*) all_tasks(loop=None)，这是一个类方法 current_task(loop=None)，这是一个类方法 asyncio.Future对象A Futureis a special low-level awaitable object that represents an eventual result of an asynchronous operation. asyncio中的Future对象是一个相对更偏向底层的可对象，通常我们不会直接用到这个对象，而是直接使用Task对象来完成任务的并和状态的追踪Task 是 Futrue的子类，和task上没有本质上的区别，没有必要去用Future，用Task就可以了Future为我们提供了异步编程中的 最终结果 的处理（Task类也具备状态处理的功能）当一个Future对象被等待的时候，协程会一直等待，直到Future已经运算完毕 asyncio中的Future类是模仿concurrent.futures.Future类而设计的示例1：1234567891011async def main():    # 获取当前事件循环    loop = asyncio.get_running_loop()        # # 创建一个任务（Future对象），这个任务什么都不干。    fut = loop.create_future()        # 等待任务最终结果（Future对象），没有结果则会一直等下去。    await fut    asyncio.run(main())示例2：12345678910111213141516171819202122import asyncioasync def set_after(fut):    await asyncio.sleep(2)    fut.set_result(&quot;666&quot;) # 通常是不会这样设置的，这里只是演示async def main():    # 获取当前事件循环    loop = asyncio.get_running_loop()        # 创建一个任务（Future对象），没绑定任何行为，则这个任务永远不知道什么时候结束。    fut = loop.create_future()        # 创建一个任务（Task对象），绑定了set_after函数，函数内部在2s之后，会给fut赋值。    # 即手动设置future任务的最终结果，那么fut就可以结束了。    await loop.create_task(set_after(fut))        # 等待 Future对象获取 最终结果，否则一直等下去    data = await fut    print(data)asyncio.run(main())Future对象本身函数进行绑定，所以想要让事件循环获取Future的结果，则需要手动设置而Task对象继承了Future对象，其实就对Future进行扩展，他可以实现在对应绑定的函数执行完成之后，自动执行set_result，从而实现自动结束虽然，平时使用的是Task对象，但对于结果的处理本质是基于Future对象来实现的扩展：支持 await 对象语 法的对象课成为可等待对象，所以 协程对象、Task对象、Future对象 都可以被成为可等待对象。asyncio中关于Future的几个方法asyncio.isfuture(obj)：判断一个对象是不是Future，注意python中一切皆对象哦，包括函数，当obj是下面几种情况时返回trueasyncio.Future的实例对象asyncio.Task的实例对象一个具有 _asyncio_future_blocking属性的对象asyncio.ensure_future(obj, *, loop=None)。将一个obj包装成Futureasyncio.wrap_future(future, *, loop=None)将concurrent.futures.Future对象包装成一个 asyncio.Future 对象。Future对象的常用方法result()：返回Future执行的结果返回值如果Future被执行完成，如果使用set_result()方法设置了一个结果，那个设置的value就会被返回如果Future被执行完成，如果使用set_exception()方法设置了一个异常，那么使用这个方法也会触发异常如果Future被取消了，那么使用这个方法会触发CancelledError异常如果Future的结果不可用或者是不可达，那么使用这个方法也会触发InvalidStateError异常set_result(result)：标记Future已经执行完毕，并且设置它的返回值set_exception(exception)：标记Future已经执行完毕，并且触发一个异常done()：如果Future1执行完毕，则返回 True cancelled()：判断任务是否取消add_done_callback(callback, *, context=None)：在Future完成之后，给它添加一个回调方法这个方法就相当于是loop.call_soon()方法，如果要回调带有关键字参数的函数，也需要使用partial方法哦remove_done_callback(callback)cancel()exception()get_loop()：返回Future所绑定的事件循环futures.Future对象在Python的concurrent.futures模块中也有一个Future对象，这个对象是基于线程池和进程池实现异步操作时使用的对象。123456789101112131415import timefrom concurrent.futures import Futurefrom concurrent.futures.thread import ThreadPoolExecutorfrom concurrent.futures.process import ProcessPoolExecutordef func(value):    time.sleep(1)    print(value)    pool = ThreadPoolExecutor(max_workers=5)# 或 pool = ProcessPoolExecutor(max_workers=5)for i in range(10):    fut = pool.submit(func, i)    print(fut)两个Future对象是不同的，他们是为不同的应用场景而设计例如：concurrent.futures.Future不支持await语法 等官方提示两对象之间不同：unlike asyncio Futures, concurrent.futures.Future instances cannot be awaited.asyncio.Future.result() and asyncio.Future.exception() do not accept the timeout argument.asyncio.Future.result() and asyncio.Future.exception() raise an InvalidStateError exception when the Future is not done.Callbacks registered with asyncio.Future.add_done_callback() are not called immediately. They are scheduled with loop.call_soon() instead.asyncio Future is not compatible with the concurrent.futures.wait() and concurrent.futures.as_completed() functions.在Python提供了一个将futures.Future 对象包装成asyncio.Future对象的函数 asynic.wrap_future为什么python会提供这种功能？一般在程序开发中我们要么统一使用 asycio 的协程实现异步操作、要么都使用进程池和线程池实现异步操作但如果 协程的异步和 进程池/线程池的异步 混搭时，那么就会用到此功能了12345678910111213141516171819202122232425262728293031import timeimport asyncioimport concurrent.futuresdef func1():    # 某个耗时操作    time.sleep(2)    return &quot;defaulf pool&quot;async def main():    loop = asyncio.get_running_loop()    # 1. Run in the default loop's executor ( 默认ThreadPoolExecutor )    # 第一步：内部会先调用 ThreadPoolExecutor 的 submit 方法去线程池中申请一个线程去执行func1函数，并返回一个concurrent.futures.Future对象    # 第二步：调用asyncio.wrap_future将concurrent.futures.Future对象包装为asycio.Future对象。    # 因为concurrent.futures.Future对象不支持await语法，所以需要包装为 asycio.Future对象 才能使用。    fut = loop.run_in_executor(None, func1)    result = await fut    print('default thread pool', result)        # # 2. Run in a custom thread pool:    # with concurrent.futures.ThreadPoolExecutor() as pool:    #     result = await loop.run_in_executor(pool, func1)    #     print('custom thread pool', result)    # 3. Run in a custom process pool:    # with concurrent.futures.ProcessPoolExecutor() as pool:    #     result = await loop.run_in_executor(pool, func1)    #     print('custom process pool', result)asyncio.run(main())应用场景：当项目以协程式的异步编程开发时，如果要使用一个第三方模块，而第三方模块不支持协程方式异步编程时，就需要用到这个功能1234567891011121314151617181920212223242526import asyncioimport requestsasync def download_image(url):    # 发送网络请求，下载图片（遇到网络下载图片的IO请求，自动化切换到其他任务）    print(&quot;开始下载:&quot;, url)    loop = asyncio.get_event_loop()    # requests模块默认不支持异步操作，所以就使用线程池来配合实现了。    future = loop.run_in_executor(None, requests.get, url)    response = await future    print('下载完成')    # 图片保存到本地文件    file_name = url.rsplit('_')[-1]    with open(file_name, mode='wb') as file_object:        file_object.write(response.content)if __name__ == '__main__':    url_list = [        'https://www3.autoimg.cn/newsdfs/g26/M02/35/A9/120x90_0_autohomecar__ChsEe12AXQ6AOOH_AAFocMs8nzU621.jpg',        'https://www2.autoimg.cn/newsdfs/g30/M01/3C/E2/120x90_0_autohomecar__ChcCSV2BBICAUntfAADjJFd6800429.jpg',        'https://www3.autoimg.cn/newsdfs/g26/M0B/3C/65/120x90_0_autohomecar__ChcCP12BFCmAIO83AAGq7vK0sGY193.jpg'    ]    tasks = [download_image(url) for url in url_list]    loop = asyncio.get_event_loop()    loop.run_until_complete(asyncio.wait(tasks))这个方法返回一个 asyncio.Future 对象，使用 functools.partial() 传递关键字参数 给 func asyncio.``wrap_future(future, **, loop=None*)将一个 concurrent.futures.Future 对象封装到 asyncio.Future 对象中async/await 关键字python3.5 用于定义协程的关键字，async定义一个协程，await用于挂起阻塞的异步调用接口 await是一个只能在协程函数中使用的关键字，用于遇到IO操作时挂起 当前协程(任务)当前协程(任务)挂起过程中 事件循环可以去执行其他的协程(任务)，当前协程IO处理完成时，可以再次切换回来执行await之后的代码 1234567891011121314151617import asyncioasync def others():    print(&quot;start&quot;)    await asyncio.sleep(2)    print('end')    return '返回值'async def func():    print(&quot;执行协程函数内部代码&quot;)    # 遇到IO操作挂起当前协程（任务），等IO操作完成之后再继续往下执行。当前协程挂起时，事件循环可以去执行其他协程（任务）。    response1 = await others()    print(&quot;IO请求结束，结果为：&quot;, response1)    response2 = await others()    print(&quot;IO请求结束，结果为：&quot;, response2)    asyncio.run( func() )事件循环的任务列表中只有一个任务，所以在IO等待时无法演示切换到其他任务效果在程序想要创建多个任务对象，需要使用Task对象来实现可暂停等待的对象： 有三类对象是可等待的，即 coroutines, Tasks, and Futurescoroutine：本质上就是一个函数，一前面的生成器yield和yield from为基础，不再赘述Tasks: 任务，顾名思义，就是要完成某件事情，其实就是对协程函数进一步的封装Future：它是一个“更底层”的概念，他代表一个一步操作的最终结果，因为一步操作一般用于耗时操作，结果不会立即得到，会在“将来”得到异步运行的结果，故而命名为Future三者的关系，coroutine可以自动封装成task，而Task是Future的子类event_loop 事件循环python Event_loop(事件循环)Python 协程与事件循环程序开启一个无限循环，把一些函数注册到事件循环上，当满足事件发生的时候，调用相应的协程函数12345678任务列表 = [ 任务1, 任务2, 任务3,... ]while True:    可执行的任务列表，已完成的任务列表 = 去任务列表中检查所有的任务，将'可执行'和'已完成'的任务返回    for 就绪任务 in 已准备就绪的任务列表:        执行已就绪的任务    for 已完成的任务 in 已完成的任务列表:        在任务列表中移除 已完成的任务    如果 任务列表 中的任务都已完成，则终止循环将多线程比喻为工厂里的多个车间，那么协程就是一个车间内的多台机器。在线程级程序中，一台机器开始工作，车间内的其它机器不能同时工作，需要等上一台机器停止，但其它车间内的机器可以同时启动，这样就可以显著提高工作效率。在协程程序中，一个车间内的不同机器可以同时运转，启动机器、暂停运转、延时启动、停止机器等操作都可以人为设置事件循环能够控制任务运行流程，也就是任务的调用方协程函数，不是像普通函数那样直接调用运行的，必须添加到事件循环中，然后由事件循环去运行，单独运行协程函数是不会有结果的1234567891011121314151617import timeimport asyncioasync def say_after_time(delay,what):        await asyncio.sleep(delay)        print(what) async def main():        print(f&quot;开始时间为： {time.time()}&quot;)        await say_after_time(1,&quot;hello&quot;)        await say_after_time(2,&quot;world&quot;)        print(f&quot;结束时间为： {time.time()}&quot;) loop=asyncio.get_event_loop()    #创建事件循环对象#loop=asyncio.new_event_loop()   #与上面等价，创建新的事件循环loop.run_until_complete(main())  #通过事件循环对象运行协程函数loop.close()获取事件循环对象的几种方式下面几种方式可以用来获取、设置、创建事件循环对象looploop=asyncio.get_running_loop() 返回（获取）在当前线程中正在运行的事件循环，如果没有正在运行的事件循环，则会显示错误；它是python3.7中新添加的loop=asyncio.get_event_loop() 获得一个事件循环，如果当前线程还没有事件循环，则创建一个新的事件循环loop；loop=asyncio.set_event_loop(loop) 设置一个事件循环为当前线程的事件循环；loop=asyncio.new_event_loop() 创建一个新的事件循环通过事件循环运行协程函数的两种方式创建事件循环对象loop，即asyncio.get_event_loop()，通过事件循环运行协程函数直接通过asyncio.run(function_name)运行协程函数需要注意的是，首先run函数是python3.7版本新添加的，前面的版本是没有的；其次，这个run函数总是会创建一个新的事件循环并在run结束之后关闭事件循环所以，如果在同一个线程中已经有了一个事件循环，则不能再使用这个函数了，因为同一个线程不能有两个事件循环，而且这个run函数不能同时运行两次，因为他已经创建一个了即同一个线程中是不允许有多个事件循环loop的运行和停止事件循环loop.run_until_complete(future)。运行事件循环，直到future运行结束loop.run_forever()。在python3.7中已经取消了，表示事件循环会一直运行，直到遇到stop。loop.stop()。停止事件循环loop.is_running()。如果事件循环依然在运行，则返回Trueloop.is_closed()。如果事件循环已经close，则返回Trueloop.close()。关闭事件循环创建Future和Taskloop.create_future(coroutine) ，返回future对象loop.create_task(corootine) ，返回task对象loop.set_task_factory(factory)loop.get_task_factory()事件循环的时钟loop.time()。可以这么理解，事件循环内部也维护着一个时钟，可以查看事件循环现在运行的时间点是多少，就像普通的time.time()类似，它返回的是一个浮点数值123456789101112131415161718192021222324import asyncio async def hello1(a,b):    print('准备做加法运算')    await asyncio.sleep(3)    return a+bloop=asyncio.get_event_loop()t1=loop.time()  #开始时间print(t1)loop.run_until_complete(hello1(3,4))t2=loop.time()  #结束时间print(t2)print(t2-t1)    #时间间隔'''运行结果为：28525.671准备做加法运算28528.7033.0320000000028813'''计划执行回调函数(CallBacks)loop.call_later(delay, callback, *args, context=None)首先简单的说一下它的含义，就是事件循环在delay多长时间之后才执行callback函数，它的返回值是asyncio.TimerHandle类的一个实例对象loop.call_at(when, callback, *args, context=None)即在某一个时刻进行调用计划的回调函数，第一个参数不再是delay而是when，表示一个绝对的时间点，结合前面的loop.time使用，它的使用方法和call_later()很类似。它的返回值是asyncio.TimerHandle类的一个实例对象loop.call_soon(callback, *args, context=None)在下一个迭代的时间循环中立刻调用回调函数，用法同上面。它的返回值是asyncio.Handle类的一个实例对象loop.call_soon_threadsafe(callback, *args, context=None)这是call_soon()函数的线程安全版本，计划回调函数必须在另一个线程中使用需要注意的是：上面的几个回调函数都只使用了“位置参数”哦，asyncio中，大部分的计划回调函数都不支持“关键字参数”，如果是想要使用关键字参数，则推荐使用functools.aprtial()对方法进一步包装总结注意事项CallBack函数只能够定义为同步方法，不能够定义为async方法，及不能使用async和@asyncio.coroutine修饰每一个CallBack方法只会调用一次，如果在同一个时刻有另个CallBack方法需要调用，则他们的执行顺序是不确定的注意使用functools.partial（）去修饰带有关键字参数的CallBack方法如何理解对于一般的异步函数，我们需要将它放在时间循环里面，然后通过事件循环去循环调用它，而因为CallBack并不是异步函数它是定义为普通的同步方法，所以不能够放在时间循环里面，但是如果我依然想要让事件循环去执行它怎么办呢那就不放进事件循环，直接让事件循环“立即、稍后、在什么时候”去执行它不就行了嘛，call的含义就是“执行”消息队列rabbitmq和redis用作消息队列的区别基本使用协程入门例子123456789101112131415161718192021222324252627import timeimport asynciodef main():    start = time.time()    @asyncio.coroutine  # 1    def do_some_work():  # 2        print('Start coroutine')        time.sleep(0.1)  # 3        print('This is a coroutine')    loop = asyncio.get_event_loop()  # 4    coroutine = do_some_work()  # 5    loop.run_until_complete(coroutine)  # 6    end = time.time()    print('运行耗时：{:.4f}'.format(end - start))  # 7main()In [53]: one()Start coroutineThis is a coroutine运行耗时：0.1062代码说明：1、使用协程装饰器创建协程函数2、协程函数3、模拟 IO 操作4、创建事件循环。每个线程中只能有一个事件循环，get_event_loop 方法会获取当前已经存在的事件循环，如果当前线程中没有，新建一个5、调用协程函数获取协程对象6、将协程对象注入到事件循环，协程的运行由事件循环控制。事件循环的 run_until_complete 方法会阻塞运行，直到任务全部完成。协程对象作为 run_until_complete 方法的参数，loop 会自动将协程对象包装成任务来运行。后面我们会讲到多个任务注入事件循环的情况7、打印程序运行耗时异步工具函数异步转yield函数12345678910111213141516171819202122async def get_next(ait):    try:        asy_obj = await ait.__anext__()        return False, asy_obj    except StopAsyncIteration:        return True, Nonedef iter_over_async(ait, loop):    &quot;&quot;&quot;    异步转yield    @param ait: 异步函数    @param loop: 事件循环    @return:    &quot;&quot;&quot;    ait = ait.__aiter__()    while True:        done, asy_obj = loop.run_until_complete(get_next(ait))        if done:            break        yield asy_obj异步调用post请求1234567891011121314151617181920212223242526272829303132from json import JSONDecodeErrorfrom typing import Dict, Callableimport aiohttpasync def asy_http_post(url: str, param_dic: Dict, encoding=&quot;utf8&quot;, call_back: Callable = None):    &quot;&quot;&quot;    异步调用post请求    @param url: url地址    @param param_dic: 参数字典    @param encoding: 编码    @param call_back: 回调函数，对结果的处理    &quot;&quot;&quot;    async with aiohttp.ClientSession() as session:        async with session.post(url, json=param_dic) as response:            if response.status == 200:                async for data in response.content.iter_any():  # 每次接收已有的全部数据                    # 处理流式数据                    try:                        yield call_back(data.decode(encoding)) if call_back else data.decode(encoding)                    except JSONDecodeError as e:                        # 在解码过程中出现异常，处理错误情况                        print(f&quot;JSON decode error: {e}&quot;)                    except UnicodeDecodeError as e:                        # 在解码数据时出现编码错误                        print(f&quot;Unicode decode error: {e}&quot;)                    except Exception as e:                        # 其他异常                        print(f&quot;Error: {e}&quot;)            else:                print(&quot;请求失败，状态码:&quot;, response.status)协程对象运行协程对象不能直接运行，必须放入事件循环中或者由 yield from 语句调用将协程对象注入事件循环的时候，其实是 run_until_complete 方法将协程包装成了一个任务（task）对象，任务对象保存了协程运行后的状态，用于未来获取协程的结果12345678910111213141516171819202122232425262728293031import timeimport asynciodef main():    start = time.time()    @asyncio.coroutine    def do_some_work():        print('Start coroutine')        time.sleep(0.1)        print('This is a coroutine')    loop = asyncio.get_event_loop()    coroutine = do_some_work()    task = loop.create_task(coroutine)  # 1    print('task 是不是 asyncio.Task 的实例?', isinstance(task, asyncio.Task))  # 2    print('Task state:', task._state)  # 3    loop.run_until_complete(task)  # 4    print('Task state:', task._state)    end = time.time()    print('运行耗时：{:.4f}'.format(end - start))main()task 是不是 asyncio.Task 的实例? TrueTask state: PENDINGStart coroutineThis is a coroutineTask state: FINISHED运行耗时：0.1052代码说明：事件循环的 create_task 方法可以创建任务，另外 asyncio.ensure_future 方法也可以创建任务，参数须为协程对象task 是 asyncio.Task 类的实例，为什么要使用协程对象创建任务？因为在这个过程中 asyncio.Task 做了一些工作，包括预激协程、协程运行中遇到某些异常时的处理task 对象的 _state 属性保存当前任务的运行状态，任务的运行状态有 PENDING 和 FINISHED 两种将任务注入事件循环，阻塞运行在 Python 3.5 中新增了 async / await 关键字用来定义协程函数这两个关键字是一个组合，其作用等同于 asyncio.coroutine 装饰器和 yield from 语句。此后协程与生成器就彻底泾渭分明了结果获取通过result获取12345678910111213141516171819202122import asyncioasync def hello1(a, b):    print(&quot;Hello world 01 begin&quot;)    await asyncio.sleep(3)  # 模拟耗时任务3秒    print(&quot;Hello again 01 end&quot;)    return a + bcoroutine = hello1(10, 5)loop = asyncio.get_event_loop()  # 第一步：创建事件循环task = asyncio.ensure_future(coroutine)  # 第二步:将多个协程函数包装成任务列表loop.run_until_complete(task)  # 第三步：通过事件循环运行print('-------------------------------------')print(task.result())loop.close()'''运行结果为Hello world 01 beginHello again 01 end-------------------------------------15'''回调绑定有了 asyncio / await 关键字，我们继续学习 asyncio 模块的基本功能。假如协程包含一个 IO 操作（这几乎是肯定的），等它处理完数据后，我们希望得到通知，以便下一步数据处理这一需求可以通过向 future 对象中添加回调来实现那么什么是 future 对象task 对象就是 future 对象，我们可以这样认为，因为 asyncio.Task 是 asyncio.Future 的子类也就是说，task 对象可以添加回调函数回调函数的最后一个参数是 future 或 task 对象，通过该对象可以获取协程返回值如果回调需要多个参数，可以通过偏函数导入简言之，一个任务完成后需要捎带运行的代码可以放到回调函数中修改上一个程序如下：12345678910111213141516171819202122232425262728293031In [64]: def three():    ...:     start = time.time()    ...:    ...:     # @asyncio.coroutine    ...:     async def corowork():      # 1    ...:         print('[corowork] Start coroutine')    ...:         time.sleep(0.1)    ...:         print('[corowork] This is a coroutine')    ...:    ...:     def callback(name, task):  # 2     ...:         print('[callback] Hello {}'.format(name))    ...:         print('[callback] coroutine state: {}'.format(task._state))    ...:    ...:     loop = asyncio.get_event_loop()    ...:     coroutine = corowork()    ...:     task = loop.create_task(coroutine)    ...:     task.add_done_callback(functools.partial(callback, 'Shiyanlou'))  # 3    ...:     loop.run_until_complete(task)    ...:    ...:     end = time.time()    ...:     print('运行耗时：{:.4f}'.format(end - start))    ...:In [65]: import functoolsIn [66]: three()[corowork] Start coroutine[corowork] This is a coroutine[callback] Hello Shiyanlou[callback] coroutine state: FINISHED运行耗时：0.1051代码说明：使用 async 关键字替代 asyncio.coroutine 装饰器创建协程函数回调函数，协程终止后需要顺便运行的代码写入这里，回调函数的参数有要求，最后一个位置参数须为 task 对象task 对象的 add_done_callback 方法可以添加回调函数，注意参数必须是回调函数，这个方法不能传入回调函数的参数，这一点需要通过 functools 模块的 partial 方法解决，将回调函数和其参数 name 作为 partial 方法的参数，此方法的返回值就是偏函数，偏函数可作为 task.add_done_callback 方法的参数异步迭代器什么是异步迭代器实现了 __aiter__() 和 __anext__() 方法的对象。__anext__ 必须返回一个 awaitable 对象。async for 会处理异步迭代器的 __anext__() 方法所返回的可等待对象，直到其引发一个 StopAsyncIteration 异常。由 PEP 492 引入。什么是异步可迭代对象可在 async for 语句中被使用的对象。必须通过它的 __aiter__() 方法返回一个 asynchronous iterator。由 PEP 492 引入。12345678910111213141516171819202122232425import asyncioclass Reader(object):    &quot;&quot;&quot; 自定义异步迭代器（同时也是异步可迭代对象） &quot;&quot;&quot;    def __init__(self):        self.count = 0    async def readline(self):        # await asyncio.sleep(1)        self.count += 1        if self.count == 100:            return None        return self.count    def __aiter__(self):        return self    async def __anext__(self):        val = await self.readline()        if val == None:            raise StopAsyncIteration        return valasync def func():    # 创建异步可迭代对象    async_iter = Reader()    # async for 必须要放在async def函数内，否则语法错误。    async for item in async_iter:        print(item)asyncio.run(func())异步迭代器其实没什么太大的作用，只是支持了async for语法而已异步上下文管理器此种对象通过定义 __aenter__() 和 __aexit__() 方法来对 async with 语句中的环境进行控制。由 PEP 492 引入12345678910111213141516171819import asyncioclass AsyncContextManager:    def __init__(self):        self.conn = None    async def do_something(self):        # 异步操作数据库        return 666    async def __aenter__(self):        # 异步链接数据库        self.conn = await asyncio.sleep(1)        return self    async def __aexit__(self, exc_type, exc, tb):        # 异步关闭数据库链接        await asyncio.sleep(1)async def func():    async with AsyncContextManager() as f:        result = await f.do_something()        print(result)asyncio.run(func())异步上下文管理器还是比较有用的，平时在开发过程中 打开、处理、关闭 操作时，就可以用这种方式来处理进阶使用uvloopuvloop是 asyncio 中的事件循环的替代方案，替换后可以使得asyncio性能提高uvloop实现了asyncio.AbstractEventLoop接口，这意味着它提供了asyncio事件循环的直接替换 事实上，uvloop要比nodejs、gevent等其他python异步框架至少要快2倍，性能可以比肩Go语言安装uvloop: pip3 install uvloop在项目中想要使用uvloop替换asyncio的事件循环也非常简单，只要在代码中这么做就行123456import asyncioimport uvloopasyncio.set_event_loop_policy(uvloop.EventLoopPolicy())# 编写asyncio的代码，与之前写的代码一致。# 内部的事件循环自动化会变为uvloopasyncio.run(...)注意：知名的asgi uvicorn内部就是使用的uvloop的事件循环 uvloop用Cython编写，并建立在libuv之上。libuv是nodejs使用的高性能，多平台异步I/O库。由于nodejs的普及和流行，libuv既快速又稳定。uvloop实现所有异步事件循环API。高级Python对象包装了低级libuv结构和函数。继承用于保持代码DRY并确保任何手动内存管理与libuv原语的寿命保持同步。asyncio附带下列内置策略:class asyncio.``DefaultEventLoopPolicy默认asyncio策略。在Unix和Windows平台上都使用 SelectorEventLoop 不需要手动安装默认策略，asyncio已配置成自动使用默认策略。class asyncio.``WindowsProactorEventLoopPolicy使用 ProactorEventLoop 事件循环实现的另一种事件循环策略，可用性: Windows自定义策略要实现一个新的事件循环策略，建议子类化 DefaultEventLoopPolicy 并重写需要定制行为的方法1234567891011class MyEventLoopPolicy(asyncio.DefaultEventLoopPolicy):    def get_event_loop(self):        &quot;&quot;&quot;Get the event loop.        This may be None or an instance of EventLoop.        &quot;&quot;&quot;        loop = super().get_event_loop()        # Do something with loop ...        return loopasyncio.set_event_loop_policy(MyEventLoopPolicy())异步Redis当通过python去操作redis时，链接、设置值、获取值 这些都涉及网络IO请求使用asycio异步的方式可以在IO等待时去做一些其他任务，从而提升性能安装Python异步操作redis模块：pip3 install aioredis示例1：异步操作redis123456789101112131415161718#!/usr/bin/env python# -*- coding:utf-8 -*-import asyncioimport aioredisasync def execute(address, password):    print(&quot;开始执行&quot;, address)    # 网络IO操作：创建redis连接    redis = await aioredis.create_redis(address, password=password)    # 网络IO操作：在redis中设置哈希值car，内部在设三个键值对，即： redis = { car:{key1:1,key2:2,key3:3}}    await redis.hmset_dict('car', key1=1, key2=2, key3=3)    # 网络IO操作：去redis中获取值    result = await redis.hgetall('car', encoding='utf-8')    print(result)    redis.close()    # 网络IO操作：关闭redis连接    await redis.wait_closed()    print(&quot;结束&quot;, address)asyncio.run(execute('redis://47.93.4.198:6379', &quot;root!2345&quot;))示例2：连接多个redis做操作（遇到IO会切换其他任务，提供了性能）1234567891011121314151617181920import asyncioimport aioredisasync def execute(address, password):    print(&quot;开始执行&quot;, address)    # 网络IO操作：先去连接 47.93.4.197:6379，遇到IO则自动切换任务，去连接47.93.4.198:6379    redis = await aioredis.create_redis_pool(address, password=password)    # 网络IO操作：遇到IO会自动切换任务    await redis.hmset_dict('car', key1=1, key2=2, key3=3)    # 网络IO操作：遇到IO会自动切换任务    result = await redis.hgetall('car', encoding='utf-8')    print(result)    redis.close()    # 网络IO操作：遇到IO会自动切换任务    await redis.wait_closed()    print(&quot;结束&quot;, address)task_list = [    execute('redis://47.93.4.197:6379', &quot;root!2345&quot;),    execute('redis://47.93.4.198:6379', &quot;root!2345&quot;)]asyncio.run(asyncio.wait(task_list)) 更多redis操作参考aioredis官网异步MySQL当通过python去操作MySQL时，连接、执行SQL、关闭都涉及网络IO请求使用asycio异步的方式可以在IO等待时去做一些其他任务，从而提升性能安装Python异步操作mysql模块：pip3 install aiomysql示例1：12345678910111213141516import asyncioimport aiomysqlasync def execute():    # 网络IO操作：连接MySQL    conn = await aiomysql.connect(host='127.0.0.1', port=3306, user='root', password='123', db='mysql', )    # 网络IO操作：创建CURSOR    cur = await conn.cursor()    # 网络IO操作：执行SQL    await cur.execute(&quot;SELECT Host,User FROM user&quot;)    # 网络IO操作：获取SQL结果    result = await cur.fetchall()    print(result)    # 网络IO操作：关闭链接    await cur.close()    conn.close()asyncio.run(execute())示例2：123456789101112131415161718192021222324#!/usr/bin/env python# -*- coding:utf-8 -*-import asyncioimport aiomysqlasync def execute(host, password):    print(&quot;开始&quot;, host)    # 网络IO操作：先去连接 47.93.40.197，遇到IO则自动切换任务，去连接47.93.40.198:6379    conn = await aiomysql.connect(host=host, port=3306, user='root', password=password, db='mysql')    # 网络IO操作：遇到IO会自动切换任务    cur = await conn.cursor()    # 网络IO操作：遇到IO会自动切换任务    await cur.execute(&quot;SELECT Host,User FROM user&quot;)    # 网络IO操作：遇到IO会自动切换任务    result = await cur.fetchall()    print(result)    # 网络IO操作：遇到IO会自动切换任务    await cur.close()    conn.close()    print(&quot;结束&quot;, host)task_list = [    execute('47.93.40.197', &quot;root!2345&quot;),    execute('47.93.40.197', &quot;root!2345&quot;)]asyncio.run(asyncio.wait(task_list))FastAPI框架FastAPI是一款用于构建API的高性能web框架，框架基于Python3.6+的 type hints搭建接下里的异步示例以FastAPI和uvicorn来讲解（uvicorn是一个支持异步的asgi）安装FastAPI web 框架：pip3 install fastapi安装uvicorn：pip3 install uvicorn本质上为web提供socket server的支持的asgi（一般支持异步称asgi、不支持异步称wsgi）示例：12345678910111213141516171819202122232425262728293031#!/usr/bin/env python# -*- coding:utf-8 -*-import asyncioimport uvicornimport aioredisfrom aioredis import Redisfrom fastapi import FastAPIapp = FastAPI()REDIS_POOL = aioredis.ConnectionsPool('redis://47.193.14.198:6379', password=&quot;root123&quot;, minsize=1, maxsize=10)@app.get(&quot;/&quot;)def index():    &quot;&quot;&quot; 普通操作接口 &quot;&quot;&quot;    return {&quot;message&quot;: &quot;Hello World&quot;}@app.get(&quot;/red&quot;)async def red():    &quot;&quot;&quot; 异步操作接口 &quot;&quot;&quot;    print(&quot;请求来了&quot;)    await asyncio.sleep(3)    # 连接池获取一个连接    conn = await REDIS_POOL.acquire()    redis = Redis(conn)    # 设置值    await redis.hmset_dict('car', key1=1, key2=2, key3=3)    # 读取值    result = await redis.hgetall('car', encoding='utf-8')    print(result)    # 连接归还连接池    REDIS_POOL.release(conn)    return resultif __name__ == '__main__':    uvicorn.run(&quot;luffy:app&quot;, host=&quot;127.0.0.1&quot;, port=5000, log_level=&quot;info&quot;)在有多个用户并发请求的情况下，异步方式来编写的接口可以在IO等待过程中去处理其他的请求，提供性能。例如：同时有两个用户并发来向接口 http://127.0.0.1:5000/red 发送请求，服务端只有一个线程，同一时刻只有一个请求被处理异步处理可以提供并发是因为：当视图函数在处理第一个请求时，第二个请求此时是等待被处理的状态，当第一个请求遇到IO等待时，会自动切换去接收并处理第二个请求，当遇到IO时自动化切换至其他请求，一旦有请求IO执行完毕，则会再次回到指定请求向下继续执行其功能代码。基于上下文管理，来实现自动化管理的案例示例1：redis123456789101112131415161718192021222324252627#!/usr/bin/env python# -*- coding:utf-8 -*-import asyncioimport uvicornimport aioredisfrom aioredis import Redisfrom fastapi import FastAPIapp = FastAPI()REDIS_POOL = aioredis.ConnectionsPool('redis://47.193.14.198:6379', password=&quot;root123&quot;, minsize=1, maxsize=10)@app.get(&quot;/&quot;)def index():    &quot;&quot;&quot; 普通操作接口 &quot;&quot;&quot;    return {&quot;message&quot;: &quot;Hello World&quot;}@app.get(&quot;/red&quot;)async def red():    &quot;&quot;&quot; 异步操作接口 &quot;&quot;&quot;    print(&quot;请求来了&quot;)    async with REDIS_POOL.get() as conn:        redis = Redis(conn)        # 设置值        await redis.hmset_dict('car', key1=1, key2=2, key3=3)        # 读取值        result = await redis.hgetall('car', encoding='utf-8')        print(result)    return resultif __name__ == '__main__':    uvicorn.run(&quot;fast3:app&quot;, host=&quot;127.0.0.1&quot;, port=5000, log_level=&quot;info&quot;)示例2：mysql12345678910111213141516171819202122232425#!/usr/bin/env python# -*- coding:utf-8 -*-import asyncioimport uvicornfrom fastapi import FastAPIimport aiomysqlapp = FastAPI()# 创建数据库连接池pool = aiomysql.Pool(host='127.0.0.1', port=3306, user='root', password='123', db='mysql',                     minsize=1, maxsize=10, echo=False, pool_recycle=-1, loop=asyncio.get_event_loop())@app.get(&quot;/red&quot;)async def red():    &quot;&quot;&quot; 异步操作接口 &quot;&quot;&quot;    # 去数据库连接池申请链接    async with pool.acquire() as conn:        async with conn.cursor() as cur:            # 网络IO操作：执行SQL            await cur.execute(&quot;SELECT Host,User FROM user&quot;)            # 网络IO操作：获取SQL结果            result = await cur.fetchall()            print(result)            # 网络IO操作：关闭链接    return {&quot;result&quot;: &quot;ok&quot;}if __name__ == '__main__':    uvicorn.run(&quot;fast2:app&quot;, host=&quot;127.0.0.1&quot;, port=5000, log_level=&quot;info&quot;)爬虫在编写爬虫应用时，需要通过网络IO去请求目标数据，这种情况适合使用异步编程来提升性能接下来我们使用支持异步编程的aiohttp模块来实现安装aiohttp模块: pip3 install aiohttp示例：123456789101112131415161718import aiohttpimport asyncioasync def fetch(session, url):    print(&quot;发送请求：&quot;, url)    async with session.get(url, verify_ssl=False) as response:        text = await response.text()        print(&quot;得到结果：&quot;, url, len(text))async def main():    async with aiohttp.ClientSession() as session:        url_list = [            'https://python.org',            'https://www.baidu.com',            'https://www.pythonav.com'        ]        tasks = [asyncio.create_task(fetch(session, url)) for url in url_list]        await asyncio.wait(tasks)if __name__ == '__main__':    asyncio.run(main())多任务实际项目中，往往有多个协程创建多个任务对象，同时在一个 loop 里运行为了把多个协程交给 loop，需要借助 asyncio.gather 方法，任务的 result 方法可以获得对应的协程函数的 return 值123456789101112131415161718192021222324252627282930In [67]: def four():    ...:     start = time.time()    ...:    ...:     async def corowork(name, t):    ...:         print('[corowork] Start coroutine', name)    ...:         await asyncio.sleep(t)                  # 1    ...:         print('[corowork] Stop coroutine', name)    ...:         return 'Coroutine {} OK'.format(name)   # 2    ...:    ...:     loop = asyncio.get_event_loop()    ...:     coroutine1 = corowork('ONE', 3)             # 3    ...:     coroutine2 = corowork('TWO', 1)             # 3    ...:     task1 = loop.create_task(coroutine1)        # 4    ...:     task2 = loop.create_task(coroutine2)        # 4    ...:     gather = asyncio.gather(task1, task2)       # 5    ...:     loop.run_until_complete(gather)             # 6    ...:     print('[task1] ', task1.result())           # 7    ...:     print('[task2] ', task2.result())           # 7    ...:    ...:     end = time.time()    ...:     print('运行耗时：{:.4f}'.format(end - start))In [68]: four()[corowork] Start coroutine ONE[corowork] Start coroutine TWO[corowork] Stop coroutine TWO[corowork] Stop coroutine ONE[task1]  Coroutine ONE OK[task2]  Coroutine TWO OK运行耗时：3.0070代码说明：await 关键字等同于 Python 3.4 中的 yield from 语句，后面接协程对象。asyncio.sleep 方法的返回值为协程对象，这一步为阻塞运行。asyncio.sleep 与 time.sleep 是不同的，前者阻塞当前协程，即 corowork 函数的运行，而 time.sleep 会阻塞整个线程，所以这里必须用前者，阻塞当前协程，CPU 可以在线程内的其它协程中执行协程函数的 return 值可以在协程运行结束后保存到对应的 task 对象的 result 方法中创建两个协程对象，在协程内部分别阻塞 3 秒和 1 秒创建两个任务对象将任务对象作为参数，asyncio.gather 方法创建任务收集器。注意，asyncio.gather 方法中参数的顺序决定了协程的启动顺序将任务收集器作为参数传入事件循环的 run_until_complete 方法，阻塞运行，直到全部任务完成任务结束后，事件循环停止，打印任务的 result 方法返回值，即协程函数的 return 值到这一步，大家应该可以看得出，上面的代码已经是异步编程的结构了，在事件循环内部，两个协程是交替运行完成的简单叙述一下程序协程部分的运行过程：首先运行 task1打印 [corowork] Start coroutine ONE遇到 asyncio.sleep 阻塞释放 CPU 转到 task2 中执行打印 [corowork] Start coroutine TWO再次遇到 asyncio.sleep 阻塞这次没有其它协程可以运行了，只能等阻塞结束task2 的阻塞时间较短，阻塞 1 秒后先结束，打印 [corowork] Stop coroutine TWO又过了 2 秒，阻塞 3 秒的 task1 也结束了阻塞，打印 [corowork] Stop coroutine ONE至此两个任务全部完成，事件循环停止打印两个任务的 result打印程序运行时间程序全部结束需要额外说明的几点：多数情况下无需调用 task 的 add_done_callback 方法，可以直接把回调函数中的代码写入 await 语句后面，协程是可以暂停和恢复的多数情况下同样无需调用 task 的 result 方法获取协程函数的 return 值，因为事件循环的 run_until_complete 方法的返回值就是协程函数的 return 值。修改上文 # 6 、7 的代码如下：12result = loop.run_until_complete(gather)print(result)再次运行结果为：1234567In [73]: four()[corowork] Start coroutine ONE[corowork] Start coroutine TWO[corowork] Stop coroutine TWO[corowork] Stop coroutine ONE['Coroutine ONE OK', 'Coroutine TWO OK']  # 变量 result 的值运行耗时：3.0045事件循环有一个 stop 方法用来停止循环和一个 close 方法用来关闭循环。以上示例中都没有调用 loop.close 方法，似乎并没有什么问题。所以到底要不要调用 loop.close 呢？简单来说，loop 只要不关闭，就还可以再次运行 run_until_complete 方法，关闭后则不可运行。有人会建议调用 loop.close，彻底清理 loop 对象防止误用，其实多数情况下根本没有这个必要asyncio 模块提供了 asyncio.gather 和 asyncio.wait 两个任务收集方法，它们的作用相同，都是将协程任务按顺序排定，再将返回值作为参数加入到事件循环中前者在上文已经用到，后者与前者的区别是它可以获取任务的执行状态（PENING &amp; FINISHED），当有一些特别的需求例如在某些情况下取消任务，可以使用 asyncio.wait 方法。多任务实现方式使用gather同时注册多个任务，实现并发awaitable asyncio.gather(**aws*, *loop=None*, *return_exceptions=False*)注意事项：gather的返回值是它所绑定的所有任务的执行结果，而且顺序是不变的，即返回的result的顺序和绑定的顺序是保持一致的除此之外，它是awaitable的，所以，如果需要获取多个任务的返回值，既然是awaitable的，就需要将它放在一个函数里面，所以我们引入一个包装多个任务的入口main，这也是python3.7的思想123456789101112131415161718192021222324252627282930313233343536373839404142434445import asyncioimport time async def hello1(a,b):    print(&quot;Hello world 01 begin&quot;)    await asyncio.sleep(3)  #模拟耗时任务3秒    print(&quot;Hello again 01 end&quot;)    return a+b async def hello2(a,b):    print(&quot;Hello world 02 begin&quot;)    await asyncio.sleep(2)   #模拟耗时任务2秒    print(&quot;Hello again 02 end&quot;)    return a-b async def hello3(a,b):    print(&quot;Hello world 03 begin&quot;)    await asyncio.sleep(4)   #模拟耗时任务4秒    print(&quot;Hello again 03 end&quot;)    return a*b async def main():  #封装多任务的入口函数    task1=asyncio.ensure_future(hello1(10,5))    task2=asyncio.ensure_future(hello2(10,5))    task3=asyncio.ensure_future(hello3(10,5))    results=await asyncio.gather(task1,task2,task3)       for result in results:    #通过迭代获取函数的结果，每一个元素就是相对应的任务的返回值，顺序都没变        print(result) loop = asyncio.get_event_loop()               loop.run_until_complete(main())loop.close()                                  '''运行结果为：Hello world 01 beginHello world 02 beginHello world 03 beginHello again 02 endHello again 01 endHello again 03 end15550'''使用wait可以同时注册多个任务，实现并发await asyncio.wait(aws, *, loop=None, timeout=None, return_when=ALL_COMPLETED)它与gather不同的地方是他的参数是集合类型，而且他的返回类型是这样一个形式，即 (done, pending)返回dones是已经完成的任务，pending是未完成的任务，都是集合类型，不同的是每一个元素不再是返回值，而是某一个task相同的是它依然也是awaitable的，故而也需要定义在一个异步函数main()中，如下：1234567891011121314#前面的代码和上面一样async def main():  #封装多任务的入口函数    task1=asyncio.ensure_future(hello1(10,5))    task2=asyncio.ensure_future(hello2(10,5))    task3=asyncio.ensure_future(hello3(10,5))    done,pending=await asyncio.wait([task1,task2,task3])       for done_task in done:        print(done_task.result())  #这里返回的是一个任务，不是直接的返回值，故而需要使用result函数进行获取 loop = asyncio.get_event_loop()               loop.run_until_complete(main())loop.close()   #运行结果也一样使用as_completed可以同时注册多个任务，实现并发这个方法使用的比较少，与前面的两个gather和wait不同的是，它不是awaitable主调方获取任务的运行结果1234567891011121314async def main():  #封装多任务的入口函数    task1=asyncio.ensure_future(hello1(10,5))    task2=asyncio.ensure_future(hello2(10,5))    task3=asyncio.ensure_future(hello3(10,5))    return await asyncio.gather(task1,task2,task3)  #不在这里获取结果，只是返回loop = asyncio.get_event_loop()               results=loop.run_until_complete(main())  #在这里再获取返回函数值,然后迭代获取for result in results:    print(result)loop.close()     #y运行结果同上    或者是如下： 123456789101112async def main():  #封装多任务的入口函数    task1=asyncio.ensure_future(hello1(10,5))    task2=asyncio.ensure_future(hello2(10,5))    task3=asyncio.ensure_future(hello3(10,5))     return await asyncio.wait([task1,task2,task3])  #不在这里获取结果，只是返回 loop = asyncio.get_event_loop()               done,pending=loop.run_until_complete(main())  #在这里再获取返回函数值,然后迭代获取for done_task in done:    print(done_task.result())loop.close()协程锁按照字面意思来看，asyncio.lock 应该叫做异步 IO 锁，之所以叫协程锁，是因为它通常使用在子协程中，其作用是将协程内部的一段代码锁住，直到这段代码运行完毕解锁协程锁的固定用法是使用 async with 创建协程锁的上下文环境，将代码块写入其中举例说明，将以下代码写入 async_lock.py 文件：12345678910111213141516171819202122232425262728293031323334353637383940import asynciol = []lock = asyncio.Lock()   # 协程锁async def work(name):    print('lalalalalalalala')     # 打印此信息是为了测试协程锁的控制范围    # 这里加个锁，第一次调用该协程，运行到这个语句块，上锁    # 当语句块结束后解锁，开锁前该语句块不可被运行第二次    # 如果上锁后有其它任务调用了这个协程函数，运行到这步会被阻塞，直至解锁    # with 是普通上下文管理器关键字，async with 是异步上下文管理器关键字    # 能够使用 with 关键字的对象须有 __enter__ 和 __exit__ 方法    # 能够使用 async with 关键字的对象须有 __aenter__ 和 __aexit__ 方法    # async with 会自动运行 lock 的 __aenter__ 方法，该方法会调用 acquire 方法上锁    # 在语句块结束时自动运行 __aexit__ 方法，该方法会调用 release 方法解锁    # 这和 with 一样，都是简化 try ... finally 语句    async with lock:        print('{} start'.format(name))  # 头一次运行该协程时打印        if 'x' in l:                    # 如果判断成功            return name                 # 直接返回结束协程，不再向下执行        await asyncio.sleep(0); print('----------')  # 阻塞 0 秒，切换协程        l.append('x')        print('{} end'.format(name))        return nameasync def one():    name = await work('one')    print('{} ok'.format(name))async def two():    name = await work('two')    print('{} ok'.format(name))def main():    loop = asyncio.get_event_loop()    tasks = asyncio.wait([one(), two()])    loop.run_until_complete(tasks)if __name__ == '__main__':    main()运行程序如下：123456789$ python3 async_lock.pylalalalalalalalaone startlalalalalalalala----------one endone oktwo starttwo okAPI索引高级API索引高级API索引任务运行异步程序，创建Task对象，等待多件事运行超时的公共集run()创建事件循环，运行一个协程，关闭事件循环。create_task()启动一个asyncio的Task对象。await sleep()休眠几秒。await gather()并发执行所有事件的调度和等待。await wait_for()有超时控制的运行。await shield()屏蔽取消操作await wait()完成情况的监控器current_task()返回当前Task对象all_tasks()返回事件循环中所有的task对象。TaskTask对象run_coroutine_threadsafe()从其他OS线程中调度一个协程。for in as_completed()用 for 循环监控完成情况。例子使用 asyncio.gather() 并行运行使用 asyncio.wait_for() 强制超时撤销协程asyncio.sleep() 的用法请主要参阅 协程与任务文档同步能被用于Task对象集的，类似线程的同步基元组件Lock互斥锁。Event事件对象。Condition条件对象Semaphore信号量BoundedSemaphore有界的信号量。例子asyncio.Event 的用法请参阅asyncio文档 synchronization primitives异常asyncio.TimeoutError类似 wait_for() 等函数在超时时候被引发。请注意 asyncio.TimeoutError 与内建异常 TimeoutError 无关。asyncio.CancelledError当一个Task对象被取消的时候被引发。请参阅 Task.cancel()。例子在取消请求发生的运行代码中如何处理CancelledError异常请参阅完整的 asyncio 专用异常 列表底层API索引底层API索引获取事件循环asyncio.get_running_loop()获取当前运行的事件循环 首选 函数。asyncio.get_event_loop()获得一个事件循环实例(当前或通过策略)。asyncio.set_event_loop()通过当前策略将事件循环设置当前事件循环。asyncio.new_event_loop()创建一个新的事件循环。例子使用asyncio.get_running_loop()。生命周期loop.run_until_complete()运行一个期程/任务/可等待对象直到完成。loop.run_forever()一直运行事件循环。loop.stop()停止事件循环。loop.close()关闭事件循环。loop.is_running()返回 True ， 如果事件循环正在运行。loop.is_closed()返回 True ，如果事件循环已经被关闭 。await loop.shutdown_asyncgens()关闭异步生成器。调试loop.set_debug()开启或禁用调试模式。loop.get_debug()获取当前测试模式。调度回调函数loop.call_soon()尽快调用回调。loop.call_soon_threadsafe()loop.call_soon() 方法线程安全的变体。loop.call_later()在给定时间 之后 调用回调函数。loop.call_at()在 指定 时间调用回调函数。线程/进程池await loop.run_in_executor()在 concurrent.futures 执行器中运行一个独占CPU或其它阻塞函数。loop.set_default_executor()设置 loop.run_in_executor() 默认执行器。任务与期程loop.create_future()创建一个 Future 对象。loop.create_task()将协程当作 Task 一样调度。loop.set_task_factory()设置 loop.create_task() 使用的工厂，它将用来创建 Tasks 。loop.get_task_factory()获取 loop.create_task() 使用的工厂，它用来创建 Tasks 。错误处理loop.call_exception_handler()调用异常处理器。loop.set_exception_handler()设置一个新的异常处理器。loop.get_exception_handler()获取当前异常处理器。loop.default_exception_handler()默认异常处理器实现。例子使用 asyncio.get_event_loop() 和 loop.run_forever()使用 loop.call_later()使用 loop.create_connection() 实现 echo客户端使用 loop.create_connection() 去 链接socket使用add_reader()监听FD(文件描述符)的读取事件使用loop.add_signal_handler()使用loop.add_signal_handler()asyncio与gevent比较Python中asyncio与gevent有什么区别?asycio 需要自己在代码中让出CPU，控制权在自己手上gevent 用会替换标准库，你以为调用的是标准库的方法实际已经被替换成gevent自己的实现，遇到阻塞调用，gevent会自动让出CPU，像不像手动挡和自动挡的区别·····gevent是第三方库，通过greenlet实现协程，其基本思路是：当一个greenlet遇到IO操作时，就自动切换到其他的greenlet，等到IO操作完成，再在适当的时候切换回来继续执行asyncio是Python 3.4版本引入的标准库，直接内置了对异步IO的支持，不需要第三方的支持编程模型比较asyncio的编程模型就是一个消息循环。我们从asyncio模块中直接获取一个EventLoop的引用，然后把需要执行的协程扔到EventLoop中执行，就实现了异步IO。很多异步io操作这两个库都可以用，只是他们在不同场景下的效率和易用性可能有区别，当然这个得进行深入的测试和研究，单就现在普通的场景来说，区别并不大gevent 是补丁，asyncio 是 python 3 原生；都能做到 异步 IO。如果现在写异步IO程序，应该用 asynciogevent 需要 patch，个人感觉不洁癖，比如有些时候不需要patch，或者自己实现的东西和patch冲突就麻烦了警告或错误提示asyncio提前到编译阶段event只能运行阶段asyncio 系统自带，官方无忧前期：async await 一入门思维有点绕，gevent程序员一开始比较习惯后期：由于 async 和 await 关键字，asyncio 才是所见即所得！gevent入口去掉第一个spawn就是普通函数差不多了处理blocking，asyncio 是 run_in_executor 好用，gevent 是 ThreadPool，不好用启动asyncio提供run_until_completegevent 是 join（spawn）同时启动多个asyncio 用 asyncio.wait 包装 或 gathergevent 是 joinall未来：官方既然出了 asyncio，感觉gevent 将会过时web框架【python 异步框架tornado】Tornado原理理解及应用场景基础知识python中常见的web框架主要包括了flask、django、tornado那么这些漂亮的框架的应用场景是什么呢？要性能，Tornado 首选要开发速度，Django 和Flask 都行区别是：Flask 把许多功能交给第三方库去完成了，因此Flask 更为灵活Django适合初学者或者小团队的快速开发，适合做管理类、博客类网站、功能复杂需求多的网站Tornado适合高度定制，适合访问量大，异步情况多的网站。也可以用于定制api服务WSGI协议中国有三家有名的通信运营商，分别是移动、联通和电信，这三家通信商的手机号是可以跨平台拨打的，假设三家通信商负责通信的协议不同且无法互通，用移动的手机号就无法给联通电信的手机打电话，为了方便通信就需要一个统一的规范WSGI协议的角色就是这个统一的规范，是描述webserver如何与web application通信的规范，要实现WSGI协议，就必须同时实现web server和webapplication，目前常见的有Tornado、Flask和DjangoWSGI是个同步模型，不支持非阻塞的请求方式，Tornado默认是不推荐使用WSGI的，如果在Tornado中使用WSGI，将无法使用Tornado的异步非阻塞的处理方式，相应的异步接口也就无法使用，性能方面也就大打折扣，这个也是Tornado性能如此优越的原因django这类框架，采用WSGI协议与服务器对接的，而这类服务器通常是基于多线程/多进程的，也就是说每有一个网络请求，服务器都会有一个线程/进程进行处理epoll技术epoll是Linux内核为处理bai大批量文件描述du符而作了改zhi进的poll，是Linux下多路复用daoIO接口zhuanselect/poll的增强shu本，它能显著提高程序权在大量并发连接中只有少量活跃的情况下的系统CPU利用率。另一点原因就是获取事件的时候，它无须遍历整个被侦听的描述符集，只要遍历那些被内核IO事件异步唤醒而加入Ready队列的描述符集合就行了。epoll除了提供select/poll那种IO事件的水平触发（Level Triggered）外，还提供了边缘触发（Edge Triggered），这就使得用户空间程序有可能缓存IO状态，减少epoll_wait/epoll_pwait的调用，提高应用程序效率。异步框架tornado逻辑流程【python 异步框架tornado】Tornado原理理解及应用场景Epoll原理解析分析下Torando.ioloop.IOLoop.current().start()代码上Torando.ioloop是Tornado的核心模块ioloop模块IOLoop是ioloop模块的一个类current()是IOLoop类的一个方法，结果是返回一个当前线程的IOLoop的实例start()也是IOLoop的方法，调用后开启循环123456789101112131415# -*- encoding=utf-8 -*-import tornado.ioloopimport tornado.webclass IndexHandler(tornado.web.RequestHandler):    def get(self):        self.write(&quot;Hello, world&quot;)def make_app():    return tornado.web.Application([(r&quot;/&quot;, IndexHandler),])if __name__ == &quot;__main__&quot;:    app = make_app()    app.listen(8888)    tornado.ioloop.IOLoop.current().start()整体的流程如下图所示：首先Tornado需要建立监听，会创建一个socket用于监听，如果有客户端A请求建立连接之后，Tornado会基于原先的socket新创建一个包含客户端A连接的有关信息的socket(分配新的监听端口)，用于监听和客户端A的请求此时对Tornado来说就有两个socket需要进行监控，原先的socket继续用来监听建立新连接，新的socket用于和客户端A进行通信，假如没有epoll技术的话，Tornado需要自己去循环询问哪个socket有新的请求有了epoll技术，Tornado只需要把所有的socket丢给epoll，epoll作为管家帮忙监控，然后Torando.ioloop.IOLoop.current().start()开启循环，不断的去询问epoll是否有请求需要处理，这就是ioloop所做的工作，也是Tornado的核心部分当有客户端进行请求，epoll就发现有socket可处理，当ioloop再次询问epoll时，epoll就把需要处理的socket交由Tornado处理Tornado对请求进行处理，取出报文，从报文中获取请求路径，然后从tornado.web.Applcation里配置的路由映射中把请求路径映射成对应的处理类，如上图IndexHandler就是处理类处理类处理完成后，生成响应，将响应内容封装成http报文，通过请求时建立的连接（尚未中断）将响应内容返回给客户端当有多个请求同时发生，Tornado会按顺序挨个处理看了上面的流程，假如Tornado在处理一个非常耗时的请求时，后面的请求是不是就会被卡死呢？答案是肯定的，所以提到了Tornado的另一个特性—异步处理，当一个请求特别耗时，Tornado就把它丢在那处理，然后继续处理下一个请求，确保后面的请求不会被卡死。Tornado异步：原生Tornado框架提供异步网络库IOLoop和IOStream以及异步协程库tornado.gen(必须使用Tornado的web框架和HTTP服务器，否则异步接口可能无法使用)，方便用户通过更直接的方法实现异步编程，而不是回调的方式，官方推荐yield协程方式完成异步通过上面所讲，基本上已经对Tornado的整个处理流程了解了，总结一下Tornado之所以能同时处理大量连接的原因：利用高效的epoll技术处理请求，单线程/单进程同时处理大量连接没使用传统的wsgi协议，而是利用Tornado自己的web框架和http服务形成了一整套WSGI方案进行处理异步处理方式，Tornado提供了异步接口可供调用对比其它框架ornado 除了web框架之外，数据库或者其他几乎都是阻塞的 Tornado自身可以写出非阻塞的代码，但是连数据库，想用ORM的时候却不行，所以也不是特别方便。因此很多人选择使用Flask或者是DjangoDjango优缺：重量级web框架，功能大而全，注重高效开发 内置管理后台 内置封装完善的ORM操作 session功能 后台管理 缺陷：高耦合 Tornado优缺：轻量级web框架，功能少而精，注重性能优越 HTTP服务器 异步编程 WebSocket 缺陷：入门门槛较高 关键知识点tornado的基础web框架模块RequestHandler封装了请求和响应。Application核心应用类，类似于flask的app，是和服务器对接的接口，保存了路由信息listen方法绑定端口创建http服务器实例，但并未监听，与socket的listen不同/ Tornado的优缺优势轻量级web框架异步非阻塞IO处理方式出色的抗负载能力优异的处理性能，不依赖多进程/多线程，一定程度上解决C10K问题WSGI全栈替代产品，推荐同时使用其web框架和HTTP服务器 分布式系统分布式任务队列huey使用python的分布式任务队列huey实现任务的异步化Celery什么是celery框架使用Celery踩过的坑Celery 分布式任务队列入门面试必问的CELERY，你了解多少？简介celery是一个任务分发系统目的：利用后端待命的无数worker实现一系列任务的快速处理业务模式：生产者消费者模型可以选择Broker:RabbitMQRedisother brokersweb框架Python 四大主流 Web 编程框架web三大主流框架 同步框架：Django、Flask；异步框架：Tornado、Sanic  关系型数据库：MySQL、PostgreSQL、Oracle；非关系型数据库：Redis、Mongo 企业级DjangoDjango于2003年诞生于美国堪萨斯（Kansas）州，最初用来制作在线新闻Web站点，于2005年加入了BSD许可证家族，成为开源网络框架Django根据比利时的爵士音乐家Django Reinhardt命名，作者这样命名Django意味着Django能优雅地演奏（开发）功能丰富的乐曲（Web应用）它是当前Python世界里最负盛名且最成熟的网络框架最初用来制作在线新闻的Web站点，目前已发展为应用最广泛的Python网络框架Django的各模块之间结合得比较紧密，所以在功能强大的同时又是一个相对封闭的系统，但是其健全的在线文档及开发社区，使开发者在遇到问题时能找到解决方法。框架特点功能最完整：Django定义了服务发布、路由映射、模板编程、数据处理的一整套功能模块间紧密耦合：开发者需要学习Django自己定义的这一整套技术完善的文档：广泛的应用和完善的在线文档，开发者遇到问题时可以搜索在线文档寻求解决方案集成数据访问组件：Django的Model层自带数据库ORM组件，无须学习其他数据库访问技术（dbi、SQLAlchemy等）强大的URL映射技术：Django使用正则表达式管理URL映射，给开发者带来了极高的灵活性后台管理系统自动生成：简单的几行配置和代码就可以实现完整的后台数据管理Web控制台错误信息完整：提供非常完整的错误信息帮助开发者定位问题，比如缺少xxx组件的配置引用等Django是遵循MVC架构的Web开发框架，其主要由以下几部分组成管理工具（Management）：一套内置的创建站点、迁移数据、维护静态文件的命令工具模型（Model）：提供数据访问接口和模块，包括数据字段、元数据、数据关系等的定义及操作视图（View）：Django的视图层封装了HTTP Request和Response的一系列操作和数据流，其主要功能包括URL映射机制、绑定模板等模板（Template）：是一套Django自己的页面渲染模板语言，用若干内置的tags和filters定义页面的生成方表单（Form）：通过内置的数据类型和控件生成HTML表单管理站（Admin）：通过声明需要管理的Model，快速生成后台数据管理网站高并发TornadoTornado是使用Python编写的一个强大的可扩展的Web服务器它在处理高网络流量时表现得足够强健，却在创建和编写时有着足够的轻量级，并能够被用在大量的应用和工具中Tornado作为FriendFeed网站的基础框架，于2009年9月10日发布，目前已经获得了很多社区的支持，并且在一系列不同的场合中得到应用除FriendFeed和Facebook外，还有很多公司在生产上转向Tornado，包括Quora、Turntable.fm、Bit.ly、Hipmunk及MyYearbook等相对于其他Python网络框架，Tornado有如下特点完备的Web框架：与Django、Flask等一样，Tornado也提供了URL路由映射、Request上下文、基于模板的页面渲染技术等开发Web应用的必备工具高效的网络库，性能与Twisted、Gevent等底层Python框架相媲美：**提供了异步I/O支持、超时事件处理。这使得Tornado除了可以作为Web应用服务器框架，还可以用来做爬虫应用、物联网关、游戏服务器等后台应用高效HTTPClient：除了服务器端框架，Tornado还提供了基于异步框架的HTTP客户端高效的内部HTTP服务器：虽然其他Python网络框架（Django、Flask）也提供了内部HTTP服务器，但它们的HTTP服务器由于性能原因只能用于测试环境。而Tornado的HTTP服务器与Tornado异步调用紧密结合，可以直接用于生产环境完备的WebSocket支持：WebSocket是HTML5的一种新标准，实现了浏览器与服务器之间的双向实时通信因为Tornado的上述特点，Tornado常被用作大型站点的接口服务框架，而不像Django那样着眼于建立完整的大型网站主要关注Tornado的异步及协程编程、身份认证框架、独特的非WSGI部署方式快速建站FlaskFlask是Python Web框架族里比较年轻的一个，于2010年出现，这使得它吸收了其他框架的优点，并且把自己的主要领域定义在了微小项目上同时，它是可扩展的，Flask让开发者自己选择用什么数据库插件存储他们的数据很多功能简单但性能卓越的网站就是基于Flask框架而搭建的，比如http://httpbin.org/就是一个功能简单但性能强大的HTTP测试项目Flask是一个面向简单需求和小型应用的微框架相对于其他Python语言的Web框架而言，Flask的特点可以归结如下。内置开发服务器和调试器：网络程序调试是在将编制好的网站投入实际运行前，用手工或编译程序等方法进行测试，修正语法错误和逻辑错误的过程。有经验的开发者都知道，这是保证网站系统能够正式应用的必要步骤。Flask 自带的开发服务器使开发者在调试程序时无须再安装其他任何网络服务器，比如Tomcat、JBoss、Apache等。Flask默认处于调试状态，使得运行中的任何错误会同时向两个目标发送信息：一个是Python Console，即启动Python程序的控制台；另一个是HTTP客户端，即Flask开发服务器将调试信息传递给了客户端。与Python单元测试功能无缝衔接：单元测试是对最小软件开发单元的测试，其重点测试程序的内部结构，主要采用白盒测试方法，由开发人员负责。单元测试的主要目标是保证函数在给定的输入状态下，能够得到预想的输出，在不符合要求时能够提醒开发人员进行检查。Flask提供了一个与Python自带的单元测试框架unitest无缝衔接的测试接口，即Flask对象的test_client()函数。通过test_client()函数，测试程序可以模拟进行HTTP访问的客户端来调用Flask路由处理函数，并且获取函数的输出来进行自定义的验证。使用Jinja2模板将HTML页面与后台应用程序联系起来一直是网站程序框架的一个重要目标。Flask通过使用Jinja2模板技术解决了这个问题。Jinja2是一个非常灵活的HTML模板技术，它是从Django模板发展而来的，但是比Django模板使用起来更加自由且更加高效。Jinja2模板使用配制的语义系统，提供灵活的模板继承技术，自动抗击XSS跨站攻击并且易于调试。完全兼容WSGI 1.0标准WSGI（Web Server Gateway Interface）具有很强的伸缩性且能运行于多线程或多进程环境下，因为Python线程全局锁的存在，使得WSGI的这个特性至关重要。WSGI已经是Python界的一个主要标准，各种大型网路服务器对其都有良好的支持。WSGI位于Web应用程序与Web服务器之间，与WSGI完全兼容使得Flask能够配置到各种大型网络服务器中。基于Unicode编码Flask是完全基于Unicode的。这对制作非纯ASCII字符集的网站来说非常方便。HTTP本身是基于字节的，也就是说任何编码格式都可以在HTTP中传输。但是，HTTP要求在HTTP Head中显式地声明在本次传输中所应用的编码格式。在默认情况下，Flask会自动添加一个UTF-8编码格式的HTTP Head，使程序员无须担心编码的问题。自定义协议Twisted以上讲到的3个Python Web框架都是围绕着应用层HTTP展开的，而Twisted是一个例外Twisted是一个用Python语言编写的事件驱动的网络框架对于追求服务器程序性能的应用，Twisted框架是一个很好的选择Twisted是一个有着10多年历史的开源事件驱动框架wisted支持很多种协议，包括传输层的UDP、TCP、TLS，以及应用层的HTTP、FTP等对于所有这些协议，Twisted提供了客户端和服务器方面的开发工具。Twisted框架的历史悠久，其主要发行版本都以Python 2为基础，最新的版本为基于Python 2.7的Twisted-15.4.0Twisted社区正在开发基于Python 3的版本，但目前为止尚没有基于Python 3的Twisted稳定发行版Twisted是一个高性能的编程框架在不同的操作系统平台上，Twisted利用不同的底层技术实现了高效能通信在Windows中，Twisted的实现基于I/O完成端口（IOCP，Input/Output Completion Port）技术，它保证了底层高效地将I/O事件通知给框架及应用程序在Linux中，Twisted的实现基于epoll技术，epoll是Linux下多路复用I/O接口select/poll的增强版本，它能显著提高程序在大量并发连接中只有少量活跃的情况下的系统CPU利用率在开发方法上，Twisted引导程序员使用异步编程模型。Twisted提供了丰富的Defer、Threading等特性来支持异步编程。消息队列RabbitMQPython消息队列（RabbitMQ）RedisPython学习之Redis交互详解相关疑问多线程存在意义多线程存在意义协程常见问题很多个协程一起运行有创建新的线程吗？协程运行时，都是在一个线程中运行的，没有创建新的线程。如下 1234567891011121314151617181920212223242526272829303132333435363738394041import asyncioimport timeimport threading a=time.time() async def hello1():    print(f&quot;Hello world 01 begin,my thread is:{threading.currentThread()}&quot;)    await asyncio.sleep(3)    print(&quot;Hello again 01 end&quot;) async def hello2():    print(f&quot;Hello world 02 begin,my thread is:{threading.currentThread()}&quot;)    await asyncio.sleep(2)    print(&quot;Hello again 02 end&quot;) async def hello3():    print(f&quot;Hello world 03 begin,my thread is:{threading.currentThread()}&quot;)    await asyncio.sleep(1)    print(&quot;Hello again 03 end&quot;) loop = asyncio.get_event_loop()tasks = [hello1(), hello2(),hello3()]loop.run_until_complete(asyncio.wait(tasks)) loop.close()  b=time.time()print('---------------------------------------')print(b-a)'''运行结果为：Hello world 03 begin,my thread is:&lt;_MainThread(MainThread, started 4168)&gt;Hello world 02 begin,my thread is:&lt;_MainThread(MainThread, started 4168)&gt;Hello world 01 begin,my thread is:&lt;_MainThread(MainThread, started 4168)&gt;Hello again 03 endHello again 02 endHello again 01 end---------------------------------------2.994506597518921'''从上面那个可以看出，三个不同的协程函数都是在一个线程完成的。但是并不是意味着，多个协程函数只能在一个线程中执行，同样可以创建新的线程，其实我们完全可以在新的线程中重新创建一个事件循环，具体的实例参见后面。 线程一定效率更高吗？ 也不是绝对的，当然在一般情况下，异步方式的执行效率是更高的，就比如上面的三个函数，如果按照同步的方式执行，则一共需要6秒的时间，但是采用协程则只需要最长的那个时间3秒，这自然是提高了工作效率，那是不是一定会提高呢？也不一定，这与协程的调用方式是由密切关系的。如下所示： 123456789101112131415161718192021222324252627282930313233343536373839404142import asyncioimport timeimport threading a=time.time() async def hello1():    print(f&quot;Hello world 01 begin,my thread is:{threading.currentThread()}&quot;)    await asyncio.sleep(3)    print(&quot;Hello again 01 end&quot;) async def hello2():    print(f&quot;Hello world 02 begin,my thread is:{threading.currentThread()}&quot;)    await asyncio.sleep(2)    print(&quot;Hello again 02 end&quot;) async def hello3():    print(f&quot;Hello world 03 begin,my thread is:{threading.currentThread()}&quot;)    await hello2()    await hello1()    print(&quot;Hello again 03 end&quot;) loop = asyncio.get_event_loop()tasks = [hello3()]loop.run_until_complete(asyncio.wait(tasks)) loop.close() b=time.time()print('---------------------------------------')print(b-a) '''运行结果为：Hello world 03 begin,my thread is:&lt;_MainThread(MainThread, started 13308)&gt;Hello world 02 begin,my thread is:&lt;_MainThread(MainThread, started 13308)&gt;Hello again 02 endHello world 01 begin,my thread is:&lt;_MainThread(MainThread, started 13308)&gt;Hello again 01 endHello again 03 end---------------------------------------5.008373498916626'''我们发现一个问题，上面执行的顺序完全不是异步执行，执行的时间也没有得到改善，究其原因，是因为上面是通过hello3去调用hello1和hello2的，这和同步调用的方式完全是一样的，即使我定义的都是异步方法，它既没有提高执行效率，还会有阻塞。结论：在有很多个异步方式的时候，一定要尽量避免这种异步函数的直接调用，这和同步是没什么区别的，一定要通过事件循环loop，“让事件循环在各个异步函数之间不停游走”，这样才不会造成阻塞。协程会不会有阻塞呢？异步方式依然会有阻塞的，当我们定义的很多个异步方法彼此之间有一来的时候，比如，我必须要等到函数1执行完毕，函数2需要用到函数1的返回值，如上面的例子2所示，就会造成阻塞，这也是异步编程的难点之一，如何合理配置这些资源，尽量减少函数之间的明确依赖，这是很重要的。 异步方法假死（freezing）123456789101112131415161718192021222324252627282930 import asyncioimport timeimport threading #定义一个异步操作async def hello1(a,b):    print(f&quot;异步函数开始执行&quot;)    await asyncio.sleep(3)    print(&quot;异步函数执行结束&quot;)    return a+b #在一个异步操作里面调用另一个异步操作async def main():    c=await hello1(10,20)    print(c)    print(&quot;主函数执行&quot;) loop = asyncio.get_event_loop()tasks = [main()]loop.run_until_complete(asyncio.wait(tasks)) loop.close() '''运行结果为：异步函数开始执行（在此处要等待3秒）异步函数执行结束30主函数执行'''注意一个问题：我们前面所讲的例子中，没有出现等待，是因为各个异步方法之间是“完全并列”关系，彼此之间没有依赖，所以，我可以将所有的异步操作“gather”起来，然后通过事件循环，让事件循环在多个异步方法之间来回调用，永不停止，故而没有出现等待。但是，现实中不可能所有的异步方法都是完全独立的，没有任何关系的，在上面的这个例子中，就是很好的说明，hello1是一个耗时任务，耗时大约3秒，main也是一个异步方法，但是main中需要用到hello1中的返回结果，所以他必须要等待hello1运行结束之后再才能继续执行，这就是为什么会得到上面结果的原因。这也再一次说明，异步依然是会有阻塞的。我们也可以这样理解，因为我给事件循环只注册了一个异步方法，那就是main，当在main里面遇到了await，事件循环挂起，转而寻找其他的异步方法，但是由于只注册了一个异步方法给事件循环，他没有其他的方法可执行了，所以只能等待，让hello1执行完了，再继续执行。multithreading+asyncio总结1234567891011121314151617181920212223242526272829303132333435363738import tkinter as tk          # 导入 Tkinter 库import timeimport asyncioimport threading class Form:    def __init__(self):        self.root=tk.Tk()        self.root.geometry('500x300')        self.root.title('窗体程序')  #设置窗口标题                self.button=tk.Button(self.root,text=&quot;开始计算&quot;,command=self.change_form_state)        self.label=tk.Label(master=self.root,text=&quot;等待计算结果&quot;)         self.button.pack()        self.label.pack()         self.root.mainloop()     async def calculate(self):        await asyncio.sleep(3)        self.label[&quot;text&quot;]=300     def get_loop(self,loop):        self.loop=loop        asyncio.set_event_loop(self.loop)        self.loop.run_forever()    def change_form_state(self):        coroutine1 = self.calculate()        new_loop = asyncio.new_event_loop()                        #在当前线程下创建时间循环，（未启用），在start_loop里面启动它        t = threading.Thread(target=self.get_loop,args=(new_loop,))   #通过当前线程开启新的线程去启动事件循环        t.start()         asyncio.run_coroutine_threadsafe(coroutine1,new_loop)  #这几个是关键，代表在新线程中事件循环不断“游走”执行  if __name__=='__main__':    form=Form()定义需要异步执行的一系列操作，及一系列协程函数；在主线程中定义一个新的线程，然后在新线程中产生一个新的事件循环；在主线程中，通过asyncio.run_coroutine_threadsafe(coroutine,loop)这个方法将一系列异步方法注册到新线程的loop里面去，这样就是新线程负责事件循环的执行。asyncio.run_coroutine_threadsafe(coroutine，loop)的意思很简单，就是我在新线程中创建一个事件循环loop然后在新线程的loop中不断不停的运行一个或者是多个coroutine</content>
	</entry>
</search>
