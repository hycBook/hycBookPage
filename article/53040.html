<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>pytorch学习_进阶知识 | 兼一书虫</title><meta name="keywords" content="pytorch"><meta name="author" content="narutohyc"><meta name="copyright" content="narutohyc"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="pytorch学习_进阶知识">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch学习_进阶知识">
<meta property="og:url" content="https://study.hycbook.com/article/53040.html">
<meta property="og:site_name" content="兼一书虫">
<meta property="og:description" content="pytorch学习_进阶知识">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic.hycbook.com/i/hexo/post_cover/%E8%95%BE%E5%A7%865.webp">
<meta property="article:published_time" content="2024-02-20T06:38:53.000Z">
<meta property="article:modified_time" content="2024-02-26T02:34:21.096Z">
<meta property="article:author" content="narutohyc">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic.hycbook.com/i/hexo/post_cover/%E8%95%BE%E5%A7%865.webp"><link rel="shortcut icon" href="https://pic.hycbook.com/i//hexo/config_imgs/网站图标.webp"><link rel="canonical" href="https://study.hycbook.com/article/53040"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="manifest" href="/manifest.json"/><meta name="msapplication-TileColor" content="#c6ff7a"/><link rel="apple-touch-icon" sizes="180x180" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/favicon-16x16.png"/><link rel="mask-icon" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/safari-pinned-tab.svg" color="#5bbad5"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?68340394dfd808cea9826e8a57f87aa6";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":120,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":200,"languages":{"author":"作者: narutohyc","link":"链接: ","source":"来源: 兼一书虫","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'pytorch学习_进阶知识',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-02-26 10:34:21'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/mainColor/heoMainColor.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/404/404.css"><script src="https://npm.elemecdn.com/echarts@4.9.0/dist/echarts.min.js"></script><link href="https://cdn.bootcdn.net/ajax/libs/toastr.js/2.1.4/toastr.min.css" rel="stylesheet"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/categoryBar/categoryBar.css"><link rel="stylesheet" href="/css/hyc_udf.css"><link rel="stylesheet" href="/css/udf_css.css"><link rel="stylesheet" href="/css/year.css"><svg aria-hidden="true" style="position:absolute; overflow:hidden; width:0; height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248L626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/person_img/兼一头像.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">124</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">172</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fangwu"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 归档</span></a></li><li><a class="site-page child" href="/categories"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> 分类</span></a></li><li><a class="site-page child" href="/tags"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yuedu">             </use></svg><span> gitbook版</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://common.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> common</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://dl.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> deep learning</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://python.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> python</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://snooby.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> snooby</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://hycbook.flowus.cn"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang">                   </use></svg><span> flowus</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xuegao">             </use></svg><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yinle">                   </use></svg><span> 音乐</span></a></li><li><a class="site-page child" href="/bangumis"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wodezhuifan">                   </use></svg><span> 追番</span></a></li><li><a class="site-page child" href="/gallery"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xiangce">                   </use></svg><span> 相册</span></a></li><li><a class="site-page child" href="/video"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-shipin">                   </use></svg><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/charts"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xigua"></use></svg><span> 统计图</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-suannai">             </use></svg><span> 网盘</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://pan.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 私月盘</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://share.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> 共享盘</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifeiji">             </use></svg><span> 导航</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/comments"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-TIFFANYSROOM_huaban">                   </use></svg><span> 留言板</span></a></li><li><a class="site-page child" href="/link"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-friends">                   </use></svg><span> 友链</span></a></li><li><a class="site-page child" href="/about"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-aixin">                   </use></svg><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic.hycbook.com/i/hexo/post_imgs/蕾姆5.webp')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">兼一书虫</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fangwu"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 归档</span></a></li><li><a class="site-page child" href="/categories"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> 分类</span></a></li><li><a class="site-page child" href="/tags"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yuedu">             </use></svg><span> gitbook版</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://common.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> common</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://dl.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> deep learning</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://python.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> python</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://snooby.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> snooby</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://hycbook.flowus.cn"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang">                   </use></svg><span> flowus</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xuegao">             </use></svg><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yinle">                   </use></svg><span> 音乐</span></a></li><li><a class="site-page child" href="/bangumis"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wodezhuifan">                   </use></svg><span> 追番</span></a></li><li><a class="site-page child" href="/gallery"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xiangce">                   </use></svg><span> 相册</span></a></li><li><a class="site-page child" href="/video"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-shipin">                   </use></svg><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/charts"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xigua"></use></svg><span> 统计图</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-suannai">             </use></svg><span> 网盘</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://pan.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 私月盘</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://share.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> 共享盘</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifeiji">             </use></svg><span> 导航</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/comments"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-TIFFANYSROOM_huaban">                   </use></svg><span> 留言板</span></a></li><li><a class="site-page child" href="/link"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-friends">                   </use></svg><span> 友链</span></a></li><li><a class="site-page child" href="/about"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-aixin">                   </use></svg><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">pytorch学习_进阶知识</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-20T06:38:53.000Z" title="发表于 2024-02-20 14:38:53">2024-02-20</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-02-26T02:34:21.096Z" title="更新于 2024-02-26 10:34:21">2024-02-26</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/deep-learning/">deep-learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">7.9k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>42分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title="pytorch学习_进阶知识"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="twikoo_visitors"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/article/53040.html#post-comment"><span id="twikoo-count"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><hr>
<blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.pytorchtutorial.com/docs/package_references/torch/">pytorch中文文档</a></p>
</blockquote>
<h1 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h1><p><code>torch.Tensor</code>是一种包含单一数据类型元素的多维矩阵</p>
<p>Torch定义了10种CPU tensor类型和GPU tensor类型：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Data type</th>
<th>dtype</th>
<th>CPU tensor</th>
<th>GPU tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td>32-bit floating point</td>
<td><code>torch.float32</code> or <code>torch.float</code></td>
<td><code>torch.FloatTensor</code></td>
<td><code>torch.cuda.FloatTensor</code></td>
</tr>
<tr>
<td>64-bit floating point</td>
<td><code>torch.float64</code> or <code>torch.double</code></td>
<td><code>torch.DoubleTensor</code></td>
<td><code>torch.cuda.DoubleTensor</code></td>
</tr>
<tr>
<td>16-bit floating point <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/tensors.html#id4">[1]</a></td>
<td><code>torch.float16</code> or <code>torch.half</code></td>
<td><code>torch.HalfTensor</code></td>
<td><code>torch.cuda.HalfTensor</code></td>
</tr>
<tr>
<td>16-bit floating point <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/tensors.html#id5">[2]</a></td>
<td><code>torch.bfloat16</code></td>
<td><code>torch.BFloat16Tensor</code></td>
<td><code>torch.cuda.BFloat16Tensor</code></td>
</tr>
<tr>
<td>32-bit complex</td>
<td><code>torch.complex32</code> or <code>torch.chalf</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>64-bit complex</td>
<td><code>torch.complex64</code> or <code>torch.cfloat</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>128-bit complex</td>
<td><code>torch.complex128</code> or <code>torch.cdouble</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>8-bit integer (unsigned)</td>
<td><code>torch.uint8</code></td>
<td><code>torch.ByteTensor</code></td>
<td><code>torch.cuda.ByteTensor</code></td>
</tr>
<tr>
<td>8-bit integer (signed)</td>
<td><code>torch.int8</code></td>
<td><code>torch.CharTensor</code></td>
<td><code>torch.cuda.CharTensor</code></td>
</tr>
<tr>
<td>16-bit integer (signed)</td>
<td><code>torch.int16</code> or <code>torch.short</code></td>
<td><code>torch.ShortTensor</code></td>
<td><code>torch.cuda.ShortTensor</code></td>
</tr>
<tr>
<td>32-bit integer (signed)</td>
<td><code>torch.int32</code> or <code>torch.int</code></td>
<td><code>torch.IntTensor</code></td>
<td><code>torch.cuda.IntTensor</code></td>
</tr>
<tr>
<td>64-bit integer (signed)</td>
<td><code>torch.int64</code> or <code>torch.long</code></td>
<td><code>torch.LongTensor</code></td>
<td><code>torch.cuda.LongTensor</code></td>
</tr>
<tr>
<td>Boolean</td>
<td><code>torch.bool</code></td>
<td><code>torch.BoolTensor</code></td>
<td><code>torch.cuda.BoolTensor</code></td>
</tr>
<tr>
<td>quantized 8-bit integer (unsigned)</td>
<td><code>torch.quint8</code></td>
<td><code>torch.ByteTensor</code></td>
<td>/</td>
</tr>
<tr>
<td>quantized 8-bit integer (signed)</td>
<td><code>torch.qint8</code></td>
<td><code>torch.CharTensor</code></td>
<td>/</td>
</tr>
<tr>
<td>quantized 32-bit integer (signed)</td>
<td><code>torch.qint32</code></td>
<td><code>torch.IntTensor</code></td>
<td>/</td>
</tr>
<tr>
<td>quantized 4-bit integer (unsigned) <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/tensors.html#id6">[3]</a></td>
<td><code>torch.quint4x2</code></td>
<td><code>torch.ByteTensor</code></td>
<td>/</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>创建</p>
</blockquote>
<p>一个张量tensor可以从Python的<code>list</code>或序列构建</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure>
<p>根据可选择的大小和数据新建一个tensor。 如果没有提供参数，将会返回一个空的零维张量。如果提供了<code>numpy.ndarray</code>,<code>torch.Tensor</code>或<code>torch.Storage</code>，将会返回一个有同样参数的tensor.如果提供了python序列，将会从序列的副本创建一个tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 接口 一个空张量tensor可以通过规定其大小来构建</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.Tensor</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.Tensor(*sizes)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.Tensor(size)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.Tensor(sequence)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.Tensor(ndarray)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.Tensor(tensor)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.Tensor(storage)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化</span></span><br><span class="line">torch.IntTensor(<span class="number">2</span>, <span class="number">4</span>).zero_()</span><br></pre></td></tr></table></figure>
<p>可以用python的索引和切片来获取和修改一个张量tensor中的内容</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">x[<span class="number">1</span>][<span class="number">2</span>]</span><br><span class="line">Out[<span class="number">0</span>]: tensor(<span class="number">6.</span>)</span><br><span class="line">    </span><br><span class="line">x[<span class="number">0</span>][<span class="number">1</span>] = <span class="number">8</span></span><br><span class="line">x</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">8.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure>
<p>每一个张量tensor都有一个相应的<code>torch.Storage</code>用来保存其数据。类tensor提供了一个存储的多维的、横向视图，并且定义了在数值运算</p>
<p><strong>会改变tensor的函数操作会用一个下划线后缀来标示</strong>。比如，<code>torch.FloatTensor.abs_()</code>会在原地计算绝对值，并返回改变后的tensor，而<code>tensor.FloatTensor.abs()</code>将会在一个新的tensor中计算结果</p>
<blockquote>
<p>关键属性和方法</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_tensor.html#torch.Tensor.new_tensor"><code>Tensor.new_tensor</code></a></th>
<th>Returns a new Tensor with <code>data</code> as the tensor data.</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_full.html#torch.Tensor.new_full"><code>Tensor.new_full</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with <code>fill_value</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_empty.html#torch.Tensor.new_empty"><code>Tensor.new_empty</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with uninitialized data.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_ones.html#torch.Tensor.new_ones"><code>Tensor.new_ones</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with <code>1</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_zeros.html#torch.Tensor.new_zeros"><code>Tensor.new_zeros</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with <code>0</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_cuda.html#torch.Tensor.is_cuda"><code>Tensor.is_cuda</code></a></td>
<td>Is <code>True</code> if the Tensor is stored on the GPU, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_quantized.html#torch.Tensor.is_quantized"><code>Tensor.is_quantized</code></a></td>
<td>Is <code>True</code> if the Tensor is quantized, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_meta.html#torch.Tensor.is_meta"><code>Tensor.is_meta</code></a></td>
<td>Is <code>True</code> if the Tensor is a meta tensor, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.device.html#torch.Tensor.device"><code>Tensor.device</code></a></td>
<td>Is the <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device"><code>torch.device</code></a> where this Tensor is.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html#torch.Tensor.grad"><code>Tensor.grad</code></a></td>
<td>This attribute is <code>None</code> by default and becomes a Tensor the first time a call to <code>backward()</code> computes gradients for <code>self</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ndim.html#torch.Tensor.ndim"><code>Tensor.ndim</code></a></td>
<td>Alias for <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim"><code>dim()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.real.html#torch.Tensor.real"><code>Tensor.real</code></a></td>
<td>Returns a new tensor containing real values of the <code>self</code> tensor for a complex-valued input tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.imag.html#torch.Tensor.imag"><code>Tensor.imag</code></a></td>
<td>Returns a new tensor containing imaginary values of the <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.abs.html#torch.Tensor.abs"><code>Tensor.abs</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs"><code>torch.abs()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.abs_.html#torch.Tensor.abs_"><code>Tensor.abs_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.abs.html#torch.Tensor.abs"><code>abs()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.absolute.html#torch.Tensor.absolute"><code>Tensor.absolute</code></a></td>
<td>Alias for <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs"><code>abs()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.absolute_.html#torch.Tensor.absolute_"><code>Tensor.absolute_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.absolute.html#torch.Tensor.absolute"><code>absolute()</code></a> Alias for <code>abs_()</code></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.acos.html#torch.Tensor.acos"><code>Tensor.acos</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.acos.html#torch.acos"><code>torch.acos()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.acos_.html#torch.Tensor.acos_"><code>Tensor.acos_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.acos.html#torch.Tensor.acos"><code>acos()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccos.html#torch.Tensor.arccos"><code>Tensor.arccos</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos"><code>torch.arccos()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccos_.html#torch.Tensor.arccos_"><code>Tensor.arccos_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccos.html#torch.Tensor.arccos"><code>arccos()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.add.html#torch.Tensor.add"><code>Tensor.add</code></a></td>
<td>Add a scalar or tensor to <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.add_.html#torch.Tensor.add_"><code>Tensor.add_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.add.html#torch.Tensor.add"><code>add()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm"><code>Tensor.addbmm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"><code>torch.addbmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addbmm_.html#torch.Tensor.addbmm_"><code>Tensor.addbmm_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm"><code>addbmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv"><code>Tensor.addcdiv</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"><code>torch.addcdiv()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcdiv_.html#torch.Tensor.addcdiv_"><code>Tensor.addcdiv_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv"><code>addcdiv()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul"><code>Tensor.addcmul</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul"><code>torch.addcmul()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcmul_.html#torch.Tensor.addcmul_"><code>Tensor.addcmul_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul"><code>addcmul()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmm.html#torch.Tensor.addmm"><code>Tensor.addmm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm"><code>torch.addmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmm_.html#torch.Tensor.addmm_"><code>Tensor.addmm_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmm.html#torch.Tensor.addmm"><code>addmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sspaddmm.html#torch.Tensor.sspaddmm"><code>Tensor.sspaddmm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sspaddmm.html#torch.sspaddmm"><code>torch.sspaddmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmv.html#torch.Tensor.addmv"><code>Tensor.addmv</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.addmv.html#torch.addmv"><code>torch.addmv()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmv_.html#torch.Tensor.addmv_"><code>Tensor.addmv_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmv.html#torch.Tensor.addmv"><code>addmv()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addr.html#torch.Tensor.addr"><code>Tensor.addr</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.addr.html#torch.addr"><code>torch.addr()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addr_.html#torch.Tensor.addr_"><code>Tensor.addr_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addr.html#torch.Tensor.addr"><code>addr()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.adjoint.html#torch.Tensor.adjoint"><code>Tensor.adjoint</code></a></td>
<td>Alias for <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.adjoint.html#torch.adjoint"><code>adjoint()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.allclose.html#torch.Tensor.allclose"><code>Tensor.allclose</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose"><code>torch.allclose()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.amax.html#torch.Tensor.amax"><code>Tensor.amax</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.amax.html#torch.amax"><code>torch.amax()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.amin.html#torch.Tensor.amin"><code>Tensor.amin</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.amin.html#torch.amin"><code>torch.amin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.aminmax.html#torch.Tensor.aminmax"><code>Tensor.aminmax</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.aminmax.html#torch.aminmax"><code>torch.aminmax()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.angle.html#torch.Tensor.angle"><code>Tensor.angle</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.angle.html#torch.angle"><code>torch.angle()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.apply_.html#torch.Tensor.apply_"><code>Tensor.apply_</code></a></td>
<td>Applies the function <code>callable</code> to each element in the tensor, replacing each element with the value returned by <code>callable</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.argmax.html#torch.Tensor.argmax"><code>Tensor.argmax</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"><code>torch.argmax()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.argmin.html#torch.Tensor.argmin"><code>Tensor.argmin</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.argmin.html#torch.argmin"><code>torch.argmin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.argsort.html#torch.Tensor.argsort"><code>Tensor.argsort</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort"><code>torch.argsort()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.argwhere.html#torch.Tensor.argwhere"><code>Tensor.argwhere</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.argwhere.html#torch.argwhere"><code>torch.argwhere()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.asin.html#torch.Tensor.asin"><code>Tensor.asin</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.asin.html#torch.asin"><code>torch.asin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.asin_.html#torch.Tensor.asin_"><code>Tensor.asin_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.asin.html#torch.Tensor.asin"><code>asin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin"><code>Tensor.arcsin</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.arcsin.html#torch.arcsin"><code>torch.arcsin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsin_.html#torch.Tensor.arcsin_"><code>Tensor.arcsin_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin"><code>arcsin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided"><code>Tensor.as_strided</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"><code>torch.as_strided()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan.html#torch.Tensor.atan"><code>Tensor.atan</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.atan.html#torch.atan"><code>torch.atan()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan_.html#torch.Tensor.atan_"><code>Tensor.atan_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan.html#torch.Tensor.atan"><code>atan()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan.html#torch.Tensor.arctan"><code>Tensor.arctan</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.arctan.html#torch.arctan"><code>torch.arctan()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan_.html#torch.Tensor.arctan_"><code>Tensor.arctan_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan.html#torch.Tensor.arctan"><code>arctan()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan2.html#torch.Tensor.atan2"><code>Tensor.atan2</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2"><code>torch.atan2()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan2_.html#torch.Tensor.atan2_"><code>Tensor.atan2_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan2.html#torch.Tensor.atan2"><code>atan2()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan2.html#torch.Tensor.arctan2"><code>Tensor.arctan2</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.arctan2.html#torch.arctan2"><code>torch.arctan2()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan2_.html#torch.Tensor.arctan2_"><code>Tensor.arctan2_</code></a></td>
<td>atan2_(other) -&gt; Tensor</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.all.html#torch.Tensor.all"><code>Tensor.all</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"><code>torch.all()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.any.html#torch.Tensor.any"><code>Tensor.any</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"><code>torch.any()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward"><code>Tensor.backward</code></a></td>
<td>Computes the gradient of current tensor w.r.t.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm"><code>Tensor.baddbmm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm"><code>torch.baddbmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.baddbmm_.html#torch.Tensor.baddbmm_"><code>Tensor.baddbmm_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm"><code>baddbmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli.html#torch.Tensor.bernoulli"><code>Tensor.bernoulli</code></a></td>
<td>Returns a result tensor where each \texttt{result[i]}result[i] is independently sampled from \text{Bernoulli}(\texttt{self[i]})Bernoulli(self[i]).</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_"><code>Tensor.bernoulli_</code></a></td>
<td>Fills each location of <code>self</code> with an independent sample from \text{Bernoulli}(\texttt{p})Bernoulli(p).</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bfloat16.html#torch.Tensor.bfloat16"><code>Tensor.bfloat16</code></a></td>
<td><code>self.bfloat16()</code> is equivalent to <code>self.to(torch.bfloat16)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bincount.html#torch.Tensor.bincount"><code>Tensor.bincount</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"><code>torch.bincount()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not"><code>Tensor.bitwise_not</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"><code>torch.bitwise_not()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_not_.html#torch.Tensor.bitwise_not_"><code>Tensor.bitwise_not_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not"><code>bitwise_not()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and"><code>Tensor.bitwise_and</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bitwise_and.html#torch.bitwise_and"><code>torch.bitwise_and()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_and_.html#torch.Tensor.bitwise_and_"><code>Tensor.bitwise_and_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and"><code>bitwise_and()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or"><code>Tensor.bitwise_or</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bitwise_or.html#torch.bitwise_or"><code>torch.bitwise_or()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_or_.html#torch.Tensor.bitwise_or_"><code>Tensor.bitwise_or_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or"><code>bitwise_or()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor"><code>Tensor.bitwise_xor</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor"><code>torch.bitwise_xor()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_xor_.html#torch.Tensor.bitwise_xor_"><code>Tensor.bitwise_xor_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor"><code>bitwise_xor()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift"><code>Tensor.bitwise_left_shift</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift"><code>torch.bitwise_left_shift()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_left_shift_.html#torch.Tensor.bitwise_left_shift_"><code>Tensor.bitwise_left_shift_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift"><code>bitwise_left_shift()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift"><code>Tensor.bitwise_right_shift</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift"><code>torch.bitwise_right_shift()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_right_shift_.html#torch.Tensor.bitwise_right_shift_"><code>Tensor.bitwise_right_shift_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift"><code>bitwise_right_shift()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bmm.html#torch.Tensor.bmm"><code>Tensor.bmm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm"><code>torch.bmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bool.html#torch.Tensor.bool"><code>Tensor.bool</code></a></td>
<td><code>self.bool()</code> is equivalent to <code>self.to(torch.bool)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.byte.html#torch.Tensor.byte"><code>Tensor.byte</code></a></td>
<td><code>self.byte()</code> is equivalent to <code>self.to(torch.uint8)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.broadcast_to.html#torch.Tensor.broadcast_to"><code>Tensor.broadcast_to</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to"><code>torch.broadcast_to()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_"><code>Tensor.cauchy_</code></a></td>
<td>Fills the tensor with numbers drawn from the Cauchy distribution:</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ceil.html#torch.Tensor.ceil"><code>Tensor.ceil</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.ceil.html#torch.ceil"><code>torch.ceil()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ceil_.html#torch.Tensor.ceil_"><code>Tensor.ceil_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ceil.html#torch.Tensor.ceil"><code>ceil()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.char.html#torch.Tensor.char"><code>Tensor.char</code></a></td>
<td><code>self.char()</code> is equivalent to <code>self.to(torch.int8)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cholesky.html#torch.Tensor.cholesky"><code>Tensor.cholesky</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"><code>torch.cholesky()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cholesky_inverse.html#torch.Tensor.cholesky_inverse"><code>Tensor.cholesky_inverse</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse"><code>torch.cholesky_inverse()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cholesky_solve.html#torch.Tensor.cholesky_solve"><code>Tensor.cholesky_solve</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cholesky_solve.html#torch.cholesky_solve"><code>torch.cholesky_solve()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.chunk.html#torch.Tensor.chunk"><code>Tensor.chunk</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk"><code>torch.chunk()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp.html#torch.Tensor.clamp"><code>Tensor.clamp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp"><code>torch.clamp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_"><code>Tensor.clamp_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp.html#torch.Tensor.clamp"><code>clamp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clip.html#torch.Tensor.clip"><code>Tensor.clip</code></a></td>
<td>Alias for <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp.html#torch.Tensor.clamp"><code>clamp()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clip_.html#torch.Tensor.clip_"><code>Tensor.clip_</code></a></td>
<td>Alias for <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_"><code>clamp_()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clone.html#torch.Tensor.clone"><code>Tensor.clone</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.clone.html#torch.clone"><code>torch.clone()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous"><code>Tensor.contiguous</code></a></td>
<td>Returns a contiguous in memory tensor containing the same data as <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.copy_.html#torch.Tensor.copy_"><code>Tensor.copy_</code></a></td>
<td>Copies the elements from <code>src</code> into <code>self</code> tensor and returns <code>self</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj.html#torch.Tensor.conj"><code>Tensor.conj</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj"><code>torch.conj()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical"><code>Tensor.conj_physical</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.conj_physical.html#torch.conj_physical"><code>torch.conj_physical()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj_physical_.html#torch.Tensor.conj_physical_"><code>Tensor.conj_physical_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical"><code>conj_physical()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.resolve_conj.html#torch.Tensor.resolve_conj"><code>Tensor.resolve_conj</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.resolve_conj.html#torch.resolve_conj"><code>torch.resolve_conj()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.resolve_neg.html#torch.Tensor.resolve_neg"><code>Tensor.resolve_neg</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.resolve_neg.html#torch.resolve_neg"><code>torch.resolve_neg()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.copysign.html#torch.Tensor.copysign"><code>Tensor.copysign</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.copysign.html#torch.copysign"><code>torch.copysign()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.copysign_.html#torch.Tensor.copysign_"><code>Tensor.copysign_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.copysign.html#torch.Tensor.copysign"><code>copysign()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cos.html#torch.Tensor.cos"><code>Tensor.cos</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos"><code>torch.cos()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cos_.html#torch.Tensor.cos_"><code>Tensor.cos_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cos.html#torch.Tensor.cos"><code>cos()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cosh.html#torch.Tensor.cosh"><code>Tensor.cosh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh"><code>torch.cosh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cosh_.html#torch.Tensor.cosh_"><code>Tensor.cosh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cosh.html#torch.Tensor.cosh"><code>cosh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.corrcoef.html#torch.Tensor.corrcoef"><code>Tensor.corrcoef</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.corrcoef.html#torch.corrcoef"><code>torch.corrcoef()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.count_nonzero.html#torch.Tensor.count_nonzero"><code>Tensor.count_nonzero</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero"><code>torch.count_nonzero()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cov.html#torch.Tensor.cov"><code>Tensor.cov</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cov.html#torch.cov"><code>torch.cov()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.acosh.html#torch.Tensor.acosh"><code>Tensor.acosh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.acosh.html#torch.acosh"><code>torch.acosh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.acosh_.html#torch.Tensor.acosh_"><code>Tensor.acosh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.acosh.html#torch.Tensor.acosh"><code>acosh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccosh.html#torch.Tensor.arccosh"><code>Tensor.arccosh</code></a></td>
<td>acosh() -&gt; Tensor</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccosh_.html#torch.Tensor.arccosh_"><code>Tensor.arccosh_</code></a></td>
<td>acosh_() -&gt; Tensor</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html#torch.Tensor.cpu"><code>Tensor.cpu</code></a></td>
<td>Returns a copy of this object in CPU memory.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cross.html#torch.Tensor.cross"><code>Tensor.cross</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross"><code>torch.cross()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cuda.html#torch.Tensor.cuda"><code>Tensor.cuda</code></a></td>
<td>Returns a copy of this object in CUDA memory.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logcumsumexp.html#torch.Tensor.logcumsumexp"><code>Tensor.logcumsumexp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logcumsumexp.html#torch.logcumsumexp"><code>torch.logcumsumexp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cummax.html#torch.Tensor.cummax"><code>Tensor.cummax</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cummax.html#torch.cummax"><code>torch.cummax()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cummin.html#torch.Tensor.cummin"><code>Tensor.cummin</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cummin.html#torch.cummin"><code>torch.cummin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod"><code>Tensor.cumprod</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod"><code>torch.cumprod()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumprod_.html#torch.Tensor.cumprod_"><code>Tensor.cumprod_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod"><code>cumprod()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum"><code>Tensor.cumsum</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cumsum.html#torch.cumsum"><code>torch.cumsum()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumsum_.html#torch.Tensor.cumsum_"><code>Tensor.cumsum_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum"><code>cumsum()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.chalf.html#torch.Tensor.chalf"><code>Tensor.chalf</code></a></td>
<td><code>self.chalf()</code> is equivalent to <code>self.to(torch.complex32)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cfloat.html#torch.Tensor.cfloat"><code>Tensor.cfloat</code></a></td>
<td><code>self.cfloat()</code> is equivalent to <code>self.to(torch.complex64)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cdouble.html#torch.Tensor.cdouble"><code>Tensor.cdouble</code></a></td>
<td><code>self.cdouble()</code> is equivalent to <code>self.to(torch.complex128)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.data_ptr.html#torch.Tensor.data_ptr"><code>Tensor.data_ptr</code></a></td>
<td>Returns the address of the first element of <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.deg2rad.html#torch.Tensor.deg2rad"><code>Tensor.deg2rad</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.deg2rad.html#torch.deg2rad"><code>torch.deg2rad()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dequantize.html#torch.Tensor.dequantize"><code>Tensor.dequantize</code></a></td>
<td>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.det.html#torch.Tensor.det"><code>Tensor.det</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.det.html#torch.det"><code>torch.det()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim"><code>Tensor.dense_dim</code></a></td>
<td>Return the number of dense dimensions in a <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/sparse.html#sparse-docs">sparse tensor</a> <code>self</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html#torch.Tensor.detach"><code>Tensor.detach</code></a></td>
<td>Returns a new Tensor, detached from the current graph.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.detach_.html#torch.Tensor.detach_"><code>Tensor.detach_</code></a></td>
<td>Detaches the Tensor from the graph that created it, making it a leaf.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.diag.html#torch.Tensor.diag"><code>Tensor.diag</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"><code>torch.diag()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.diag_embed.html#torch.Tensor.diag_embed"><code>Tensor.diag_embed</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.diag_embed.html#torch.diag_embed"><code>torch.diag_embed()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.diagflat.html#torch.Tensor.diagflat"><code>Tensor.diagflat</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.diagflat.html#torch.diagflat"><code>torch.diagflat()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.diagonal.html#torch.Tensor.diagonal"><code>Tensor.diagonal</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal"><code>torch.diagonal()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.diagonal_scatter.html#torch.Tensor.diagonal_scatter"><code>Tensor.diagonal_scatter</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.diagonal_scatter.html#torch.diagonal_scatter"><code>torch.diagonal_scatter()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fill_diagonal_.html#torch.Tensor.fill_diagonal_"><code>Tensor.fill_diagonal_</code></a></td>
<td>Fill the main diagonal of a tensor that has at least 2-dimensions.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmax.html#torch.Tensor.fmax"><code>Tensor.fmax</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax"><code>torch.fmax()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmin.html#torch.Tensor.fmin"><code>Tensor.fmin</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin"><code>torch.fmin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.diff.html#torch.Tensor.diff"><code>Tensor.diff</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.diff.html#torch.diff"><code>torch.diff()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.digamma.html#torch.Tensor.digamma"><code>Tensor.digamma</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.digamma.html#torch.digamma"><code>torch.digamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.digamma_.html#torch.Tensor.digamma_"><code>Tensor.digamma_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.digamma.html#torch.Tensor.digamma"><code>digamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim"><code>Tensor.dim</code></a></td>
<td>Returns the number of dimensions of <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dist.html#torch.Tensor.dist"><code>Tensor.dist</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist"><code>torch.dist()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.div.html#torch.Tensor.div"><code>Tensor.div</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"><code>torch.div()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.div_.html#torch.Tensor.div_"><code>Tensor.div_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.div.html#torch.Tensor.div"><code>div()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.divide.html#torch.Tensor.divide"><code>Tensor.divide</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.divide.html#torch.divide"><code>torch.divide()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.divide_.html#torch.Tensor.divide_"><code>Tensor.divide_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.divide.html#torch.Tensor.divide"><code>divide()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dot.html#torch.Tensor.dot"><code>Tensor.dot</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot"><code>torch.dot()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.double.html#torch.Tensor.double"><code>Tensor.double</code></a></td>
<td><code>self.double()</code> is equivalent to <code>self.to(torch.float64)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dsplit.html#torch.Tensor.dsplit"><code>Tensor.dsplit</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit"><code>torch.dsplit()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.element_size.html#torch.Tensor.element_size"><code>Tensor.element_size</code></a></td>
<td>Returns the size in bytes of an individual element.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.eq.html#torch.Tensor.eq"><code>Tensor.eq</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.eq.html#torch.eq"><code>torch.eq()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.eq_.html#torch.Tensor.eq_"><code>Tensor.eq_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.eq.html#torch.Tensor.eq"><code>eq()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.equal.html#torch.Tensor.equal"><code>Tensor.equal</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.equal.html#torch.equal"><code>torch.equal()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erf.html#torch.Tensor.erf"><code>Tensor.erf</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.erf.html#torch.erf"><code>torch.erf()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erf_.html#torch.Tensor.erf_"><code>Tensor.erf_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erf.html#torch.Tensor.erf"><code>erf()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfc.html#torch.Tensor.erfc"><code>Tensor.erfc</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.erfc.html#torch.erfc"><code>torch.erfc()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfc_.html#torch.Tensor.erfc_"><code>Tensor.erfc_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfc.html#torch.Tensor.erfc"><code>erfc()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv"><code>Tensor.erfinv</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.erfinv.html#torch.erfinv"><code>torch.erfinv()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfinv_.html#torch.Tensor.erfinv_"><code>Tensor.erfinv_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv"><code>erfinv()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.exp.html#torch.Tensor.exp"><code>Tensor.exp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp"><code>torch.exp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.exp_.html#torch.Tensor.exp_"><code>Tensor.exp_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.exp.html#torch.Tensor.exp"><code>exp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.expm1.html#torch.Tensor.expm1"><code>Tensor.expm1</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.expm1.html#torch.expm1"><code>torch.expm1()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.expm1_.html#torch.Tensor.expm1_"><code>Tensor.expm1_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.expm1.html#torch.Tensor.expm1"><code>expm1()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html#torch.Tensor.expand"><code>Tensor.expand</code></a></td>
<td>Returns a new view of the <code>self</code> tensor with singleton dimensions expanded to a larger size.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as"><code>Tensor.expand_as</code></a></td>
<td>Expand this tensor to the same size as <code>other</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_"><code>Tensor.exponential_</code></a></td>
<td>Fills <code>self</code> tensor with elements drawn from the exponential distribution:</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fix.html#torch.Tensor.fix"><code>Tensor.fix</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.fix.html#torch.fix"><code>torch.fix()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fix_.html#torch.Tensor.fix_"><code>Tensor.fix_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fix.html#torch.Tensor.fix"><code>fix()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fill_.html#torch.Tensor.fill_"><code>Tensor.fill_</code></a></td>
<td>Fills <code>self</code> tensor with the specified value.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.flatten.html#torch.Tensor.flatten"><code>Tensor.flatten</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten"><code>torch.flatten()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.flip.html#torch.Tensor.flip"><code>Tensor.flip</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"><code>torch.flip()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fliplr.html#torch.Tensor.fliplr"><code>Tensor.fliplr</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"><code>torch.fliplr()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.flipud.html#torch.Tensor.flipud"><code>Tensor.flipud</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"><code>torch.flipud()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.float.html#torch.Tensor.float"><code>Tensor.float</code></a></td>
<td><code>self.float()</code> is equivalent to <code>self.to(torch.float32)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.float_power.html#torch.Tensor.float_power"><code>Tensor.float_power</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.float_power.html#torch.float_power"><code>torch.float_power()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.float_power_.html#torch.Tensor.float_power_"><code>Tensor.float_power_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.float_power.html#torch.Tensor.float_power"><code>float_power()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor.html#torch.Tensor.floor"><code>Tensor.floor</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.floor.html#torch.floor"><code>torch.floor()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_.html#torch.Tensor.floor_"><code>Tensor.floor_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor.html#torch.Tensor.floor"><code>floor()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide"><code>Tensor.floor_divide</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.floor_divide.html#torch.floor_divide"><code>torch.floor_divide()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_divide_.html#torch.Tensor.floor_divide_"><code>Tensor.floor_divide_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide"><code>floor_divide()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmod.html#torch.Tensor.fmod"><code>Tensor.fmod</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"><code>torch.fmod()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmod_.html#torch.Tensor.fmod_"><code>Tensor.fmod_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmod.html#torch.Tensor.fmod"><code>fmod()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.frac.html#torch.Tensor.frac"><code>Tensor.frac</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.frac.html#torch.frac"><code>torch.frac()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.frac_.html#torch.Tensor.frac_"><code>Tensor.frac_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.frac.html#torch.Tensor.frac"><code>frac()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.frexp.html#torch.Tensor.frexp"><code>Tensor.frexp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"><code>torch.frexp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.gather.html#torch.Tensor.gather"><code>Tensor.gather</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather"><code>torch.gather()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.gcd.html#torch.Tensor.gcd"><code>Tensor.gcd</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd"><code>torch.gcd()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.gcd_.html#torch.Tensor.gcd_"><code>Tensor.gcd_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.gcd.html#torch.Tensor.gcd"><code>gcd()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ge.html#torch.Tensor.ge"><code>Tensor.ge</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.ge.html#torch.ge"><code>torch.ge()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ge_.html#torch.Tensor.ge_"><code>Tensor.ge_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ge.html#torch.Tensor.ge"><code>ge()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal"><code>Tensor.greater_equal</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.greater_equal.html#torch.greater_equal"><code>torch.greater_equal()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_equal_.html#torch.Tensor.greater_equal_"><code>Tensor.greater_equal_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal"><code>greater_equal()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_"><code>Tensor.geometric_</code></a></td>
<td>Fills <code>self</code> tensor with elements drawn from the geometric distribution:</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.geqrf.html#torch.Tensor.geqrf"><code>Tensor.geqrf</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"><code>torch.geqrf()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ger.html#torch.Tensor.ger"><code>Tensor.ger</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.ger.html#torch.ger"><code>torch.ger()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.get_device.html#torch.Tensor.get_device"><code>Tensor.get_device</code></a></td>
<td>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.gt.html#torch.Tensor.gt"><code>Tensor.gt</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt"><code>torch.gt()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.gt_.html#torch.Tensor.gt_"><code>Tensor.gt_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.gt.html#torch.Tensor.gt"><code>gt()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater.html#torch.Tensor.greater"><code>Tensor.greater</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.greater.html#torch.greater"><code>torch.greater()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_.html#torch.Tensor.greater_"><code>Tensor.greater_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater.html#torch.Tensor.greater"><code>greater()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.half.html#torch.Tensor.half"><code>Tensor.half</code></a></td>
<td><code>self.half()</code> is equivalent to <code>self.to(torch.float16)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.hardshrink.html#torch.Tensor.hardshrink"><code>Tensor.hardshrink</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink"><code>torch.nn.functional.hardshrink()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.heaviside.html#torch.Tensor.heaviside"><code>Tensor.heaviside</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside"><code>torch.heaviside()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.histc.html#torch.Tensor.histc"><code>Tensor.histc</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.histc.html#torch.histc"><code>torch.histc()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.histogram.html#torch.Tensor.histogram"><code>Tensor.histogram</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.histogram.html#torch.histogram"><code>torch.histogram()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.hsplit.html#torch.Tensor.hsplit"><code>Tensor.hsplit</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"><code>torch.hsplit()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.hypot.html#torch.Tensor.hypot"><code>Tensor.hypot</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.hypot.html#torch.hypot"><code>torch.hypot()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.hypot_.html#torch.Tensor.hypot_"><code>Tensor.hypot_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.hypot.html#torch.Tensor.hypot"><code>hypot()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.i0.html#torch.Tensor.i0"><code>Tensor.i0</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.i0.html#torch.i0"><code>torch.i0()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.i0_.html#torch.Tensor.i0_"><code>Tensor.i0_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.i0.html#torch.Tensor.i0"><code>i0()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.igamma.html#torch.Tensor.igamma"><code>Tensor.igamma</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"><code>torch.igamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.igamma_.html#torch.Tensor.igamma_"><code>Tensor.igamma_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.igamma.html#torch.Tensor.igamma"><code>igamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.igammac.html#torch.Tensor.igammac"><code>Tensor.igammac</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.igammac.html#torch.igammac"><code>torch.igammac()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.igammac_.html#torch.Tensor.igammac_"><code>Tensor.igammac_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.igammac.html#torch.Tensor.igammac"><code>igammac()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_"><code>Tensor.index_add_</code></a></td>
<td>Accumulate the elements of <code>alpha</code> times <code>source</code> into the <code>self</code> tensor by adding to the indices in the order given in <code>index</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_add.html#torch.Tensor.index_add"><code>Tensor.index_add</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_"><code>torch.Tensor.index_add_()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_"><code>Tensor.index_copy_</code></a></td>
<td>Copies the elements of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"><code>tensor</code></a> into the <code>self</code> tensor by selecting the indices in the order given in <code>index</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_copy.html#torch.Tensor.index_copy"><code>Tensor.index_copy</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_"><code>torch.Tensor.index_copy_()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_"><code>Tensor.index_fill_</code></a></td>
<td>Fills the elements of the <code>self</code> tensor with value <code>value</code> by selecting the indices in the order given in <code>index</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_fill.html#torch.Tensor.index_fill"><code>Tensor.index_fill</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_"><code>torch.Tensor.index_fill_()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_"><code>Tensor.index_put_</code></a></td>
<td>Puts values from the tensor <code>values</code> into the tensor <code>self</code> using the indices specified in <code>indices</code> (which is a tuple of Tensors).</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_put.html#torch.Tensor.index_put"><code>Tensor.index_put</code></a></td>
<td>Out-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_"><code>index_put_()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_reduce_.html#torch.Tensor.index_reduce_"><code>Tensor.index_reduce_</code></a></td>
<td>Accumulate the elements of <code>source</code> into the <code>self</code> tensor by accumulating to the indices in the order given in <code>index</code> using the reduction given by the <code>reduce</code> argument.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_reduce.html#torch.Tensor.index_reduce"><code>Tensor.index_reduce</code></a></td>
<td></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_select.html#torch.Tensor.index_select"><code>Tensor.index_select</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.index_select.html#torch.index_select"><code>torch.index_select()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.indices.html#torch.Tensor.indices"><code>Tensor.indices</code></a></td>
<td>Return the indices tensor of a <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs">sparse COO tensor</a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.inner.html#torch.Tensor.inner"><code>Tensor.inner</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner"><code>torch.inner()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.int.html#torch.Tensor.int"><code>Tensor.int</code></a></td>
<td><code>self.int()</code> is equivalent to <code>self.to(torch.int32)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.int_repr.html#torch.Tensor.int_repr"><code>Tensor.int_repr</code></a></td>
<td>Given a quantized Tensor, <code>self.int_repr()</code> returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.inverse.html#torch.Tensor.inverse"><code>Tensor.inverse</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.inverse.html#torch.inverse"><code>torch.inverse()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.isclose.html#torch.Tensor.isclose"><code>Tensor.isclose</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose"><code>torch.isclose()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.isfinite.html#torch.Tensor.isfinite"><code>Tensor.isfinite</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"><code>torch.isfinite()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.isinf.html#torch.Tensor.isinf"><code>Tensor.isinf</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.isinf.html#torch.isinf"><code>torch.isinf()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.isposinf.html#torch.Tensor.isposinf"><code>Tensor.isposinf</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.isposinf.html#torch.isposinf"><code>torch.isposinf()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.isneginf.html#torch.Tensor.isneginf"><code>Tensor.isneginf</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.isneginf.html#torch.isneginf"><code>torch.isneginf()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.isnan.html#torch.Tensor.isnan"><code>Tensor.isnan</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.isnan.html#torch.isnan"><code>torch.isnan()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_contiguous.html#torch.Tensor.is_contiguous"><code>Tensor.is_contiguous</code></a></td>
<td>Returns True if <code>self</code> tensor is contiguous in memory in the order specified by memory format.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_complex.html#torch.Tensor.is_complex"><code>Tensor.is_complex</code></a></td>
<td>Returns True if the data type of <code>self</code> is a complex data type.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_conj.html#torch.Tensor.is_conj"><code>Tensor.is_conj</code></a></td>
<td>Returns True if the conjugate bit of <code>self</code> is set to true.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_floating_point.html#torch.Tensor.is_floating_point"><code>Tensor.is_floating_point</code></a></td>
<td>Returns True if the data type of <code>self</code> is a floating point data type.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_inference.html#torch.Tensor.is_inference"><code>Tensor.is_inference</code></a></td>
<td>See <code>torch.is_inference()</code></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf"><code>Tensor.is_leaf</code></a></td>
<td>All Tensors that have <code>requires_grad</code> which is <code>False</code> will be leaf Tensors by convention.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_pinned.html#torch.Tensor.is_pinned"><code>Tensor.is_pinned</code></a></td>
<td>Returns true if this tensor resides in pinned memory.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_set_to.html#torch.Tensor.is_set_to"><code>Tensor.is_set_to</code></a></td>
<td>Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_shared.html#torch.Tensor.is_shared"><code>Tensor.is_shared</code></a></td>
<td>Checks if tensor is in shared memory.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_signed.html#torch.Tensor.is_signed"><code>Tensor.is_signed</code></a></td>
<td>Returns True if the data type of <code>self</code> is a signed data type.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse"><code>Tensor.is_sparse</code></a></td>
<td>Is <code>True</code> if the Tensor uses sparse storage layout, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.istft.html#torch.Tensor.istft"><code>Tensor.istft</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"><code>torch.istft()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.isreal.html#torch.Tensor.isreal"><code>Tensor.isreal</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal"><code>torch.isreal()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.item.html#torch.Tensor.item"><code>Tensor.item</code></a></td>
<td>Returns the value of this tensor as a standard Python number.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.kthvalue.html#torch.Tensor.kthvalue"><code>Tensor.kthvalue</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"><code>torch.kthvalue()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lcm.html#torch.Tensor.lcm"><code>Tensor.lcm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.lcm.html#torch.lcm"><code>torch.lcm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lcm_.html#torch.Tensor.lcm_"><code>Tensor.lcm_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lcm.html#torch.Tensor.lcm"><code>lcm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp"><code>Tensor.ldexp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.ldexp.html#torch.ldexp"><code>torch.ldexp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ldexp_.html#torch.Tensor.ldexp_"><code>Tensor.ldexp_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp"><code>ldexp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.le.html#torch.Tensor.le"><code>Tensor.le</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.le.html#torch.le"><code>torch.le()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.le_.html#torch.Tensor.le_"><code>Tensor.le_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.le.html#torch.Tensor.le"><code>le()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal"><code>Tensor.less_equal</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.less_equal.html#torch.less_equal"><code>torch.less_equal()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_equal_.html#torch.Tensor.less_equal_"><code>Tensor.less_equal_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal"><code>less_equal()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lerp.html#torch.Tensor.lerp"><code>Tensor.lerp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.lerp.html#torch.lerp"><code>torch.lerp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lerp_.html#torch.Tensor.lerp_"><code>Tensor.lerp_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lerp.html#torch.Tensor.lerp"><code>lerp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma"><code>Tensor.lgamma</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.lgamma.html#torch.lgamma"><code>torch.lgamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lgamma_.html#torch.Tensor.lgamma_"><code>Tensor.lgamma_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma"><code>lgamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log.html#torch.Tensor.log"><code>Tensor.log</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.log.html#torch.log"><code>torch.log()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log_.html#torch.Tensor.log_"><code>Tensor.log_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log.html#torch.Tensor.log"><code>log()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logdet.html#torch.Tensor.logdet"><code>Tensor.logdet</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logdet.html#torch.logdet"><code>torch.logdet()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log10.html#torch.Tensor.log10"><code>Tensor.log10</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.log10.html#torch.log10"><code>torch.log10()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log10_.html#torch.Tensor.log10_"><code>Tensor.log10_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log10.html#torch.Tensor.log10"><code>log10()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log1p.html#torch.Tensor.log1p"><code>Tensor.log1p</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.log1p.html#torch.log1p"><code>torch.log1p()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log1p_.html#torch.Tensor.log1p_"><code>Tensor.log1p_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log1p.html#torch.Tensor.log1p"><code>log1p()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log2.html#torch.Tensor.log2"><code>Tensor.log2</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.log2.html#torch.log2"><code>torch.log2()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log2_.html#torch.Tensor.log2_"><code>Tensor.log2_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log2.html#torch.Tensor.log2"><code>log2()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_"><code>Tensor.log_normal_</code></a></td>
<td>Fills <code>self</code> tensor with numbers samples from the log-normal distribution parameterized by the given mean \mu<em>μ</em> and standard deviation \sigma<em>σ</em>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logaddexp.html#torch.Tensor.logaddexp"><code>Tensor.logaddexp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"><code>torch.logaddexp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logaddexp2.html#torch.Tensor.logaddexp2"><code>Tensor.logaddexp2</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logaddexp2.html#torch.logaddexp2"><code>torch.logaddexp2()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logsumexp.html#torch.Tensor.logsumexp"><code>Tensor.logsumexp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp"><code>torch.logsumexp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and"><code>Tensor.logical_and</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logical_and.html#torch.logical_and"><code>torch.logical_and()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_and_.html#torch.Tensor.logical_and_"><code>Tensor.logical_and_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and"><code>logical_and()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not"><code>Tensor.logical_not</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logical_not.html#torch.logical_not"><code>torch.logical_not()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_not_.html#torch.Tensor.logical_not_"><code>Tensor.logical_not_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not"><code>logical_not()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or"><code>Tensor.logical_or</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or"><code>torch.logical_or()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_or_.html#torch.Tensor.logical_or_"><code>Tensor.logical_or_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or"><code>logical_or()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor"><code>Tensor.logical_xor</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logical_xor.html#torch.logical_xor"><code>torch.logical_xor()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_xor_.html#torch.Tensor.logical_xor_"><code>Tensor.logical_xor_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor"><code>logical_xor()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logit.html#torch.Tensor.logit"><code>Tensor.logit</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logit.html#torch.logit"><code>torch.logit()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logit_.html#torch.Tensor.logit_"><code>Tensor.logit_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logit.html#torch.Tensor.logit"><code>logit()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.long.html#torch.Tensor.long"><code>Tensor.long</code></a></td>
<td><code>self.long()</code> is equivalent to <code>self.to(torch.int64)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lt.html#torch.Tensor.lt"><code>Tensor.lt</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.lt.html#torch.lt"><code>torch.lt()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lt_.html#torch.Tensor.lt_"><code>Tensor.lt_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lt.html#torch.Tensor.lt"><code>lt()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.less.html#torch.Tensor.less"><code>Tensor.less</code></a></td>
<td>lt(other) -&gt; Tensor</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_.html#torch.Tensor.less_"><code>Tensor.less_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.less.html#torch.Tensor.less"><code>less()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lu.html#torch.Tensor.lu"><code>Tensor.lu</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"><code>torch.lu()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lu_solve.html#torch.Tensor.lu_solve"><code>Tensor.lu_solve</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve"><code>torch.lu_solve()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.as_subclass.html#torch.Tensor.as_subclass"><code>Tensor.as_subclass</code></a></td>
<td>Makes a <code>cls</code> instance with the same data pointer as <code>self</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.map_.html#torch.Tensor.map_"><code>Tensor.map_</code></a></td>
<td>Applies <code>callable</code> for each element in <code>self</code> tensor and the given <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"><code>tensor</code></a> and stores the results in <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_"><code>Tensor.masked_scatter_</code></a></td>
<td>Copies elements from <code>source</code> into <code>self</code> tensor at positions where the <code>mask</code> is True.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_scatter.html#torch.Tensor.masked_scatter"><code>Tensor.masked_scatter</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_"><code>torch.Tensor.masked_scatter_()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_"><code>Tensor.masked_fill_</code></a></td>
<td>Fills elements of <code>self</code> tensor with <code>value</code> where <code>mask</code> is True.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill.html#torch.Tensor.masked_fill"><code>Tensor.masked_fill</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_"><code>torch.Tensor.masked_fill_()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_select.html#torch.Tensor.masked_select"><code>Tensor.masked_select</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.masked_select.html#torch.masked_select"><code>torch.masked_select()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.matmul.html#torch.Tensor.matmul"><code>Tensor.matmul</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"><code>torch.matmul()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power"><code>Tensor.matrix_power</code></a></td>
<td>NOTE<a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power"><code>matrix_power()</code></a> is deprecated, use <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power"><code>torch.linalg.matrix_power()</code></a> instead.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.matrix_exp.html#torch.Tensor.matrix_exp"><code>Tensor.matrix_exp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.matrix_exp.html#torch.matrix_exp"><code>torch.matrix_exp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.max.html#torch.Tensor.max"><code>Tensor.max</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"><code>torch.max()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.maximum.html#torch.Tensor.maximum"><code>Tensor.maximum</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum"><code>torch.maximum()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mean.html#torch.Tensor.mean"><code>Tensor.mean</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"><code>torch.mean()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nanmean.html#torch.Tensor.nanmean"><code>Tensor.nanmean</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nanmean.html#torch.nanmean"><code>torch.nanmean()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.median.html#torch.Tensor.median"><code>Tensor.median</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"><code>torch.median()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nanmedian.html#torch.Tensor.nanmedian"><code>Tensor.nanmedian</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian"><code>torch.nanmedian()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.min.html#torch.Tensor.min"><code>Tensor.min</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"><code>torch.min()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.minimum.html#torch.Tensor.minimum"><code>Tensor.minimum</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.minimum.html#torch.minimum"><code>torch.minimum()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mm.html#torch.Tensor.mm"><code>Tensor.mm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm"><code>torch.mm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.smm.html#torch.Tensor.smm"><code>Tensor.smm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.smm.html#torch.smm"><code>torch.smm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mode.html#torch.Tensor.mode"><code>Tensor.mode</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"><code>torch.mode()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.movedim.html#torch.Tensor.movedim"><code>Tensor.movedim</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim"><code>torch.movedim()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.moveaxis.html#torch.Tensor.moveaxis"><code>Tensor.moveaxis</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.moveaxis.html#torch.moveaxis"><code>torch.moveaxis()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.msort.html#torch.Tensor.msort"><code>Tensor.msort</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.msort.html#torch.msort"><code>torch.msort()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mul.html#torch.Tensor.mul"><code>Tensor.mul</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"><code>torch.mul()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mul_.html#torch.Tensor.mul_"><code>Tensor.mul_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mul.html#torch.Tensor.mul"><code>mul()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.multiply.html#torch.Tensor.multiply"><code>Tensor.multiply</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.multiply.html#torch.multiply"><code>torch.multiply()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.multiply_.html#torch.Tensor.multiply_"><code>Tensor.multiply_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.multiply.html#torch.Tensor.multiply"><code>multiply()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.multinomial.html#torch.Tensor.multinomial"><code>Tensor.multinomial</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"><code>torch.multinomial()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mv.html#torch.Tensor.mv"><code>Tensor.mv</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv"><code>torch.mv()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma"><code>Tensor.mvlgamma</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.mvlgamma.html#torch.mvlgamma"><code>torch.mvlgamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mvlgamma_.html#torch.Tensor.mvlgamma_"><code>Tensor.mvlgamma_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma"><code>mvlgamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nansum.html#torch.Tensor.nansum"><code>Tensor.nansum</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum"><code>torch.nansum()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.narrow.html#torch.Tensor.narrow"><code>Tensor.narrow</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"><code>torch.narrow()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.narrow_copy.html#torch.Tensor.narrow_copy"><code>Tensor.narrow_copy</code></a></td>
<td>See <code>torch.narrow_copy()</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ndimension.html#torch.Tensor.ndimension"><code>Tensor.ndimension</code></a></td>
<td>Alias for <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim"><code>dim()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num"><code>Tensor.nan_to_num</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"><code>torch.nan_to_num()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nan_to_num_.html#torch.Tensor.nan_to_num_"><code>Tensor.nan_to_num_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num"><code>nan_to_num()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ne.html#torch.Tensor.ne"><code>Tensor.ne</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne"><code>torch.ne()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ne_.html#torch.Tensor.ne_"><code>Tensor.ne_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ne.html#torch.Tensor.ne"><code>ne()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal"><code>Tensor.not_equal</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.not_equal.html#torch.not_equal"><code>torch.not_equal()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.not_equal_.html#torch.Tensor.not_equal_"><code>Tensor.not_equal_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal"><code>not_equal()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.neg.html#torch.Tensor.neg"><code>Tensor.neg</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.neg.html#torch.neg"><code>torch.neg()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.neg_.html#torch.Tensor.neg_"><code>Tensor.neg_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.neg.html#torch.Tensor.neg"><code>neg()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.negative.html#torch.Tensor.negative"><code>Tensor.negative</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.negative.html#torch.negative"><code>torch.negative()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.negative_.html#torch.Tensor.negative_"><code>Tensor.negative_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.negative.html#torch.Tensor.negative"><code>negative()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nelement.html#torch.Tensor.nelement"><code>Tensor.nelement</code></a></td>
<td>Alias for <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.numel.html#torch.Tensor.numel"><code>numel()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter"><code>Tensor.nextafter</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nextafter.html#torch.nextafter"><code>torch.nextafter()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nextafter_.html#torch.Tensor.nextafter_"><code>Tensor.nextafter_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter"><code>nextafter()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nonzero.html#torch.Tensor.nonzero"><code>Tensor.nonzero</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nonzero.html#torch.nonzero"><code>torch.nonzero()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.norm.html#torch.Tensor.norm"><code>Tensor.norm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"><code>torch.norm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_"><code>Tensor.normal_</code></a></td>
<td>Fills <code>self</code> tensor with elements samples from the normal distribution parameterized by <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"><code>mean</code></a> and <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"><code>std</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.numel.html#torch.Tensor.numel"><code>Tensor.numel</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel"><code>torch.numel()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html#torch.Tensor.numpy"><code>Tensor.numpy</code></a></td>
<td>Returns the tensor as a NumPy <code>ndarray</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.orgqr.html#torch.Tensor.orgqr"><code>Tensor.orgqr</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.orgqr.html#torch.orgqr"><code>torch.orgqr()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ormqr.html#torch.Tensor.ormqr"><code>Tensor.ormqr</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"><code>torch.ormqr()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.outer.html#torch.Tensor.outer"><code>Tensor.outer</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.outer.html#torch.outer"><code>torch.outer()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.permute.html#torch.Tensor.permute"><code>Tensor.permute</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.permute.html#torch.permute"><code>torch.permute()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.pin_memory.html#torch.Tensor.pin_memory"><code>Tensor.pin_memory</code></a></td>
<td>Copies the tensor to pinned memory, if it’s not already pinned.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.pinverse.html#torch.Tensor.pinverse"><code>Tensor.pinverse</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.pinverse.html#torch.pinverse"><code>torch.pinverse()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma"><code>Tensor.polygamma</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma"><code>torch.polygamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.polygamma_.html#torch.Tensor.polygamma_"><code>Tensor.polygamma_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma"><code>polygamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.positive.html#torch.Tensor.positive"><code>Tensor.positive</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.positive.html#torch.positive"><code>torch.positive()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.pow.html#torch.Tensor.pow"><code>Tensor.pow</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow"><code>torch.pow()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.pow_.html#torch.Tensor.pow_"><code>Tensor.pow_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.pow.html#torch.Tensor.pow"><code>pow()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.prod.html#torch.Tensor.prod"><code>Tensor.prod</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"><code>torch.prod()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.put_.html#torch.Tensor.put_"><code>Tensor.put_</code></a></td>
<td>Copies the elements from <code>source</code> into the positions specified by <code>index</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.qr.html#torch.Tensor.qr"><code>Tensor.qr</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr"><code>torch.qr()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.qscheme.html#torch.Tensor.qscheme"><code>Tensor.qscheme</code></a></td>
<td>Returns the quantization scheme of a given QTensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.quantile.html#torch.Tensor.quantile"><code>Tensor.quantile</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"><code>torch.quantile()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nanquantile.html#torch.Tensor.nanquantile"><code>Tensor.nanquantile</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile"><code>torch.nanquantile()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_scale.html#torch.Tensor.q_scale"><code>Tensor.q_scale</code></a></td>
<td>Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_zero_point.html#torch.Tensor.q_zero_point"><code>Tensor.q_zero_point</code></a></td>
<td>Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_per_channel_scales.html#torch.Tensor.q_per_channel_scales"><code>Tensor.q_per_channel_scales</code></a></td>
<td>Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_per_channel_zero_points.html#torch.Tensor.q_per_channel_zero_points"><code>Tensor.q_per_channel_zero_points</code></a></td>
<td>Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_per_channel_axis.html#torch.Tensor.q_per_channel_axis"><code>Tensor.q_per_channel_axis</code></a></td>
<td>Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.rad2deg.html#torch.Tensor.rad2deg"><code>Tensor.rad2deg</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.rad2deg.html#torch.rad2deg"><code>torch.rad2deg()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.random_.html#torch.Tensor.random_"><code>Tensor.random_</code></a></td>
<td>Fills <code>self</code> tensor with numbers sampled from the discrete uniform distribution over <code>[from, to - 1]</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ravel.html#torch.Tensor.ravel"><code>Tensor.ravel</code></a></td>
<td>see <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.ravel.html#torch.ravel"><code>torch.ravel()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal"><code>Tensor.reciprocal</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.reciprocal.html#torch.reciprocal"><code>torch.reciprocal()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.reciprocal_.html#torch.Tensor.reciprocal_"><code>Tensor.reciprocal_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal"><code>reciprocal()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html#torch.Tensor.record_stream"><code>Tensor.record_stream</code></a></td>
<td>Ensures that the tensor memory is not reused for another tensor until all current work queued on <code>stream</code> are complete.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook"><code>Tensor.register_hook</code></a></td>
<td>Registers a backward hook.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.remainder.html#torch.Tensor.remainder"><code>Tensor.remainder</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"><code>torch.remainder()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.remainder_.html#torch.Tensor.remainder_"><code>Tensor.remainder_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.remainder.html#torch.Tensor.remainder"><code>remainder()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.renorm.html#torch.Tensor.renorm"><code>Tensor.renorm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.renorm.html#torch.renorm"><code>torch.renorm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.renorm_.html#torch.Tensor.renorm_"><code>Tensor.renorm_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.renorm.html#torch.Tensor.renorm"><code>renorm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.repeat.html#torch.Tensor.repeat"><code>Tensor.repeat</code></a></td>
<td>Repeats this tensor along the specified dimensions.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.repeat_interleave.html#torch.Tensor.repeat_interleave"><code>Tensor.repeat_interleave</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"><code>torch.repeat_interleave()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad"><code>Tensor.requires_grad</code></a></td>
<td>Is <code>True</code> if gradients need to be computed for this Tensor, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_"><code>Tensor.requires_grad_</code></a></td>
<td>Change if autograd should record operations on this tensor: sets this tensor’s <code>requires_grad</code> attribute in-place.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html#torch.Tensor.reshape"><code>Tensor.reshape</code></a></td>
<td>Returns a tensor with the same data and number of elements as <code>self</code> but with the specified shape.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.reshape_as.html#torch.Tensor.reshape_as"><code>Tensor.reshape_as</code></a></td>
<td>Returns this tensor as the same shape as <code>other</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.resize_.html#torch.Tensor.resize_"><code>Tensor.resize_</code></a></td>
<td>Resizes <code>self</code> tensor to the specified size.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.resize_as_.html#torch.Tensor.resize_as_"><code>Tensor.resize_as_</code></a></td>
<td>Resizes the <code>self</code> tensor to be the same size as the specified <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"><code>tensor</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.retain_grad.html#torch.Tensor.retain_grad"><code>Tensor.retain_grad</code></a></td>
<td>Enables this Tensor to have their <code>grad</code> populated during <code>backward()</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.retains_grad.html#torch.Tensor.retains_grad"><code>Tensor.retains_grad</code></a></td>
<td>Is <code>True</code> if this Tensor is non-leaf and its <code>grad</code> is enabled to be populated during <code>backward()</code>, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.roll.html#torch.Tensor.roll"><code>Tensor.roll</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"><code>torch.roll()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.rot90.html#torch.Tensor.rot90"><code>Tensor.rot90</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.rot90.html#torch.rot90"><code>torch.rot90()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round"><code>Tensor.round</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.round.html#torch.round"><code>torch.round()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.round_.html#torch.Tensor.round_"><code>Tensor.round_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round"><code>round()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt"><code>Tensor.rsqrt</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.rsqrt.html#torch.rsqrt"><code>torch.rsqrt()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.rsqrt_.html#torch.Tensor.rsqrt_"><code>Tensor.rsqrt_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt"><code>rsqrt()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter.html#torch.Tensor.scatter"><code>Tensor.scatter</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"><code>torch.Tensor.scatter_()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"><code>Tensor.scatter_</code></a></td>
<td>Writes all values from the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"><code>Tensor.scatter_add_</code></a></td>
<td>Adds all values from the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code> tensor in a similar fashion as <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"><code>scatter_()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add.html#torch.Tensor.scatter_add"><code>Tensor.scatter_add</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"><code>torch.Tensor.scatter_add_()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_"><code>Tensor.scatter_reduce_</code></a></td>
<td>Reduces all values from the <code>src</code> tensor to the indices specified in the <code>index</code> tensor in the <code>self</code> tensor using the applied reduction defined via the <code>reduce</code> argument (<code>&quot;sum&quot;</code>, <code>&quot;prod&quot;</code>, <code>&quot;mean&quot;</code>, <code>&quot;amax&quot;</code>, <code>&quot;amin&quot;</code>).</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce.html#torch.Tensor.scatter_reduce"><code>Tensor.scatter_reduce</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_"><code>torch.Tensor.scatter_reduce_()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.select.html#torch.Tensor.select"><code>Tensor.select</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.select.html#torch.select"><code>torch.select()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.select_scatter.html#torch.Tensor.select_scatter"><code>Tensor.select_scatter</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.select_scatter.html#torch.select_scatter"><code>torch.select_scatter()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.set_.html#torch.Tensor.set_"><code>Tensor.set_</code></a></td>
<td>Sets the underlying storage, size, and strides.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_"><code>Tensor.share_memory_</code></a></td>
<td>Moves the underlying storage to shared memory.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.short.html#torch.Tensor.short"><code>Tensor.short</code></a></td>
<td><code>self.short()</code> is equivalent to <code>self.to(torch.int16)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid"><code>Tensor.sigmoid</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid"><code>torch.sigmoid()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sigmoid_.html#torch.Tensor.sigmoid_"><code>Tensor.sigmoid_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid"><code>sigmoid()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sign.html#torch.Tensor.sign"><code>Tensor.sign</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sign.html#torch.sign"><code>torch.sign()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sign_.html#torch.Tensor.sign_"><code>Tensor.sign_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sign.html#torch.Tensor.sign"><code>sign()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.signbit.html#torch.Tensor.signbit"><code>Tensor.signbit</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.signbit.html#torch.signbit"><code>torch.signbit()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sgn.html#torch.Tensor.sgn"><code>Tensor.sgn</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sgn.html#torch.sgn"><code>torch.sgn()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sgn_.html#torch.Tensor.sgn_"><code>Tensor.sgn_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sgn.html#torch.Tensor.sgn"><code>sgn()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sin.html#torch.Tensor.sin"><code>Tensor.sin</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin"><code>torch.sin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sin_.html#torch.Tensor.sin_"><code>Tensor.sin_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sin.html#torch.Tensor.sin"><code>sin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinc.html#torch.Tensor.sinc"><code>Tensor.sinc</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sinc.html#torch.sinc"><code>torch.sinc()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinc_.html#torch.Tensor.sinc_"><code>Tensor.sinc_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinc.html#torch.Tensor.sinc"><code>sinc()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinh.html#torch.Tensor.sinh"><code>Tensor.sinh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"><code>torch.sinh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinh_.html#torch.Tensor.sinh_"><code>Tensor.sinh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinh.html#torch.Tensor.sinh"><code>sinh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.asinh.html#torch.Tensor.asinh"><code>Tensor.asinh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.asinh.html#torch.asinh"><code>torch.asinh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.asinh_.html#torch.Tensor.asinh_"><code>Tensor.asinh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.asinh.html#torch.Tensor.asinh"><code>asinh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh"><code>Tensor.arcsinh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.arcsinh.html#torch.arcsinh"><code>torch.arcsinh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsinh_.html#torch.Tensor.arcsinh_"><code>Tensor.arcsinh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh"><code>arcsinh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.size.html#torch.Tensor.size"><code>Tensor.size</code></a></td>
<td>Returns the size of the <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.slogdet.html#torch.Tensor.slogdet"><code>Tensor.slogdet</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.slogdet.html#torch.slogdet"><code>torch.slogdet()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.slice_scatter.html#torch.Tensor.slice_scatter"><code>Tensor.slice_scatter</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.slice_scatter.html#torch.slice_scatter"><code>torch.slice_scatter()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sort.html#torch.Tensor.sort"><code>Tensor.sort</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sort.html#torch.sort"><code>torch.sort()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.split.html#torch.Tensor.split"><code>Tensor.split</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"><code>torch.split()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask"><code>Tensor.sparse_mask</code></a></td>
<td>Returns a new <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/sparse.html#sparse-docs">sparse tensor</a> with values from a strided tensor <code>self</code> filtered by the indices of the sparse tensor <code>mask</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim"><code>Tensor.sparse_dim</code></a></td>
<td>Return the number of sparse dimensions in a <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/sparse.html#sparse-docs">sparse tensor</a> <code>self</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt"><code>Tensor.sqrt</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch.sqrt"><code>torch.sqrt()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sqrt_.html#torch.Tensor.sqrt_"><code>Tensor.sqrt_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt"><code>sqrt()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.square.html#torch.Tensor.square"><code>Tensor.square</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.square.html#torch.square"><code>torch.square()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.square_.html#torch.Tensor.square_"><code>Tensor.square_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.square.html#torch.Tensor.square"><code>square()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze"><code>Tensor.squeeze</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"><code>torch.squeeze()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze_.html#torch.Tensor.squeeze_"><code>Tensor.squeeze_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze"><code>squeeze()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.std.html#torch.Tensor.std"><code>Tensor.std</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"><code>torch.std()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.stft.html#torch.Tensor.stft"><code>Tensor.stft</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"><code>torch.stft()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.storage.html#torch.Tensor.storage"><code>Tensor.storage</code></a></td>
<td>Returns the underlying storage.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.storage_offset.html#torch.Tensor.storage_offset"><code>Tensor.storage_offset</code></a></td>
<td>Returns <code>self</code> tensor’s offset in the underlying storage in terms of number of storage elements (not bytes).</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.storage_type.html#torch.Tensor.storage_type"><code>Tensor.storage_type</code></a></td>
<td>Returns the type of the underlying storage.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.stride.html#torch.Tensor.stride"><code>Tensor.stride</code></a></td>
<td>Returns the stride of <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sub.html#torch.Tensor.sub"><code>Tensor.sub</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub"><code>torch.sub()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sub_.html#torch.Tensor.sub_"><code>Tensor.sub_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sub.html#torch.Tensor.sub"><code>sub()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.subtract.html#torch.Tensor.subtract"><code>Tensor.subtract</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.subtract.html#torch.subtract"><code>torch.subtract()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.subtract_.html#torch.Tensor.subtract_"><code>Tensor.subtract_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.subtract.html#torch.Tensor.subtract"><code>subtract()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sum.html#torch.Tensor.sum"><code>Tensor.sum</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"><code>torch.sum()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sum_to_size.html#torch.Tensor.sum_to_size"><code>Tensor.sum_to_size</code></a></td>
<td>Sum <code>this</code> tensor to <code>size</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.svd.html#torch.Tensor.svd"><code>Tensor.svd</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"><code>torch.svd()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.swapaxes.html#torch.Tensor.swapaxes"><code>Tensor.swapaxes</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.swapaxes.html#torch.swapaxes"><code>torch.swapaxes()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.swapdims.html#torch.Tensor.swapdims"><code>Tensor.swapdims</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.swapdims.html#torch.swapdims"><code>torch.swapdims()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.symeig.html#torch.Tensor.symeig"><code>Tensor.symeig</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"><code>torch.symeig()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.t.html#torch.Tensor.t"><code>Tensor.t</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.t.html#torch.t"><code>torch.t()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.t_.html#torch.Tensor.t_"><code>Tensor.t_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.t.html#torch.Tensor.t"><code>t()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tensor_split.html#torch.Tensor.tensor_split"><code>Tensor.tensor_split</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"><code>torch.tensor_split()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tile.html#torch.Tensor.tile"><code>Tensor.tile</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"><code>torch.tile()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to"><code>Tensor.to</code></a></td>
<td>Performs Tensor dtype and/or device conversion.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_mkldnn.html#torch.Tensor.to_mkldnn"><code>Tensor.to_mkldnn</code></a></td>
<td>Returns a copy of the tensor in <code>torch.mkldnn</code> layout.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.take.html#torch.Tensor.take"><code>Tensor.take</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.take.html#torch.take"><code>torch.take()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.take_along_dim.html#torch.Tensor.take_along_dim"><code>Tensor.take_along_dim</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.take_along_dim.html#torch.take_along_dim"><code>torch.take_along_dim()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tan.html#torch.Tensor.tan"><code>Tensor.tan</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tan.html#torch.tan"><code>torch.tan()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tan_.html#torch.Tensor.tan_"><code>Tensor.tan_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tan.html#torch.Tensor.tan"><code>tan()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tanh.html#torch.Tensor.tanh"><code>Tensor.tanh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tanh.html#torch.tanh"><code>torch.tanh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tanh_.html#torch.Tensor.tanh_"><code>Tensor.tanh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tanh.html#torch.Tensor.tanh"><code>tanh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atanh.html#torch.Tensor.atanh"><code>Tensor.atanh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.atanh.html#torch.atanh"><code>torch.atanh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atanh_.html#torch.Tensor.atanh_"><code>Tensor.atanh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atanh.html#torch.Tensor.atanh"><code>atanh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh"><code>Tensor.arctanh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.arctanh.html#torch.arctanh"><code>torch.arctanh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctanh_.html#torch.Tensor.arctanh_"><code>Tensor.arctanh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh"><code>arctanh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tolist.html#torch.Tensor.tolist"><code>Tensor.tolist</code></a></td>
<td>Returns the tensor as a (nested) list.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.topk.html#torch.Tensor.topk"><code>Tensor.topk</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"><code>torch.topk()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_dense.html#torch.Tensor.to_dense"><code>Tensor.to_dense</code></a></td>
<td>Creates a strided copy of <code>self</code> if <code>self</code> is not a strided tensor, otherwise returns <code>self</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse"><code>Tensor.to_sparse</code></a></td>
<td>Returns a sparse copy of the tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_csr.html#torch.Tensor.to_sparse_csr"><code>Tensor.to_sparse_csr</code></a></td>
<td>Convert a tensor to compressed row storage format (CSR).</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_csc.html#torch.Tensor.to_sparse_csc"><code>Tensor.to_sparse_csc</code></a></td>
<td>Convert a tensor to compressed column storage (CSC) format.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_bsr.html#torch.Tensor.to_sparse_bsr"><code>Tensor.to_sparse_bsr</code></a></td>
<td>Convert a CSR tensor to a block sparse row (BSR) storage format of given blocksize.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_bsc.html#torch.Tensor.to_sparse_bsc"><code>Tensor.to_sparse_bsc</code></a></td>
<td>Convert a CSR tensor to a block sparse column (BSC) storage format of given blocksize.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.trace.html#torch.Tensor.trace"><code>Tensor.trace</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.trace.html#torch.trace"><code>torch.trace()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.transpose.html#torch.Tensor.transpose"><code>Tensor.transpose</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose"><code>torch.transpose()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.transpose_.html#torch.Tensor.transpose_"><code>Tensor.transpose_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.transpose.html#torch.Tensor.transpose"><code>transpose()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.triangular_solve.html#torch.Tensor.triangular_solve"><code>Tensor.triangular_solve</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"><code>torch.triangular_solve()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tril.html#torch.Tensor.tril"><code>Tensor.tril</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tril.html#torch.tril"><code>torch.tril()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tril_.html#torch.Tensor.tril_"><code>Tensor.tril_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tril.html#torch.Tensor.tril"><code>tril()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.triu.html#torch.Tensor.triu"><code>Tensor.triu</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu"><code>torch.triu()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.triu_.html#torch.Tensor.triu_"><code>Tensor.triu_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.triu.html#torch.Tensor.triu"><code>triu()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.true_divide.html#torch.Tensor.true_divide"><code>Tensor.true_divide</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.true_divide.html#torch.true_divide"><code>torch.true_divide()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_"><code>Tensor.true_divide_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_"><code>true_divide_()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.trunc.html#torch.Tensor.trunc"><code>Tensor.trunc</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc"><code>torch.trunc()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.trunc_.html#torch.Tensor.trunc_"><code>Tensor.trunc_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.trunc.html#torch.Tensor.trunc"><code>trunc()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.type.html#torch.Tensor.type"><code>Tensor.type</code></a></td>
<td>Returns the type if dtype is not provided, else casts this object to the specified type.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.type_as.html#torch.Tensor.type_as"><code>Tensor.type_as</code></a></td>
<td>Returns this tensor cast to the type of the given tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unbind.html#torch.Tensor.unbind"><code>Tensor.unbind</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind"><code>torch.unbind()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten"><code>Tensor.unflatten</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.unflatten.html#torch.unflatten"><code>torch.unflatten()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unfold.html#torch.Tensor.unfold"><code>Tensor.unfold</code></a></td>
<td>Returns a view of the original tensor which contains all slices of size <code>size</code> from <code>self</code> tensor in the dimension <code>dimension</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_"><code>Tensor.uniform_</code></a></td>
<td>Fills <code>self</code> tensor with numbers sampled from the continuous uniform distribution:</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unique.html#torch.Tensor.unique"><code>Tensor.unique</code></a></td>
<td>Returns the unique elements of the input tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unique_consecutive.html#torch.Tensor.unique_consecutive"><code>Tensor.unique_consecutive</code></a></td>
<td>Eliminates all but the first element from every consecutive group of equivalent elements.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze"><code>Tensor.unsqueeze</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze"><code>torch.unsqueeze()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze_.html#torch.Tensor.unsqueeze_"><code>Tensor.unsqueeze_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze"><code>unsqueeze()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.values.html#torch.Tensor.values"><code>Tensor.values</code></a></td>
<td>Return the values tensor of a <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs">sparse COO tensor</a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.var.html#torch.Tensor.var"><code>Tensor.var</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.var.html#torch.var"><code>torch.var()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.vdot.html#torch.Tensor.vdot"><code>Tensor.vdot</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"><code>torch.vdot()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view"><code>Tensor.view</code></a></td>
<td>Returns a new tensor with the same data as the <code>self</code> tensor but of a different <code>shape</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.view_as.html#torch.Tensor.view_as"><code>Tensor.view_as</code></a></td>
<td>View this tensor as the same size as <code>other</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.vsplit.html#torch.Tensor.vsplit"><code>Tensor.vsplit</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"><code>torch.vsplit()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.where.html#torch.Tensor.where"><code>Tensor.where</code></a></td>
<td><code>self.where(condition, y)</code> is equivalent to <code>torch.where(condition, self, y)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy"><code>Tensor.xlogy</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy"><code>torch.xlogy()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.xlogy_.html#torch.Tensor.xlogy_"><code>Tensor.xlogy_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy"><code>xlogy()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.zero_.html#torch.Tensor.zero_"><code>Tensor.zero_</code></a></td>
<td>Fills <code>self</code> tensor with zeros.</td>
</tr>
</tbody>
</table>
</div>
<h2 id="storage"><a href="#storage" class="headerlink" title="storage"></a>storage</h2><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.jianshu.com/p/ebd7f6395bf4">tensor的数据结构、storage()、stride()、storage_offset()</a></p>
</blockquote>
<p>pytorch中一个tensor对象分为<strong>头信息区（Tensor）</strong>和<strong>存储区（Storage）</strong>两部分</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/pytorch学习_进阶知识/tensor结构示例图.webp" alt="tensor结构示例图"><br>头信息区主要保存tensor的形状（size）、步长（stride）、数据类型（type）等信息；而真正的data（数据)则以<strong>连续一维数组</strong>的形式放在存储区，由torch.Storage实例管理着</p>
<p><strong>注意：storage永远是一维数组，任何维度的tensor的实际数据都存储在一维的storage中</strong></p>
<blockquote>
<p>获取tensor的storage</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1.0</span>, <span class="number">4.0</span>],[<span class="number">2.0</span>, <span class="number">1.0</span>],[<span class="number">3.0</span>, <span class="number">5.0</span>]])</span><br><span class="line">a.storage()</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line"> <span class="number">1.0</span></span><br><span class="line"> <span class="number">4.0</span></span><br><span class="line"> <span class="number">2.0</span></span><br><span class="line"> <span class="number">1.0</span></span><br><span class="line"> <span class="number">3.0</span></span><br><span class="line"> <span class="number">5.0</span></span><br><span class="line">[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">a.storage()[<span class="number">2</span>] = <span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">id</span>(a.storage())</span><br><span class="line">Out[<span class="number">1</span>]: <span class="number">1343354913168</span></span><br></pre></td></tr></table></figure>
<h1 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h1><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/AccumulateMore/CV">小土堆+李沐课程笔记</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.bilibili.com/video/BV1hE411t7RN/?p=11&amp;vd_source=d741c08a55ba6a6a780b28e90920def0">PyTorch深度学习快速入门教程（绝对通俗易懂！）【小土堆】</a></p>
</blockquote>
<h2 id="Pytorch加载数据"><a href="#Pytorch加载数据" class="headerlink" title="Pytorch加载数据"></a>Pytorch加载数据</h2><p>Pytorch中加载数据需要Dataset、Dataloader。</p>
<ul>
<li>Dataset提供一种方式去获取每个数据及其对应的label，告诉我们总共有多少个数据。</li>
<li>Dataloader为后面的网络提供不同的数据形式，它将一批一批数据进行一个打包。</li>
</ul>
<h2 id="Tensorboard"><a href="#Tensorboard" class="headerlink" title="Tensorboard"></a>Tensorboard</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备的测试数据集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor())               </span><br><span class="line"><span class="comment"># batch_size=4 使得 img0, target0 = dataset[0]、img1, target1 = dataset[1]、img2, target2 = dataset[2]、img3, target3 = dataset[3]，然后这四个数据作为Dataloader的一个返回      </span></span><br><span class="line">test_loader = DataLoader(dataset=test_data,batch_size=<span class="number">64</span>,shuffle=<span class="literal">True</span>,num_workers=<span class="number">0</span>,drop_last=<span class="literal">False</span>)      </span><br><span class="line"><span class="comment"># 用for循环取出DataLoader打包好的四个数据</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">    imgs, targets = data <span class="comment"># 每个data都是由4张图片组成，imgs.size 为 [4,3,32,32]，四张32×32图片三通道，targets由四个标签组成             </span></span><br><span class="line">    writer.add_images(<span class="string">&quot;test_data&quot;</span>,imgs,step)</span><br><span class="line">    step = step + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h2 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h2><p>① Transforms当成工具箱的话，里面的class就是不同的工具。例如像totensor、resize这些工具。</p>
<p>② Transforms拿一些特定格式的图片，经过Transforms里面的工具，获得我们想要的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;Data/FirstTypeData/val/bees/10870992_eebeeb3a12.jpg&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)  </span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor()  <span class="comment"># 创建 transforms.ToTensor类 的实例化对象</span></span><br><span class="line">tensor_img = tensor_trans(img)  <span class="comment"># 调用 transforms.ToTensor类 的__call__的魔术方法   </span></span><br><span class="line"><span class="built_in">print</span>(tensor_img)</span><br></pre></td></tr></table></figure>
<h2 id="torchvision数据集"><a href="#torchvision数据集" class="headerlink" title="torchvision数据集"></a>torchvision数据集</h2><p>① torchvision中有很多数据集，当我们写代码时指定相应的数据集指定一些参数，它就可以自行下载。</p>
<p>② CIFAR-10数据集包含60000张32×32的彩色图片，一共10个类别，其中50000张训练图片，10000张测试图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,download=<span class="literal">True</span>) <span class="comment"># root为存放数据集的相对路线</span></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>) <span class="comment"># train=True是训练集，train=False是测试集  </span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(test_set[<span class="number">0</span>])       <span class="comment"># 输出的3是target </span></span><br><span class="line"><span class="built_in">print</span>(test_set.classes)  <span class="comment"># 测试数据集中有多少种</span></span><br><span class="line"></span><br><span class="line">img, target = test_set[<span class="number">0</span>] <span class="comment"># 分别获得图片、target</span></span><br><span class="line"><span class="built_in">print</span>(img)</span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(test_set.classes[target]) <span class="comment"># 3号target对应的种类</span></span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>① Loss损失函数一方面计算实际输出和目标之间的差距。</p>
<p>② Loss损失函数另一方面为我们更新输出提供一定的依据</p>
<blockquote>
<p>L1loss损失函数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> L1Loss</span><br><span class="line">inputs = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32)</span><br><span class="line">targets = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>],dtype=torch.float32)</span><br><span class="line">inputs = torch.reshape(inputs,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">targets = torch.reshape(targets,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">loss = L1Loss()  <span class="comment"># 默认为 maen</span></span><br><span class="line">result = loss(inputs,targets)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>MSE损失函数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> L1Loss</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">inputs = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32)</span><br><span class="line">targets = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>],dtype=torch.float32)</span><br><span class="line">inputs = torch.reshape(inputs,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">targets = torch.reshape(targets,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">loss_mse = nn.MSELoss()</span><br><span class="line">result_mse = loss_mse(inputs,targets)</span><br><span class="line"><span class="built_in">print</span>(result_mse)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>交叉熵损失函数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> L1Loss</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>])</span><br><span class="line">y = torch.tensor([<span class="number">1</span>])</span><br><span class="line">x = torch.reshape(x,(<span class="number">1</span>,<span class="number">3</span>)) <span class="comment"># 1的 batch_size，有三类</span></span><br><span class="line">loss_cross = nn.CrossEntropyLoss()</span><br><span class="line">result_cross = loss_cross(x,y)</span><br><span class="line"><span class="built_in">print</span>(result_cross)</span><br></pre></td></tr></table></figure>
<h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>① 损失函数调用backward方法，就可以调用损失函数的反向传播方法，就可以求出我们需要调节的梯度，我们就可以利用我们的优化器就可以根据梯度对参数进行调整，达到整体误差降低的目的。</p>
<p>② 梯度要清零，如果梯度不清零会导致梯度累加</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss() <span class="comment"># 交叉熵    </span></span><br><span class="line">tudui = Tudui()</span><br><span class="line">optim = torch.optim.SGD(tudui.parameters(),lr=<span class="number">0.01</span>)   <span class="comment"># 随机梯度下降优化器</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    outputs = tudui(imgs)</span><br><span class="line">    result_loss = loss(outputs, targets) <span class="comment"># 计算实际输出与目标输出的差距</span></span><br><span class="line">    optim.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">    result_loss.backward() <span class="comment"># 反向传播，计算损失函数的梯度</span></span><br><span class="line">    optim.step()   <span class="comment"># 根据梯度，对网络的参数进行调优</span></span><br><span class="line">    <span class="built_in">print</span>(result_loss) <span class="comment"># 对数据只看了一遍，只看了一轮，所以loss下降不大</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>神经网络学习率优化</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>,drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui, self).__init__()        </span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>,<span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">loss = nn.CrossEntropyLoss() <span class="comment"># 交叉熵    </span></span><br><span class="line">tudui = Tudui()</span><br><span class="line">optim = torch.optim.SGD(tudui.parameters(),lr=<span class="number">0.01</span>)   <span class="comment"># 随机梯度下降优化器</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=<span class="number">5</span>, gamma=<span class="number">0.1</span>) <span class="comment"># 每过 step_size 更新一次优化器，更新是学习率为原来的学习率的 0.1 倍    </span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = tudui(imgs)</span><br><span class="line">        result_loss = loss(outputs, targets) <span class="comment"># 计算实际输出与目标输出的差距</span></span><br><span class="line">        optim.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">        result_loss.backward() <span class="comment"># 反向传播，计算损失函数的梯度</span></span><br><span class="line">        optim.step()   <span class="comment"># 根据梯度，对网络的参数进行调优</span></span><br><span class="line">        scheduler.step() <span class="comment"># 学习率太小了，所以20个轮次后，相当于没走多少</span></span><br><span class="line">        running_loss = running_loss + result_loss</span><br><span class="line">    <span class="built_in">print</span>(running_loss) <span class="comment"># 对这一轮所有误差的总和</span></span><br></pre></td></tr></table></figure>
<h2 id="网络模型使用及修改"><a href="#网络模型使用及修改" class="headerlink" title="网络模型使用及修改"></a>网络模型使用及修改</h2><blockquote>
<p>网络模型添加</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line">vgg16_true = torchvision.models.vgg16(pretrained=<span class="literal">True</span>) <span class="comment"># 下载卷积层对应的参数是多少、池化层对应的参数时多少，这些参数时ImageNet训练好了的</span></span><br><span class="line">vgg16_true.add_module(<span class="string">&#x27;add_linear&#x27;</span>,nn.Linear(<span class="number">1000</span>,<span class="number">10</span>)) <span class="comment"># 在VGG16后面添加一个线性层，使得输出为适应CIFAR10的输出，CIFAR10需要输出10个种类</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vgg16_true)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>网络模型修改</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=<span class="literal">False</span>) <span class="comment"># 没有预训练的参数     </span></span><br><span class="line"><span class="built_in">print</span>(vgg16_false)</span><br><span class="line">vgg16_false.classifier[<span class="number">6</span>] = nn.Linear(<span class="number">4096</span>,<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(vgg16_false)</span><br></pre></td></tr></table></figure>
<h2 id="网络模型保存与读取"><a href="#网络模型保存与读取" class="headerlink" title="网络模型保存与读取"></a>网络模型保存与读取</h2><blockquote>
<p>模型结构 + 模型参数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">torch.save(vgg16,<span class="string">&quot;./model/vgg16_method1.pth&quot;</span>) <span class="comment"># 保存方式一：模型结构 + 模型参数      </span></span><br><span class="line"><span class="built_in">print</span>(vgg16)</span><br><span class="line"></span><br><span class="line">model = torch.load(<span class="string">&quot;./model/vgg16_method1.pth&quot;</span>) <span class="comment"># 保存方式一对应的加载模型    </span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>模型参数（官方推荐），不保存网络模型结构</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">torch.save(vgg16.state_dict(),<span class="string">&quot;./model/vgg16_method2.pth&quot;</span>) <span class="comment"># 保存方式二：模型参数（官方推荐）,不再保存网络模型结构  </span></span><br><span class="line"><span class="built_in">print</span>(vgg16)</span><br><span class="line"></span><br><span class="line">model = torch.load(<span class="string">&quot;./model/vgg16_method2.pth&quot;</span>) <span class="comment"># 导入模型参数   </span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<h2 id="固定模型参数"><a href="#固定模型参数" class="headerlink" title="固定模型参数"></a>固定模型参数</h2><p>在训练过程中可能需要固定一部分模型的参数，只更新另一部分参数，有两种思路实现这个目标</p>
<ol>
<li>一个是设置不要更新参数的<a target="_blank" rel="noopener external nofollow noreferrer" href="https://so.csdn.net/so/search?q=网络层&amp;spm=1001.2101.3001.7020">网络层</a>为false</li>
<li>另一个就是在定义优化器时只传入要更新的参数</li>
</ol>
<p>当然最优的做法是，优化器中只传入requires_grad=True的参数，这样占用的内存会更小一点，效率也会更高</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_class=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(net, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">4</span>, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fc2(self.fc1(x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 冻结fc1层的参数</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;fc1&quot;</span> <span class="keyword">in</span> name:</span><br><span class="line">        param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只传入requires_grad = True的参数</span></span><br><span class="line">optimizer = optim.SGD(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, net.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;model.fc1.weight&quot;</span>, model.fc1.weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;model.fc2.weight&quot;</span>, model.fc2.weight)</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    x = torch.randn((<span class="number">3</span>, <span class="number">8</span>))</span><br><span class="line">    label = torch.randint(<span class="number">0</span>, <span class="number">3</span>, [<span class="number">3</span>]).long()</span><br><span class="line">    output = model(x)</span><br><span class="line"></span><br><span class="line">    loss = loss_fn(output, label)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;model.fc1.weight&quot;</span>, model.fc1.weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;model.fc2.weight&quot;</span>, model.fc2.weight)</span><br></pre></td></tr></table></figure>
<h2 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h2><blockquote>
<p>DataLoader加载数据集</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line"></span><br><span class="line"><span class="comment"># length 长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># 如果train_data_size=10，则打印：训练数据集的长度为：10</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试数据集的长度：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用 Dataloader 来加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data_size, batch_size=<span class="number">64</span>)        </span><br><span class="line">test_dataloader = DataLoader(test_data_size, batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>测试网络正确</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搭建神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui, self).__init__()        </span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  <span class="comment"># 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),  <span class="comment"># 展平后变成 64*4*4 了</span></span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    tudui = Tudui()</span><br><span class="line">    <span class="built_in">input</span> = torch.ones((<span class="number">64</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">    output = tudui(<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)  <span class="comment"># 测试输出的尺寸是不是我们想要的</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>网络训练数据</p>
</blockquote>
<p>① model.train()和model.eval()的区别主要在于Batch Normalization和Dropout两层。</p>
<p>② 如果模型中有BN层(Batch Normalization）和 Dropout，需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。</p>
<p>③ 不启用 Batch Normalization 和 Dropout。 如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。</p>
<p>④ 训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的性质。</p>
<p>⑤ 在做one classification的时候，训练集和测试集的样本分布是不一样的，尤其需要注意这一点</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui, self).__init__()        </span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  <span class="comment"># 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),  <span class="comment"># 展平后变成 64*4*4 了</span></span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line"></span><br><span class="line"><span class="comment"># length 长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># 如果train_data_size=10，则打印：训练数据集的长度为：10</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试数据集的长度：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用 Dataloader 来加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=<span class="number">64</span>)        </span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line">tudui = Tudui() </span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss() <span class="comment"># 交叉熵，fn 是 fuction 的缩写</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">learning = <span class="number">0.01</span>  <span class="comment"># 1e-2 就是 0.01 的意思</span></span><br><span class="line">optimizer = torch.optim.SGD(tudui.parameters(),learning)   <span class="comment"># 随机梯度下降优化器  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置网络的一些参数</span></span><br><span class="line"><span class="comment"># 记录训练的次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 记录测试的次数</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练的轮次</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加 tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-----第 &#123;&#125; 轮训练开始-----&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练步骤开始</span></span><br><span class="line">    tudui.train() <span class="comment"># 当网络中有dropout层、batchnorm层时，这些层能起作用</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = tudui(imgs)</span><br><span class="line">        loss = loss_fn(outputs, targets) <span class="comment"># 计算实际输出与目标输出的差距</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 优化器对模型调优</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">        loss.backward() <span class="comment"># 反向传播，计算损失函数的梯度</span></span><br><span class="line">        optimizer.step()   <span class="comment"># 根据梯度，对网络的参数进行调优</span></span><br><span class="line">        </span><br><span class="line">        total_train_step = total_train_step + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数：&#123;&#125;，Loss：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step,loss.item()))  <span class="comment"># 方式二：获得loss值</span></span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>,loss.item(),total_train_step)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）</span></span><br><span class="line">    tudui.<span class="built_in">eval</span>()  <span class="comment"># 当网络中有dropout层、batchnorm层时，这些层不能起作用</span></span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 没有梯度了</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader: <span class="comment"># 测试数据集提取数据</span></span><br><span class="line">            imgs, targets = data</span><br><span class="line">            outputs = tudui(imgs)</span><br><span class="line">            loss = loss_fn(outputs, targets) <span class="comment"># 仅data数据在网络模型上的损失</span></span><br><span class="line">            total_test_loss = total_test_loss + loss.item() <span class="comment"># 所有loss</span></span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy = total_accuracy + accuracy</span><br><span class="line">            </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>,total_test_loss,total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>,total_accuracy/test_data_size,total_test_step)  </span><br><span class="line">    total_test_step = total_test_step + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    torch.save(tudui, <span class="string">&quot;./model/tudui_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i)) <span class="comment"># 保存每一轮训练后的结果</span></span><br><span class="line">    <span class="comment">#torch.save(tudui.state_dict(),&quot;tudui_&#123;&#125;.path&quot;.format(i)) # 保存方式二         </span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型已保存&quot;</span>)</span><br><span class="line">    </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/narutohyc">narutohyc</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://study.hycbook.com/article/53040.html">https://study.hycbook.com/article/53040.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://study.hycbook.com" target="_blank">兼一书虫</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post_share"><div class="social-share" data-image="https://pic.hycbook.com/i/hexo/post_cover/蕾姆5.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><link rel="stylesheet" href="/" media="defer" onload="this.media='all'"/><div class="post-reward"><button class="tip-button reward-button"><span class="tip-button__text">打赏</span><div class="coin-wrapper"><div class="coin"><div class="coin__middle"></div><div class="coin__back"></div><div class="coin__front"></div></div></div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic.hycbook.com/i//hexo/qr_codes/hyc_wechat.webp" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/qr_codes/hyc_wechat.webp" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://pic.hycbook.com/i//hexo/qr_codes/hyc_alipay.webp" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/qr_codes/hyc_alipay.webp" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></button></div><audio id="coinAudio" src="https://s1.vika.cn/space/2022/10/29/6db0ad2bccf949f09054b3b206dcc66f?attname=马里奥游戏投币叮当.mp3"></audio><script defer="defer" src="/"></script><nav class="pagination-post" id="pagination"><div class="prev-post pull-full"><a href="/article/57252.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆4.webp" onerror="onerror=null;src='https://pic.hycbook.com/i/hexo/config_imgs/404.svg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">LLM Tokenizer分词系列</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/article/53039.html" title="pytorch学习_基础知识"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆2.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-04-16</div><div class="title">pytorch学习_基础知识</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Twikoo</span><span class="switch-btn"></span><span class="second-comment">Valine</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Tensor"><span class="toc-number">1.</span> <span class="toc-text">Tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#storage"><span class="toc-number">1.1.</span> <span class="toc-text">storage</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B"><span class="toc-number">2.</span> <span class="toc-text">实例</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Pytorch%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">2.1.</span> <span class="toc-text">Pytorch加载数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensorboard"><span class="toc-number">2.2.</span> <span class="toc-text">Tensorboard</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transforms"><span class="toc-number">2.3.</span> <span class="toc-text">Transforms</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torchvision%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">2.4.</span> <span class="toc-text">torchvision数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">2.5.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">2.6.</span> <span class="toc-text">优化器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E5%8F%8A%E4%BF%AE%E6%94%B9"><span class="toc-number">2.7.</span> <span class="toc-text">网络模型使用及修改</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96"><span class="toc-number">2.8.</span> <span class="toc-text">网络模型保存与读取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BA%E5%AE%9A%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">2.9.</span> <span class="toc-text">固定模型参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="toc-number">2.10.</span> <span class="toc-text">训练流程</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://pic.hycbook.com/i/hexo/config_imgs/footer_bg.webp')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By narutohyc</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><p><a target="_blank" href="https://hexo.io/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为Hexo"></a>&nbsp;<a target="_blank" href="https://demo.jerryc.me/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用butterfly"></a>&nbsp;<a target="_blank" href="https://vercel.com/ " rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vervel-brightgreen?style=flat&logo=Vercel" title="本站采用双线部署，默认线路托管于Vercel"></a>&nbsp;<a target="_blank" href="https://github.com/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由Gtihub托管"></a>&nbsp;<a target="_blank" href="https://zixiaoyun.com" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/图床-薄荷图床-green" title="薄荷图床"></a></p><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=35020502000647" rel="external nofollow noreferrer"><img style="position:relative;top:4px" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/config_imgs//备案图标.webp" alt="ICP"/>闽公网安备35020502000647号  </a><a href="https://beian.miit.gov.cn/" rel="external nofollow noreferrer" target="_blank">闽ICP备2023021562号-1</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><!-- button#darkmode(type="button" title=_p('rightside.night_mode_title'))--><!--  i.fas.fa-adjust--><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="fa-solid fa-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="fa-solid fa-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="fa-solid fa-arrow-rotate-right"></i></div><div class="rightMenu-item" id="menu-home"><i class="fa-solid fa-house"></i></div></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:window.open(&quot;https://www.baidu.com/s?wd=&quot;+window.getSelection().toString());window.location.reload();" rel="external nofollow noreferrer"><i class="fas fa-comment"></i><span>百度搜索</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" href="/archives/"><i class="fa-solid fa-archive"></i><span>文章归档</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="fa-solid fa-folder-open"></i><span>文章分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="fa-solid fa-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuNormal"><a class="rightMenu-item menu-link" id="menu-radompage"><i class="fa-solid fa-shoe-prints"></i><span>随便逛逛</span></a><div class="rightMenu-item" id="menu-translate"><i class="fa-solid fa-earth-asia"></i><span>繁简切换</span></div><div class="rightMenu-item" id="menu-darkmode"><i class="fa-solid fa-moon"></i><span>切换模式</span></div></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? '' : ''

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://vercel.hycbook.com',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://vercel.hycbook.com',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'ncn88uooQf0IO2rrGE7Vniwp-gzGzoHsz',
      appKey: 'Yghpzg1QfBMFJ0MxxHubVzKL',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Twikoo' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'https://vercel.hycbook.com',
        region: '',
        pageSize: 3,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script defer src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script defer data-pjax src="/zhheo/random.js"></script><script data-pjax src="/js/coin.js"></script><script defer src="https://npm.elemecdn.com/vue@2.6.11"></script><script async src="//at.alicdn.com/t/c/font_3670467_a0sijt8frxo.js"></script><script defer src="/live2d-widget/autoload.js"></script><script defer src="https://cdnjs.cloudflare.com/ajax/libs/toastr.js/latest/toastr.min.js"></script><script data-pjax defer src="https://npm.elemecdn.com/tzy-blog/lib/js/theme/chocolate.js"></script><script defer data-pjax src="/js/rightMenu.js"></script><script defer data-pjax src="/js/udf_mouse.js"></script><script defer data-pjax src="/js/udf_js.js"></script><script defer src="/js/udf_js.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><script>window.$crisp = [];
window.CRISP_WEBSITE_ID = "561b80db-3f0f-45cb-b3b1-aae7355939e6";
(function () {
  d = document;
  s = d.createElement("script");
  s.src = "https://client.crisp.chat/l.js";
  s.async = 1;
  d.getElementsByTagName("head")[0].appendChild(s);
})();
$crisp.push(["safe", true])

if (false) {
  $crisp.push(["do", "chat:hide"])
  $crisp.push(["on", "chat:closed", function() {
    $crisp.push(["do", "chat:hide"])
  }])
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      $crisp.push(["do", "chat:show"])
      $crisp.push(["do", "chat:open"])

    });
  }
  chatBtnFn()
} else {
  if (false) {
    function chatBtnHide () {
      $crisp.push(["do", "chat:hide"])
    }
    function chatBtnShow () {
      $crisp.push(["do", "chat:show"])
    }
  }
}</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="app-refresh" id="app-refresh" style="position: fixed;top: -2.2rem;left: 0;right: 0;z-index: 99999;padding: 0 1rem;font-size: 15px;height: 2.2rem;transition: all 0.3s ease;"><div class="app-refresh-wrap" style=" display: flex;color: #fff;height: 100%;align-items: center;justify-content: center;"><label>✨ 兼一书虫上新啦！ 👉</label><a href="javascript:void(0)" rel="external nofollow noreferrer" onclick="location.reload()"><span style="color: #fff;text-decoration: underline;cursor: pointer;">🍭查看新品🍬</span></a></div></div><script>if ('serviceWorker' in navigator) {
  if (navigator.serviceWorker.controller) {
    navigator.serviceWorker.addEventListener('controllerchange', function() {
      showNotification()
    })
  }
  window.addEventListener('load', function() {
    navigator.serviceWorker.register('/sw.js')
  })
}

function showNotification() {
  if (GLOBAL_CONFIG.Snackbar) {
    var snackbarBg =
      document.documentElement.getAttribute('data-theme') === 'light' ?
      GLOBAL_CONFIG.Snackbar.bgLight :
      GLOBAL_CONFIG.Snackbar.bgDark
    var snackbarPos = GLOBAL_CONFIG.Snackbar.position
    Snackbar.show({
      text: '✨ 兼一书虫上新啦！ 👉',
      backgroundColor: snackbarBg,
      duration: 500000,
      pos: snackbarPos,
      actionText: '🍭查看新品🍬',
      actionTextColor: '#fff',
      onActionClick: function(e) {
        location.reload()
      },
    })
  } else {
    var showBg =
      document.documentElement.getAttribute('data-theme') === 'light' ?
      '#49b1f5' :
      '#1f1f1f'
    var cssText = `top: 0; background: ${showBg};`
    document.getElementById('app-refresh')
      .style.cssText = cssText
  }
}</script></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>