<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>机器学习_线性代数与矩阵论(3) | 兼一书虫</title><meta name="keywords" content="机器学习数学,线性代数,矩阵论"><meta name="author" content="narutohyc"><meta name="copyright" content="narutohyc"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="机器学习的数学基础入门知识">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习_线性代数与矩阵论(3)">
<meta property="og:url" content="https://study.hycbook.com/article/29825.html">
<meta property="og:site_name" content="兼一书虫">
<meta property="og:description" content="机器学习的数学基础入门知识">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic.hycbook.com/i/hexo/post_cover/%E8%95%BE%E5%A7%866.webp">
<meta property="article:published_time" content="2023-09-08T01:47:22.000Z">
<meta property="article:modified_time" content="2023-09-08T03:54:55.037Z">
<meta property="article:author" content="narutohyc">
<meta property="article:tag" content="机器学习数学">
<meta property="article:tag" content="线性代数">
<meta property="article:tag" content="矩阵论">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic.hycbook.com/i/hexo/post_cover/%E8%95%BE%E5%A7%866.webp"><link rel="shortcut icon" href="https://pic.hycbook.com/i//hexo/config_imgs/网站图标.webp"><link rel="canonical" href="https://study.hycbook.com/article/29825"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="manifest" href="/manifest.json"/><meta name="msapplication-TileColor" content="#c6ff7a"/><link rel="apple-touch-icon" sizes="180x180" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/favicon-16x16.png"/><link rel="mask-icon" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/safari-pinned-tab.svg" color="#5bbad5"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?68340394dfd808cea9826e8a57f87aa6";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":120,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":200,"languages":{"author":"作者: narutohyc","link":"链接: ","source":"来源: 兼一书虫","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '机器学习_线性代数与矩阵论(3)',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-09-08 11:54:55'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/hyc_udf.css"><link rel="stylesheet" href="/css/udf_css.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/mainColor/heoMainColor.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/404/404.css"><script src="https://npm.elemecdn.com/echarts@4.9.0/dist/echarts.min.js"></script><link href="https://cdn.bootcdn.net/ajax/libs/toastr.js/2.1.4/toastr.min.css" rel="stylesheet"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/person_img/兼一头像.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">120</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">169</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fangwu"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 归档</span></a></li><li><a class="site-page child" href="/categories"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> 分类</span></a></li><li><a class="site-page child" href="/tags"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> gitbook版</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://common.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> common</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://python.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> python</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://dl.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 深度学习</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xuegao">             </use></svg><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yinle">                   </use></svg><span> 音乐</span></a></li><li><a class="site-page child" href="/bangumis"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wodezhuifan">                   </use></svg><span> 追番</span></a></li><li><a class="site-page child" href="/gallery"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xiangce">                   </use></svg><span> 相册</span></a></li><li><a class="site-page child" href="/video"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-shipin">                   </use></svg><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/charts"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xigua"></use></svg><span> 统计图</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-suannai">             </use></svg><span> 网盘</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://pan.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 私月盘</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://share.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> 共享盘</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifeiji">             </use></svg><span> 导航</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/comments"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-TIFFANYSROOM_huaban">                   </use></svg><span> 留言板</span></a></li><li><a class="site-page child" href="/link"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-changyonglianjie">                   </use></svg><span> 友链</span></a></li><li><a class="site-page child" href="/about"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-aixin">                   </use></svg><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic.hycbook.com/i/hexo/post_imgs/蕾姆6.webp')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">兼一书虫</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fangwu"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 归档</span></a></li><li><a class="site-page child" href="/categories"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> 分类</span></a></li><li><a class="site-page child" href="/tags"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> gitbook版</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://common.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> common</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://python.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> python</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://dl.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 深度学习</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xuegao">             </use></svg><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yinle">                   </use></svg><span> 音乐</span></a></li><li><a class="site-page child" href="/bangumis"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wodezhuifan">                   </use></svg><span> 追番</span></a></li><li><a class="site-page child" href="/gallery"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xiangce">                   </use></svg><span> 相册</span></a></li><li><a class="site-page child" href="/video"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-shipin">                   </use></svg><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/charts"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xigua"></use></svg><span> 统计图</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-suannai">             </use></svg><span> 网盘</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://pan.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 私月盘</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://share.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> 共享盘</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifeiji">             </use></svg><span> 导航</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/comments"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-TIFFANYSROOM_huaban">                   </use></svg><span> 留言板</span></a></li><li><a class="site-page child" href="/link"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-changyonglianjie">                   </use></svg><span> 友链</span></a></li><li><a class="site-page child" href="/about"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-aixin">                   </use></svg><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">机器学习_线性代数与矩阵论(3)</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-09-08T01:47:22.000Z" title="发表于 2023-09-08 09:47:22">2023-09-08</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-09-08T03:54:55.037Z" title="更新于 2023-09-08 11:54:55">2023-09-08</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/math/">math</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">1.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>4分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title="机器学习_线性代数与矩阵论(3)"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="twikoo_visitors"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/article/29825.html#post-comment"><span id="twikoo-count"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><hr>
<h1 id="二次型"><a href="#二次型" class="headerlink" title="二次型"></a>二次型</h1><p>二次型是一种特殊的二次函数，只含有二次项，它在线性代数与多元函数微积分中被广泛使用</p>
<p>在机器学习中二次型经常作为目标函数出现</p>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><p><code>二次型</code>(Quadric Form) 是由纯二次项构成的函数，即二次齐次多项式，如下面的函数</p>
<script type="math/tex; mode=display">
2 x^{2}-3 x y+y^{2}+z^{2}</script><p>二次型可以写成矩阵形式</p>
<script type="math/tex; mode=display">
x^{\mathrm{T}} \boldsymbol{A x}</script><p>其中$\boldsymbol{A}$是$n$阶对称矩阵，$\boldsymbol{x}$是一个列向量，上面的二次型展开之后为</p>
<script type="math/tex; mode=display">
\sum_{i=1}^{n} \sum_{j=1}^{n} a_{i j} x_{i} x_{j}</script><p>这里要求<script type="math/tex">a_{i j}=a_{j i}</script>，需要注意的是，一般的二次函数不一定是二次型，它可能有一次项和常数项</p>
<p>上式的二次型对应的矩阵为</p>
<script type="math/tex; mode=display">
\left(\begin{array}{ccc}
2 & -1.5 & 0 \\
-1.5 & 1 & 0 \\
0 & 0 & 1
\end{array}\right)</script><p>平方项<script type="math/tex">a x_{i}^{2}</script>的系数是矩阵的主对角线元素，交叉乘积项<script type="math/tex">a x_{i} x_{j}</script>的系数由<script type="math/tex">a_{i j}</script>与<script type="math/tex">a_{j i}</script>均分，实对称矩阵与二次型一一对应</p>
<h2 id="正定二次型与正定矩阵"><a href="#正定二次型与正定矩阵" class="headerlink" title="正定二次型与正定矩阵"></a>正定二次型与正定矩阵</h2><p>在某些数学证明或计算中，会将二次函数配方成完全平方的形式以得到想要的结果，如下面的例子</p>
<script type="math/tex; mode=display">
\left(x_{1}-2\right)^{2}+\left(x_{2}+5\right)^{2}+\left(x_{3}-7\right)^{2}</script><p>平方项是非负的，$(2,-5,7)$是该函数的极小值，由此引入二次型和矩阵正定的概念，如果一个二次型对于任意非$\boldsymbol{0}$向量$\boldsymbol{x}$都有</p>
<script type="math/tex; mode=display">
\boldsymbol{x}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{x}>0</script><p>则称该二次型为<code>正定</code>(Positive Definite)二次型，矩阵$\boldsymbol{A}$为正定矩阵，如果对于任意非$\boldsymbol{0}$向量$\boldsymbol{x}$都有</p>
<script type="math/tex; mode=display">
\boldsymbol{x}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{x} \geqslant 0</script><p>则该二次型为<code>半正定</code>(Positive Semi-definite)二次型，矩阵$\boldsymbol{A}$为半正定矩阵，如果对于任意非0向量$\boldsymbol{x}$都在</p>
<script type="math/tex; mode=display">
\boldsymbol{x}^{\mathrm{T}} \boldsymbol{A x}<0</script><p>则该二次型为<code>负定</code>(Negative Definite)二次型，矩阵$A$为负定矩阵，类似地可以定义半负定的概念</p>
<p>如果既不正定也不负定，则称为<code>不定</code></p>
<p>下面的二次型为正定二次型</p>
<script type="math/tex; mode=display">
f\left(x_{1},x_{2},x_{3}\right)=x_{1}^{2}+2 x_{2}^{2}+x_{3}^{2}</script><p>其对应的矩阵为正定矩阵</p>
<script type="math/tex; mode=display">
\left(\begin{array}{lll}
1 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 1
\end{array}\right)</script><p>下面的二次型为半正定二次型</p>
<script type="math/tex; mode=display">
f\left(x_{1},x_{2},x_{3}\right)=x_{1}^{2}+2 x_{2}^{2}</script><p>其对应的矩阵为半正定矩阵</p>
<script type="math/tex; mode=display">
\left(\begin{array}{lll}
1 & 0 & 0 \\
0 & 2 & 0 \\
0 & 0 & 0
\end{array}\right)</script><p>如果令<script type="math/tex">x_{1}=0，x_{2}=0，x_{3}=1</script>，二次型的值为0</p>
<p>下面的二次型是负定二次型</p>
<script type="math/tex; mode=display">
f\left(x_{1},x_{2},x_{3}\right)=-x_{1}^{2}-2 x_{2}^{2}-x_{3}^{2}</script><p>其对应的矩阵为负定矩阵</p>
<script type="math/tex; mode=display">
\left(\begin{array}{ccc}
-1 & 0 & 0 \\
0 & -2 & 0 \\
0 & 0 & -1
\end{array}\right)</script><p><strong>正定二次型被用于多元函数极值的判定法则</strong></p>
<p>正定矩阵的所有主对角线元素$a_{i i}&gt;0，i=1，\cdots，n$</p>
<p>根据正定的定义，由于对于任意非0向量$\boldsymbol{x}$都有$\boldsymbol{x}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{x}&gt;0$，因此可以构造一个第$i$个分量为1，其他分量均为0的向量$\boldsymbol{x}$</p>
<script type="math/tex; mode=display">
\left(\begin{array}{lllll}
0 & \cdots & 1 & \cdots & 0
\end{array}\right)^{\mathrm{T}}</script><p>则有</p>
<script type="math/tex; mode=display">
\boldsymbol{x}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{x}=a_{i i}>0</script><p>因此结论成立</p>
<p>证明一个对称矩阵$\boldsymbol{A}$正定可以按照定义进行，除此之外，还可以采用下面的方法</p>
<ol>
<li>矩阵<script type="math/tex">\boldsymbol{A}</script>的<script type="math/tex">n</script>个特征值<script type="math/tex">\lambda_{1}，\cdots，\lambda_{n}</script>均大于0</li>
<li>存在可逆矩阵$\boldsymbol{P}$使得$\boldsymbol{A}=\boldsymbol{P}^{\mathrm{T}} \boldsymbol{P}$</li>
<li>如果$\boldsymbol{A}$是正定矩阵，则$\boldsymbol{A}^{\mathrm{T}}$也是正定矩阵</li>
<li>矩阵$\boldsymbol{A}$的所有顺序主子式均为正</li>
</ol>
<p>第一条判定规则可以通过正交变换将二次型化为标准型证明，化为标准型(对应于对角矩阵)之后为正定二次型</p>
<p>下面证明第2条判定规则，对于任意曲$\boldsymbol{\theta}$向量$\boldsymbol{x}$在</p>
<script type="math/tex; mode=display">
\boldsymbol{x}^{\mathrm{T}} A \boldsymbol{x}=\boldsymbol{x}^{\mathrm{T}} \boldsymbol{P}^{\mathrm{T}} \boldsymbol{P} \boldsymbol{x}=\left(\boldsymbol{P}_{\boldsymbol{x}}\right)^{\mathrm{T}} \boldsymbol{P} \boldsymbol{x}>0</script><p>因为$P$可逆，对于任意非$\boldsymbol{0}$向量$\boldsymbol{x}$有$\boldsymbol{P x} \neq \mathbf{0}$</p>
<p>下面证明第3条判定规则，如果$A$是正定矩阵，对于任意非0向量$x$都有$x^{\mathrm{T}} A \boldsymbol{x}&gt;0$，对于任意非$\boldsymbol{0}$向量$\boldsymbol{x}$有</p>
<script type="math/tex; mode=display">
\left(\boldsymbol{x}^{\mathrm{T}} \boldsymbol{A}^{\mathrm{T}} \boldsymbol{x}\right)^{\mathrm{T}}=\boldsymbol{x}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{x}>0</script><p>对于$n$阶矩阵$A$</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left(\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 n} \\
a_{21} & a_{22} & \cdots & a_{2 n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n 1} & a_{n 2} & \cdots & a_{n n}
\end{array}\right)</script><p>其前$k, 1 \leqslant k \leqslant n$行前$k$列元素形成的行列式</p>
<script type="math/tex; mode=display">
\left|\begin{array}{ccc}
a_{11} & \cdots & a_{1 k} \\
\vdots & \ddots & \vdots \\
a_{k 1} & \cdots & a_{k k}
\end{array}\right|</script><p>称为<code>顺序主子式</code>，这是矩阵左上角的子方阵形成的行列式，对于下面的4阶矩阵</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left(\begin{array}{cccc}
1 & 2 & 3 & 4 \\
5 & 6 & 7 & 8 \\
9 & 10 & 11 & 12 \\
13 & 14 & 15 & 16
\end{array}\right)</script><p>其1阶顺序主子式为</p>
<script type="math/tex; mode=display">
|1|</script><p>2阶顺序主子式为</p>
<script type="math/tex; mode=display">
\left|\begin{array}{ll}
1 & 2 \\
5 & 6
\end{array}\right|</script><p>3阶顺序主子式为</p>
<script type="math/tex; mode=display">
\left|\begin{array}{ccc}
1 & 2 & 3 \\
5 & 6 & 7 \\
9 & 10 & 11
\end{array}\right|</script><p>4阶顺序主子式为</p>
<script type="math/tex; mode=display">
\left|\begin{array}{cccc}
1 & 2 & 3 & 4 \\
5 & 6 & 7 & 8 \\
9 & 10 & 11 & 12 \\
13 & 14 & 15 & 16
\end{array}\right|</script><p>矩阵$A$不是正定的，因为其二阶顺序主子式为负</p>
<script type="math/tex; mode=display">
\left|\begin{array}{ll}
1 & 2 \\
5 & 6
\end{array}\right|=1 \times 6-2 \times 5<0</script><p>对于任意的$m \times n$矩阵$\boldsymbol{A}$，$\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}$是对称半正定矩阵，下面给出证明，显然该矩阵是对称的</p>
<script type="math/tex; mode=display">
\left(\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}\right)^{\mathrm{T}}=\boldsymbol{A}^{\mathrm{T}}\left(\boldsymbol{A}^{\mathrm{T}}\right)^{\mathrm{T}}=\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}</script><p>对于任意非$\boldsymbol{0}$向量$\boldsymbol{x}$，有</p>
<script type="math/tex; mode=display">
\boldsymbol{x}^{\mathrm{T}} \boldsymbol{A}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{x}=(\boldsymbol{A} \boldsymbol{x})^{\mathrm{T}}(\boldsymbol{A} \boldsymbol{x}) \geqslant 0</script><p>类似地可以证明$\boldsymbol{A A ^ { \mathrm { T } }}$也是对称半正定矩阵</p>
<p>在机器学习中，这种矩阵经常出现，如向量组的格拉姆矩阵，包括线性回归、支持向量机以及logistic回归等线性模型</p>
<p>它们目标函数的黑塞矩阵为这种类型的矩阵，因此是凸函数，可以保证求得全局极小值点</p>
<p>类似地，实对称矩阵负定可以通过下面的方法进行判定</p>
<ol>
<li>矩阵<script type="math/tex">A</script>的<script type="math/tex">n</script>个特征值<script type="math/tex">\lambda_{1}, \cdots, \lambda_{n}</script>均小于0</li>
<li>存在可逆矩阵$\boldsymbol{P}$使得$\boldsymbol{A}=-\boldsymbol{P}^{\mathrm{T}} \boldsymbol{P}$</li>
<li>矩阵$A$的所有奇数阶顺序主子式均为负，偶数阶顺序主子式均为正</li>
</ol>
<h2 id="标准型"><a href="#标准型" class="headerlink" title="标准型"></a>标准型</h2><p>标准型指对于任意的<script type="math/tex">i \neq j</script>，二次型中项<script type="math/tex">a_{i j} x_{i} x_{j}</script>的系数均为0，二次型由纯平方项构成，可写成如下形式</p>
<script type="math/tex; mode=display">
\boldsymbol{x}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{x}=d_{1} x_{1}^{2}+d_{2} x_{2}^{2}+\cdots+d_{n} x_{n}^{2}</script><p>下面是一个标准型</p>
<script type="math/tex; mode=display">
x_{1}^{2}-3 x_{2}^{2}+x_{3}^{2}</script><p>标准型对应的矩阵为对角矩阵，上面的标准型对应的矩阵为</p>
<script type="math/tex; mode=display">
\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & -3 & 0 \\
0 & 0 & 1
\end{array}\right)</script><p>在标准型中，正平方项的数量称为<code>正惯性指数</code>，负平方项的数量称为<code>负惯性指数</code></p>
<p>上面的标准型的正惯性指数为2，负惯性指数为1</p>
<p>由于二次型的矩阵为对称矩阵，因此一定可以对角化</p>
<p>通过正交变换可以将二次型化为标准型，与实对称矩阵的正交变换对角化相同</p>
<p>对于二次型$x^{\mathrm{T}} A \boldsymbol{x}$，通过正交变换将$A$化为对角矩阵</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\boldsymbol{P} \boldsymbol{\Lambda} \boldsymbol{P}^{\mathrm{T}}</script><p>从而有</p>
<script type="math/tex; mode=display">
\boldsymbol{x}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{x}=\boldsymbol{x}^{\mathrm{T}} \boldsymbol{P} \boldsymbol{\Lambda} \boldsymbol{P}^{\mathrm{T}} \boldsymbol{x}=\left(\boldsymbol{P}^{\mathrm{T}} \boldsymbol{x}\right)^{\mathrm{T}} \boldsymbol{\Lambda}\left(\boldsymbol{P}^{\mathrm{T}} \boldsymbol{x}\right)</script><p>这里$\boldsymbol{P}$是正交矩阵，如果令$\boldsymbol{y}=\boldsymbol{P}^{\mathrm{T}} \boldsymbol{x}$或者$\boldsymbol{x}=\boldsymbol{P} \boldsymbol{y}$，则$\boldsymbol{y}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{y}$是标准型</p>
<p>这对应于通过将$\boldsymbol{x}$换元为$y$，使得换元之后的二次型为标准型</p>
<p>如果矩阵<script type="math/tex">\boldsymbol{A}</script>的<script type="math/tex">n</script>个特征值<script type="math/tex">\lambda_{1}, \cdots, \lambda_{n}</script>均大于0，则矩阵<script type="math/tex">\boldsymbol{A}</script>正定</p>
<p>对于任意非0向量$x$，由于$P$是正交矩阵，$y=P^{\mathrm{T}} x \neq 0$，因此$A$正定</p>
<p>下面举例说明，对于下面的二次型</p>
<script type="math/tex; mode=display">
x_{1}^{2}+5 x_{2}^{2}+5 x_{3}^{2}+2 x_{1} x_{2}-4 x_{1} x_{3}</script><p>其对应的系数矩阵为</p>
<script type="math/tex; mode=display">
A=\left(\begin{array}{ccc}
1 & 1 & -2 \\
1 & 5 & 0 \\
-2 & 0 & 5
\end{array}\right)</script><p>特征多项式为</p>
<script type="math/tex; mode=display">
\begin{array}{l}
|A-\lambda I| 
=\left|\begin{array}{ccc}
1-\lambda & 1 & -2 \\
1 & 5-\lambda & 0 \\
-2 & 0 & 5-\lambda
\end{array}\right| \stackrel{r_{3}+2 r_{2}}{\longrightarrow}\left|\begin{array}{ccc}
1-\lambda & 1 & -2 \\
1 & 5-\lambda & 0 \\
0 & 2(5-\lambda) & 5-\lambda
\end{array}\right| \stackrel{c_{2}-2 \times c_{3}}{\longrightarrow}\left|\begin{array}{cccc}
1-\lambda & 5 & -2 \\
1 & 5-\lambda & 0 \\
0 & 0 & 5-\lambda
\end{array}\right| \\
=(5-\lambda)\left(\lambda^{2}-6 \lambda\right)
\end{array}</script><p>解得特征值为$0,5,6$</p>
<p>当$\lambda=5$时，有</p>
<script type="math/tex; mode=display">
A-\lambda I=\left(\begin{array}{ccc}
-4 & 1 & -2 \\
1 & 0 & 0 \\
-2 & 0 & 0
\end{array}\right) \rightarrow\left(\begin{array}{ccc}
1 & 0 & 0 \\
0 & 1 & -2 \\
0 & 0 & 0
\end{array}\right)</script><p>方程$(A-\lambda I) x=0$的解为</p>
<script type="math/tex; mode=display">
\boldsymbol{x}_{1}=\left(\begin{array}{lll}
0 & 2 & 1
\end{array}\right)^{\mathrm{T}}</script><p>当$\lambda=6$时，有</p>
<script type="math/tex; mode=display">
\boldsymbol{A}-\lambda \boldsymbol{I}=\left(\begin{array}{ccc}
-5 & 1 & -2 \\
1 & -1 & 0 \\
-2 & 0 & -1
\end{array}\right) \rightarrow\left(\begin{array}{ccc}
1 & 0 & 1 / 2 \\
0 & 1 & 1 / 2 \\
0 & 0 & 0
\end{array}\right)</script><p>方程$(A-\lambda I) x=0$的解为</p>
<script type="math/tex; mode=display">
x_{2}=\left(\begin{array}{lll}
1 & 1 & -2
\end{array}\right)^{\mathrm{T}}</script><p>当$\lambda=0$时，有</p>
<script type="math/tex; mode=display">
\boldsymbol{A}-\lambda I=\left(\begin{array}{ccc}
1 & 1 & -2 \\
1 & 5 & 0 \\
-2 & 0 & 5
\end{array}\right) \rightarrow\left(\begin{array}{ccc}
1 & 0 & -5 / 2 \\
0 & 1 & 1 / 2 \\
0 & 0 & 0
\end{array}\right)</script><p>方程$(A-\lambda I) x=0$的解为</p>
<script type="math/tex; mode=display">
\boldsymbol{x}_{3}=\left(\begin{array}{lll}
5 & -1 & 2
\end{array}\right)^{\mathrm{T}}</script><p>由于二次型的系数矩阵是实对称矩阵，其不同特征值对应的特征向量相互正交，因此只需要将这些特征向量单位化即可</p>
<script type="math/tex; mode=display">
\alpha_{1}=\frac{1}{\sqrt{5}}\left(\begin{array}{l}
0 \\
2 \\
1
\end{array}\right), \alpha_{2}=\frac{1}{\sqrt{6}}\left(\begin{array}{c}
1 \\
1 \\
-2
\end{array}\right), \alpha_{3}=\frac{1}{\sqrt{30}}\left(\begin{array}{c}
5 \\
-1 \\
2
\end{array}\right)</script><p>令</p>
<script type="math/tex; mode=display">
P=\left(\begin{array}{ccc}
0 & \frac{1}{\sqrt{6}} & \frac{5}{\sqrt{30}} \\
\frac{2}{\sqrt{5}} & \frac{1}{\sqrt{6}} & -\frac{1}{\sqrt{30}} \\
\frac{1}{\sqrt{5}} & -\frac{2}{\sqrt{6}} & \frac{2}{\sqrt{30}}
\end{array}\right)</script><p>通过正交变换$x=\boldsymbol{P y}$可将二次型化为如下的标准型</p>
<script type="math/tex; mode=display">
5 y_{1}^{2}+6 y_{2}^{2}</script><h1 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h1><p>矩阵分解是矩阵分析的重要内容，这种技术将一个矩阵分解为若干矩阵的乘积，通常为2个或3个矩阵的乘积</p>
<p>在求解线性方程组，计算逆矩阵、行列式以及特征值，多重积分换元等问题上，矩阵分解有广泛的应用</p>
<h2 id="楚列斯基分解"><a href="#楚列斯基分解" class="headerlink" title="楚列斯基分解"></a>楚列斯基分解</h2><p>对于$n$阶对称半正定矩阵$\boldsymbol{A}$，<code>楚列斯基</code>(Cholesky)分解将其分解为$n$阶下三角矩阵$L$以及其转置$L^{\mathrm{T}}$的乘积</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\boldsymbol{L} \boldsymbol{L}^{\mathrm{T}}</script><p>如果$A$是实对称正定矩阵，则上式的分解唯一</p>
<p>下面是对称矩阵楚列斯基分解的一个 例子</p>
<script type="math/tex; mode=display">
\left(\begin{array}{ccc}
4 & 12 & -16 \\
12 & 37 & -43 \\
-16 & -43 & 98
\end{array}\right)=\left(\begin{array}{ccc}
2 & 0 & 0 \\
6 & 1 & 0 \\
-8 & 5 & 3
\end{array}\right)\left(\begin{array}{ccc}
2 & 6 & -8 \\
0 & 1 & 5 \\
0 & 0 & 3
\end{array}\right)</script><p>楚列斯基分解可用于求解线性方程组，对于如下的线性方程组</p>
<script type="math/tex; mode=display">
\boldsymbol{A x}=\boldsymbol{b}</script><p>如果$A$是对称正定矩阵，它可以分解为$L L^{\mathrm{T}}$，则有</p>
<script type="math/tex; mode=display">
\boldsymbol{L} \boldsymbol{L}^{\mathrm{T}} \boldsymbol{x}=\boldsymbol{b}</script><p>如果令</p>
<script type="math/tex; mode=display">
\boldsymbol{L}^{\mathrm{T}} \boldsymbol{x}=\boldsymbol{y}</script><p>则可先求解线性方程组</p>
<script type="math/tex; mode=display">
L y=b</script><p>得到$y$。然后求解</p>
<script type="math/tex; mode=display">
\boldsymbol{L}^{\mathrm{T}} \boldsymbol{x}=\boldsymbol{y}</script><p>得到$x$，这两个方程组的系数矩阵分别为下三角和上三角矩阵，均可高效地求解</p>
<p>在实际应用中，如果系数矩阵$A$不变而常数向量$b$会改变，则预先将$A$进行楚列斯基分解，每次对于不同的$b$均可高效地求解</p>
<p>在求解最优化问题的拟牛顿法中，需要求解如下的方程组</p>
<script type="math/tex; mode=display">
\boldsymbol{B}_{k} \boldsymbol{d}=-\boldsymbol{g}_{k}</script><p>其中<script type="math/tex">B_{k}</script>为第<script type="math/tex">k</script>次迭代时的黑塞(Hessian)矩阵的近似矩阵，<script type="math/tex">d</script>为牛顿方向，<script type="math/tex">g_{k}</script>为第<script type="math/tex">k</script>次迭代时的梯度值</p>
<p>此方程可以使用楚列斯基分解求解</p>
<blockquote>
<p>楚列斯基分解还可以用于检查矩阵的正定性</p>
</blockquote>
<p>对一个矩阵进行楚列斯基分解，如果分解失败，则说明矩阵不是半正定矩阵；否则为半正定矩阵</p>
<p>下面以3阶矩阵为例推导楚列斯基分解的计算公式，如果</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left(\begin{array}{lll}
a_{11} & a_{21} & a_{31} \\
a_{21} & a_{22} & a_{32} \\
a_{31} & a_{32} & a_{33}
\end{array}\right)=\boldsymbol{L} \boldsymbol{L}^{\mathrm{T}}=\left(\begin{array}{ccc}
l_{11} & 0 & 0 \\
l_{21} & l_{22} & 0 \\
l_{31} & l_{32} & l_{33}
\end{array}\right)\left(\begin{array}{ccc}
l_{11} & l_{21} & l_{31} \\
0 & l_{22} & l_{32} \\
0 & 0 & l_{33}
\end{array}\right)</script><p>则有</p>
<script type="math/tex; mode=display">
\left(\begin{array}{ccc}
l_{11}^{2} & l_{21} l_{11} & l_{31} l_{11} \\
l_{21} l_{11} & l_{21}^{2}+l_{22}^{2} & l_{31} l_{21}+l_{32} l_{22} \\
l_{31} l_{11} & l_{31} l_{21}+l_{32} l_{22} & l_{31}^{2}+l_{32}^{2}+l_{33}^{2}
\end{array}\right)=\left(\begin{array}{lll}
a_{11} & a_{21} & a_{31} \\
a_{21} & a_{22} & a_{32} \\
a_{31} & a_{32} & a_{33}
\end{array}\right)</script><p>首先可以得到主对角的第一个元素</p>
<script type="math/tex; mode=display">
l_{11}=\sqrt{a_{11}}</script><p>根据$l_{11}$可以得到第2行的所有元素</p>
<script type="math/tex; mode=display">
l_{21}=\frac{a_{21}}{l_{11}}, l_{22}=\sqrt{a_{22}-l_{21}^{2}}</script><p>进一步得到第3行的元素</p>
<script type="math/tex; mode=display">
l_{31}=\frac{a_{31}}{l_{11}}, l_{32}=\frac{1}{l_{22}}\left(a_{32}-l_{31} l_{21}\right), l_{33}=\sqrt{a_{33}-\left(l_{31}^{2}+l_{32}^{2}\right)}</script><p>所有元素逐行算出，首先计算出第1行的元素<script type="math/tex">l_{11}</script>，然后计算第2行的元素<script type="math/tex">l_{21}, l_{22}</script>，接下来计算<script type="math/tex">l_{31}, l_{32}, l_{33}</script>，依此类推</p>
<p>这里<script type="math/tex">l_{i j}, 1<j \leqslant i</script>与<script type="math/tex">l_{p q}, p \leqslant i, q<j</script>有关，这些值已经被算出</p>
<p>对于$n$阶矩阵，楚列斯基分解的计算公式为</p>
<script type="math/tex; mode=display">
l_{i i}=\left(a_{i i}-\sum_{k=1}^{i-1} l_{i k}^{2}\right)^{\frac{1}{2}} \qquad l_{j i}=\frac{1}{l_{i i}}\left(a_{j i}-\sum_{k=1}^{i-1} l_{i k} l_{j k}\right), j=i+1, \cdots, n</script><p>Python中linalg的cholesky函数实现了对称正定矩阵的楚列斯基分解</p>
<p>函数的输入是被分解矩阵$\boldsymbol{A}$，输出为下三角矩阵$\boldsymbol{L}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">A = np.array([[<span class="number">6</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">8</span>],[<span class="number">3</span>,<span class="number">6</span>,<span class="number">5</span>,<span class="number">1</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">10</span>,<span class="number">7</span>],[<span class="number">8</span>,<span class="number">1</span>,<span class="number">7</span>,<span class="number">25</span>]])</span><br><span class="line">L = np<span class="number">.1</span>inalg.cholesky(A)</span><br><span class="line"><span class="built_in">print</span>(L)</span><br></pre></td></tr></table></figure>
<p>程序输出结果为</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="number">2.44948974</span>  <span class="number">0.0</span>         <span class="number">0.0</span>         <span class="number">0.0</span> ],</span><br><span class="line"> [<span class="number">1.22474487</span>  <span class="number">2.12132034</span>  <span class="number">0.0</span>         <span class="number">0.0</span>],</span><br><span class="line"> [<span class="number">1.63299316</span>  <span class="number">1.41421356</span>  <span class="number">2.30940108</span>  <span class="number">0.0</span>],</span><br><span class="line"> [<span class="number">3.26598632</span>  -<span class="number">1.41421356</span> <span class="number">1.58771324</span>  <span class="number">3.13249102</span>]]</span><br></pre></td></tr></table></figure>
<p>可以验证矩阵$\boldsymbol{L}$与其转置的乘积即为矩阵$\boldsymbol{A}$</p>
<h2 id="QR-分解"><a href="#QR-分解" class="headerlink" title="QR 分解"></a>QR 分解</h2><p><code>QR分解</code>(正交三角分解)将矩阵分解为正交矩阵与上三角矩阵的乘积，这种分解被广泛地应用于求解某些问题，如矩阵的特征值</p>
<p>事实上，$\mathrm{QR}$分解是格拉姆-施密特正交化的另外一种表现形式</p>
<p>首先考虑方阵的情况，对于任意的$n$阶方阵$\boldsymbol{A}$，$\mathrm{QR}$分解将其分解为一个$n$阶正交矩阵$\boldsymbol{Q}$与一个$n$阶上三角矩阵$\boldsymbol{R}$的乘积</p>
<script type="math/tex; mode=display">
A=Q R</script><p>如果矩阵$\boldsymbol{A}$可逆且要求矩阵$\boldsymbol{R}$的主对角元为正，则上式的分解唯一</p>
<p>如果$\boldsymbol{A}$有$m(m \leqslant n$) 个线性无关的列，则$\boldsymbol{Q}$的前$m$个列构成$\boldsymbol{A}$的列空间的标准正交基</p>
<p>下面来看$\mathrm{QR}$分解的实际例子，对于如下矩阵</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left(\begin{array}{ll}
7 & 2 \\
2 & 4
\end{array}\right)</script><p>其$\mathrm{QR}$分解的结果为</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\boldsymbol{Q R}=\left(\begin{array}{ll}
7 & 2 \\
2 & 4
\end{array}\right)=\left(\begin{array}{cc}
0.962 & -0.275 \\
0.275 & 0.962
\end{array}\right)\left(\begin{array}{cc}
7.28 & 3.02 \\
0 & 3.30
\end{array}\right)</script><p>下面考虑非方阵的情况，对于$m \times n, m&gt;n$的矩阵$\boldsymbol{A}$，QR 分解将其分解为一个$m$阶正交矩阵与如下形式的$m \times n$矩阵$\boldsymbol{R}$的乘积</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\boldsymbol{Q} \boldsymbol{R}=\boldsymbol{Q}\left(\begin{array}{c}
\boldsymbol{R}_{n} \\
\boldsymbol{0}_{(m-n) \times n}
\end{array}\right)</script><p>其中$\boldsymbol{R}_{n}$是$n$阶上三角矩阵，$\boldsymbol{0}$是一个$(m-n) \times n$的零矩阵。 如果$m&lt;n$, 则分解的结果为</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\boldsymbol{Q R}=\boldsymbol{Q}\left(\boldsymbol{R}_{m} \boldsymbol{B}_{m \times(n-m)}\right)</script><p>其中$\boldsymbol{Q}$是一个$m$阶正交矩阵，<script type="math/tex">\boldsymbol{R}_{m}</script>是<script type="math/tex">m</script>阶上三角矩阵，<script type="math/tex">\boldsymbol{B}_{m \times(n-m)}</script>是一个<script type="math/tex">m \times(n-m)</script>的矩阵</p>
<blockquote>
<p>$\mathrm{QR}$分解有 3 种实现方式</p>
</blockquote>
<p>分别是格拉姆-施密特正交化、豪斯霍尔德变换以及吉文斯(Givens)旋转</p>
<p>下面介绍格拉姆-施密特正交化以及豪斯霍尔德变换</p>
<p>考虑$A$为$n$阶方阵的情况，使用格拉姆-施密特正交化技术对矩阵$A$的列进行正交化，将矩阵$\boldsymbol{A}$按列分块</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left(\begin{array}{lll}
a_{1} & \cdots & a_{n}
\end{array}\right)</script><p>假设这些列向量线性无关，首先将它的列正交化</p>
<script type="math/tex; mode=display">
\begin{array}{ll}
u_{1}=\boldsymbol{a}_{1} \qquad \boldsymbol{u}_{2}=\boldsymbol{a}_{2}-\frac{\boldsymbol{a}_{2}^{\mathrm{T}} \boldsymbol{u}_{1}}{\boldsymbol{u}_{1}^{\mathrm{T}} \boldsymbol{u}_{1}} \boldsymbol{u}_{1} \qquad \boldsymbol{u}_{3}=\boldsymbol{a}_{3}-\frac{\boldsymbol{a}_{3}^{\mathrm{T}} \boldsymbol{u}_{1}}{\boldsymbol{u}_{1}^{\mathrm{T}} \boldsymbol{u}_{1}} \boldsymbol{u}_{1}-\frac{\boldsymbol{a}_{3}^{\mathrm{T}} \boldsymbol{u}_{2}}{\boldsymbol{u}_{2}^{\mathrm{T}} \boldsymbol{u}_{2}} \boldsymbol{u}_{2} \\
\ldots \qquad \boldsymbol{u}_{n}=\boldsymbol{a}_{n}-\sum_{i=1}^{n-1} \frac{\boldsymbol{a}_{n}^{\mathrm{T}} \boldsymbol{u}_{i}}{\boldsymbol{u}_{i}^{\mathrm{T}} \boldsymbol{u}_{i}} \boldsymbol{u}_{i}
\end{array}</script><p>然后进行单位化</p>
<script type="math/tex; mode=display">
\boldsymbol{e}_{i}=\frac{\boldsymbol{u}_{i}}{\left\|\boldsymbol{u}_{i}\right\|^{\prime}}, i=1, \cdots, n</script><p>$A$的各个列向量在标准正交基下的坐标为其在各个基向量上的投影，由于在进行格拉姆-施密特正交化时<script type="math/tex">e_{i}</script>只与<script type="math/tex">a_{1}, \cdots, a_{i}</script>有关</p>
<p>因此<script type="math/tex">a_{i}</script>在<script type="math/tex">e_{i+1}, \cdots, e_{n}</script>方向的投影均为0，有</p>
<script type="math/tex; mode=display">
\begin{array}{l}
\boldsymbol{a}_{1}=\boldsymbol{a}_{1}^{\mathrm{T}} \boldsymbol{e}_{1} \boldsymbol{e}_{1} \qquad \boldsymbol{a}_{2}=\boldsymbol{a}_{2}^{\mathrm{T}} \boldsymbol{e}_{1} \boldsymbol{e}_{1}+\boldsymbol{a}_{2}^{\mathrm{T}} \boldsymbol{e}_{2} \boldsymbol{e}_{2} \qquad \boldsymbol{a}_{3}=\boldsymbol{a}_{3}^{\mathrm{T}} \boldsymbol{e}_{1} \boldsymbol{e}_{1}+\boldsymbol{a}_{3}^{\mathrm{T}} \boldsymbol{e}_{2} \boldsymbol{e}_{2}+\boldsymbol{a}_{3}^{\mathrm{T}} \boldsymbol{e}_{3} \boldsymbol{e}_{3}
\\
\ldots  \qquad \boldsymbol{a}_{n}=\sum_{i=1}^{n} \boldsymbol{a}_{n}^{\mathrm{T}} e_{i} e_{i} \\
\end{array}</script><p>写成矩阵形式为</p>
<script type="math/tex; mode=display">
\left(a_{1} \cdots a_{n}\right)=\left(\begin{array}{lll}e_{1} & \cdots & e_{n}\end{array}\right)\left(\begin{array}{cccc}\boldsymbol{a}_{1}^{\mathrm{T}} \boldsymbol{e}_{1} & \boldsymbol{a}_{2}^{\mathrm{T}} \boldsymbol{e}_{1} & \boldsymbol{a}_{3}^{\mathrm{T}} \boldsymbol{e}_{1} & \cdots \\ 0 & \boldsymbol{a}_{2}^{\mathrm{T}} \boldsymbol{e}_{2} & \boldsymbol{a}_{3}^{\mathrm{T}} \boldsymbol{e}_{2} & \cdots \\ 0 & 0 & \boldsymbol{a}_{3}^{\mathrm{T}} \boldsymbol{e}_{3} & \cdots \\ \vdots & \vdots & \vdots & \end{array}\right)</script><p>令<script type="math/tex">Q=\left(e_{1} \cdots e_{n}\right)</script>，以及</p>
<script type="math/tex; mode=display">
\boldsymbol{R}=\left(\begin{array}{cccc}
\boldsymbol{a}_{1}^{\mathrm{T}} \boldsymbol{e}_{1} & \boldsymbol{a}_{2}^{\mathrm{T}} \boldsymbol{e}_{1} & \boldsymbol{a}_{3}^{\mathrm{T}} \boldsymbol{e}_{1} & \cdots \\
0 & \boldsymbol{a}_{2}^{\mathrm{T}} \boldsymbol{e}_{2} & \boldsymbol{a}_{3}^{\mathrm{T}} \boldsymbol{e}_{2} & \cdots \\
0 & 0 & \boldsymbol{a}_{3}^{\mathrm{T}} \boldsymbol{e}_{3} & \cdots \\
\vdots & \vdots & \vdots &
\end{array}\right)</script><p>$Q$的列是用$A$的列构造的标准正交基，$R$的第$i$列为$\boldsymbol{A}$的第$i$列在前$i$个基向量方向的投影，此即$Q R$分解结果</p>
<blockquote>
<p>例子</p>
</blockquote>
<p>下面举例说明，对于如下的矩阵</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left(\begin{array}{ccc}
12 & -51 & 4 \\
6 & 167 & -68 \\
-4 & 24 & -41
\end{array}\right)</script><p>首先对它的列向量进行正交化，得到如下矩阵</p>
<script type="math/tex; mode=display">
\boldsymbol{U}=\left(\begin{array}{lll}
\boldsymbol{u}_{1} & \boldsymbol{u}_{2} & \boldsymbol{u}_{3}
\end{array}\right)=\left(\begin{array}{ccc}
12 & -69 & -58 / 5 \\
6 & 158 & 6 / 5 \\
-4 & 30 & -33
\end{array}\right)</script><p>然后将该矩阵的列单位化，可以得到</p>
<script type="math/tex; mode=display">
\boldsymbol{Q}=\left(\begin{array}{lll}
\frac{\boldsymbol{u}_{1}}{\left\|\boldsymbol{u}_{1}\right\|} & \frac{\boldsymbol{u}_{2}}{\left\|\boldsymbol{u}_{2}\right\|} & \frac{\boldsymbol{u}_{3}}{\left\|\boldsymbol{u}_{3}\right\|}
\end{array}\right)=\left(\begin{array}{ccc}
6 / 7 & -69 / 175 & -58 / 175 \\
3 / 7 & 158 / 175 & 6 / 175 \\
-2 / 7 & 6 / 35 & -33 / 35
\end{array}\right)</script><p>由此可以得到上三角矩阵</p>
<script type="math/tex; mode=display">
\boldsymbol{R}=\boldsymbol{Q}^{\mathrm{T}} \boldsymbol{A}=\left(\begin{array}{ccc}
14 & 21 & -14 \\
0 & 175 & -70 \\
0 & 0 & 35
\end{array}\right)</script><p>用豪斯霍尔德变换进行$Q R$分解的思路与之前讲述的类似，首先用矩阵$A$的第1列构造第1个豪斯霍尔德矩阵$\boldsymbol{P}_{1}$</p>
<script type="math/tex; mode=display">
\left(\begin{array}{cccc}
p_{11} & p_{12} & \cdots & p_{1 n} \\
p_{21} & p_{22} & \cdots & p_{2 n} \\
\vdots & \vdots & & \vdots \\
p_{n 1} & p_{n 2} & \cdots & p_{n n}
\end{array}\right)</script><p>左乘该矩阵将$\boldsymbol{A}$的第1列后面$n-1$个元素全部零化</p>
<script type="math/tex; mode=display">
\boldsymbol{P}_{1} \boldsymbol{A}=\left(\begin{array}{cccc}
a_{11} & a_{12} & \cdots & a_{1 n} \\
0 & a_{22} & \cdots & a_{2 n} \\
\vdots & \vdots & & \vdots \\
0 & a_{n 2} & \cdots & a_{n n}
\end{array}\right)</script><p>接下来构造第2个豪斯霍尔德矩阵$P_{2}$，为如下形式</p>
<script type="math/tex; mode=display">
\left(\begin{array}{cccc}
1 & 0 & \cdots & 0 \\
0 & p_{22} & \cdots & p_{2 n} \\
\vdots & \vdots & & \vdots \\
0 & p_{n 2} & \cdots & p_{n n}
\end{array}\right)</script><p>其中</p>
<script type="math/tex; mode=display">
\left(\begin{array}{ccc}
p_{22} & \cdots & p_{2 n} \\
\vdots & & \vdots \\
p_{n 2} & \cdots & p_{n n}
\end{array}\right)</script><p>使用<script type="math/tex">P_{1} \boldsymbol{A}</script>的第2列的后面<script type="math/tex">n-1</script>个元素构造，将<script type="math/tex">P_{1} A</script>左乘<script type="math/tex">P_{2}</script>，可以将其第2列后面<script type="math/tex">n-2</script>个元素零化</p>
<script type="math/tex; mode=display">
\boldsymbol{P}_{2} \boldsymbol{P}_{1} \boldsymbol{A}=\left(\begin{array}{ccccc}
a_{11} & a_{12} & a_{13} & \cdots & a_{1 n} \\
0 & a_{22} & a_{23} & \cdots & a_{2 n} \\
0 & 0 & a_{33} & \cdots & a_{3 n} \\
\vdots & \vdots & \vdots & & \vdots \\
0 & 0 & a_{n 3} & \cdots & a_{n n}
\end{array}\right)</script><p>构造第3个豪斯霍尔德矩阵$\boldsymbol{P}_{3}$，为如下形式</p>
<script type="math/tex; mode=display">
\left(\begin{array}{ccccc}
1 & 0 & 0 & \cdots & 0 \\
0 & 1 & 0 & \cdots & 0 \\
0 & 0 & p_{33} & \cdots & p_{3 n} \\
\vdots & \vdots & \vdots & & \vdots \\
0 & 0 & p_{n 3} & \cdots & p_{n n}
\end{array}\right)</script><p>其中</p>
<script type="math/tex; mode=display">
\left(\begin{array}{ccc}
p_{33} & \cdots & p_{3 n} \\
\vdots & & \vdots \\
p_{n 3} & \cdots & p_{n n}
\end{array}\right)</script><p>用<script type="math/tex">P_{2} P_{1} A</script>的第3列的后面<script type="math/tex">n-2</script>个元素构造，将<script type="math/tex">P_{2} P_{1} A</script>左乘<script type="math/tex">P_{3}</script>，可以将其第3列后面<script type="math/tex">n-3</script>个元素零化</p>
<script type="math/tex; mode=display">
\boldsymbol{P}_{3} \boldsymbol{P}_{2} \boldsymbol{P}_{1} \boldsymbol{A}=\left(\begin{array}{cccccc}
a_{11} & a_{12} & a_{13} & a_{14} & \cdots & a_{1 n} \\
0 & a_{22} & a_{23} & a_{24} & \cdots & a_{2 n} \\
0 & 0 & a_{33} & a_{34} & \cdots & a_{3 n} \\
0 & 0 & 0 & a_{44} & \cdots & a_{4 n} \\
\vdots & \vdots & \vdots & \vdots & & \vdots \\
0 & 0 & 0 & a_{n 4} & \cdots & a_{n n}
\end{array}\right)</script><p>依此类推，经过$n-1$次豪斯霍尔德变换，可以将$\boldsymbol{A}$化为上三角矩阵</p>
<script type="math/tex; mode=display">
P_{n-1} \cdots P_{2} P_{1} A=R</script><p>令</p>
<script type="math/tex; mode=display">
Q=\left(P_{n-1} \cdots P_{2} P_{1}\right)^{-1}=P_{1}^{-1} P_{2}^{-1} \cdots P_{n-1}^{-1}=P_{1} P_{2} P_{n-1}</script><p>由于$P_{0}, i=1, \cdots, n-1$都是正交矩阵，因此$Q$也是一个正交矩阵，这就是$\mathrm{QR}$分解的结果</p>
<p>$\mathrm{QR}$分解可以由Python中linalg的qr函数实现，函数的输入为被分解矩阵$A$，输出为正交矩阵$Q$和上三角矩阵$R$</p>
<p>下面用例子进行说明，首先考虑方阵，对于如下的方阵</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left(\begin{array}{lll}
1 & 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{array}\right)</script><p>其$\mathrm{QR}$分解的代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]])</span><br><span class="line">Q, R = np.linalg.qr(A)</span><br><span class="line"><span class="built_in">print</span>(Q)</span><br><span class="line"><span class="built_in">print</span>(R)</span><br></pre></td></tr></table></figure>
<p>程序运行结果如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[-<span class="number">0.12309149</span>  <span class="number">0.90453403</span>    <span class="number">0.40824829</span>],</span><br><span class="line"> [-<span class="number">0.49236596</span>  <span class="number">0.30151134</span>    -<span class="number">0.81649658</span>],</span><br><span class="line"> [-<span class="number">0.86164044</span>  -<span class="number">0.301511340</span>  <span class="number">0.40824829</span>]]</span><br><span class="line"></span><br><span class="line">[[-<span class="number">8.12403840e+00</span>  -<span class="number">9.60113630e+00</span>  -<span class="number">1.10782342e+01</span>],</span><br><span class="line"> [<span class="number">0.00000000e+00</span>   <span class="number">9.04534034e-01</span>   <span class="number">1.80906807e+00</span>],</span><br><span class="line"> [<span class="number">0.00000000e+00</span>   <span class="number">0.00000000e+00</span>   -<span class="number">8.88178420e-16</span>]] </span><br></pre></td></tr></table></figure>
<p>可以验证这两个矩阵的乘积就是原始矩阵$\boldsymbol{A}$，接下来考虑不是方阵的情况，对于如下的矩阵</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left(\begin{array}{lll}
1 & 2 & 3 \\
4 & 5 & 6
\end{array}\right)</script><p>其$\mathrm{QR}$分解的代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]])</span><br><span class="line">Q, R = np.linalg.qr(A)</span><br><span class="line"><span class="built_in">print</span>(Q)</span><br><span class="line"><span class="built_in">print</span>(R)</span><br></pre></td></tr></table></figure>
<p>程序运行结果如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[[-<span class="number">0.24253563</span>-<span class="number">0.9701425</span>]</span><br><span class="line"> [-<span class="number">0.97014250</span> <span class="number">.24253563</span>]]</span><br><span class="line"></span><br><span class="line">[[-<span class="number">4.12310563</span>-<span class="number">5.33578375</span>-<span class="number">6.54846188</span>]</span><br><span class="line"> [<span class="number">0</span>,-<span class="number">0.72760688</span>-<span class="number">1.45521375</span>]]</span><br></pre></td></tr></table></figure>
<h2 id="特征值分解"><a href="#特征值分解" class="headerlink" title="特征值分解"></a>特征值分解</h2><blockquote>
<p>定义</p>
</blockquote>
<p><code>特征值分解</code>(Eigen Decomposition)也称为<code>谱分解</code>(Spectral Decomposition)，是矩阵相似对角化的另一种表述</p>
<p>对于$n$阶矩阵$\boldsymbol{A}$，如果它有$n$个线性无关的特征向量，则可将其分解为如下3个矩阵的乘积</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\boldsymbol{Q} \Lambda \boldsymbol{Q}^{-1}</script><p>其中$\Lambda$为对角矩阵，矩阵$\Lambda$的对角线元素为矩阵$A$的特征值</p>
<script type="math/tex; mode=display">
\Lambda=\left(\begin{array}{lll}
\lambda_{1} & & \\
& \ddots & \\
& & \lambda_{n}
\end{array}\right)</script><p>$Q$为$n$阶矩阵，它的列为$A$的特征向量，与对角矩阵中特征值的排列顺序一致</p>
<script type="math/tex; mode=display">
\boldsymbol{Q}=\left(\begin{array}{lll}
x_{1} & \cdots & \boldsymbol{x}_{n}
\end{array}\right)</script><p>一个$n$阶矩阵可以进行特征值分解的充分必要条件是它有$n$个线性无关的特征向量，通常情况下，这些特征向量$x_{i}$都是单位化的</p>
<blockquote>
<p>用于计算逆矩阵</p>
</blockquote>
<p>特征值分解可以用于计算逆矩阵，如果矩阵$\boldsymbol{A}$可以进行特征值分解，且其所有特征值都非0，则</p>
<script type="math/tex; mode=display">
A=Q \Lambda Q^{-1}</script><p>其逆矩阵为</p>
<script type="math/tex; mode=display">
\boldsymbol{A}^{-1}=\left(\boldsymbol{Q} \boldsymbol{A} \boldsymbol{Q}^{-1}\right)^{-1}=\boldsymbol{Q} \boldsymbol{\Lambda}^{-1} \boldsymbol{Q}^{-1}</script><p>对角矩阵的逆矩阵容易计算，是主对角线所有元素的倒数</p>
<p>特征值分解还可用于计算矩阵的多项式或者幂，对于如下多项式</p>
<script type="math/tex; mode=display">
f(x)=a_{n} x^{n}+a_{n-1} x^{n-1}+\cdots+a_{1} x</script><p>如果矩阵$\boldsymbol{A}$可以进行特征值分解，且</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\boldsymbol{Q A} \boldsymbol{Q}^{-1}</script><p>则有</p>
<script type="math/tex; mode=display">
\begin{aligned}
f(\boldsymbol{A}) & =f\left(\boldsymbol{Q \Lambda} \boldsymbol{Q}^{-1}\right)=a_{1} \boldsymbol{Q \Lambda} Q^{-1}+a_{2} \boldsymbol{Q \Lambda} Q^{-1} Q \Lambda Q^{-1}+\cdots=a_{1} \boldsymbol{Q} \boldsymbol{\Lambda} Q^{-1}+a_{2} \boldsymbol{Q} \boldsymbol{\Lambda}^{2} Q^{-1}+\cdots \\
& =\boldsymbol{Q}\left(a_{1} \boldsymbol{\Lambda}+a_{2} \Lambda^{2}+\cdots\right) Q^{-1}=Q f(\boldsymbol{\Lambda}) Q^{-1}
\end{aligned}</script><p>对角矩阵的幂仍然是对角矩阵，是主对角线元素分别求幂，因此有</p>
<script type="math/tex; mode=display">
f(\boldsymbol{\Lambda})_{i i}=f\left(\Lambda_{i i}\right)</script><p>借助于特征值分解，可以高效地计算出$f(A)$，特别地，有</p>
<script type="math/tex; mode=display">
A^{n}=Q A^{n} Q^{-1}</script><p>如果$A$是实对称矩阵，可对其特征向量进行正交化，特征值分解为</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=Q \Lambda Q^{\mathrm{T}}</script><p>其中$Q$为正交矩阵，它的列是$A$的正交化特征向量，$A$同样为$A$的所有特征值构成的对角矩阵</p>
<p>特征值分解可以借助于$\mathrm{QR}$箕法实现，机器学习中常用的矩阵如协方差矩阵等都是实对称矩阵，因此都可以进行特征值分解</p>
<p>特征值分解可以由Python中linalg的eig函数实现，函数的输入为被分解矩阵$A$，输出为所有特征值，以及这些特征值对应的单位化特征向量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">A = np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span>]])</span><br><span class="line">V, U = np.linalg.eig(A)</span><br><span class="line"><span class="built_in">print</span>(U)</span><br><span class="line"><span class="built_in">print</span>(V)</span><br></pre></td></tr></table></figure>
<p>程序结果如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[[-<span class="number">0.23197069</span>-<span class="number">0.785830240</span> <span class="number">.40824829</span>]</span><br><span class="line"> [-<span class="number">0.52532209</span>-<span class="number">0.08675134</span>-<span class="number">0.81649658</span>]</span><br><span class="line"> [-<span class="number">0.81867350</span> <span class="number">.612327560</span> <span class="number">.40824829</span>]]</span><br><span class="line"></span><br><span class="line">[<span class="number">1.61168440e+01</span>   -<span class="number">1.11684397e+00</span>  -<span class="number">1.30367773e-15</span>]</span><br></pre></td></tr></table></figure>
<p>这里的V所有特征值形成的向量，U的列是单位化的特征向量</p>
<h2 id="奇异值分解"><a href="#奇异值分解" class="headerlink" title="奇异值分解"></a>奇异值分解</h2><p>特征值分解只适用于方阵，且要求方阵有$n$个线性无关的特征向量</p>
<p><code>奇异值分解</code>(Singular Value Decomposition, SVD)是对它的推广，对于任意的矩阵均可用特征值与特征向量进行分解</p>
<p>其思路是对$A A^{\mathrm{T}}$和$\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}$进行特征值分解，对于任意矩阵$\boldsymbol{A}$，这两个矩阵都是<code>对称半正定矩阵</code>，一定能进行特征值分解</p>
<p>并且这两个矩阵的特征值都是非负的，后面将会证明它们有相同的非0特征值</p>
<p>假设$A \in \mathbb{R}^{m \times n}$，其中$m \geqslant n$，则有</p>
<script type="math/tex; mode=display">
\boldsymbol{U}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{V}=\mathbf{\Sigma}</script><p>其中$\boldsymbol{U}$为$m$阶正交矩阵，其列称为矩阵$\boldsymbol{A}$的左奇异向量，也是$\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}}$的特征向量，$\boldsymbol{\Sigma}$为如下形式的$m \times n$矩阵</p>
<script type="math/tex; mode=display">
\boldsymbol{\Sigma}=\left(\begin{array}{cccc}
\sigma_{1} & 0 & \cdots & 0 \\
0 & \sigma_{2} & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots \\
0 & 0 & \cdots & \sigma_{n} \\
0 & 0 & \cdots & 0 \\
\vdots & \vdots & \vdots & \vdots \\
0 & 0 & \cdots & \cdots
\end{array}\right)=\left(\begin{array}{c}
\boldsymbol{\Sigma}_{n} \\
\mathbf{0}_{(m-n) \times n}
\end{array}\right)</script><p>其尺寸与$\boldsymbol{A}$相同，在这里$\boldsymbol{\Sigma}_{n}$是$n$阶对角矩阵且主对角线元素按照其值大小降序排列</p>
<script type="math/tex; mode=display">
\boldsymbol{\Sigma}_{n}=\operatorname{diag}\left(\sigma_{1}, \cdots, \sigma_{n}\right), \sigma_{1} \geqslant \sigma_{2} \geqslant \cdots \geqslant \sigma_{n} \geqslant 0</script><p>$\sigma_{i}$称为$\boldsymbol{A}$的奇异值，是$\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}}$特征值的非负平方根，也是$\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}$特征值的非负平方根</p>
<p>$\boldsymbol{V}$为$n$阶正交矩阵，其行称为矩阵$\boldsymbol{A}$的右奇异向量，也是$\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}$的特征向量</p>
<p>式1两边左乘$\boldsymbol{U}$，右乘$\boldsymbol{V}^{\mathrm{T}}$，由于$\boldsymbol{U}$、$\boldsymbol{V}$都是正交矩阵，因此有</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{\boldsymbol { V } ^ { \mathrm { T } }}</script><p>上式称为矩阵的<code>奇异值分解</code>，对于$m \leqslant n$的情况，有类似的结果，此时</p>
<script type="math/tex; mode=display">
\boldsymbol{\Sigma}=\left(\begin{array}{ccccccc}
\sigma_{1} & 0 & \cdots & 0 & 0 & \cdots & \cdots \\
0 & \sigma_{2} & \cdots & 0 & 0 & \cdots & \cdots \\
\cdots & \cdots & \cdots & \cdots & 0 & \cdots & \cdots \\
0 & 0 & \cdots & \sigma_{m} & 0 & \cdots & \cdots
\end{array}\right)=\left(\boldsymbol{\Sigma}_{m} \mathbf{0}_{m \times(n-m)}\right)</script><p>下面证明$\boldsymbol{A} A^{\mathrm{T}}$与$\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}$有相同的非0特征值，假设$\lambda \neq 0$是$A A^{\mathrm{T}}$的特征值,$\boldsymbol{x}$是对应的特征向量，则有</p>
<script type="math/tex; mode=display">
\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}} \boldsymbol{x}=\lambda \boldsymbol{x}</script><p>上式两边同时左乘$\boldsymbol{A}^{\mathrm{T}}$可以得到</p>
<script type="math/tex; mode=display">
\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{A}^{\mathrm{T}} \boldsymbol{x}=\boldsymbol{A}^{\mathrm{T}} \lambda \boldsymbol{x}</script><p>即</p>
<script type="math/tex; mode=display">
\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}\left(\boldsymbol{A}^{\mathrm{T}} \boldsymbol{x}\right)=\lambda\left(\boldsymbol{A}^{\mathrm{T}} \boldsymbol{x}\right)</script><p>下面证明$\boldsymbol{A}^{\mathrm{T}} \boldsymbol{x} \neq \mathbf{0}$，式 (2.65) 两边同时左乘$\boldsymbol{x}^{\mathrm{T}}$, 由于$\lambda \neq 0, \boldsymbol{x} \neq \mathbf{0}$</p>
<script type="math/tex; mode=display">
\boldsymbol{x}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{A}^{\mathrm{T}} \boldsymbol{x}=\left(\boldsymbol{A}^{\mathrm{T}} \boldsymbol{x}\right)^{\mathrm{T}} \boldsymbol{A}^{\mathrm{T}} \boldsymbol{x}=\lambda \boldsymbol{x}^{\mathrm{T}} \boldsymbol{x}>0</script><p>因此$A^{\mathrm{T}} x \neq 0$，$\lambda$是$A^{\mathrm{T}} A$的特征值，$\boldsymbol{A}^{\mathrm{T}} \boldsymbol{x}$是对应的特征向量</p>
<p>同样，如果$\lambda \neq 0$是$A^{\mathrm{T}} A$的特征值,$\boldsymbol{x}$是对应的特征向量，则有</p>
<script type="math/tex; mode=display">
\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{x}=\lambda \boldsymbol{x}</script><p>上式两边同时左乘$\boldsymbol{A}$可以得到</p>
<script type="math/tex; mode=display">
\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}} \boldsymbol{A x}=\boldsymbol{A} \lambda \boldsymbol{x}</script><p>即</p>
<script type="math/tex; mode=display">
\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}}(\boldsymbol{A} \boldsymbol{x})=\lambda(\boldsymbol{A} \boldsymbol{x})</script><p>下面证明$A x \neq 0$，上上上式两边同时左乘$x^{\mathrm{T}}$，由于$\lambda \neq 0, x \neq 0$</p>
<script type="math/tex; mode=display">
\boldsymbol{x}^{\mathrm{T}} \boldsymbol{A}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{x}=(\boldsymbol{A} \boldsymbol{x})^{\mathrm{T}} \boldsymbol{A} \boldsymbol{x}=\boldsymbol{\lambda} \boldsymbol{x}^{\mathrm{T}} \boldsymbol{x}>0</script><p>因此$A x \neq 0$，$\lambda$是$A A^{\mathrm{T}}$的特征值，$A \boldsymbol{x}$是对应的特征向量</p>
<p>需要注意的是，$\boldsymbol{A A ^ { \mathrm { T } }}$的0特征值不一定是$A^{\mathrm{T}} \boldsymbol{A}$的0特征值，下面举例说明，对于如下的矩阵</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left(\begin{array}{ll}
1 & 0 \\
0 & 1 \\
0 & 0
\end{array}\right)</script><p>有</p>
<script type="math/tex; mode=display">
\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}}=\left(\begin{array}{ll}
1 & 0 \\
0 & 1 \\
0 & 0
\end{array}\right)\left(\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0
\end{array}\right)=\left(\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 0
\end{array}\right)</script><p>$A A^{\mathrm{T}}$的特征值为<script type="math/tex">\lambda_{1}=1, \lambda_{2}=1, \lambda_{3}=0</script></p>
<script type="math/tex; mode=display">
\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}=\left(\begin{array}{lll}
1 & 0 & 0 \\
0 & 1 & 0
\end{array}\right)\left(\begin{array}{ll}
1 & 0 \\
0 & 1 \\
0 & 0
\end{array}\right)=\left(\begin{array}{ll}
1 & 0 \\
0 & 1
\end{array}\right)</script><p>可知<script type="math/tex">\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}</script>特征值为<script type="math/tex">\lambda_{1}=1, \lambda_{2}=1</script>，0是<script type="math/tex">\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}}</script>的特征值但不是<script type="math/tex">\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}</script>的特征值</p>
<p>下面来看奇异值分解的一个例子。对于如下的矩阵</p>
<script type="math/tex; mode=display">
\boldsymbol{A}=\left(\begin{array}{cc}
-1 & 3 \\
3 & 1 \\
1 & 1
\end{array}\right)</script><p>有</p>
<script type="math/tex; mode=display">
\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}}=\left(\begin{array}{ccc}
10 & 0 & 2 \\
0 & 10 & 4 \\
2 & 4 & 2
\end{array}\right)</script><p>以及</p>
<script type="math/tex; mode=display">
\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}=\left(\begin{array}{cc}
11 & 1 \\
1 & 11
\end{array}\right)</script><p>这里<script type="math/tex">A A^{\mathrm{T}}</script>的特征值为<script type="math/tex">\lambda_{1}=12, \lambda_{2}=10, \lambda_{3}=0</script>，<script type="math/tex">\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}</script>的特征值为<script type="math/tex">\lambda_{1}=12, \lambda_{2}=10</script></p>
<p>因此$\boldsymbol{A}$的非0奇异值为<script type="math/tex">\sigma_{1}=\sqrt{12}</script>、<script type="math/tex">\sigma_{2}=\sqrt{10}</script></p>
<p>计算$\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}}$与$\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}$的特征向量并进行单位化，最后得到奇异值分解结果为</p>
<script type="math/tex; mode=display">
\boldsymbol{U}^{\mathrm{T}} \boldsymbol{A} \boldsymbol{V}=\left(\begin{array}{ccc}
\frac{1}{\sqrt{6}} & \frac{2}{\sqrt{6}} & \frac{1}{\sqrt{6}} \\
\frac{2}{\sqrt{5}} & -\frac{1}{\sqrt{5}} & 0 \\
\frac{1}{\sqrt{30}} & \frac{2}{\sqrt{30}} & -\frac{5}{\sqrt{30}}
\end{array}\right)^{\mathrm{T}}\left(\begin{array}{cc}
-1 & 3 \\
3 & 1 \\
1 & 1
\end{array}\right)\left(\begin{array}{cc}
\frac{1}{\sqrt{2}} & \frac{1}{\sqrt{2}} \\
\frac{1}{\sqrt{2}} & -\frac{1}{\sqrt{2}}
\end{array}\right)=\left(\begin{array}{cc}
\sqrt{12} & 0 \\
0 & \sqrt{10} \\
0 & 0
\end{array}\right)</script><p>如果$m \geqslant n$</p>
<script type="math/tex; mode=display">
\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}=\left(\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{\mathrm{T}}\right)^{\mathrm{T}} \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{\mathrm{T}}=\boldsymbol{V} \boldsymbol{\Sigma}^{\mathrm{T}} \boldsymbol{U}^{\mathrm{T}} \boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{\mathrm{T}}=\boldsymbol{V} \boldsymbol{\Sigma}^{\mathrm{T}} \boldsymbol{\Sigma} \boldsymbol{V}^{\mathrm{T}}</script><p>即</p>
<script type="math/tex; mode=display">
\boldsymbol{A}^{\mathrm{T}} \boldsymbol{A}=\boldsymbol{V} \boldsymbol{\Sigma}^{\mathrm{T}} \boldsymbol{\Sigma} \boldsymbol{V}^{\mathrm{T}}</script><p>在这里</p>
<script type="math/tex; mode=display">
\boldsymbol{\Sigma}^{\mathrm{T}} \boldsymbol{\Sigma}=\left(\begin{array}{c}
\boldsymbol{\Sigma}_{n} \\
\mathbf{0}_{(m-n) \times n}
\end{array}\right)^{\mathrm{T}}\left(\begin{array}{c}
\boldsymbol{\Sigma}_{n} \\
\mathbf{0}_{(m-n) \times n}
\end{array}\right)=\left(\boldsymbol{\Sigma}_{n} \mathbf{0}_{n \times(m-n)}\right)\left(\begin{array}{c}
\boldsymbol{\Sigma}_{n} \\
\mathbf{0}_{(m-n) \times n}
\end{array}\right)=\boldsymbol{\Sigma}_{n}^{2}</script><p>是$n$阶对角阵，上式就是$A^{\mathrm{T}} \boldsymbol{A}$的特征值分解</p>
<p>类似地有</p>
<script type="math/tex; mode=display">
\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}}=\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{\mathrm{T}}\left(\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{\mathrm{T}}\right)^{\mathrm{T}}=\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{\mathrm{T}} \boldsymbol{V} \boldsymbol{\Sigma}^{\mathrm{T}} \boldsymbol{U}^{\mathrm{T}}=\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{\Sigma}^{\mathrm{T}} \boldsymbol{U}^{\mathrm{T}}</script><p>即</p>
<script type="math/tex; mode=display">
\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}}=\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{\Sigma}^{\mathrm{T}} \boldsymbol{U}^{\mathrm{T}}</script><p>在这里</p>
<script type="math/tex; mode=display">
\begin{aligned}
\boldsymbol{\Sigma} \boldsymbol{\Sigma}^{\mathrm{T}} & =\left(\begin{array}{c}
\boldsymbol{\Sigma}_{n} \\
\mathbf{0}_{(m-n) \times n}
\end{array}\right)\left(\begin{array}{c}
\boldsymbol{\Sigma}_{n} \\
\mathbf{0}_{(m-n) \times n}
\end{array}\right)^{\mathrm{T}}=\left(\begin{array}{c}
\boldsymbol{\Sigma}_{n} \\
\mathbf{0}_{(m-n) \times n}
\end{array}\right)\left(\begin{array}{cc}
\boldsymbol{\Sigma}_{n} & \mathbf{0}_{n \times(m-n)}
\end{array}\right) \\
& =\left(\begin{array}{cc}
\boldsymbol{\Sigma}_{n}^{2} & \boldsymbol{\Sigma}_{n} \times \boldsymbol{0}_{n \times(m-n)} \\
\mathbf{0}_{(m-n) \times n} \times \boldsymbol{\Sigma}_{n} & \mathbf{0}_{(m-n) \times n} \times \mathbf{0}_{n \times(m-n)}
\end{array}\right)=\left(\begin{array}{cc}
\boldsymbol{\Sigma}_{n}^{2} & \boldsymbol{0}_{n \times(m-n)} \\
\mathbf{0}_{(m-n) \times n} & \mathbf{0}_{(m-n) \times(m-n)}
\end{array}\right)
\end{aligned}</script><p>是$m$阶对角阵，上上式就是$A A^{\mathrm{T}}$的特征值分解，对于$m \leqslant n$有相同的结论</p>
<p>如果$A$是对称矩阵，则$A^{\mathrm{T}} A=A A^{\mathrm{T}}=A \boldsymbol{A}$，因此$A^{\mathrm{T}} A$和$\boldsymbol{A} \boldsymbol{A}^{\mathrm{T}}$的特征值分解是相同的，这意味着$U$和$V$相同</p>
<p>假设$\lambda$是$A$的特征值，根据特征值的性质，$\lambda^{2}$是$A^{\mathrm{T}} \boldsymbol{A}$与$\boldsymbol{A A ^ { \mathrm { T } }}$的特征值，因此$A$的奇异值为其特征值的绝对值</p>
<script type="math/tex; mode=display">
\sigma=\sqrt{\lambda^{2}}=|\lambda|</script><p>Python中linalg的svd函数实现了奇异值分解，函数的输入值为被分解矩阵$A$，输出为正交矩阵$U$和$V^{\mathrm{T}}$，以及非0奇异值$\sigma_{i}$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line">data = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line">u, sigma, vt = linalg.svd(data)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(u)</span><br><span class="line"><span class="built_in">print</span>(sigma)</span><br><span class="line"><span class="built_in">print</span>(vt)</span><br></pre></td></tr></table></figure>
<p>输出结果如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[[-<span class="number">0.3863177</span>   -<span class="number">0.92236578</span>]</span><br><span class="line"> [-<span class="number">0.92236578</span>   <span class="number">0.3863177</span>]</span><br><span class="line"> [<span class="number">9.508032</span>      <span class="number">0.77286964</span>]] </span><br><span class="line"> </span><br><span class="line">[[-<span class="number">0.42866713</span>   -<span class="number">0.56630692</span>   -<span class="number">0.7039467</span>]</span><br><span class="line"> [<span class="number">0.80596391</span>    <span class="number">0.11238241</span>    -<span class="number">0.58119908</span>]</span><br><span class="line"> [<span class="number">0.40824829</span>   -<span class="number">0.81649658</span>    <span class="number">0.40824829</span>]]  </span><br></pre></td></tr></table></figure>
<p>这里的u是公式中的$U$，$\mathrm{vt}$是公式中的$V^{\mathrm{T}}$，sigma是所有非0奇异值，它们构成如下$2 \times 3$的矩阵$\boldsymbol{\Sigma}$</p>
<script type="math/tex; mode=display">
\left(\begin{array}{ccc}
9.508032 & 0 & 0 \\
0 & 0.7728694 & 0
\end{array}\right)</script><p>可以验证，这3个矩阵的乘积为原始矩阵</p>
<blockquote>
<p>奇异值分解的几何意义</p>
</blockquote>
<p>向量$\boldsymbol{x}$左乘任意矩阵$\boldsymbol{A}$所实现的线性变换可以分解为3次变换</p>
<script type="math/tex; mode=display">
\boldsymbol{A} \boldsymbol{x}=\boldsymbol{U} \boldsymbol{\Sigma} \boldsymbol{V}^{\mathrm{T}} \boldsymbol{x}</script><p>首先是$\boldsymbol{x}$左乘正交矩阵$V^{\mathrm{T}}$所代表的旋转变换，接下来是$V^{\mathrm{T}} x$左乘矩阵$\boldsymbol{\Sigma}$所代表的拉伸变换</p>
<p>最后是$\Sigma V^{\mathrm{T}} x$左乘正交矩阵$U$所代表的旋转变换</p>
<p>奇异值分解揭示了矩阵的本质特征，对分析矩阵的性质有重要的价值</p>
<p>在对人工神经网络权重矩阵的理论分析中，奇异值和奇异向量经常被使用，在图像压缩与推荐系统中，奇异值分解也有应用</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/narutohyc">narutohyc</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://study.hycbook.com/article/29825.html">https://study.hycbook.com/article/29825.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://study.hycbook.com" target="_blank">兼一书虫</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E6%95%B0%E5%AD%A6/">机器学习数学</a><a class="post-meta__tags" href="/tags/%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0/">线性代数</a><a class="post-meta__tags" href="/tags/%E7%9F%A9%E9%98%B5%E8%AE%BA/">矩阵论</a></div><div class="post_share"><div class="social-share" data-image="https://pic.hycbook.com/i/hexo/post_cover/蕾姆6.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><link rel="stylesheet" href="/" media="defer" onload="this.media='all'"/><div class="post-reward"><button class="tip-button reward-button"><span class="tip-button__text">打赏</span><div class="coin-wrapper"><div class="coin"><div class="coin__middle"></div><div class="coin__back"></div><div class="coin__front"></div></div></div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic.hycbook.com/i//hexo/qr_codes/hyc_wechat.webp" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/qr_codes/hyc_wechat.webp" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://pic.hycbook.com/i//hexo/qr_codes/hyc_alipay.webp" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/qr_codes/hyc_alipay.webp" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></button></div><audio id="coinAudio" src="https://s1.vika.cn/space/2022/10/29/6db0ad2bccf949f09054b3b206dcc66f?attname=马里奥游戏投币叮当.mp3"></audio><script defer="defer" src="/"></script><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/article/58496.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆5.webp" onerror="onerror=null;src='https://pic.hycbook.com/i/hexo/config_imgs/404.svg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">机器学习_线性代数与矩阵论(2)</div></div></a></div><div class="next-post pull-right"><a href="/article/41340.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆7.webp" onerror="onerror=null;src='https://pic.hycbook.com/i/hexo/config_imgs/404.svg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">机器学习_概率论(2)</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/article/58496.html" title="机器学习_线性代数与矩阵论(2)"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆5.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-08</div><div class="title">机器学习_线性代数与矩阵论(2)</div></div></a></div><div><a href="/article/47032.html" title="机器学习_线性代数与矩阵论(1)"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆1.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-10-11</div><div class="title">机器学习_线性代数与矩阵论(1)</div></div></a></div><div><a href="/article/8171.html" title="机器学习_一元函数微积分(2)"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆4.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-08</div><div class="title">机器学习_一元函数微积分(2)</div></div></a></div><div><a href="/article/10706.html" title="机器学习_一元函数微积分(1)"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆0.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-09-10</div><div class="title">机器学习_一元函数微积分(1)</div></div></a></div><div><a href="/article/41340.html" title="机器学习_概率论(2)"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆7.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-09-08</div><div class="title">机器学习_概率论(2)</div></div></a></div><div><a href="/article/58730.html" title="机器学习_最优化方法(1)"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆3.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-12-31</div><div class="title">机器学习_最优化方法(1)</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Twikoo</span><span class="switch-btn"></span><span class="second-comment">Valine</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BA%8C%E6%AC%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">二次型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">1.1.</span> <span class="toc-text">基本概念</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AD%A3%E5%AE%9A%E4%BA%8C%E6%AC%A1%E5%9E%8B%E4%B8%8E%E6%AD%A3%E5%AE%9A%E7%9F%A9%E9%98%B5"><span class="toc-number">1.2.</span> <span class="toc-text">正定二次型与正定矩阵</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%87%E5%87%86%E5%9E%8B"><span class="toc-number">1.3.</span> <span class="toc-text">标准型</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3"><span class="toc-number">2.</span> <span class="toc-text">矩阵分解</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A5%9A%E5%88%97%E6%96%AF%E5%9F%BA%E5%88%86%E8%A7%A3"><span class="toc-number">2.1.</span> <span class="toc-text">楚列斯基分解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#QR-%E5%88%86%E8%A7%A3"><span class="toc-number">2.2.</span> <span class="toc-text">QR 分解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%BE%81%E5%80%BC%E5%88%86%E8%A7%A3"><span class="toc-number">2.3.</span> <span class="toc-text">特征值分解</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3"><span class="toc-number">2.4.</span> <span class="toc-text">奇异值分解</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://pic.hycbook.com/i/hexo/config_imgs/footer_bg.webp')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By narutohyc</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><p><a target="_blank" href="https://hexo.io/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为Hexo"></a>&nbsp;<a target="_blank" href="https://demo.jerryc.me/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用butterfly"></a>&nbsp;<a target="_blank" href="https://vercel.com/ " rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vervel-brightgreen?style=flat&logo=Vercel" title="本站采用双线部署，默认线路托管于Vercel"></a>&nbsp;<a target="_blank" href="https://github.com/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由Gtihub托管"></a>&nbsp;<a target="_blank" href="https://zixiaoyun.com" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/图床-薄荷图床-green" title="薄荷图床"></a></p><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=35020502000647" rel="external nofollow noreferrer"><img style="position:relative;top:4px" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/config_imgs//备案图标.webp" alt="ICP"/>闽公网安备35020502000647号  </a><a href="https://beian.miit.gov.cn/" rel="external nofollow noreferrer" target="_blank">闽ICP备2022013843号-1</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="fa-solid fa-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="fa-solid fa-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="fa-solid fa-arrow-rotate-right"></i></div><div class="rightMenu-item" id="menu-home"><i class="fa-solid fa-house"></i></div></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:window.open(&quot;https://www.baidu.com/s?wd=&quot;+window.getSelection().toString());window.location.reload();" rel="external nofollow noreferrer"><i class="fas fa-comment"></i><span>百度搜索</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" href="/archives/"><i class="fa-solid fa-archive"></i><span>文章归档</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="fa-solid fa-folder-open"></i><span>文章分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="fa-solid fa-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuNormal"><a class="rightMenu-item menu-link" id="menu-radompage"><i class="fa-solid fa-shoe-prints"></i><span>随便逛逛</span></a><div class="rightMenu-item" id="menu-translate"><i class="fa-solid fa-earth-asia"></i><span>繁简切换</span></div><div class="rightMenu-item" id="menu-darkmode"><i class="fa-solid fa-moon"></i><span>切换模式</span></div></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? '' : ''

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://vercel.hycbook.com',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://vercel.hycbook.com',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'ncn88uooQf0IO2rrGE7Vniwp-gzGzoHsz',
      appKey: 'Yghpzg1QfBMFJ0MxxHubVzKL',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Twikoo' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'https://vercel.hycbook.com',
        region: '',
        pageSize: 3,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script defer src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script defer data-pjax src="/js/rightMenu.js"></script><script defer data-pjax src="/js/udf_mouse.js"></script><script defer data-pjax src="/js/udf_js.js"></script><script defer data-pjax src="/zhheo/random.js"></script><script data-pjax src="/js/coin.js"></script><script defer src="https://npm.elemecdn.com/vue@2.6.11"></script><script async src="//at.alicdn.com/t/c/font_3670467_a0sijt8frxo.js"></script><script defer src="/live2d-widget/autoload.js"></script><script defer src="/js/udf_js.js"></script><script defer src="https://cdnjs.cloudflare.com/ajax/libs/toastr.js/latest/toastr.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><script>window.$crisp = [];
window.CRISP_WEBSITE_ID = "561b80db-3f0f-45cb-b3b1-aae7355939e6";
(function () {
  d = document;
  s = d.createElement("script");
  s.src = "https://client.crisp.chat/l.js";
  s.async = 1;
  d.getElementsByTagName("head")[0].appendChild(s);
})();
$crisp.push(["safe", true])

if (false) {
  $crisp.push(["do", "chat:hide"])
  $crisp.push(["on", "chat:closed", function() {
    $crisp.push(["do", "chat:hide"])
  }])
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      $crisp.push(["do", "chat:show"])
      $crisp.push(["do", "chat:open"])

    });
  }
  chatBtnFn()
} else {
  if (false) {
    function chatBtnHide () {
      $crisp.push(["do", "chat:hide"])
    }
    function chatBtnShow () {
      $crisp.push(["do", "chat:show"])
    }
  }
}</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="app-refresh" id="app-refresh" style="position: fixed;top: -2.2rem;left: 0;right: 0;z-index: 99999;padding: 0 1rem;font-size: 15px;height: 2.2rem;transition: all 0.3s ease;"><div class="app-refresh-wrap" style=" display: flex;color: #fff;height: 100%;align-items: center;justify-content: center;"><label>✨ 兼一书虫上新啦！ 👉</label><a href="javascript:void(0)" rel="external nofollow noreferrer" onclick="location.reload()"><span style="color: #fff;text-decoration: underline;cursor: pointer;">🍭查看新品🍬</span></a></div></div><script>if ('serviceWorker' in navigator) {
  if (navigator.serviceWorker.controller) {
    navigator.serviceWorker.addEventListener('controllerchange', function() {
      showNotification()
    })
  }
  window.addEventListener('load', function() {
    navigator.serviceWorker.register('/sw.js')
  })
}

function showNotification() {
  if (GLOBAL_CONFIG.Snackbar) {
    var snackbarBg =
      document.documentElement.getAttribute('data-theme') === 'light' ?
      GLOBAL_CONFIG.Snackbar.bgLight :
      GLOBAL_CONFIG.Snackbar.bgDark
    var snackbarPos = GLOBAL_CONFIG.Snackbar.position
    Snackbar.show({
      text: '✨ 兼一书虫上新啦！ 👉',
      backgroundColor: snackbarBg,
      duration: 500000,
      pos: snackbarPos,
      actionText: '🍭查看新品🍬',
      actionTextColor: '#fff',
      onActionClick: function(e) {
        location.reload()
      },
    })
  } else {
    var showBg =
      document.documentElement.getAttribute('data-theme') === 'light' ?
      '#49b1f5' :
      '#1f1f1f'
    var cssText = `top: 0; background: ${showBg};`
    document.getElementById('app-refresh')
      .style.cssText = cssText
  }
}</script></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>