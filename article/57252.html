<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>LLM Tokenizer分词系列 | 兼一书虫</title><meta name="keywords" content="深度学习,Tokenizer"><meta name="author" content="narutohyc"><meta name="copyright" content="narutohyc"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="LLM Tokenizer分词系列">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM Tokenizer分词系列">
<meta property="og:url" content="https://study.hycbook.com/article/57252.html">
<meta property="og:site_name" content="兼一书虫">
<meta property="og:description" content="LLM Tokenizer分词系列">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic.hycbook.com/i/hexo/post_cover/%E8%95%BE%E5%A7%864.webp">
<meta property="article:published_time" content="2024-02-15T00:39:15.000Z">
<meta property="article:modified_time" content="2024-10-24T02:23:30.168Z">
<meta property="article:author" content="narutohyc">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Tokenizer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic.hycbook.com/i/hexo/post_cover/%E8%95%BE%E5%A7%864.webp"><link rel="shortcut icon" href="https://pic.hycbook.com/i//hexo/config_imgs/网站图标.webp"><link rel="canonical" href="https://study.hycbook.com/article/57252"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="manifest" href="/manifest.json"/><meta name="msapplication-TileColor" content="#c6ff7a"/><link rel="apple-touch-icon" sizes="180x180" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/favicon-16x16.png"/><link rel="mask-icon" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/safari-pinned-tab.svg" color="#5bbad5"/><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/hycBook/hycBookPage@resources/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?68340394dfd808cea9826e8a57f87aa6";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":120,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":200,"languages":{"author":"作者: narutohyc","link":"链接: ","source":"来源: 兼一书虫","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM Tokenizer分词系列',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-10-24 10:23:30'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/mainColor/heoMainColor.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/404/404.css"><script src="https://npm.elemecdn.com/echarts@4.9.0/dist/echarts.min.js"></script><link href="https://cdn.bootcdn.net/ajax/libs/toastr.js/2.1.4/toastr.min.css" rel="stylesheet"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/categoryBar/categoryBar.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/hycBook/hycBookPage@resources/css/hyc_udf.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/hycBook/hycBookPage@resources/css/udf_css.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/hycBook/hycBookPage@resources/css/year.css"><svg aria-hidden="true" style="position:absolute; overflow:hidden; width:0; height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248L626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/person_img/兼一头像.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">125</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">173</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fangwu"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 归档</span></a></li><li><a class="site-page child" href="/categories"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> 分类</span></a></li><li><a class="site-page child" href="/tags"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yuedu">             </use></svg><span> gitbook版</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://common.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> common</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://dl.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> deep learning</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://python.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> python</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://snooby.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> snooby</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://hycbook.flowus.cn"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang">                   </use></svg><span> flowus</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xuegao">             </use></svg><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yinle">                   </use></svg><span> 音乐</span></a></li><li><a class="site-page child" href="/bangumis"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wodezhuifan">                   </use></svg><span> 追番</span></a></li><li><a class="site-page child" href="/gallery"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xiangce">                   </use></svg><span> 相册</span></a></li><li><a class="site-page child" href="/video"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-shipin">                   </use></svg><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/charts"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xigua"></use></svg><span> 统计图</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-suannai">             </use></svg><span> 网盘</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://pan.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 私月盘</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://share.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> 共享盘</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifeiji">             </use></svg><span> 导航</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/comments"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-TIFFANYSROOM_huaban">                   </use></svg><span> 留言板</span></a></li><li><a class="site-page child" href="/link"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-friends">                   </use></svg><span> 友链</span></a></li><li><a class="site-page child" href="/about"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-aixin">                   </use></svg><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic.hycbook.com/i/hexo/post_imgs/蕾姆4.webp')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">兼一书虫</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fangwu"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 归档</span></a></li><li><a class="site-page child" href="/categories"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> 分类</span></a></li><li><a class="site-page child" href="/tags"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yuedu">             </use></svg><span> gitbook版</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://common.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> common</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://dl.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> deep learning</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://python.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> python</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://snooby.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> snooby</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://hycbook.flowus.cn"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang">                   </use></svg><span> flowus</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xuegao">             </use></svg><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yinle">                   </use></svg><span> 音乐</span></a></li><li><a class="site-page child" href="/bangumis"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wodezhuifan">                   </use></svg><span> 追番</span></a></li><li><a class="site-page child" href="/gallery"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xiangce">                   </use></svg><span> 相册</span></a></li><li><a class="site-page child" href="/video"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-shipin">                   </use></svg><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/charts"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xigua"></use></svg><span> 统计图</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-suannai">             </use></svg><span> 网盘</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://pan.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 私月盘</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://share.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> 共享盘</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifeiji">             </use></svg><span> 导航</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/comments"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-TIFFANYSROOM_huaban">                   </use></svg><span> 留言板</span></a></li><li><a class="site-page child" href="/link"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-friends">                   </use></svg><span> 友链</span></a></li><li><a class="site-page child" href="/about"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-aixin">                   </use></svg><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">LLM Tokenizer分词系列</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2024-02-15T00:39:15.000Z" title="发表于 2024-02-15 08:39:15">2024-02-15</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-10-24T02:23:30.168Z" title="更新于 2024-10-24 10:23:30">2024-10-24</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/deep-learning/">deep-learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">6.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>22分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title="LLM Tokenizer分词系列"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="twikoo_visitors"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/article/57252.html#post-comment"><span id="twikoo-count"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><hr>
<h1 id="tokenizer"><a href="#tokenizer" class="headerlink" title="tokenizer"></a>tokenizer</h1><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/tokenizers/index">hugging face Tokenizer文档</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/tokenizer_summary">huggingface的分词器的摘要</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.360doc.com/content/23/0605/20/7673502_1083606595.shtml">【LLM系列之Tokenizer】如何科学地训练一个LLM分词器</a></p>
</blockquote>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>文本分词的过程涉及将文本拆分成多个单词或子单词。接着，这些单词或子单词会被映射到特定的ID，转换过程涉及一个查找表，这是一种简单的对应关系</p>
<p>因此，我们的主要关注点在于解析文本为一系列的单词或子单词</p>
<p>更具体地说，我们将探讨🤗 Transformers库中常用的三种主要分词器类型：<strong>Byte-Pair Encoding (BPE)</strong>、<strong>WordPiece</strong>和<strong>SentencePiece</strong>，并且我们将提供实例说明哪种模型采用了哪种分词器</p>
<p>要了解特定预训练模型使用了哪种分词器，你可以参考每个模型主页上的文档说明，例如BertTokenizer，你会发现模型采用的是<strong>WordPiece分词器</strong></p>
<h2 id="分词例子"><a href="#分词例子" class="headerlink" title="分词例子"></a>分词例子</h2><p>将一段文本分词到小块是一个比它看起来更加困难的任务，并且有很多方式来实现分词，举个例子，让我们看看这个句子</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&quot;Don&#x27;t you love 🤗 Transformers? We sure <span class="keyword">do</span>.&quot;</span><br></pre></td></tr></table></figure>
<p>对这段文本分词的一个简单方式，就是使用空格来分词，得到的结果是：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&quot;Don&#x27;t&quot;, &quot;you&quot;, &quot;love&quot;, &quot;🤗&quot;, &quot;Transformers?&quot;, &quot;We&quot;, &quot;sure&quot;, &quot;<span class="keyword">do</span>.&quot;]</span><br></pre></td></tr></table></figure>
<p>上面的分词是一个明智的开始，但是如果我们查看token <code>&quot;Transformers?&quot;</code> 和 <code>&quot;do.&quot;</code>，我们可以观察到标点符号附在单词<code>&quot;Transformer&quot;</code> 和 <code>&quot;do&quot;</code>的后面，这并不是最理想的情况</p>
<p>我们应该将标点符号考虑进来，这样一个模型就没必要学习一个单词和每个可能跟在后面的 标点符号的不同的组合，这么组合的话，模型需要学习的组合的数量会急剧上升。将标点符号也考虑进来，对范例文本进行分词的结果就是：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&quot;Don&quot;, &quot;&#x27;&quot;, &quot;t&quot;, &quot;you&quot;, &quot;love&quot;, &quot;🤗&quot;, &quot;Transformers&quot;, &quot;?&quot;, &quot;We&quot;, &quot;sure&quot;, &quot;<span class="keyword">do</span>&quot;, &quot;.&quot;]</span><br></pre></td></tr></table></figure>
<p>分词的结果更好了，然而，这么做也是不好的，分词怎么处理单词<code>&quot;Don&#39;t&quot;</code>，<code>&quot;Don&#39;t&quot;</code>的含义是<code>&quot;do not&quot;</code>，所以这么分词<code>[&quot;Do&quot;, &quot;n&#39;t&quot;]</code> 会更好</p>
<p>现在开始事情就开始变得复杂起来了，部分的原因是每个模型都有它自己的分词类型</p>
<p>依赖于我们应用在文本分词上的规则， 相同的文本会产生不同的分词输出</p>
<p>用在训练数据上的分词规则，被用来对输入做分词操作，一个预训练模型才会正确的执行</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://spacy.io/">spaCy</a> and <a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.statmt.org/moses/?n=Development.GetStarted">Moses</a> 是两个受欢迎的<code>基于规则的分词器</code>，将这两个分词器应用在示例文本上，<em>spaCy</em> 和 <em>Moses</em>会输出类似下面的结果：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&quot;<span class="keyword">Do</span>&quot;, &quot;n&#x27;t&quot;, &quot;you&quot;, &quot;love&quot;, &quot;🤗&quot;, &quot;Transformers&quot;, &quot;?&quot;, &quot;We&quot;, &quot;sure&quot;, &quot;<span class="keyword">do</span>&quot;, &quot;.&quot;]</span><br></pre></td></tr></table></figure>
<p>可见上面的分词使用到了空格和标点符号的分词方式，以及基于规则的分词方式</p>
<p>空格和标点符号分词以及基于规则的分词都是单词分词的例子，<code>不那么严格的来说，单词分词的定义就是将句子分割到很多单词</code></p>
<p>然而将文本分割到更小的块是符合直觉的，当处理大型文本语料库时，上面的 分词方法会导致很多问题</p>
<p>在这种情况下，空格和标点符号分词通常会产生一个非常大的词典（使用到的所有不重复的单词和tokens的集合）</p>
<p>像：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/transformerxl">Transformer XL</a>使用空格和标点符号分词，结果会产生一个大小是267,735的词典</p>
<p>这么大的一个词典容量，迫使模型有着一个巨大的embedding矩阵，以及巨大的输入和输出层，这会增加内存使用量，也会提高时间复杂度</p>
<p>通常情况下，transformers模型几乎没有词典容量大于50,000的，特别是只在一种语言上预训练的模型</p>
<blockquote>
<p>所以如果简单的空格和标点符号分词让人不满意，为什么不简单的对字符分词</p>
</blockquote>
<p>尽管字符分词是非常简单的，并且能极大的减少内存使用，降低时间复杂度，但是这样做会让模型很难学到有意义的输入表达</p>
<p>像： 比起学到单词<code>&quot;today&quot;</code>的一个有意义的上下文独立的表达，学到字母<code>&quot;t&quot;</code>的一个有意义的上下文独立的表达是相当困难的</p>
<p>因此，字符分词经常会伴随着性能的下降。所以为了获得最好的结果，transformers模型在单词级别分词和字符级别分词之间使用了一个折中的方案被称作<strong>子词分词</strong></p>
<h2 id="分词粒度"><a href="#分词粒度" class="headerlink" title="分词粒度"></a>分词粒度</h2><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/weixin_37447415/article/details/126583754">NLP中Tokenizers总结（BPE、WordPiece、Unigram和SentencePiece）</a></p>
</blockquote>
<p>在<a target="_blank" rel="noopener external nofollow noreferrer" href="https://so.csdn.net/so/search?q=NLP&amp;spm=1001.2101.3001.7020">NLP</a>中，模型如Bert、GPT）的输入通常需要先进行tokenize，其目的是<strong>将输入的文本流，切分为一个个子串，每个子串都有完整的语义</strong>，便于学习embedding表达和后续模型的使用。tokenize有三种粒度：<strong>word/subword/char</strong></p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/huggingface基本使用教程/LLM%20Tokenizer分词系列/Tokenizer不同粒度.svg" alt="Tokenizer不同粒度"></p>
<ul>
<li><p><strong>word/词</strong>：词是最自然的语言单元，对于英文来说其天然存在空格进行，切分相对容易，常用的分词器有<a target="_blank" rel="noopener external nofollow noreferrer" href="https://spacy.io/">spaCy</a>和<a target="_blank" rel="noopener external nofollow noreferrer" href="http://www2.statmt.org/moses/?n=Development.GetStarted">Moses</a> </p>
<p>中文不具备这样的分割符，所以相对困难一些，不过目前也有Jieba、HanLP、LTP等分词器，这些分词器基于规则与模型，可以取得良好的分词效果</p>
<p>使用词时会有2个问题，通常情况下<strong>词表大小不超过5w</strong>：</p>
<ol>
<li>词表通常是基于语料进行分词获得，但遇到新的语料时可能会出现<strong>OOV</strong>的情况</li>
<li>词表过于庞大，对于模型来说大部分参数都集中在输入输出层，不利于模型学习，且容易<strong>爆内存</strong>（显存）</li>
</ol>
</li>
<li><p><strong>char/字符</strong>：字符是一种语言最基本的组成单元，如英文中的’a’、’b’、’c’或中文中的‘你’、‘我’、‘他’等，使用字符有如下问题：</p>
<ol>
<li>字符数量是有限的通常数量较少，这样在学习每个字符的embedding向量时，每个字符中包含非常多的语义，<strong>学习起来比较困难</strong></li>
<li>以字符分割，会造成序列长度过长，对后续应用造成较大限制</li>
</ol>
</li>
<li><p><strong>subword/子词</strong>：它介于char和word之间，可以很好的<strong>平衡词汇量和语义独立性</strong>，它的切分准则是<strong>常用的词不被切分，而不常见的词切分为子词</strong></p>
</li>
</ul>
<h1 id="子词分词"><a href="#子词分词" class="headerlink" title="子词分词"></a>子词分词</h1><blockquote>
<p>子词分词原则</p>
</blockquote>
<p><strong>子词分词算法</strong>依赖这样的原则：</p>
<ol>
<li>频繁使用的单词不应该被分割成更小的子词</li>
<li>很少使用的单词应该被分解到有意义的子词</li>
</ol>
<p>举个例子： <code>&quot;annoyingly&quot;</code>能被看作一个很少使用的单词，能被分解成<code>&quot;annoying&quot;</code>和<code>&quot;ly&quot;</code></p>
<p><code>&quot;annoying&quot;</code>和<code>&quot;ly&quot;</code>作为独立地子词，出现的次数都很频繁，而且与此同时单词<code>&quot;annoyingly&quot;</code>的含义可以通过组合<code>&quot;annoying&quot;</code>和<code>&quot;ly&quot;</code>的含义来获得</p>
<hr>
<p>在粘合和胶水语言上，像Turkish语言，这么做是相当有用的，在这样的语言里，通过线性组合子词，大多数情况下你能形成任意长的复杂的单词</p>
<p>子词分词允许模型有一个合理的词典大小，而且能学到有意义的上下文独立地表达</p>
<p>除此以外，子词分词可以让模型处理以前从来没见过的单词， 方式是通过分解这些单词到已知的子词，举个例子：<code>BertTokenizer</code>对句子<code>&quot;I have a new GPU!&quot;</code>分词的结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer = BertTokenizer.from_pretrained(<span class="string">&quot;bert-base-uncased&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tokenizer.tokenize(<span class="string">&quot;I have a new GPU!&quot;</span>)</span><br><span class="line">[<span class="string">&quot;i&quot;</span>, <span class="string">&quot;have&quot;</span>, <span class="string">&quot;a&quot;</span>, <span class="string">&quot;new&quot;</span>, <span class="string">&quot;gp&quot;</span>, <span class="string">&quot;##u&quot;</span>, <span class="string">&quot;!&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>因为我们正在考虑不区分大小写的模型，句子首先被转换成小写字母形式</p>
<p>我们可以见到单词<code>[&quot;i&quot;, &quot;have&quot;, &quot;a&quot;, &quot;new&quot;]</code>在分词器的词典内，但是这个单词<code>&quot;gpu&quot;</code>不在词典内</p>
<p>所以，分词器将<code>&quot;gpu&quot;</code>分割成已知的子词<code>[&quot;gp&quot; and &quot;##u&quot;]</code></p>
<p><code>&quot;##&quot;</code>意味着剩下的 token应该附着在前面那个token的后面，不带空格的附着（分词的解码或者反向）</p>
<p>另外一个例子，<code>XLNetTokenizer</code>对前面的文本例子分词结果如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> XLNetTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = XLNetTokenizer.from_pretrained(<span class="string">&quot;xlnet-base-cased&quot;</span>)</span><br><span class="line">tokenizer.tokenize(<span class="string">&quot;Don&#x27;t you love 🤗 Transformers? We sure do.&quot;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>[<span class="string">&quot;▁Don&quot;</span>, <span class="string">&quot;&#x27;&quot;</span>, <span class="string">&quot;t&quot;</span>, <span class="string">&quot;▁you&quot;</span>, <span class="string">&quot;▁love&quot;</span>, <span class="string">&quot;▁&quot;</span>, <span class="string">&quot;🤗&quot;</span>, <span class="string">&quot;▁&quot;</span>, <span class="string">&quot;Transform&quot;</span>, <span class="string">&quot;ers&quot;</span>, <span class="string">&quot;?&quot;</span>, <span class="string">&quot;▁We&quot;</span>, <span class="string">&quot;▁sure&quot;</span>, <span class="string">&quot;▁do&quot;</span>, <span class="string">&quot;.&quot;</span>]</span><br></pre></td></tr></table></figure>
<p>当我们查看<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/tokenizer_summary#sentencepiece">SentencePiece</a>时会回过头来解释这些<code>&quot;▁&quot;</code>符号的含义。正如你能见到的，很少使用的单词 <code>&quot;Transformers&quot;</code>能被分割到更加频繁使用的子词<code>&quot;Transform&quot;</code>和<code>&quot;ers&quot;</code></p>
<p>现在让我们来看看不同的子词分割算法是怎么工作的，注意到所有的这些分词算法依赖于某些训练的方式，这些训练通常在语料库上完成， 相应的模型也是在这个语料库上训练的</p>
<h2 id="Byte-Pair-Encoding-BPE"><a href="#Byte-Pair-Encoding-BPE" class="headerlink" title="Byte-Pair Encoding (BPE)"></a>Byte-Pair Encoding (BPE)</h2><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/abs/1508.07909">BPE-Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://leimao.github.io/blog/Byte-Pair-Encoding/">Byte Pair Encoding</a></p>
</blockquote>
<p>BPE依赖于一个预分词器，这个预分词器会将训练数据分割成单词。预分词可以是简单的 空格分词，像：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/gpt2">GPT-2</a>，<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/roberta">RoBERTa</a></p>
<p>更加先进的预分词方式包括了基于规则的分词，像：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/xlm">XLM</a>，<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/flaubert">FlauBERT</a>，FlauBERT在大多数语言使用了Moses，或者<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/gpt">GPT</a>，GPT使用了Spacy和ftfy，统计了训练语料库中每个单词的频次</p>
<p>在预分词以后，生成了单词的集合，也确定了训练数据中每个单词出现的频次</p>
<p>下一步，<strong>BPE产生了一个基础词典，包含了集合中所有的符号</strong>，<code>BPE学习融合的规则-组合基础词典中的两个符号来形成一个新的符号</code></p>
<p>BPE会一直学习直到词典的大小满足了期望的词典大小的要求。注意到 期望的词典大小是一个超参数，在训练这个分词器以前就需要人为指定</p>
<p>举个例子，让我们假设在预分词以后，下面的单词集合以及他们的频次都已经确定好了：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(&quot;hug&quot;, <span class="number">10</span>), (&quot;pug&quot;, <span class="number">5</span>), (&quot;pun&quot;, <span class="number">12</span>), (&quot;bun&quot;, <span class="number">4</span>), (&quot;hugs&quot;, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>所以，基础的词典是<code>[&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;]</code>，将所有单词分割成基础词典内的符号，就可以获得：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(&quot;h&quot; &quot;u&quot; &quot;g&quot;, <span class="number">10</span>), (&quot;p&quot; &quot;u&quot; &quot;g&quot;, <span class="number">5</span>), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, <span class="number">12</span>), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, <span class="number">4</span>), (&quot;h&quot; &quot;u&quot; &quot;g&quot; &quot;s&quot;, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>BPE接着会统计每个可能的符号对的频次，然后挑出出现最频繁的的符号对，在上面的例子中，<code>&quot;h&quot;</code>跟了<code>&quot;u&quot;</code>出现了10 + 5 = 15次 （10次是出现了10次<code>&quot;hug&quot;</code>，5次是出现了5次<code>&quot;hugs&quot;</code>）</p>
<p>然而，最频繁的符号对是<code>&quot;u&quot;</code>后面跟了个<code>&quot;g&quot;</code>，总共出现了10 + 5 + 5 = 20次</p>
<p>因此，分词器学到的第一个融合规则是组合所有的<code>&quot;u&quot;</code>后面跟了个<code>&quot;g&quot;</code>符号</p>
<p>下一步，<code>&quot;ug&quot;</code>被加入到了词典内。单词的集合就变成了：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(&quot;h&quot; &quot;ug&quot;, <span class="number">10</span>), (&quot;p&quot; &quot;ug&quot;, <span class="number">5</span>), (&quot;p&quot; &quot;u&quot; &quot;n&quot;, <span class="number">12</span>), (&quot;b&quot; &quot;u&quot; &quot;n&quot;, <span class="number">4</span>), (&quot;h&quot; &quot;ug&quot; &quot;s&quot;, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>BPE接着会统计出下一个最普遍的出现频次最大的符号对，也就是<code>&quot;u&quot;</code>后面跟了个<code>&quot;n&quot;</code>，出现了16次，<code>&quot;u&quot;</code>，<code>&quot;n&quot;</code>被融合成了<code>&quot;un&quot;</code>。</p>
<p>也被加入到了词典中，再下一个出现频次最大的符号对是<code>&quot;h&quot;</code>后面跟了个<code>&quot;ug&quot;</code>，出现了15次</p>
<p>又一次这个符号对被融合成了<code>&quot;hug&quot;</code>， 也被加入到了词典中</p>
<p>在当前这步，词典是<code>[&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;, &quot;hug&quot;]</code>，我们的单词集合则是：</p>
<figure class="highlight cmd"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(&quot;hug&quot;, <span class="number">10</span>), (&quot;p&quot; &quot;ug&quot;, <span class="number">5</span>), (&quot;p&quot; &quot;un&quot;, <span class="number">12</span>), (&quot;b&quot; &quot;un&quot;, <span class="number">4</span>), (&quot;hug&quot; &quot;s&quot;, <span class="number">5</span>)</span><br></pre></td></tr></table></figure>
<p>假设，the Byte-Pair Encoding在这个时候停止训练，学到的融合规则并应用到其他新的单词上（只要这些新单词不包括不在基础词典内的符号 就行）</p>
<p>举个例子，单词<code>&quot;bug&quot;</code>会被分词到<code>[&quot;b&quot;, &quot;ug&quot;]</code>，但是<code>&quot;mug&quot;</code>会被分词到<code>[&quot;&lt;unk&gt;&quot;, &quot;ug&quot;]</code>，因为符号<code>&quot;m&quot;</code>不在基础词典内</p>
<p>通常来看的话，单个字母像<code>&quot;m&quot;</code>不会被<code>&quot;&lt;unk&gt;&quot;</code>符号替换掉，因为训练数据通常包括了每个字母，每个字母至少出现了一次，但是在特殊的符号 中也可能发生像emojis</p>
<p>就像之前提到的那样，词典的大小，举个例子，基础词典的大小 + 融合的数量，是一个需要配置的超参数</p>
<p>举个例子：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/gpt">GPT</a> 的词典大小是40,478，因为GPT有着478个基础词典内的字符，在40,000次融合以后选择了停止训练</p>
<h3 id="Byte-level-BPE"><a href="#Byte-level-BPE" class="headerlink" title="Byte-level BPE"></a>Byte-level BPE</h3><p>一个包含了所有可能的基础字符的基础字典可能会非常大，如果考虑将所有的unicode字符作为基础字符</p>
<p>为了拥有一个更好的基础词典，<a target="_blank" rel="noopener external nofollow noreferrer" href="https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">GPT-2</a>使用了字节 作为基础词典，这是一个非常聪明的技巧，迫使基础词典是256大小，而且确保了所有基础字符包含在这个词典内。使用了其他的规则来处理标点符号，这个GPT2的分词器能对每个文本进行分词，不需要使用到<unk>符号。<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/gpt">GPT-2</a>有一个大小是50,257 的词典，对应到256字节的基础tokens，一个特殊的文本结束token，这些符号经过了50,000次融合学习</p>
<h2 id="WordPiece"><a href="#WordPiece" class="headerlink" title="WordPiece"></a>WordPiece</h2><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf">Japanese and Korean Voice Search (Schuster et al., 2012)</a></p>
</blockquote>
<p>WordPiece是子词分词算法，被用在<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/bert">BERT</a>，<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/distilbert">DistilBERT</a>，和<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/electra">Electra</a>，和BPE非常相似</p>
<p>WordPiece首先初始化一个词典，这个词典包含了出现在训练数据中的每个字符，然后递进的学习一个给定数量的融合规则</p>
<p>和BPE相比较， WordPiece不会选择出现频次最大的符号对，而是选择了加入到字典以后能最大化训练数据似然值的符号对</p>
<p>所以这到底意味着什么？参考前面的例子，最大化训练数据的似然值，等价于找到一个符号对，它们的概率除以这个符号对中第一个符号的概率，接着除以第二个符号的概率，在所有的符号对中商最大</p>
<p>像：如果<code>&quot;ug&quot;</code>的概率除以<code>&quot;u&quot;</code>除以<code>&quot;g&quot;</code>的概率的商，比其他任何符号对更大， 这个时候才能融合<code>&quot;u&quot;</code>和<code>&quot;g&quot;</code></p>
<p>直觉上，WordPiece，和BPE有点点不同，WordPiece是评估融合两个符号会失去的量，来确保这么做是值得的</p>
<h2 id="Unigram"><a href="#Unigram" class="headerlink" title="Unigram"></a>Unigram</h2><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/pdf/1804.10959.pdf">Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, 2018)</a></p>
</blockquote>
<p>Unigram是一个子词分词器算法，和BPE或者WordPiece相比较 ，Unigram使用大量的符号来初始化它的基础字典，然后逐渐的精简每个符号来获得一个更小的词典。举例来看基础词典能够对应所有的预分词 的单词以及最常见的子字符串。Unigram没有直接用在任何transformers的任何模型中，但是和<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/tokenizer_summary#sentencepiece">SentencePiece</a>一起联合使用。</p>
<p>在每个训练的步骤，Unigram算法在当前词典的训练数据上定义了一个损失函数（经常定义为log似然函数的），还定义了一个unigram语言模型。 然后，对词典内的每个符号，算法会计算如果这个符号从词典内移除，总的损失会升高多少</p>
<p>Unigram然后会移除百分之p的符号，这些符号的loss 升高是最低的（p通常是10%或者20%），像：这些在训练数据上对总的损失影响最小的符号</p>
<p>重复这个过程，直到词典已经达到了期望的大小。 为了任何单词都能被分词，Unigram算法总是保留基础的字符</p>
<p>因为Unigram不是基于融合规则（和BPE以及WordPiece相比较），在训练以后算法有几种方式来分词，如果一个训练好的Unigram分词器 的词典是这个：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&quot;b&quot;, &quot;g&quot;, &quot;h&quot;, &quot;n&quot;, &quot;p&quot;, &quot;s&quot;, &quot;u&quot;, &quot;ug&quot;, &quot;un&quot;, &quot;hug&quot;],</span><br></pre></td></tr></table></figure>
<p><code>&quot;hugs&quot;</code>可以被分词成<code>[&quot;hug&quot;, &quot;s&quot;]</code>, <code>[&quot;h&quot;, &quot;ug&quot;, &quot;s&quot;]</code>或者<code>[&quot;h&quot;, &quot;u&quot;, &quot;g&quot;, &quot;s&quot;]</code></p>
<p>所以选择哪一个呢？Unigram在保存词典的时候还会保存训练语料库内每个token的概率，所以在训练以后可以计算每个可能的分词结果的概率</p>
<p>实际上算法简单的选择概率最大的那个分词结果，但是也会提供概率来根据分词结果的概率来采样一个可能的分词结果</p>
<p>分词器在损失函数上训练，这些损失函数定义了这些概率</p>
<p>假设训练数据包含了这些单词 <script type="math/tex">x*{1}, \dots, x*{N}</script>，一个单词<script type="math/tex">x*{i}</script>的所有可能的分词结果的集合定义为<script type="math/tex">S(x*{i})</script>，然后总的损失就可以定义为：</p>
<script type="math/tex; mode=display">
L = - \sum _{i=1}^{N}{log ( \sum _{x \in S(x_i)}{p(x)})}</script><h2 id="SentencePiece"><a href="#SentencePiece" class="headerlink" title="SentencePiece"></a>SentencePiece</h2><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/pdf/1808.06226.pdf">SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Kudo et al., 2018)</a></p>
</blockquote>
<p>目前为止描述的所有分词算法都有相同的问题：它们都假设输入的文本使用空格来分开单词，然而，不是所有的语言都使用空格来分开单词</p>
<p>一个可能的解决方案是使用某种语言特定的预分词器。像：<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/xlm">XLM</a>使用了一个特定的中文、日语和Thai的预分词器</p>
<p>为了更加广泛的解决这个问题，<code>SentencePiece</code>将输入文本看作一个原始的输入流，因此使用的符合集合中也包括了空格</p>
<p>SentencePiece然后会使用BPE或者unigram算法来产生合适的词典</p>
<p>举例来说，<code>XLNetTokenizer</code>使用了SentencePiece，这也是为什么上面的例子中<code>&quot;▁&quot;</code>符号包含在词典内</p>
<p>SentencePiece解码是非常容易的，因为所有的tokens能被concatenate起来，然后将<code>&quot;▁&quot;</code>替换成空格</p>
<p>库内所有使用了SentencePiece的transformers模型，会和unigram组合起来使用，像：使用了SentencePiece的模型是<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/albert">ALBERT</a>, <a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/xlnet">XLNet</a>，<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/marian">Marian</a>，和<a target="_blank" rel="noopener external nofollow noreferrer" href="https://huggingface.co/docs/transformers/v4.37.2/zh/model_doc/t5">T5</a></p>
<h1 id="训练分词器"><a href="#训练分词器" class="headerlink" title="训练分词器"></a>训练分词器</h1><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/625715830">大模型基础知识系列：从头训练一个自己的Tokenizer</a></p>
</blockquote>
<p>当前，预训练语言模型已成为NLP算法工程师的工具箱中的常客。在实际应用中，几乎所有的NLP模型都依赖于分词器（Tokenizer）来处理文本数据</p>
<p>虽然通常我们会倾向于使用现成的分词器，但有时候创建一个定制化的分词器也是必要的</p>
<p>对于分词器的构建，通常可以选择使用sentencepiece或者huggingface的tokenizers库，我们可以采用tokenizers库来训练我们自己的分词器，确保tokenizers库已经安装在你的系统上</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install tokenizers</span><br></pre></td></tr></table></figure>
<p>有了tokenizers库，我们可以开始构建我们的Tokenizer。这个过程包括配置多个组件以自定Tokenizer的行为，包括但不限于：</p>
<ul>
<li><strong>模型（Models）</strong>：这是Tokenizer的核心，负责实际的分词操作。可选的模型包括WordLevel、BPE、Unigram和WordPiece</li>
<li><strong>规范化器（Normalizers）</strong>：规范化器用于预处理输入文本，将其转换为标准化的格式，如进行Unicode规范化或转换为小写，同时跟踪与原始文本的对齐关系</li>
<li><strong>预分词器（PreTokenizers）</strong>：预分词器按照一定规则拆分输入文本，以确保底层模型按照这些预设边界构建令牌</li>
<li><strong>后处理器（PostProcessors）</strong>：在分词流程完成后，后处理器负责在标记化后的字符串中插入特殊标记，比如说为模型提供标准格式的字符串</li>
<li><strong>解码器（Decoders）</strong>：解码器能够将分词器生成的ID转换回人类可读的文本</li>
</ul>
<p>这些组件的组合使得Tokenizer不仅能够执行基本的分词任务，还能为特定的NLP问题提供定制化的解决方案</p>
<p>在调用 Tokenizer.encode 或 Tokenizer.encode_batch 时，输入文本将经过以下流程：</p>
<ol>
<li>规范化</li>
<li>预分词</li>
<li>模型处理</li>
<li>后处理</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env Python</span></span><br><span class="line"><span class="comment"># -- coding: utf-8 --</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">@version: v1.0</span></span><br><span class="line"><span class="string">@author: huangyc</span></span><br><span class="line"><span class="string">@file: train_tokenizer.py</span></span><br><span class="line"><span class="string">@Description: </span></span><br><span class="line"><span class="string">@time: 2024/2/15 9:42</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> load_dataset</span><br><span class="line"><span class="keyword">from</span> tokenizers.models <span class="keyword">import</span> BPE</span><br><span class="line"><span class="keyword">from</span> tokenizers.normalizers <span class="keyword">import</span> NFD, StripAccents</span><br><span class="line"><span class="keyword">from</span> tokenizers.pre_tokenizers <span class="keyword">import</span> Whitespace, Punctuation, Digits, ByteLevel</span><br><span class="line"><span class="keyword">from</span> tokenizers.trainers <span class="keyword">import</span> BpeTrainer</span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> Tokenizer, normalizers, pre_tokenizers, decoders</span><br><span class="line"><span class="keyword">from</span> tokenizers <span class="keyword">import</span> tokenizers</span><br><span class="line"><span class="keyword">from</span> tokenizers.processors <span class="keyword">import</span> TemplateProcessing</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">batch_iterator</span>(<span class="params">batch_size=<span class="number">10000</span></span>):</span><br><span class="line">    dataset = load_dataset(<span class="string">&quot;TurboPascal/tokenizers_example_zh_en&quot;</span>, cache_dir=<span class="string">&#x27;./cache/&#x27;</span>)</span><br><span class="line">    <span class="built_in">print</span>(dataset)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, <span class="built_in">len</span>(dataset), batch_size):</span><br><span class="line">        <span class="keyword">yield</span> dataset[<span class="string">&#x27;train&#x27;</span>][i: i + batch_size][<span class="string">&quot;text&quot;</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_tokenizer</span>():</span><br><span class="line">    <span class="comment"># 自定数据集</span></span><br><span class="line">    data_files = [<span class="string">r&quot;.\data\dataset_hyc.txt&quot;</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义tokenizer</span></span><br><span class="line">    tokenizer = Tokenizer(BPE(unk_token=<span class="string">&quot;[UNK]&quot;</span>))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义一个归一化对象</span></span><br><span class="line">    normalizer = normalizers.<span class="type">Sequence</span>([NFD(), StripAccents()])</span><br><span class="line">    tokenizer.normalizer = normalizer</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 我们主要使用四类分割，空白、标点符号、数字、Bytelevel</span></span><br><span class="line">    <span class="comment"># pre_tokenizer = pre_tokenizers.Sequence([Whitespace(), Punctuation(), Digits(individual_digits=True), ByteLevel()])</span></span><br><span class="line">    <span class="comment"># tokenizer.pre_tokenizer = pre_tokenizer</span></span><br><span class="line">    <span class="comment"># 使用空白分割</span></span><br><span class="line">    tokenizer.pre_tokenizer = Whitespace()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 解码器</span></span><br><span class="line">    tokenizer.decoder = decoders.ByteLevel(add_prefix_space=<span class="literal">True</span>, use_regex=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 字节级 BPE 可能在生成的令牌中包括空白。如果您不希望偏移量包含这些空格，那么必须使用这个 PostProcessor。</span></span><br><span class="line">    tokenizer.post_processor = tokenizers.processors.ByteLevel()</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 定义一个BpeTrainer</span></span><br><span class="line">    trainer = BpeTrainer(special_tokens=[<span class="string">&quot;[UNK]&quot;</span>, <span class="string">&quot;[CLS]&quot;</span>, <span class="string">&quot;[SEP]&quot;</span>, <span class="string">&quot;[PAD]&quot;</span>, <span class="string">&quot;[MASK]&quot;</span>])</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 开始训练</span></span><br><span class="line">    <span class="comment"># 方式一</span></span><br><span class="line">    tokenizer.train(data_files, trainer)</span><br><span class="line">    <span class="comment"># 方式二</span></span><br><span class="line">    <span class="comment"># tokenizer.train_from_iterator(batch_iterator(), trainer=trainer, length=len(dataset[&#x27;train&#x27;]))</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># tokenizer保存</span></span><br><span class="line">    tokenizer.save(<span class="string">&quot;data/tokenizer-wiki.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># tokenizer加载</span></span><br><span class="line">    tokenizer = Tokenizer.from_file(<span class="string">&quot;data/tokenizer-wiki.json&quot;</span>)</span><br><span class="line"></span><br><span class="line">    sentence = <span class="string">&quot;我尝试了很多的方法&quot;</span></span><br><span class="line">    output = tokenizer.encode(sentence)</span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(output.tokens)</span><br><span class="line">    <span class="comment"># [&#x27;我&#x27;, &#x27;[UNK]&#x27;, &#x27;[UNK]&#x27;, &#x27;了&#x27;, &#x27;很&#x27;, &#x27;多的&#x27;, &#x27;方&#x27;, &#x27;法&#x27;]</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(output.ids)</span><br><span class="line">    <span class="comment"># [376, 0, 0, 44, 339, 1561, 438, 524]</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(output.offsets[<span class="number">5</span>])</span><br><span class="line">    <span class="comment"># (5, 7)</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(sentence[output.offsets[<span class="number">5</span>][<span class="number">0</span>]:output.offsets[<span class="number">5</span>][<span class="number">1</span>]])</span><br><span class="line">    <span class="comment"># &#x27;多的&#x27;</span></span><br><span class="line"></span><br><span class="line">    tokenizer.token_to_id(<span class="string">&quot;[SEP]&quot;</span>)</span><br><span class="line">    <span class="comment"># 2</span></span><br><span class="line">	</span><br><span class="line">    <span class="comment"># 后续处理</span></span><br><span class="line">    tokenizer.post_processor = TemplateProcessing(single=<span class="string">&quot;[CLS] $A [SEP]&quot;</span>, pair=<span class="string">&quot;[CLS] $A [SEP] $B:1 [SEP]:1&quot;</span>,</span><br><span class="line">                                                  special_tokens=[(<span class="string">&quot;[CLS]&quot;</span>, tokenizer.token_to_id(<span class="string">&quot;[CLS]&quot;</span>)),</span><br><span class="line">                                                                  (<span class="string">&quot;[SEP]&quot;</span>, tokenizer.token_to_id(<span class="string">&quot;[SEP]&quot;</span>)), ], )</span><br><span class="line"></span><br><span class="line">    output = tokenizer.encode_batch([sentence])</span><br><span class="line">    <span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line">    output = tokenizer.encode_batch([[<span class="string">&quot;我尝试了许多的方法&quot;</span>, <span class="string">&quot;却始终没有成功&quot;</span>], [<span class="string">&quot;自己说过的话&quot;</span>, <span class="string">&quot;就必须要努力去践行&quot;</span>]])</span><br><span class="line">    <span class="built_in">print</span>(output)</span><br><span class="line"></span><br><span class="line">    tokenizer.enable_padding(pad_id=<span class="number">3</span>, pad_token=<span class="string">&quot;[PAD]&quot;</span>)</span><br><span class="line">    output = tokenizer.encode_batch([<span class="string">&quot;我尝试了许多的方法&quot;</span>, <span class="string">&quot;却始终没有成功&quot;</span>])</span><br><span class="line">    <span class="built_in">print</span>(output[<span class="number">0</span>].tokens, output[<span class="number">1</span>].tokens)</span><br><span class="line">    <span class="comment"># [&#x27;[CLS]&#x27;, &#x27;我&#x27;, &#x27;[UNK]&#x27;, &#x27;[UNK]&#x27;, &#x27;了&#x27;, &#x27;许&#x27;, &#x27;多的&#x27;, &#x27;方&#x27;, &#x27;法&#x27;, &#x27;[SEP]&#x27;] [&#x27;[CLS]&#x27;, &#x27;[UNK]&#x27;, &#x27;[UNK]&#x27;, &#x27;[UNK]&#x27;, &#x27;没有&#x27;, &#x27;成&#x27;, &#x27;功&#x27;, &#x27;[SEP]&#x27;, &#x27;[PAD]&#x27;, &#x27;[PAD]&#x27;]</span></span><br><span class="line"></span><br><span class="line">    <span class="built_in">print</span>(output[<span class="number">1</span>].attention_mask)</span><br><span class="line">    <span class="comment"># [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    train_tokenizer()</span><br></pre></td></tr></table></figure>
<h2 id="扩展"><a href="#扩展" class="headerlink" title="扩展"></a>扩展</h2><blockquote>
<p><strong>qwen扩展自已的词表</strong></p>
</blockquote>
<ol>
<li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/QwenLM/Qwen/blob/main/tokenization_note_zh.md">tokenization_note_zh.md</a></li>
<li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/QwenLM/Qwen/issues/164">请问Tokenizer的训练工具使用的是什么？</a></li>
<li><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/QwenLM/Qwen/issues/399">SentencePiece词表合并问题 #399</a></li>
</ol>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/narutohyc">narutohyc</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://study.hycbook.com/article/57252.html">https://study.hycbook.com/article/57252.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://study.hycbook.com" target="_blank">兼一书虫</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/Tokenizer/">Tokenizer</a></div><div class="post_share"><div class="social-share" data-image="https://pic.hycbook.com/i/hexo/post_cover/蕾姆4.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><link rel="stylesheet" href="/" media="defer" onload="this.media='all'"/><div class="post-reward"><button class="tip-button reward-button"><span class="tip-button__text">打赏</span><div class="coin-wrapper"><div class="coin"><div class="coin__middle"></div><div class="coin__back"></div><div class="coin__front"></div></div></div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic.hycbook.com/i//hexo/qr_codes/hyc_wechat.webp" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/qr_codes/hyc_wechat.webp" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://pic.hycbook.com/i//hexo/qr_codes/hyc_alipay.webp" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/qr_codes/hyc_alipay.webp" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></button></div><audio id="coinAudio" src="https://cdn.jsdelivr.net/gh/hycBook/static_reources@resources/resources/hexo/reward_music/马里奥游戏投币叮当.mp3"></audio><script defer="defer" src="/"></script><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/article/61183.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆1.webp" onerror="onerror=null;src='https://pic.hycbook.com/i/hexo/config_imgs/404.svg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">咕呱锻炼随笔</div></div></a></div><div class="next-post pull-right"><a href="/article/53040.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆5.webp" onerror="onerror=null;src='https://pic.hycbook.com/i/hexo/config_imgs/404.svg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">pytorch学习_进阶知识</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/article/47450.html" title="图神经网络"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆3.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-16</div><div class="title">图神经网络</div></div></a></div><div><a href="/article/53377.html" title="深度学习模型压缩技术"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆11.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-04</div><div class="title">深度学习模型压缩技术</div></div></a></div><div><a href="/article/24897.html" title="LLM模型部署调试推理"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆0.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-12</div><div class="title">LLM模型部署调试推理</div></div></a></div><div><a href="/article/35455.html" title="深度学习在图像领域的应用"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆1.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-02-23</div><div class="title">深度学习在图像领域的应用</div></div></a></div><div><a href="/article/42898.html" title="nlp关键词和摘要提取技术整理"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆2.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-22</div><div class="title">nlp关键词和摘要提取技术整理</div></div></a></div><div><a href="/article/46832.html" title="深度学习核心之损失函数"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆8.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-26</div><div class="title">深度学习核心之损失函数</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Twikoo</span><span class="switch-btn"></span><span class="second-comment">Valine</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#tokenizer"><span class="toc-number">1.</span> <span class="toc-text">tokenizer</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E8%AF%8D%E4%BE%8B%E5%AD%90"><span class="toc-number">1.2.</span> <span class="toc-text">分词例子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E8%AF%8D%E7%B2%92%E5%BA%A6"><span class="toc-number">1.3.</span> <span class="toc-text">分词粒度</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AD%90%E8%AF%8D%E5%88%86%E8%AF%8D"><span class="toc-number">2.</span> <span class="toc-text">子词分词</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Byte-Pair-Encoding-BPE"><span class="toc-number">2.1.</span> <span class="toc-text">Byte-Pair Encoding (BPE)</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Byte-level-BPE"><span class="toc-number">2.1.1.</span> <span class="toc-text">Byte-level BPE</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#WordPiece"><span class="toc-number">2.2.</span> <span class="toc-text">WordPiece</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Unigram"><span class="toc-number">2.3.</span> <span class="toc-text">Unigram</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#SentencePiece"><span class="toc-number">2.4.</span> <span class="toc-text">SentencePiece</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E5%88%86%E8%AF%8D%E5%99%A8"><span class="toc-number">3.</span> <span class="toc-text">训练分词器</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A9%E5%B1%95"><span class="toc-number">3.1.</span> <span class="toc-text">扩展</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://pic.hycbook.com/i/hexo/config_imgs/footer_bg.webp')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By narutohyc</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><p><a target="_blank" href="https://hexo.io/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为Hexo"></a>&nbsp;<a target="_blank" href="https://demo.jerryc.me/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用butterfly"></a>&nbsp;<a target="_blank" href="https://vercel.com/ " rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vervel-brightgreen?style=flat&logo=Vercel" title="本站采用双线部署，默认线路托管于Vercel"></a>&nbsp;<a target="_blank" href="https://github.com/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由Gtihub托管"></a>&nbsp;<a target="_blank" href="https://zixiaoyun.com" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/图床-薄荷图床-green" title="薄荷图床"></a></p><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=35020502000647" rel="external nofollow noreferrer"><img style="position:relative;top:4px" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/config_imgs//备案图标.webp" alt="ICP"/>闽公网安备35020502000647号  </a><a href="https://beian.miit.gov.cn/" rel="external nofollow noreferrer" target="_blank">闽ICP备2023021562号-1</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><!-- button#darkmode(type="button" title=_p('rightside.night_mode_title'))--><!--  i.fas.fa-adjust--><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="fa-solid fa-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="fa-solid fa-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="fa-solid fa-arrow-rotate-right"></i></div><div class="rightMenu-item" id="menu-home"><i class="fa-solid fa-house"></i></div></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:window.open(&quot;https://www.baidu.com/s?wd=&quot;+window.getSelection().toString());window.location.reload();" rel="external nofollow noreferrer"><i class="fas fa-comment"></i><span>百度搜索</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" href="/archives/"><i class="fa-solid fa-archive"></i><span>文章归档</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="fa-solid fa-folder-open"></i><span>文章分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="fa-solid fa-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuNormal"><a class="rightMenu-item menu-link" id="menu-radompage"><i class="fa-solid fa-shoe-prints"></i><span>随便逛逛</span></a><div class="rightMenu-item" id="menu-translate"><i class="fa-solid fa-earth-asia"></i><span>繁简切换</span></div><div class="rightMenu-item" id="menu-darkmode"><i class="fa-solid fa-moon"></i><span>切换模式</span></div></div></div><div id="rightmenu-mask"></div><div><script src="https://cdn.jsdelivr.net/gh/hycBook/hycBookPage@resources/js/utils.js"></script><script src="https://cdn.jsdelivr.net/gh/hycBook/hycBookPage@resources/js/main.js"></script><script src="https://cdn.jsdelivr.net/gh/hycBook/hycBookPage@resources/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? '' : ''

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://vercel.hycbook.com',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://vercel.hycbook.com',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'ncn88uooQf0IO2rrGE7Vniwp-gzGzoHsz',
      appKey: 'Yghpzg1QfBMFJ0MxxHubVzKL',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Twikoo' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'https://vercel.hycbook.com',
        region: '',
        pageSize: 3,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script defer src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script defer data-pjax src="https://cdn.jsdelivr.net/gh/hycBook/hycBookPage@resources/zhheo/random.js"></script><script data-pjax src="https://cdn.jsdelivr.net/gh/hycBook/hycBookPage@resources/js/coin.js"></script><script defer src="https://npm.elemecdn.com/vue@2.6.11"></script><script async src="//at.alicdn.com/t/c/font_3670467_a0sijt8frxo.js"></script><script defer src="/live2d-widget/autoload.js"></script><script defer src="https://cdnjs.cloudflare.com/ajax/libs/toastr.js/latest/toastr.min.js"></script><script data-pjax defer src="https://npm.elemecdn.com/tzy-blog/lib/js/theme/chocolate.js"></script><script defer data-pjax src="https://cdn.jsdelivr.net/gh/hycBook/hycBookPage@resources/js/rightMenu.js"></script><script defer data-pjax src="https://cdn.jsdelivr.net/gh/hycBook/hycBookPage@resources/js/udf_mouse.js"></script><script defer data-pjax src="https://cdn.jsdelivr.net/gh/hycBook/hycBookPage@resources/js/udf_js.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><script>window.$crisp = [];
window.CRISP_WEBSITE_ID = "561b80db-3f0f-45cb-b3b1-aae7355939e6";
(function () {
  d = document;
  s = d.createElement("script");
  s.src = "https://client.crisp.chat/l.js";
  s.async = 1;
  d.getElementsByTagName("head")[0].appendChild(s);
})();
$crisp.push(["safe", true])

if (false) {
  $crisp.push(["do", "chat:hide"])
  $crisp.push(["on", "chat:closed", function() {
    $crisp.push(["do", "chat:hide"])
  }])
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      $crisp.push(["do", "chat:show"])
      $crisp.push(["do", "chat:open"])

    });
  }
  chatBtnFn()
} else {
  if (false) {
    function chatBtnHide () {
      $crisp.push(["do", "chat:hide"])
    }
    function chatBtnShow () {
      $crisp.push(["do", "chat:show"])
    }
  }
}</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="app-refresh" id="app-refresh" style="position: fixed;top: -2.2rem;left: 0;right: 0;z-index: 99999;padding: 0 1rem;font-size: 15px;height: 2.2rem;transition: all 0.3s ease;"><div class="app-refresh-wrap" style=" display: flex;color: #fff;height: 100%;align-items: center;justify-content: center;"><label>✨ 兼一书虫上新啦！ 👉</label><a href="javascript:void(0)" rel="external nofollow noreferrer" onclick="location.reload()"><span style="color: #fff;text-decoration: underline;cursor: pointer;">🍭查看新品🍬</span></a></div></div><script>if ('serviceWorker' in navigator) {
  if (navigator.serviceWorker.controller) {
    navigator.serviceWorker.addEventListener('controllerchange', function() {
      showNotification()
    })
  }
  window.addEventListener('load', function() {
    navigator.serviceWorker.register('/sw.js')
  })
}

function showNotification() {
  if (GLOBAL_CONFIG.Snackbar) {
    var snackbarBg =
      document.documentElement.getAttribute('data-theme') === 'light' ?
      GLOBAL_CONFIG.Snackbar.bgLight :
      GLOBAL_CONFIG.Snackbar.bgDark
    var snackbarPos = GLOBAL_CONFIG.Snackbar.position
    Snackbar.show({
      text: '✨ 兼一书虫上新啦！ 👉',
      backgroundColor: snackbarBg,
      duration: 500000,
      pos: snackbarPos,
      actionText: '🍭查看新品🍬',
      actionTextColor: '#fff',
      onActionClick: function(e) {
        location.reload()
      },
    })
  } else {
    var showBg =
      document.documentElement.getAttribute('data-theme') === 'light' ?
      '#49b1f5' :
      '#1f1f1f'
    var cssText = `top: 0; background: ${showBg};`
    document.getElementById('app-refresh')
      .style.cssText = cssText
  }
}</script></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>