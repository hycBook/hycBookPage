<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>pytorch学习记录 | 兼一书虫</title><meta name="keywords" content="pytorch"><meta name="author" content="narutohyc"><meta name="copyright" content="narutohyc"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="pytorch学习记录">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch学习记录">
<meta property="og:url" content="https://study.hycbook.com/article/53039.html">
<meta property="og:site_name" content="兼一书虫">
<meta property="og:description" content="pytorch学习记录">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic.hycbook.com/i/hexo/post_cover/%E8%95%BE%E5%A7%862.webp">
<meta property="article:published_time" content="2023-04-16T08:39:14.000Z">
<meta property="article:modified_time" content="2023-08-13T03:23:59.449Z">
<meta property="article:author" content="narutohyc">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic.hycbook.com/i/hexo/post_cover/%E8%95%BE%E5%A7%862.webp"><link rel="shortcut icon" href="https://pic.hycbook.com/i//hexo/config_imgs/网站图标.webp"><link rel="canonical" href="https://study.hycbook.com/article/53039"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="manifest" href="/manifest.json"/><meta name="msapplication-TileColor" content="#c6ff7a"/><link rel="apple-touch-icon" sizes="180x180" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/favicon-16x16.png"/><link rel="mask-icon" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/safari-pinned-tab.svg" color="#5bbad5"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?68340394dfd808cea9826e8a57f87aa6";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":120,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":200,"languages":{"author":"作者: narutohyc","link":"链接: ","source":"来源: 兼一书虫","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'pytorch学习记录',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-08-13 11:23:59'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="/css/hyc_udf.css"><link rel="stylesheet" href="/css/udf_css.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/mainColor/heoMainColor.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/404/404.css"><script src="https://npm.elemecdn.com/echarts@4.9.0/dist/echarts.min.js"></script><link href="https://cdn.bootcdn.net/ajax/libs/toastr.js/2.1.4/toastr.min.css" rel="stylesheet"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="兼一书虫" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/person_img/兼一头像.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">114</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">168</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fangwu"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 归档</span></a></li><li><a class="site-page child" href="/categories"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> 分类</span></a></li><li><a class="site-page child" href="/tags"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xuegao">             </use></svg><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yinle">                   </use></svg><span> 音乐</span></a></li><li><a class="site-page child" href="/bangumis"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wodezhuifan">                   </use></svg><span> 追番</span></a></li><li><a class="site-page child" href="/gallery"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xiangce">                   </use></svg><span> 相册</span></a></li><li><a class="site-page child" href="/video"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-shipin">                   </use></svg><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/charts"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xigua"></use></svg><span> 统计图</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-suannai">             </use></svg><span> 网盘</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://pan.hycbook.com/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 私月盘</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://share.hycbook.com/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> 共享盘</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifeiji">             </use></svg><span> 导航</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/comments"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-TIFFANYSROOM_huaban">                   </use></svg><span> 留言板</span></a></li><li><a class="site-page child" href="/link"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-changyonglianjie">                   </use></svg><span> 友链</span></a></li><li><a class="site-page child" href="/about"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-aixin">                   </use></svg><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic.hycbook.com/i/hexo/post_imgs/蕾姆2.webp')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">兼一书虫</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fangwu"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 归档</span></a></li><li><a class="site-page child" href="/categories"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> 分类</span></a></li><li><a class="site-page child" href="/tags"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xuegao">             </use></svg><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yinle">                   </use></svg><span> 音乐</span></a></li><li><a class="site-page child" href="/bangumis"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wodezhuifan">                   </use></svg><span> 追番</span></a></li><li><a class="site-page child" href="/gallery"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xiangce">                   </use></svg><span> 相册</span></a></li><li><a class="site-page child" href="/video"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-shipin">                   </use></svg><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/charts"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xigua"></use></svg><span> 统计图</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-suannai">             </use></svg><span> 网盘</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://pan.hycbook.com/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 私月盘</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://share.hycbook.com/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> 共享盘</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifeiji">             </use></svg><span> 导航</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/comments"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-TIFFANYSROOM_huaban">                   </use></svg><span> 留言板</span></a></li><li><a class="site-page child" href="/link"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-changyonglianjie">                   </use></svg><span> 友链</span></a></li><li><a class="site-page child" href="/about"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-aixin">                   </use></svg><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">pytorch学习记录</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-04-16T08:39:14.000Z" title="发表于 2023-04-16 16:39:14">2023-04-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-08-13T03:23:59.449Z" title="更新于 2023-08-13 11:23:59">2023-08-13</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/deep-learning/">deep_learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">16.8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>84分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title="pytorch学习记录"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="twikoo_visitors"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/article/53039.html#post-comment"><span id="twikoo-count"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><hr>
<blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.pytorchtutorial.com/docs/package_references/torch/">pytorch中文文档</a></p>
</blockquote>
<h1 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h1><h2 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h2><blockquote>
<p><strong>torch.is_tensor</strong>: 如果obj是一个pytorch张量，则返回True</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">torch.is_tensor(x)</span><br><span class="line"></span><br><span class="line">Out[<span class="number">0</span>]: <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.is_storage</strong>: 如何obj是一个pytorch storage对象，则返回True</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">torch.is_storage(x)</span><br><span class="line"></span><br><span class="line">Out[<span class="number">0</span>]: <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.numel</strong>: 返回<code>input</code> 张量中的元素个数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">torch.numel(a)</span><br><span class="line">Out[<span class="number">0</span>]: <span class="number">120</span></span><br><span class="line"></span><br><span class="line">a = torch.zeros(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">torch.numel(a)</span><br><span class="line">Out[<span class="number">1</span>]: <span class="number">16</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.set_printoptions</strong>: 设置打印选项</p>
<p>参数:</p>
<ul>
<li>precision – 浮点数输出的精度位数 (默认为8 )</li>
<li>threshold – 阈值，触发汇总显示而不是完全显示(repr)的数组元素的总数 （默认为1000）</li>
<li>edgeitems – 汇总显示中，每维（轴）两端显示的项数（默认值为3）</li>
<li>linewidth – 用于插入行间隔的每行字符数（默认为80）。Thresholded matricies will ignore this parameter.</li>
<li>profile – pretty打印的完全默认值。 可以覆盖上述所有选项 (默认为short, full)</li>
</ul>
</blockquote>
<h2 id="创建操作-Creation-Ops"><a href="#创建操作-Creation-Ops" class="headerlink" title="创建操作 Creation Ops"></a>创建操作 Creation Ops</h2><blockquote>
<p><strong>torch.eye</strong>: 返回一个2维张量，对角线位置全1，其它位置全0</p>
<p>参数:</p>
<ul>
<li>n (<a target="_blank" rel="noopener external nofollow noreferrer" href="https://docs.python.org/2/library/functions.html#int">int</a> ) – 行数</li>
<li>m (<a target="_blank" rel="noopener external nofollow noreferrer" href="https://docs.python.org/2/library/functions.html#int">int</a>, <em>optional</em>) – 列数.如果为None,则默认为<em>n</em></li>
<li>out (<a target="_blank" rel="noopener external nofollow noreferrer" href="http://pytorch.org/docs/tensors.html#torch.Tensor"><em>Tensor</em></a>, <em>optinal</em>) - Output tensor</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.eye(<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]:  <span class="number">1</span>  <span class="number">0</span>  <span class="number">0</span></span><br><span class="line"> <span class="number">0</span>  <span class="number">1</span>  <span class="number">0</span></span><br><span class="line"> <span class="number">0</span>  <span class="number">0</span>  <span class="number">1</span></span><br><span class="line"> </span><br><span class="line">[torch.FloatTensor of size 3x3]</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>from_numpy</strong>: 将<code>numpy.ndarray</code> 转换为pytorch的 <code>Tensor</code>。 返回的张量tensor和numpy的ndarray共享同一内存空间。修改一个会导致另外一个也被修改。返回的张量不能改变大小</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = numpy.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">t = torch.from_numpy(a)</span><br><span class="line">Out[<span class="number">0</span>]: torch.LongTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">t[<span class="number">0</span>] = -<span class="number">1</span></span><br><span class="line">a</span><br><span class="line">Out[<span class="number">1</span>]: array([-<span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.linspace</strong>: 返回一个1维张量，包含在区间<code>start</code> 和 <code>end</code> 上均匀间隔的<code>steps</code>个点。 输出1维张量的长度为<code>steps</code></p>
<p>参数:</p>
<ul>
<li>start (float) – 序列的起始点</li>
<li>end (float) – 序列的最终值</li>
<li>steps (int) – 在<code>start</code> 和 <code>end</code>间生成的样本数</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.linspace(<span class="number">3</span>, <span class="number">10</span>, steps=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">  <span class="number">3.0000</span></span><br><span class="line">  <span class="number">4.7500</span></span><br><span class="line">  <span class="number">6.5000</span></span><br><span class="line">  <span class="number">8.2500</span></span><br><span class="line"> <span class="number">10.0000</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.logspace</strong>: </p>
<p>参数:</p>
<ul>
<li>start (float) – 序列的起始点</li>
<li>end (float) – 序列的最终值</li>
<li>steps (int) – 在<code>开始</code>和<code>结束</code>之间要采样的点数。默认值：100</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.logspace(start=<span class="number">0.1</span>, end=<span class="number">1.0</span>, steps=<span class="number">5</span>)</span><br><span class="line"></span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">  <span class="number">1.2589</span></span><br><span class="line">  <span class="number">2.1135</span></span><br><span class="line">  <span class="number">3.5481</span></span><br><span class="line">  <span class="number">5.9566</span></span><br><span class="line"> <span class="number">10.0000</span></span><br><span class="line">[torch.FloatTensor of size <span class="number">5</span>]</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.ones</strong>: 返回一个全为1 的张量，形状由可变参数<code>sizes</code>定义</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line"> <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span></span><br><span class="line"> <span class="number">1</span>  <span class="number">1</span>  <span class="number">1</span></span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.rand</strong>: 返回一个张量，包含了从区间[0,1)的均匀分布中抽取的一组随机数，形状由可变参数<code>sizes</code> 定义</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line"> <span class="number">0.5010</span>  <span class="number">0.5140</span>  <span class="number">0.0719</span></span><br><span class="line"> <span class="number">0.1435</span>  <span class="number">0.5636</span>  <span class="number">0.0538</span></span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.randn</strong>: 返回一个张量，包含了从标准正态分布(均值为0，方差为 1，即高斯白噪声)中抽取一组随机数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line"> <span class="number">1.4339</span>  <span class="number">0.3351</span> -<span class="number">1.0999</span></span><br><span class="line"> <span class="number">1.5458</span> -<span class="number">0.9643</span> -<span class="number">0.3558</span></span><br><span class="line">[torch.FloatTensor of size 2x3]</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.randperm</strong>: 给定参数<code>n</code>，返回一个从<code>0</code> 到<code>n -1</code> 的随机整数排列</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.randperm(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">    </span><br><span class="line">torch.randperm(<span class="number">6</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.arange</strong>: torch.arange(start, end, step=1, out=None) → Tensor</p>
<p>参数:</p>
<ul>
<li>start (float) – 序列的起始点</li>
<li>end (float) – 序列的终止点</li>
<li>step (float) – 相邻点的间隔大小</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.arange(<span class="number">1</span>, <span class="number">2.5</span>, <span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.0000</span>, <span class="number">1.5000</span>, <span class="number">2.0000</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.zeros</strong>: 返回一个全为标量 0 的张量</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"></span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="索引-切片-连接-换位"><a href="#索引-切片-连接-换位" class="headerlink" title="索引|切片|连接|换位"></a>索引|切片|连接|换位</h2><blockquote>
<p><strong>torch.cat</strong>: 给定维度上对输入的张量序列<code>seq</code> 进行连接操作</p>
<p><code>torch.cat()</code>可以看做 <code>torch.split()</code> 和 <code>torch.chunk()</code>的反操作</p>
<p>参数:</p>
<ul>
<li>inputs (<em>sequence of Tensors</em>) – 可以是任意相同Tensor 类型的python 序列</li>
<li>dimension (<em>int</em>, <em>optional</em>) – 沿着此维连接张量序列</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">torch.cat((x, x, x), <span class="number">0</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[ <span class="number">0.6030</span>, -<span class="number">0.0292</span>],</span><br><span class="line">        [ <span class="number">0.6030</span>, -<span class="number">0.0292</span>],</span><br><span class="line">        [ <span class="number">0.6030</span>, -<span class="number">0.0292</span>]])</span><br><span class="line"></span><br><span class="line">torch.cat((x, x, x), <span class="number">1</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([[ <span class="number">0.6030</span>, -<span class="number">0.0292</span>,  <span class="number">0.6030</span>, -<span class="number">0.0292</span>,  <span class="number">0.6030</span>, -<span class="number">0.0292</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.chunk</strong>: torch.chunk(tensor, chunks, dim=0)</p>
<p>在给定维度(轴)上将输入张量进行分块儿</p>
<p>参数:</p>
<ul>
<li>tensor (Tensor) – 待分块的输入张量</li>
<li>chunks (int) – 分块的个数</li>
<li>dim (int) – 沿着此维度进行分块</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.arange(<span class="number">11</span>).chunk(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: (tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]), tensor([<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]), tensor([<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]), tensor([ <span class="number">9</span>, <span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line">torch.rand((<span class="number">3</span>,<span class="number">2</span>)).chunk(<span class="number">3</span>,dim=<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">(tensor([[<span class="number">0.4479</span>, <span class="number">0.8420</span>]]),</span><br><span class="line"> tensor([[<span class="number">0.2951</span>, <span class="number">0.9858</span>]]),</span><br><span class="line"> tensor([[<span class="number">0.2795</span>, <span class="number">0.7413</span>]]))</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.gather</strong>: torch.gather(input, dim, index, out=None) → Tensor</p>
<p>沿给定轴<code>dim</code>，将输入索引张量<code>index</code>指定位置的值进行聚合</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 源张量</li>
<li>dim (int) – 索引的轴</li>
<li>index (LongTensor) – 聚合元素的下标</li>
<li>out (Tensor, optional) – 目标张量</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">t = torch.Tensor([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">torch.gather(t, <span class="number">1</span>, torch.LongTensor([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>]]))</span><br><span class="line"></span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">3.</span>]])</span><br><span class="line"></span><br><span class="line">t = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">torch.gather(t, <span class="number">1</span>, torch.LongTensor([[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">1</span>,<span class="number">0</span>]]))</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">3.</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.index_select</strong>: torch.index_select(input, dim, index, out=None) → Tensor</p>
<p>沿着指定维度对输入进行切片，取<code>index</code>中指定的相应项(<code>index</code>为一个LongTensor)，然后返回到一个新的张量， 返回的张量与原始张量<em>Tensor</em>有相同的维度(在指定轴上)</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – 索引的轴</li>
<li>index (LongTensor) – 包含索引下标的一维张量</li>
<li>out (Tensor, optional) – 目标张量</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[-<span class="number">0.8335</span>,  <span class="number">1.2611</span>,  <span class="number">0.6569</span>, -<span class="number">0.1598</span>],</span><br><span class="line">        [-<span class="number">0.1019</span>,  <span class="number">1.5010</span>, -<span class="number">1.4486</span>, -<span class="number">2.2269</span>],</span><br><span class="line">        [-<span class="number">0.6087</span>, -<span class="number">0.6940</span>, -<span class="number">0.2556</span>, -<span class="number">1.1843</span>]])</span><br><span class="line"></span><br><span class="line">indices = torch.LongTensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">torch.index_select(x, <span class="number">0</span>, indices)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[-<span class="number">0.8335</span>,  <span class="number">1.2611</span>,  <span class="number">0.6569</span>, -<span class="number">0.1598</span>],</span><br><span class="line">        [-<span class="number">0.6087</span>, -<span class="number">0.6940</span>, -<span class="number">0.2556</span>, -<span class="number">1.1843</span>]])</span><br><span class="line"></span><br><span class="line">torch.index_select(x, <span class="number">1</span>, indices)</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">tensor([[-<span class="number">0.8335</span>,  <span class="number">0.6569</span>],</span><br><span class="line">        [-<span class="number">0.1019</span>, -<span class="number">1.4486</span>],</span><br><span class="line">        [-<span class="number">0.6087</span>, -<span class="number">0.2556</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.masked_select</strong>: torch.masked_select(input, mask, out=None) → Tensor</p>
<p>根据掩码张量<code>mask</code>中的二元值，取输入张量中的指定项( <code>mask</code>为一个 <em>ByteTensor</em>)，将取值返回到一个新的1D张量</p>
<p>张量 <code>mask</code>须跟<code>input</code>张量有相同数量的元素数目，但形状或维度不需要相同</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[-<span class="number">0.8335</span>,  <span class="number">1.2611</span>,  <span class="number">0.6569</span>, -<span class="number">0.1598</span>],</span><br><span class="line">        [-<span class="number">0.1019</span>,  <span class="number">1.5010</span>, -<span class="number">1.4486</span>, -<span class="number">2.2269</span>],</span><br><span class="line">        [-<span class="number">0.6087</span>, -<span class="number">0.6940</span>, -<span class="number">0.2556</span>, -<span class="number">1.1843</span>]])</span><br><span class="line"></span><br><span class="line">mask = x.ge(<span class="number">0.5</span>)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>]])</span><br><span class="line">   </span><br><span class="line">torch.masked_select(x, mask)</span><br><span class="line">Out[<span class="number">2</span>]: tensor([<span class="number">1.2394</span>, <span class="number">0.5152</span>, <span class="number">0.5170</span>, <span class="number">1.8193</span>, <span class="number">0.7352</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.nonzero</strong>: 用于<a target="_blank" rel="noopener external nofollow noreferrer" href="https://so.csdn.net/so/search?q=输出数组&amp;spm=1001.2101.3001.7020">输出数组</a>的非零值的索引，即用来定位数组中非零的元素</p>
<p><code>as_tuple</code>：如果设为<code>False</code>，则返回一个二维张量，其中每一行都是非零值的索引</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">torch.nonzero(torch.Tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]))</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line">torch.nonzero(torch.Tensor([[<span class="number">0.6</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],</span><br><span class="line">                             [<span class="number">0.0</span>, <span class="number">0.4</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],</span><br><span class="line">                             [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.2</span>, <span class="number">0.0</span>],</span><br><span class="line">                             [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1</span>, -<span class="number">0.4</span>]]))</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.split</strong>: torch.split(tensor, split_size, dim=0)</p>
<p>将输入张量分割成相等形状的chunks(如果可分)</p>
<p>如果沿指定维的张量形状大小不能被<code>split_size</code> 整分， 则最后一个分块会小于其它分块</p>
<p>参数:</p>
<ul>
<li>tensor (Tensor) – 待分割张量</li>
<li>split_size (int) – 单个分块的形状大小</li>
<li>dim (int) – 沿着此维进行分割</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">10</span>).reshape(<span class="number">5</span>,<span class="number">2</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">        [<span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">torch.split(a, <span class="number">2</span>)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">(tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">2</span>, <span class="number">3</span>]]),</span><br><span class="line"> tensor([[<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">         [<span class="number">6</span>, <span class="number">7</span>]]),</span><br><span class="line"> tensor([[<span class="number">8</span>, <span class="number">9</span>]]))</span><br><span class="line"></span><br><span class="line">torch.split(a, [<span class="number">1</span>,<span class="number">4</span>])</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">(tensor([[<span class="number">0</span>, <span class="number">1</span>]]),</span><br><span class="line"> tensor([[<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">         [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">         [<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">         [<span class="number">8</span>, <span class="number">9</span>]]))</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.squeeze</strong>: torch.squeeze(input, dim=None, out=None)</p>
<p>将输入张量形状中的<code>1</code> 去除并返回。 如果输入是形如(A×1×B×1×C×1×D)(A×1×B×1×C×1×D)，那么输出形状就为： (A×B×C×D)</p>
<p>当给定<code>dim</code>时，那么挤压操作只在给定维度上。例如，输入形状为: (A×1×B)(A×1×B), <code>squeeze(input, 0)</code> 将会保持张量不变，只有用 <code>squeeze(input, 1)</code>，形状会变成 (A×B)(A×B)。</p>
<p>注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int, optional) – 如果给定，则<code>input</code>只会在给定维度挤压</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x = torch.zeros(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">x.size()</span><br><span class="line">Out[<span class="number">0</span>]: torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">    </span><br><span class="line">y = torch.squeeze(x)</span><br><span class="line">y.size()</span><br><span class="line">Out[<span class="number">1</span>]: torch.Size([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">y = torch.squeeze(x, <span class="number">0</span>)</span><br><span class="line">y.size()</span><br><span class="line">Out[<span class="number">2</span>]: torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">y = torch.squeeze(x, <span class="number">1</span>)</span><br><span class="line">y.size()</span><br><span class="line">Out[<span class="number">3</span>]: torch.Size([<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.unsqueeze</strong>: torch.unsqueeze(input, dim, out=None)</p>
<p>返回一个新的张量，对输入的制定位置插入维度 1</p>
<p>注意： 返回张量与输入张量共享内存，所以改变其中一个的内容会改变另一个</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">x.shape</span><br><span class="line">Out[<span class="number">0</span>]: torch.Size([<span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">torch.unsqueeze(x, <span class="number">0</span>).shape</span><br><span class="line">Out[<span class="number">1</span>]: torch.Size([<span class="number">1</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">torch.unsqueeze(x, <span class="number">1</span>).shape</span><br><span class="line">Out[<span class="number">2</span>]: torch.Size([<span class="number">4</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.stack</strong>: torch.stack(sequence, dim=0)</p>
<p>沿着一个新维度对输入张量序列进行连接。 序列中所有的张量都应该为相同形状</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">t1 = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                   [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                   [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">t2 = torch.tensor([[<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>],</span><br><span class="line">                   [<span class="number">40</span>, <span class="number">50</span>, <span class="number">60</span>],</span><br><span class="line">                   [<span class="number">70</span>, <span class="number">80</span>, <span class="number">90</span>]])</span><br><span class="line">t1.shape, t2.shape</span><br><span class="line">Out[<span class="number">0</span>]: (torch.Size([<span class="number">3</span>, <span class="number">3</span>]), torch.Size([<span class="number">3</span>, <span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line">torch.stack((t1, t2), dim=<span class="number">0</span>).shape</span><br><span class="line">Out[<span class="number">1</span>]: torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">torch.stack((t1, t2), dim=<span class="number">1</span>).shape</span><br><span class="line">Out[<span class="number">2</span>]: torch.Size([<span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">torch.stack((t1, t2), dim=<span class="number">2</span>).shape</span><br><span class="line">Out[<span class="number">3</span>]: torch.Size([<span class="number">3</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.transpose</strong>: torch.transpose(input, dim0, dim1, out=None) → Tensor</p>
<p>返回输入矩阵<code>input</code>的转置。交换维度<code>dim0</code>和<code>dim1</code>。 输出张量与输入张量共享内存，所以改变其中一个会导致另外一个也被修改。</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim0 (int) – 转置的第一维</li>
<li>dim1 (int) – 转置的第二维</li>
</ul>
<p><strong>torch.t</strong>: torch.t(input, out=None) → Tensor</p>
<p>输入一个矩阵（2维张量），并转置0, 1维。 可以被视为函数<code>transpose(input, 0, 1)</code>的简写</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">x.shape</span><br><span class="line">Out[<span class="number">0</span>]: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">torch.transpose(x, <span class="number">0</span>, <span class="number">1</span>).shape</span><br><span class="line">Out[<span class="number">1</span>]: torch.Size([<span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line">    </span><br><span class="line">torch.t(x).shape</span><br><span class="line">Out[<span class="number">2</span>]: torch.Size([<span class="number">3</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.unbind</strong>: torch.unbind(tensor, dim=0)[source]</p>
<p>移除指定维后，返回一个元组，包含了沿着指定维切片后的各个切片</p>
<p>参数:</p>
<ul>
<li>tensor (Tensor) – 输入张量</li>
<li>dim (int) – 删除的维度</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">torch.unbind(torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                           [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                           [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]))</span><br><span class="line">Out[<span class="number">0</span>]: (tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]), tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]), tensor([<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]))</span><br><span class="line"></span><br><span class="line">torch.unbind(torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                           [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                           [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]), dim=<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">1</span>]: (tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]), tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]), tensor([<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]))</span><br><span class="line"></span><br><span class="line">torch.unbind(torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                           [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                           [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]), dim=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">2</span>]: (tensor([<span class="number">1</span>, <span class="number">4</span>, <span class="number">7</span>]), tensor([<span class="number">2</span>, <span class="number">5</span>, <span class="number">8</span>]), tensor([<span class="number">3</span>, <span class="number">6</span>, <span class="number">9</span>]))</span><br></pre></td></tr></table></figure>
<h2 id="随机抽样"><a href="#随机抽样" class="headerlink" title="随机抽样"></a>随机抽样</h2><blockquote>
<p><strong>torch.seed</strong>: 设置torch cpu随机数种子</p>
<p><strong>torch.manual_seed</strong>: 设置torch cpu随机数种子，torch.manual_seed(seed)</p>
<p>设定生成随机数的种子，并返回一个torch._C.Generator对象</p>
<p><strong>torch.cuda.manual_seed</strong>: 设置torch cuda随机数种子</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">torch.seed()</span><br><span class="line">Out[<span class="number">0</span>]: <span class="number">348176808892500</span></span><br><span class="line">torch.seed()</span><br><span class="line">Out[<span class="number">1</span>]: <span class="number">348177652492500</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">2</span>]: &lt;torch._C.Generator at <span class="number">0x138bdf41a90</span>&gt;</span><br><span class="line"></span><br><span class="line">torch.cuda.manual_seed(<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.bernoulli</strong>: torch.bernoulli(input, out=None) → Tensor</p>
<p>从伯努利分布中提取二进制随机数（0或1），输入张量应为包含用于绘制二进制随机数的概率的张量。因此，输入中的所有值都必须在以下范围内(0,1)</p>
<p><strong>torch.poisson</strong>: 泊松分布用于计算一个事件在平均价值率(时间)的一定时间内发生的可能性。泊松分布是一个离散的概率分布</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor(<span class="number">3</span>, <span class="number">3</span>).uniform_(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">0.0044</span>, <span class="number">0.7257</span>, <span class="number">0.2599</span>],</span><br><span class="line">        [<span class="number">0.1663</span>, <span class="number">0.2119</span>, <span class="number">0.7875</span>],</span><br><span class="line">        [<span class="number">0.7648</span>, <span class="number">0.8838</span>, <span class="number">0.6814</span>]])</span><br><span class="line">torch.bernoulli(a)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line">a = torch.ones(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">torch.bernoulli(a)</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>]])</span><br><span class="line">a = torch.zeros(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">torch.bernoulli(a)</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">a = torch.rand(<span class="number">4</span>, <span class="number">4</span>) * <span class="number">5</span> </span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">0.2542</span>, <span class="number">1.3148</span>, <span class="number">4.2023</span>, <span class="number">2.4838</span>],</span><br><span class="line">        [<span class="number">1.2574</span>, <span class="number">0.5842</span>, <span class="number">0.1604</span>, <span class="number">0.3900</span>],</span><br><span class="line">        [<span class="number">1.9929</span>, <span class="number">3.8710</span>, <span class="number">3.8516</span>, <span class="number">0.0889</span>],</span><br><span class="line">        [<span class="number">4.0595</span>, <span class="number">0.5437</span>, <span class="number">1.9715</span>, <span class="number">1.4863</span>]])</span><br><span class="line">torch.poisson(a)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">4.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">8.</span>, <span class="number">4.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">5.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.multinomial</strong>: torch.multinomial(input, num_samples,replacement=False, out=None) → LongTensor</p>
<p>对input的每一行做n_samples次取值，输出的张量是每一次取值时input张量对应行的下标</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 包含概率值的张量</li>
<li>num_samples (int) – 抽取的样本数</li>
<li>replacement (bool, optional) – 布尔值，决定是否能重复抽取</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">weights = torch.Tensor([<span class="number">0</span>, <span class="number">10</span>, <span class="number">3</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">torch.multinomial(weights, <span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># replacement=True时 概率为0的没机会被取到</span></span><br><span class="line">torch.multinomial(weights, <span class="number">4</span>, replacement=<span class="literal">True</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.normal</strong>: torch.normal(means, std, out=None)</p>
<p>返回一个张量，包含从给定参数<code>means</code>,<code>std</code>的离散正态分布中抽取随机数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.normal(mean=torch.arange(<span class="number">1.</span>, <span class="number">11.</span>), std=torch.arange(<span class="number">1</span>, <span class="number">0</span>, -<span class="number">0.1</span>))</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([ <span class="number">0.9732</span>,  <span class="number">2.0833</span>,  <span class="number">2.5282</span>,  <span class="number">4.3588</span>,  <span class="number">5.4837</span>,  <span class="number">5.1150</span>,  <span class="number">7.0366</span>,  <span class="number">7.9774</span>,</span><br><span class="line">         <span class="number">9.1679</span>, <span class="number">10.0248</span>])</span><br><span class="line"></span><br><span class="line">torch.normal(mean=<span class="number">0.5</span>, std=torch.arange(<span class="number">1.</span>, <span class="number">6.</span>))</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">0.7067</span>,  <span class="number">2.4856</span>, -<span class="number">2.1957</span>, -<span class="number">4.3114</span>, <span class="number">16.2506</span>])</span><br><span class="line"></span><br><span class="line">torch.normal(mean=torch.arange(<span class="number">1.</span>, <span class="number">6.</span>))</span><br><span class="line">Out[<span class="number">2</span>]: tensor([<span class="number">0.7835</span>, <span class="number">4.6096</span>, <span class="number">2.7244</span>, <span class="number">5.2810</span>, <span class="number">4.8413</span>])</span><br></pre></td></tr></table></figure>
<h2 id="序列化Serialization"><a href="#序列化Serialization" class="headerlink" title="序列化Serialization"></a>序列化Serialization</h2><blockquote>
<p><strong>torch.save</strong>: 保存一个对象到一个硬盘文件上 参考: <a target="_blank" rel="noopener external nofollow noreferrer" href="http://pytorch.org/docs/notes/serialization.html#recommend-saving-models">Recommended approach for saving a model</a> </p>
<p>torch.save(<em>obj</em>, <em>f</em>, <em>pickle_module=pickle</em>, <em>pickle_protocol=DEFAULT_PROTOCOL</em>, <em>_use_new_zipfile_serialization=True</em>)</p>
<p>参数：</p>
<ul>
<li>obj – 保存对象</li>
<li>f － 类文件对象 (返回文件描述符)或一个保存文件名的字符串</li>
<li>pickle_module – 用于pickling元数据和对象的模块</li>
<li>pickle_protocol – 指定pickle protocal 可以覆盖默认参数</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to file</span></span><br><span class="line">torch.save(x, <span class="string">&#x27;tensor.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to io.BytesIO buffer</span></span><br><span class="line">buffer = io.BytesIO()</span><br><span class="line">torch.save(x, buffer)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.load</strong>: 从磁盘文件中读取一个通过<code>torch.save()</code>保存的对象</p>
<p>torch.load(<em>f</em>, <em>map_location=None</em>, <em>pickle_module=pickle</em>, <strong><em>, </em>weights_only=False<em>, </em></strong>pickle_load_args*)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.load(<span class="string">&#x27;tensors.pt&#x27;</span>, encoding=<span class="string">&#x27;ascii&#x27;</span>)</span><br><span class="line">torch.load(<span class="string">&#x27;tensors.pt&#x27;</span>, map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line">torch.load(<span class="string">&#x27;tensors.pt&#x27;</span>, map_location=&#123;<span class="string">&#x27;cuda:1&#x27;</span>:<span class="string">&#x27;cuda:0&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load from io.BytesIO buffer</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;tensor.pt&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    buffer = io.BytesIO(f.read())</span><br><span class="line">torch.load(buffer)</span><br></pre></td></tr></table></figure>
<h2 id="并行化-Parallelism"><a href="#并行化-Parallelism" class="headerlink" title="并行化 Parallelism"></a>并行化 Parallelism</h2><blockquote>
<p><strong>torch.get_num_threads</strong>: 获得用于并行化CPU操作的OpenMP线程数</p>
<p><strong>torch.set_num_threads</strong>: 设定用于并行化CPU操作的OpenMP线程数</p>
</blockquote>
<h2 id="数学操作Math-operations"><a href="#数学操作Math-operations" class="headerlink" title="数学操作Math operations"></a>数学操作Math operations</h2><blockquote>
<p><strong>torch.abs</strong>: 计算输入张量的每个元素绝对值</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">abs</span>(torch.FloatTensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.acos</strong>: torch.acos(input, out=None) → Tensor</p>
<p>返回一个新张量，包含输入张量每个元素的反余弦</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.acos(torch.FloatTensor([-<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]))</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">3.1416</span>, <span class="number">0.0000</span>, <span class="number">1.5708</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.add</strong>: torch.add(input, value, out=None)</p>
<p>对输入张量<code>input</code>逐元素加上标量值<code>value</code>，并返回结果到一个新的张量</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0.3510</span>, -<span class="number">0.2226</span>, -<span class="number">0.7971</span>, -<span class="number">0.2564</span>])</span><br><span class="line">torch.add(a, <span class="number">20</span>)</span><br><span class="line"></span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">20.3510</span>, <span class="number">19.7774</span>, <span class="number">19.2029</span>, <span class="number">19.7436</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.addcdiv</strong>: torch.addcdiv(tensor, value=1, tensor1, tensor2, out=None) → Tensor</p>
<p>用<code>tensor2</code>对<code>tensor1</code>逐元素相除，然后乘以标量值<code>value</code> 并加到<code>tensor</code></p>
<p>张量的形状不需要匹配，但元素数量必须一致</p>
<p>参数：</p>
<ul>
<li>tensor (Tensor) – 张量，对 tensor1 ./ tensor 进行相加</li>
<li>value (Number, optional) – 标量，对 tensor1 ./ tensor2 进行相乘</li>
<li>tensor1 (Tensor) – 张量，作为被除数(分子)</li>
<li>tensor2 (Tensor) –张量，作为除数(分母)</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">
out _{i}=\operatorname{input}_{i}+ value \times \frac{\text { tensor } 1_{i}}{\text { tensor 2}_{i}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">t = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">t1 = torch.randn(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">t2 = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">t, t1, t2</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">(tensor([[-<span class="number">1.2863</span>,  <span class="number">1.1267</span>, -<span class="number">1.7120</span>]]),</span><br><span class="line"> tensor([[-<span class="number">0.4294</span>],</span><br><span class="line">         [-<span class="number">0.5328</span>],</span><br><span class="line">         [-<span class="number">0.5373</span>]]),</span><br><span class="line"> tensor([[-<span class="number">0.0876</span>,  <span class="number">0.4398</span>,  <span class="number">1.3583</span>]]))</span><br><span class="line"></span><br><span class="line">torch.addcdiv(t, t1, t2, value=<span class="number">0.1</span>)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[-<span class="number">0.7958</span>,  <span class="number">1.0291</span>, -<span class="number">1.7436</span>],</span><br><span class="line">        [-<span class="number">0.6778</span>,  <span class="number">1.0056</span>, -<span class="number">1.7512</span>],</span><br><span class="line">        [-<span class="number">0.6727</span>,  <span class="number">1.0046</span>, -<span class="number">1.7515</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.addcmul</strong>: torch.addcmul(tensor, value=1, tensor1, tensor2, out=None) → Tensor</p>
<p>用<code>tensor2</code>对<code>tensor1</code>逐元素相乘，并对结果乘以标量值<code>value</code>然后加到<code>tensor</code></p>
<p>参数：</p>
<ul>
<li>tensor (Tensor) – 张量，对tensor1 ./ tensor 进行相加</li>
<li>value (Number, optional) – 标量，对 tensor1 . tensor2 进行相乘</li>
<li>tensor1 (Tensor) – 张量，作为乘子1</li>
<li>tensor2 (Tensor) –张量，作为乘子2</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">
out _{i}= input _{i}+ value \times tensor 1_{i} \times tensor 2_{i}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">t = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">t1 = torch.randn(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">t2 = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">t, t1, t2</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">(tensor([[-<span class="number">1.2863</span>,  <span class="number">1.1267</span>, -<span class="number">1.7120</span>]]),</span><br><span class="line"> tensor([[-<span class="number">0.4294</span>],</span><br><span class="line">         [-<span class="number">0.5328</span>],</span><br><span class="line">         [-<span class="number">0.5373</span>]]),</span><br><span class="line"> tensor([[-<span class="number">0.0876</span>,  <span class="number">0.4398</span>,  <span class="number">1.3583</span>]]))</span><br><span class="line"></span><br><span class="line">torch.addcmul(t, t1, t2, value=<span class="number">0.1</span>)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[-<span class="number">1.2825</span>,  <span class="number">1.1078</span>, -<span class="number">1.7703</span>],</span><br><span class="line">        [-<span class="number">1.2816</span>,  <span class="number">1.1033</span>, -<span class="number">1.7844</span>],</span><br><span class="line">        [-<span class="number">1.2816</span>,  <span class="number">1.1031</span>, -<span class="number">1.7850</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.asin</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的反正弦函数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0.2583</span>, -<span class="number">0.5285</span>,  <span class="number">0.8979</span>,  <span class="number">1.0104</span>])</span><br><span class="line"></span><br><span class="line">torch.asin(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">0.2613</span>, -<span class="number">0.5569</span>,  <span class="number">1.1149</span>,     nan])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.atan</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的反正切函数</p>
<p><strong>torch.atan2</strong>: 返回一个新张量，包含两个输入张量<code>input1</code>和<code>input2</code>的反正切函数</p>
<p>torch.atan2(input1, input2, out=None) → Tensor</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0.2583</span>, -<span class="number">0.5285</span>,  <span class="number">0.8979</span>,  <span class="number">1.0104</span>])</span><br><span class="line">b = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">2</span>]: tensor([<span class="number">0.1100</span>, <span class="number">1.4311</span>, <span class="number">1.9536</span>, <span class="number">0.7652</span>])</span><br><span class="line"></span><br><span class="line">torch.atan(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">0.2528</span>, -<span class="number">0.4862</span>,  <span class="number">0.7316</span>,  <span class="number">0.7906</span>])</span><br><span class="line"></span><br><span class="line">torch.atan2(a, b)</span><br><span class="line">Out[<span class="number">3</span>]: tensor([ <span class="number">1.1681</span>, -<span class="number">0.3538</span>,  <span class="number">0.4308</span>,  <span class="number">0.9226</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.ceil</strong>: 对输入<code>input</code>张量每个元素向上取整, 即取不小于每个元素的最小整数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.9105</span>, -<span class="number">0.7277</span>,  <span class="number">0.9516</span>, -<span class="number">0.1081</span>])</span><br><span class="line"></span><br><span class="line">torch.ceil(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([-<span class="number">0.</span>, -<span class="number">0.</span>, <span class="number">1.</span>, -<span class="number">0.</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.floor</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的floor，即不小于元素的最大整数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.5661</span>, -<span class="number">0.9135</span>,  <span class="number">1.1018</span>, -<span class="number">0.2633</span>])</span><br><span class="line"></span><br><span class="line">torch.floor(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([-<span class="number">1.</span>, -<span class="number">1.</span>,  <span class="number">1.</span>, -<span class="number">1.</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.fmod</strong>: 计算逐元素余数， 保留正负号</p>
<p><strong>torch.remainder</strong>: 计算逐元素余数， 相当于python 中的 % 操作符</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">t = torch.tensor([<span class="number">10</span>, -<span class="number">22</span>, <span class="number">31</span>, -<span class="number">47</span>])</span><br><span class="line"></span><br><span class="line">torch.fmod(t, <span class="number">5</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0</span>, -<span class="number">2</span>,  <span class="number">1</span>, -<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line">torch.remainder(t, <span class="number">5</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line">np.mod(np.array([<span class="number">10</span>, -<span class="number">22</span>, <span class="number">31</span>, -<span class="number">47</span>]), <span class="number">5</span>)</span><br><span class="line">Out[<span class="number">2</span>]: array([<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>], dtype=int32)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.clamp</strong>: torch.clamp(input, min, max, out=None) → Tensor</p>
<p>将输入<code>input</code>张量每个元素的夹紧到区间 [min,max][min,max]，并返回结果到一个新张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">      | <span class="built_in">min</span>, <span class="keyword">if</span> x_i &lt; <span class="built_in">min</span></span><br><span class="line">y_i = | x_i, <span class="keyword">if</span> <span class="built_in">min</span> &lt;= x_i &lt;= <span class="built_in">max</span></span><br><span class="line">      | <span class="built_in">max</span>, <span class="keyword">if</span> x_i &gt; <span class="built_in">max</span></span><br></pre></td></tr></table></figure>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.9105</span>, -<span class="number">0.7277</span>,  <span class="number">0.9516</span>, -<span class="number">0.1081</span>])</span><br><span class="line"></span><br><span class="line">torch.clamp(a, <span class="built_in">min</span>=-<span class="number">0.5</span>, <span class="built_in">max</span>=<span class="number">0.5</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([-<span class="number">0.5000</span>, -<span class="number">0.5000</span>,  <span class="number">0.5000</span>, -<span class="number">0.1081</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.cos</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的余弦</p>
<p><strong>torch.cosh</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的双曲余弦</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.9105</span>, -<span class="number">0.7277</span>,  <span class="number">0.9516</span>, -<span class="number">0.1081</span>])</span><br><span class="line"></span><br><span class="line">torch.cos(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">0.6133</span>, <span class="number">0.7467</span>, <span class="number">0.5804</span>, <span class="number">0.9942</span>])</span><br><span class="line"></span><br><span class="line">torch.cosh(a)</span><br><span class="line">Out[<span class="number">2</span>]: tensor([<span class="number">1.4439</span>, <span class="number">1.2766</span>, <span class="number">1.4880</span>, <span class="number">1.0058</span>])    </span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.div()</strong>: 将<code>input</code>逐元素除以标量值<code>value</code>，并返回结果到输出张量<code>out</code></p>
<p>torch.div(input, value, out=None)</p>
<p>两张量<code>input</code>和<code>other</code>逐元素相除，并将结果返回到输出</p>
<p>torch.div(<em>input</em>, <em>other</em>, <em>**, </em>rounding_mode=None<em>, </em>out=None*) → Tensor</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.9105</span>, -<span class="number">0.7277</span>,  <span class="number">0.9516</span>, -<span class="number">0.1081</span>])</span><br><span class="line"></span><br><span class="line">torch.div(a, <span class="number">0.5</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([-<span class="number">1.8210</span>, -<span class="number">1.4554</span>,  <span class="number">1.9032</span>, -<span class="number">0.2162</span>])</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">a = torch.tensor([[-<span class="number">0.3711</span>, -<span class="number">1.9353</span>, -<span class="number">0.4605</span>, -<span class="number">0.2917</span>],</span><br><span class="line">                  [ <span class="number">0.1815</span>, -<span class="number">1.0111</span>,  <span class="number">0.9805</span>, -<span class="number">1.5923</span>]])</span><br><span class="line">b = torch.tensor([ <span class="number">0.8032</span>,  <span class="number">0.2930</span>, -<span class="number">0.8113</span>, -<span class="number">0.2308</span>])</span><br><span class="line"></span><br><span class="line">torch.div(a, b, rounding_mode=<span class="string">&#x27;trunc&#x27;</span>)</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">tensor([[-<span class="number">0.</span>, -<span class="number">6.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">0.</span>, -<span class="number">3.</span>, -<span class="number">1.</span>,  <span class="number">6.</span>]])</span><br><span class="line"></span><br><span class="line">torch.div(a, b, rounding_mode=<span class="string">&#x27;floor&#x27;</span>)</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">tensor([[-<span class="number">1.</span>, -<span class="number">7.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">0.</span>, -<span class="number">4.</span>, -<span class="number">2.</span>,  <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.exp</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的指数。</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line">torch.exp(torch.Tensor([<span class="number">0</span>, math.log(<span class="number">2</span>)]))</span><br><span class="line"></span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.frac</strong>: 返回每个元素的分数部分</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.frac(torch.Tensor([<span class="number">1</span>, <span class="number">2.5</span>, -<span class="number">3.2</span>]))</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0.0000</span>,  <span class="number">0.5000</span>, -<span class="number">0.2000</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.lerp</strong>: 对两个张量以<code>start</code>，<code>end</code>做线性插值， 将结果返回到输出张量</p>
<p>torch.lerp(<em>input</em>, <em>end</em>, <em>weight</em>, <em>**, </em>out=None*)</p>
<p>参数：</p>
<ul>
<li>start (Tensor) – 起始点张量</li>
<li>end (Tensor) – 终止点张量</li>
<li>weight (float) – 插值公式的weight</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<script type="math/tex; mode=display">
out_i=start_i+weight_i∗(end_i−start_i)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">start = torch.arange(<span class="number">1.</span>, <span class="number">5.</span>)</span><br><span class="line">end = torch.empty(<span class="number">4</span>).fill_(<span class="number">10</span>)</span><br><span class="line">start, end</span><br><span class="line">Out[<span class="number">0</span>]: (tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>]), tensor([<span class="number">10.</span>, <span class="number">10.</span>, <span class="number">10.</span>, <span class="number">10.</span>]))</span><br><span class="line"></span><br><span class="line">torch.lerp(start, end, <span class="number">0.5</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">5.5000</span>, <span class="number">6.0000</span>, <span class="number">6.5000</span>, <span class="number">7.0000</span>])</span><br><span class="line">torch.lerp(start, end, torch.full_like(start, <span class="number">0.5</span>))</span><br><span class="line">Out[<span class="number">2</span>]: tensor([<span class="number">5.5000</span>, <span class="number">6.0000</span>, <span class="number">6.5000</span>, <span class="number">7.0000</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.log</strong>: 计算<code>input</code> 的自然对数</p>
<p><strong>torch.log1p</strong>: 计算$input + 1$的自然对数$y_i = log(x_i+1)$，对值比较小的输入，此函数比<code>torch.log()</code>更准确</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">5</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.3466</span>,  <span class="number">2.3803</span>, -<span class="number">0.0423</span>, -<span class="number">0.9744</span>,  <span class="number">0.4976</span>])</span><br><span class="line"></span><br><span class="line">torch.log(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([    nan,  <span class="number">0.8672</span>,     nan,     nan, -<span class="number">0.6980</span>])</span><br><span class="line"></span><br><span class="line">torch.log1p(a)</span><br><span class="line">Out[<span class="number">2</span>]: tensor([-<span class="number">0.4256</span>,  <span class="number">1.2180</span>, -<span class="number">0.0432</span>, -<span class="number">3.6633</span>,  <span class="number">0.4039</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.mul</strong>: 用标量值<code>value</code>乘以输入<code>input</code>的每个元素，并返回一个新的结果张量</p>
<p>torch.mul(input, value, out=None)</p>
<p>两个张量<code>input</code>,<code>other</code>按元素进行相乘，并返回到输出张量</p>
<p>torch.mul(input, other, out=None)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0.0603</span>, -<span class="number">0.5258</span>, -<span class="number">0.3810</span>])</span><br><span class="line">b = torch.randn(<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">1.2408</span>, -<span class="number">1.3506</span>,  <span class="number">0.9296</span>])</span><br><span class="line"></span><br><span class="line">torch.mul(a, <span class="number">100</span>)</span><br><span class="line">Out[<span class="number">2</span>]: tensor([  <span class="number">6.0299</span>, -<span class="number">52.5785</span>, -<span class="number">38.0989</span>])</span><br><span class="line"></span><br><span class="line">torch.mul(a, b)</span><br><span class="line">Out[<span class="number">3</span>]: tensor([ <span class="number">0.0748</span>,  <span class="number">0.7101</span>, -<span class="number">0.3542</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.neg</strong>: 返回一个新张量，包含输入<code>input</code> 张量按元素取负</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0.0603</span>, -<span class="number">0.5258</span>, -<span class="number">0.3810</span>])</span><br><span class="line"></span><br><span class="line">torch.neg(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([-<span class="number">0.0603</span>,  <span class="number">0.5258</span>,  <span class="number">0.3810</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.pow</strong>: torch.pow(input, exponent, out=None)</p>
<p>对输入<code>input</code>的按元素求<code>exponent</code>次幂值，并返回结果张量。 幂值<code>exponent</code> 可以为单一 <code>float</code> 数或者与<code>input</code>相同元素数的张量</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">a = torch.arange(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">exp = torch.arange(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">torch.<span class="built_in">pow</span>(a, <span class="number">2</span>)</span><br><span class="line">Out[<span class="number">2</span>]: tensor([ <span class="number">1</span>,  <span class="number">4</span>,  <span class="number">9</span>, <span class="number">16</span>])</span><br><span class="line">torch.<span class="built_in">pow</span>(a, exp)</span><br><span class="line">Out[<span class="number">3</span>]: tensor([  <span class="number">1</span>,   <span class="number">4</span>,  <span class="number">27</span>, <span class="number">256</span>])</span><br><span class="line">torch.<span class="built_in">pow</span>(<span class="number">2</span>, exp)</span><br><span class="line">Out[<span class="number">4</span>]: tensor([ <span class="number">2</span>,  <span class="number">4</span>,  <span class="number">8</span>, <span class="number">16</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.reciprocal</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的倒数，即 1.0/x</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line"></span><br><span class="line">torch.reciprocal(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">1.0000</span>, <span class="number">0.5000</span>, <span class="number">0.3333</span>, <span class="number">0.2500</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.round</strong>: 返回一个新张量，将输入<code>input</code>张量每个元素舍入到最近的整数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0.7995</span>, -<span class="number">2.0975</span>,  <span class="number">0.7273</span>,  <span class="number">0.7539</span>])</span><br><span class="line"></span><br><span class="line">torch.<span class="built_in">round</span>(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">1.</span>, -<span class="number">2.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.rsqrt</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的平方根倒数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line"></span><br><span class="line">torch.rsqrt(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">1.0000</span>, <span class="number">0.7071</span>, <span class="number">0.5774</span>, <span class="number">0.5000</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.sigmoid</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的sigmoid值</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line"></span><br><span class="line">torch.sigmoid(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">0.7311</span>, <span class="number">0.8808</span>, <span class="number">0.9526</span>, <span class="number">0.9820</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.sign</strong>: 符号函数：返回一个新张量，包含输入<code>input</code>张量每个元素的正负</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line"></span><br><span class="line">torch.sign(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.sin</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的正弦</p>
<p><strong>torch.sinh</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的双曲正弦</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line"></span><br><span class="line">torch.sin(a)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.9215</span>,  <span class="number">0.2650</span>,  <span class="number">0.8285</span>,  <span class="number">0.5914</span>])</span><br><span class="line">torch.sinh(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([-<span class="number">1.4591</span>,  <span class="number">0.2714</span>,  <span class="number">1.1392</span>,  <span class="number">0.6759</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.sqrt</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的平方根</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line"></span><br><span class="line">torch.sqrt(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">1.0000</span>, <span class="number">1.4142</span>, <span class="number">1.7321</span>, <span class="number">2.0000</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.tan</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的正切</p>
<p><strong>torch.tanh</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的双曲正切</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line"></span><br><span class="line">torch.tan(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">1.5574</span>, -<span class="number">2.1850</span>, -<span class="number">0.1425</span>,  <span class="number">1.1578</span>])</span><br><span class="line">torch.tanh(a)</span><br><span class="line">Out[<span class="number">2</span>]: tensor([<span class="number">0.7616</span>, <span class="number">0.9640</span>, <span class="number">0.9951</span>, <span class="number">0.9993</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.trunc</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的截断值(标量x的截断值是最接近其的整数)</p>
<p>简而言之，有符号数的小数部分被舍弃</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">2.1647</span>, -<span class="number">0.2294</span>,  <span class="number">0.4943</span>,  <span class="number">1.5146</span>])</span><br><span class="line"></span><br><span class="line">torch.trunc(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([-<span class="number">2.</span>, -<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>
<h2 id="Reduction-Ops"><a href="#Reduction-Ops" class="headerlink" title="Reduction Ops"></a>Reduction Ops</h2><blockquote>
<p><strong>torch.cumprod</strong>: torch.cumprod(input, dim, out=None) → Tensor</p>
<p>返回输入沿指定维度的累积，例如，如果输入是一个N 元向量，则结果也是一个N 元向量，$y<em>i= \prod </em>{i}{x_i}$</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line"></span><br><span class="line">torch.cumprod(a, dim=<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">6.</span>, <span class="number">24.</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.cumsum</strong>: torch.cumsum(input, dim, out=None) → Tensor</p>
<p>返回输入沿指定维度的累加，例如，如果输入是一个N 元向量，则结果也是一个N 元向量，$y<em>i= \sum </em>{i}{x_i}$</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line"></span><br><span class="line">torch.cumsum(a, dim=<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">1.</span>,  <span class="number">3.</span>,  <span class="number">6.</span>, <span class="number">10.</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.dist</strong>: 返回 (<code>input</code> - <code>other</code>) 的 <code>p范数</code></p>
<p>torch.dist(input, other, p=2, out=None) → Tensor</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>other (Tensor) – 右侧输入张量</li>
<li>p (float, optional) – 所计算的范数</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<script type="math/tex; mode=display">
||x||_p = (\sum _{i=1}^{n}{|x_i|^p})^{\frac {1}{p}}</script></blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">torch.dist(x, y, <span class="number">3.5</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor(<span class="number">4.0000</span>)</span><br><span class="line">torch.dist(x, y, <span class="number">3</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor(<span class="number">4.</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.norm</strong>: 返回输入张量<code>input</code> 的 p 范数</p>
<p>torch.norm(input, p=2) → float</p>
<p>返回输入张量给定维<code>dim</code> 上每行的p 范数</p>
<p>torch.norm(input, p, dim, out=None) → Tensor</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([[ <span class="number">0.7848</span>, -<span class="number">0.3629</span>,  <span class="number">0.4028</span>]])</span><br><span class="line">torch.norm(a, <span class="number">3</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor(<span class="number">0.8418</span>)</span><br><span class="line"></span><br><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">tensor([[ <span class="number">1.0718</span>,  <span class="number">3.1510</span>],</span><br><span class="line">        [-<span class="number">0.3178</span>, -<span class="number">0.9579</span>],</span><br><span class="line">        [ <span class="number">0.4065</span>,  <span class="number">0.4106</span>]])</span><br><span class="line">torch.norm(a, <span class="number">2</span>, <span class="number">1</span>)</span><br><span class="line">Out[<span class="number">3</span>]: tensor([<span class="number">3.3283</span>, <span class="number">1.0092</span>, <span class="number">0.5778</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.mean</strong>: 返回输入张量给定维度<code>dim</code>上每行的均值</p>
<p><strong>torch.median</strong>: 返回输入张量给定维度每行的中位数，同时返回一个包含中位数的索引的<code>LongTensor</code></p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>dim (int) – 缩减的维度</li>
<li>values (Tensor, optional) – 结果张量</li>
<li>indices (Tensor, optional) – 返回的索引结果张量</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line">torch.mean(x)</span><br><span class="line">Out[<span class="number">0</span>]: tensor(<span class="number">2.5000</span>)</span><br><span class="line"></span><br><span class="line">torch.median(x)</span><br><span class="line">Out[<span class="number">1</span>]: tensor(<span class="number">2.</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.mode</strong>: 返回给定维<code>dim</code>上，每行的<code>众数值</code>， 同时返回一个<code>LongTensor</code>，包含众数职的索引</p>
<p>torch.mode(input, dim=-1, values=None, indices=None) -&gt; (Tensor, LongTensor)</p>
<p>返回输入张量给定维度上每行的积</p>
<p>torch.prod(input, dim, out=None) → Tensor</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[-<span class="number">0.1038</span>,  <span class="number">0.8983</span>, -<span class="number">0.7463</span>],</span><br><span class="line">        [-<span class="number">0.6661</span>, -<span class="number">0.5061</span>,  <span class="number">0.2043</span>]])</span><br><span class="line"></span><br><span class="line">torch.mode(a, <span class="number">1</span>)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">torch.return_types.mode(</span><br><span class="line">values=tensor([-<span class="number">0.7463</span>, -<span class="number">0.6661</span>]),</span><br><span class="line">indices=tensor([<span class="number">2</span>, <span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.prod</strong>: 返回输入张量<code>input</code> 所有元素的积</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">torch.prod(x)</span><br><span class="line">Out[<span class="number">0</span>]: tensor(<span class="number">24.</span>)</span><br><span class="line">    </span><br><span class="line">y = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line">torch.prod(y, dim=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">2.</span>, <span class="number">12.</span>])    </span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.std</strong>: 返回输入张量<code>input</code> 所有元素的标准差</p>
<p>torch.std(input) → float</p>
<p>返回输入张量给定维度上每行的标准差</p>
<p>torch.std(input, dim, out=None) → Tensor</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line">torch.std(x)</span><br><span class="line">Out[<span class="number">0</span>]: tensor(<span class="number">1.2910</span>)</span><br><span class="line"></span><br><span class="line">torch.std(y, dim=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">0.7071</span>, <span class="number">0.7071</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.sum</strong>: 返回输入张量所有元素的和</p>
<p>torch.sum(input) → float</p>
<p>返回输入张量给定维度上每行的和</p>
<p>torch.sum(input, dim, out=None) → Tensor</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line">torch.var(x)</span><br><span class="line">Out[<span class="number">0</span>]: tensor(<span class="number">10.0</span>)</span><br><span class="line"></span><br><span class="line">torch.var(y, dim=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">3.</span>, <span class="number">7.</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.var</strong>: 返回输入张量所有元素的方差</p>
<p>torch.var(input) → float</p>
<p>返回输入张量给定维度上每行的方差</p>
<p>torch.var(input, dim, out=None) → Tensor</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">y = torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"></span><br><span class="line">torch.var(x)</span><br><span class="line">Out[<span class="number">0</span>]: tensor(<span class="number">1.6667</span>)</span><br><span class="line"></span><br><span class="line">torch.var(y, dim=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">0.5000</span>, <span class="number">0.5000</span>])</span><br></pre></td></tr></table></figure>
<h2 id="比较操作-Comparison-Ops"><a href="#比较操作-Comparison-Ops" class="headerlink" title="比较操作 Comparison Ops"></a>比较操作 Comparison Ops</h2><blockquote>
<p><strong>torch.eq</strong>: torch.eq(input, other, out=None) → Tensor</p>
<p>比较元素相等性。第二个参数可为一个数或与第一个参数同类型形状的张量</p>
<p><strong>torch.ge</strong>: torch.ge(input, other, out=None) → Tensor</p>
<p>逐元素比较<code>input</code>和<code>other</code>，即是否 input&gt;=other</p>
<p><strong>torch.gt</strong>: torch.gt(input, other, out=None) → Tensor</p>
<p>逐元素比较<code>input</code>和<code>other</code> ， 即是否input&gt;otherinput&gt;other </p>
<p><strong>torch.le</strong>: torch.le(input, other, out=None) → Tensor</p>
<p>逐元素比较<code>input</code>和<code>other</code> ， 即是否input&lt;=other</p>
<p><strong>torch.lt</strong>: torch.lt(input, other, out=None) → Tensor</p>
<p>逐元素比较<code>input</code>和<code>other</code> ， 即是否 input&lt;other</p>
<p><strong>torch.ne</strong>: torch.ne(input, other, out=None) → Tensor</p>
<p>逐元素比较<code>input</code>和<code>other</code>，即是否 input!=other</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">torch.eq(torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.Tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">4</span>]]))</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[ <span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">        [<span class="literal">False</span>,  <span class="literal">True</span>]])</span><br><span class="line"></span><br><span class="line">torch.ge(torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.Tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">4</span>]]))</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[ <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>,  <span class="literal">True</span>]])</span><br><span class="line"></span><br><span class="line">torch.gt(torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.Tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">4</span>]]))</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">tensor([[<span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>]])</span><br><span class="line"></span><br><span class="line">torch.le(torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.Tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">4</span>]]))</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">tensor([[ <span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">        [ <span class="literal">True</span>,  <span class="literal">True</span>]])</span><br><span class="line"></span><br><span class="line">torch.lt(torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.Tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">4</span>]]))</span><br><span class="line">Out[<span class="number">4</span>]: </span><br><span class="line">tensor([[<span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">        [ <span class="literal">True</span>, <span class="literal">False</span>]])</span><br><span class="line"></span><br><span class="line">torch.ne(torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.Tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">4</span>]]))</span><br><span class="line">Out[<span class="number">5</span>]: </span><br><span class="line">tensor([[<span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">        [ <span class="literal">True</span>, <span class="literal">False</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.equal</strong>: torch.equal(tensor1, tensor2) → bool</p>
<p>如果两个张量有相同的形状和元素值，则返回<code>True</code> ，否则 <code>False</code></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.equal(torch.Tensor([<span class="number">1</span>, <span class="number">2</span>]), torch.Tensor([<span class="number">1</span>, <span class="number">2</span>]))</span><br><span class="line">Out[<span class="number">0</span>]: <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.kthvalue</strong>: torch.kthvalue(input, k, dim=None, out=None) -&gt; (Tensor, LongTensor)</p>
<p>取输入张量<code>input</code>指定维上第k 个最小值。如果不指定<code>dim</code>，则默认为<code>input</code>的最后一维</p>
<p><strong>torch.topk</strong>: 沿给定<code>dim</code>维度返回输入张量<code>input</code>中 <code>k</code> 个最大值。 如果不指定<code>dim</code>，则默认为<code>input</code>的最后一维。 如果为<code>largest</code>为 <code>False</code> ，则返回最小的 <code>k</code> 个值</p>
<p>返回一个元组 <em>(values,indices)</em>，其中<code>indices</code>是原始输入张量<code>input</code>中测元素下标。 如果设定布尔值<code>sorted</code> 为<em>True</em>，将会确保返回的 <code>k</code> 个值被排序</p>
<p>torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>k (int) – “top-k”中的<code>k</code></li>
<li>dim (int, optional) – 排序的维</li>
<li>largest (bool, optional) – 布尔值，控制返回最大或最小值</li>
<li>sorted (bool, optional) – 布尔值，控制返回值是否排序</li>
<li>out (tuple, optional) – 可选输出张量 (Tensor, LongTensor) output buffers</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.kthvalue</span></span><br><span class="line">torch.kthvalue(x, <span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]:</span><br><span class="line">torch.return_types.kthvalue(</span><br><span class="line">values=tensor(<span class="number">4</span>),</span><br><span class="line">indices=tensor(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.topk</span></span><br><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">6</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">torch.topk(x, <span class="number">3</span>)</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">torch.return_types.topk(</span><br><span class="line">values=tensor([<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>]),</span><br><span class="line">indices=tensor([<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">torch.topk(x, <span class="number">3</span>, <span class="number">0</span>, largest=<span class="literal">False</span>)</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">torch.return_types.topk(</span><br><span class="line">values=tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]),</span><br><span class="line">indices=tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]))</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.max</strong>: 返回输入张量所有元素的最大值</p>
<p>torch.max()</p>
<p>返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引</p>
<p>torch.max(input, dim, max=None, max_indices=None) -&gt; (Tensor, LongTensor)</p>
<p><code>input</code>中<strong>逐元素</strong>与<code>other</code>相应位置的元素对比，返回最大值到输出张量</p>
<p>torch.max(input, other, out=None) → Tensor</p>
<p><strong>torch.min</strong>: 返回输入张量所有元素的最小值</p>
<p>torch.min(input) → float</p>
<p>返回输入张量给定维度上每行的最小值，并同时返回每个最小值的位置索引</p>
<p>torch.min(input, dim, min=None, min_indices=None) -&gt; (Tensor, LongTensor)</p>
<p><code>input</code>中<strong>逐元素</strong>与<code>other</code>相应位置的元素对比，返回最小值到输出张量</p>
<p>torch.min(input, other, out=None) → Tensor</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[-<span class="number">0.1204</span>, -<span class="number">0.5016</span>],</span><br><span class="line">        [ <span class="number">1.2717</span>,  <span class="number">0.7351</span>]])</span><br><span class="line">b = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[-<span class="number">1.4497</span>,  <span class="number">0.7534</span>],</span><br><span class="line">        [ <span class="number">0.5994</span>, -<span class="number">0.1490</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大值</span></span><br><span class="line">torch.<span class="built_in">max</span>(torch.arange(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">Out[<span class="number">2</span>]: tensor(<span class="number">4</span>)</span><br><span class="line">torch.<span class="built_in">max</span>(a, <span class="number">1</span>)</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">torch.return_types.<span class="built_in">max</span>(</span><br><span class="line">values=tensor([-<span class="number">0.1204</span>,  <span class="number">1.2717</span>]),</span><br><span class="line">indices=tensor([<span class="number">0</span>, <span class="number">0</span>]))</span><br><span class="line">torch.<span class="built_in">max</span>(a, b)</span><br><span class="line">Out[<span class="number">4</span>]: </span><br><span class="line">tensor([[-<span class="number">0.1204</span>,  <span class="number">0.7534</span>],</span><br><span class="line">        [ <span class="number">1.2717</span>,  <span class="number">0.7351</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小值</span></span><br><span class="line">torch.<span class="built_in">min</span>(torch.arange(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">Out[<span class="number">5</span>]: tensor(<span class="number">1</span>)</span><br><span class="line">torch.<span class="built_in">min</span>(a, <span class="number">1</span>)</span><br><span class="line">Out[<span class="number">6</span>]: </span><br><span class="line">torch.return_types.<span class="built_in">min</span>(</span><br><span class="line">values=tensor([-<span class="number">0.5016</span>,  <span class="number">0.7351</span>]),</span><br><span class="line">indices=tensor([<span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">torch.<span class="built_in">min</span>(a, b)</span><br><span class="line">Out[<span class="number">7</span>]: </span><br><span class="line">tensor([[-<span class="number">1.4497</span>, -<span class="number">0.5016</span>],</span><br><span class="line">        [ <span class="number">0.5994</span>, -<span class="number">0.1490</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.sort</strong>: torch.sort(input, dim=None, descending=False, out=None) -&gt; (Tensor, LongTensor)</p>
<p>对输入张量<code>input</code>沿着指定维按升序排序。如果不给定<code>dim</code>，则默认为输入的最后一维。如果指定参数<code>descending</code>为<code>True</code>，则按降序排序</p>
<p>返回元组 (sorted_tensor, sorted_indices) ， <code>sorted_indices</code> 为原始输入中的下标</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[-<span class="number">2.3460</span>,  <span class="number">1.3734</span>,  <span class="number">1.1444</span>, -<span class="number">0.4736</span>],</span><br><span class="line">        [-<span class="number">1.1785</span>,  <span class="number">0.8436</span>, -<span class="number">1.4403</span>, -<span class="number">0.1073</span>],</span><br><span class="line">        [-<span class="number">0.1198</span>,  <span class="number">0.7067</span>, -<span class="number">0.0734</span>, -<span class="number">1.6181</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">sorted</span>, indices = torch.sort(x)</span><br><span class="line"><span class="built_in">sorted</span>, indices</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">(tensor([[-<span class="number">2.3460</span>, -<span class="number">0.4736</span>,  <span class="number">1.1444</span>,  <span class="number">1.3734</span>],</span><br><span class="line">         [-<span class="number">1.4403</span>, -<span class="number">1.1785</span>, -<span class="number">0.1073</span>,  <span class="number">0.8436</span>],</span><br><span class="line">         [-<span class="number">1.6181</span>, -<span class="number">0.1198</span>, -<span class="number">0.0734</span>,  <span class="number">0.7067</span>]]),</span><br><span class="line"> tensor([[<span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]]))</span><br></pre></td></tr></table></figure>
<h2 id="其它操作-Other-Operations"><a href="#其它操作-Other-Operations" class="headerlink" title="其它操作 Other Operations"></a>其它操作 Other Operations</h2><blockquote>
<p><strong>torch.cross</strong>: 返回沿着维度<code>dim</code>上，两个张量<code>input</code>和<code>other</code>的向量积（叉积）。 <code>input</code>和<code>other</code> 必须有相同的形状，且指定的<code>dim</code>维上size必须为<code>3</code></p>
<p>如果不指定<code>dim</code>，则默认为第一个尺度为<code>3</code>的维</p>
<p>torch.cross(input, other, dim=-1, out=None) → Tensor</p>
</blockquote>
<script type="math/tex; mode=display">
\left[\begin{array}{l}a_{1} \\ a_{2} \\ a_{3}\end{array}\right] \times\left[\begin{array}{l}b_{1} \\ b_{2} \\ b_{3}\end{array}\right]=\left[\begin{array}{c}a_{2} b_{3}-a_{3} b_{2} \\ a_{3} b_{1}-a_{1} b_{3} \\ a_{1} b_{2}-a_{2} b_{1}\end{array}\right]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randint(<span class="number">1</span>, <span class="number">6</span>, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line">b = torch.randint(<span class="number">1</span>, <span class="number">6</span>, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line">torch.cross(a, a)</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">torch.cross(a, b)</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">tensor([[ <span class="number">3</span>, -<span class="number">5</span>,  <span class="number">1</span>],</span><br><span class="line">        [-<span class="number">8</span>,  <span class="number">1</span>, <span class="number">10</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.diag</strong>: torch.diag(input, diagonal=0, out=None) → Tensor</p>
<ul>
<li>如果输入是一个向量(1D 张量)，则返回一个以<code>input</code>为对角线元素的2D方阵</li>
<li>如果输入是一个矩阵(2D 张量)，则返回一个包含<code>input</code>对角线元素的1D张量</li>
</ul>
<p>参数<code>diagonal</code>指定对角线:</p>
<ul>
<li><code>diagonal</code> = 0, 主对角线</li>
<li><code>diagonal</code> &gt; 0, 主对角线之上</li>
<li><code>diagonal</code> &lt; 0, 主对角线之下</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果输入是一个向量(1D 张量)，则返回一个以`input`为对角线元素的2D方阵</span></span><br><span class="line">a = torch.randn(<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.3509</span>,  <span class="number">0.6176</span>, -<span class="number">1.4976</span>])</span><br><span class="line">torch.diag(a)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[-<span class="number">0.3509</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.0000</span>,  <span class="number">0.6176</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>, -<span class="number">1.4976</span>]])</span><br><span class="line">torch.diag(a, <span class="number">1</span>)</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">tensor([[ <span class="number">0.0000</span>, -<span class="number">0.3509</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.6176</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>, -<span class="number">1.4976</span>],</span><br><span class="line">        [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果输入是一个矩阵(2D 张量)，则返回一个包含`input`对角线元素的1D张量</span></span><br><span class="line"><span class="comment"># 取得给定矩阵第k个对角线:</span></span><br><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">tensor([[ <span class="number">0.8224</span>,  <span class="number">0.7792</span>,  <span class="number">0.2605</span>],</span><br><span class="line">        [-<span class="number">0.8646</span>,  <span class="number">0.2568</span>, -<span class="number">0.8189</span>],</span><br><span class="line">        [ <span class="number">1.1693</span>,  <span class="number">0.8108</span>, -<span class="number">1.9662</span>]])</span><br><span class="line">torch.diag(a, <span class="number">0</span>)</span><br><span class="line">Out[<span class="number">4</span>]: tensor([ <span class="number">0.8224</span>,  <span class="number">0.2568</span>, -<span class="number">1.9662</span>])</span><br><span class="line">torch.diag(a, <span class="number">1</span>)</span><br><span class="line">Out[<span class="number">5</span>]: tensor([ <span class="number">0.7792</span>, -<span class="number">0.8189</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.histc</strong>: torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor</p>
<p>计算输入张量的直方图。以<code>min</code>和<code>max</code>为range边界，将其均分成<code>bins</code>个直条，然后将排序好的数据划分到各个直条(bins)中</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>bins (int) – 直方图 bins(直条)的个数(默认100个)</li>
<li>min (int) – range的下边界(包含)</li>
<li>max (int) – range的上边界(包含)</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.histc(torch.FloatTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]), bins=<span class="number">4</span>, <span class="built_in">min</span>=<span class="number">0</span>, <span class="built_in">max</span>=<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">0.</span>, <span class="number">2.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br><span class="line"></span><br><span class="line">torch.histc(torch.FloatTensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]), bins=<span class="number">4</span>, <span class="built_in">min</span>=<span class="number">0</span>, <span class="built_in">max</span>=<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">0.</span>, <span class="number">3.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.renorm</strong>: torch.renorm(input, p, dim, maxnorm, out=None) → Tensor</p>
<p>返回一个张量，包含规范化后的各个子张量，使得沿着<code>dim</code>维划分的各子张量的p范数小于<code>maxnorm</code></p>
<p>如果p范数的值小于<code>maxnorm</code>，则当前子张量不需要修改</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>p (float) – 范数的p</li>
<li>dim (int) – 沿着此维切片，得到张量子集</li>
<li>maxnorm (float) – 每个子张量的范数的最大值</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">x[<span class="number">1</span>].fill_(<span class="number">2</span>)</span><br><span class="line">x[<span class="number">2</span>].fill_(<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>]])</span><br><span class="line"></span><br><span class="line">torch.renorm(x, p=<span class="number">1</span>, dim=<span class="number">0</span>, maxnorm=<span class="number">5</span>)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">        [<span class="number">1.6667</span>, <span class="number">1.6667</span>, <span class="number">1.6667</span>],</span><br><span class="line">        [<span class="number">1.6667</span>, <span class="number">1.6667</span>, <span class="number">1.6667</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.trace</strong>: 返回输入2维矩阵对角线元素的和(迹)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">10</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">torch.trace(x)</span><br><span class="line">Out[<span class="number">1</span>]: tensor(<span class="number">15</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.tril</strong>: torch.tril(input, diagonal=0, out=None) → Tensor</p>
<p>返回一个张量<code>out</code>，包含输入矩阵(2D张量)的下三角部分，<code>out</code>其余部分被设为<code>0</code></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">10</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">torch.tril(x)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">torch.tril(x, diagonal=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.triu</strong>: torch.triu(input, k=0, out=None) → Tensor</p>
<p>返回一个张量，包含输入矩阵(2D张量)的上三角部分，其余部分被设为<code>0</code>。这里所说的上三角部分为矩阵指定对角线<code>diagonal</code>之上的元素。</p>
<p>参数<code>k</code>控制对角线: - <code>k</code> = 0, 主对角线 - <code>k</code> &gt; 0, 主对角线之上 - <code>k</code> &lt; 0, 主对角线之下</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">10</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">torch.triu(x)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">torch.triu(x, diagonal=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure>
<h2 id="BLAS-and-LAPACK-Operations"><a href="#BLAS-and-LAPACK-Operations" class="headerlink" title="BLAS and LAPACK Operations"></a>BLAS and LAPACK Operations</h2><blockquote>
<p>torch.addbmm</p>
<p>torch.addmm</p>
<p>torch.addmv</p>
<p>torch.addr</p>
<p>torch.baddbmm</p>
<p>torch.bmm</p>
<p>torch.btrifact</p>
<p>torch.btrisolve</p>
<p><strong>torch.dot</strong>: 计算两个张量的点乘(内乘),两个张量都为1-D 向量</p>
<p>torch.dot(tensor1, tensor2) → float</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.dot(torch.Tensor([<span class="number">2</span>, <span class="number">3</span>]), torch.Tensor([<span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line">Out[<span class="number">0</span>]: tensor(<span class="number">7.</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.linalg.eig</strong>: 计算实方阵<code>a</code> 的特征值和特征向量</p>
<p>torch.linalg.eig(A, * , out=None)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">A = torch.randn(<span class="number">2</span>, <span class="number">2</span>, dtype=torch.complex128)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[-<span class="number">0.2029</span>-<span class="number">0.0673j</span>, -<span class="number">0.5188</span>-<span class="number">0.6723j</span>],</span><br><span class="line">        [-<span class="number">1.1984</span>+<span class="number">0.0585j</span>,  <span class="number">0.5786</span>-<span class="number">0.1849j</span>]], dtype=torch.complex128)</span><br><span class="line"></span><br><span class="line">L, V = torch.linalg.eig(A)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">(tensor([-<span class="number">0.7870</span>-<span class="number">0.5003j</span>,  <span class="number">1.1626</span>+<span class="number">0.2481j</span>], dtype=torch.complex128),</span><br><span class="line"> tensor([[ <span class="number">0.7596</span>+<span class="number">0.0000j</span>, -<span class="number">0.4008</span>-<span class="number">0.3285j</span>],</span><br><span class="line">         [ <span class="number">0.6258</span>-<span class="number">0.1770j</span>,  <span class="number">0.8552</span>+<span class="number">0.0000j</span>]], dtype=torch.complex128))</span><br></pre></td></tr></table></figure>
<h1 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h1><p><code>torch.Tensor</code>是一种包含单一数据类型元素的多维矩阵</p>
<p>Torch定义了10种CPU tensor类型和GPU tensor类型：</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>Data type</th>
<th>dtype</th>
<th>CPU tensor</th>
<th>GPU tensor</th>
</tr>
</thead>
<tbody>
<tr>
<td>32-bit floating point</td>
<td><code>torch.float32</code> or <code>torch.float</code></td>
<td><code>torch.FloatTensor</code></td>
<td><code>torch.cuda.FloatTensor</code></td>
</tr>
<tr>
<td>64-bit floating point</td>
<td><code>torch.float64</code> or <code>torch.double</code></td>
<td><code>torch.DoubleTensor</code></td>
<td><code>torch.cuda.DoubleTensor</code></td>
</tr>
<tr>
<td>16-bit floating point <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/tensors.html#id4">[1]</a></td>
<td><code>torch.float16</code> or <code>torch.half</code></td>
<td><code>torch.HalfTensor</code></td>
<td><code>torch.cuda.HalfTensor</code></td>
</tr>
<tr>
<td>16-bit floating point <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/tensors.html#id5">[2]</a></td>
<td><code>torch.bfloat16</code></td>
<td><code>torch.BFloat16Tensor</code></td>
<td><code>torch.cuda.BFloat16Tensor</code></td>
</tr>
<tr>
<td>32-bit complex</td>
<td><code>torch.complex32</code> or <code>torch.chalf</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>64-bit complex</td>
<td><code>torch.complex64</code> or <code>torch.cfloat</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>128-bit complex</td>
<td><code>torch.complex128</code> or <code>torch.cdouble</code></td>
<td></td>
<td></td>
</tr>
<tr>
<td>8-bit integer (unsigned)</td>
<td><code>torch.uint8</code></td>
<td><code>torch.ByteTensor</code></td>
<td><code>torch.cuda.ByteTensor</code></td>
</tr>
<tr>
<td>8-bit integer (signed)</td>
<td><code>torch.int8</code></td>
<td><code>torch.CharTensor</code></td>
<td><code>torch.cuda.CharTensor</code></td>
</tr>
<tr>
<td>16-bit integer (signed)</td>
<td><code>torch.int16</code> or <code>torch.short</code></td>
<td><code>torch.ShortTensor</code></td>
<td><code>torch.cuda.ShortTensor</code></td>
</tr>
<tr>
<td>32-bit integer (signed)</td>
<td><code>torch.int32</code> or <code>torch.int</code></td>
<td><code>torch.IntTensor</code></td>
<td><code>torch.cuda.IntTensor</code></td>
</tr>
<tr>
<td>64-bit integer (signed)</td>
<td><code>torch.int64</code> or <code>torch.long</code></td>
<td><code>torch.LongTensor</code></td>
<td><code>torch.cuda.LongTensor</code></td>
</tr>
<tr>
<td>Boolean</td>
<td><code>torch.bool</code></td>
<td><code>torch.BoolTensor</code></td>
<td><code>torch.cuda.BoolTensor</code></td>
</tr>
<tr>
<td>quantized 8-bit integer (unsigned)</td>
<td><code>torch.quint8</code></td>
<td><code>torch.ByteTensor</code></td>
<td>/</td>
</tr>
<tr>
<td>quantized 8-bit integer (signed)</td>
<td><code>torch.qint8</code></td>
<td><code>torch.CharTensor</code></td>
<td>/</td>
</tr>
<tr>
<td>quantized 32-bit integer (signed)</td>
<td><code>torch.qint32</code></td>
<td><code>torch.IntTensor</code></td>
<td>/</td>
</tr>
<tr>
<td>quantized 4-bit integer (unsigned) <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/tensors.html#id6">[3]</a></td>
<td><code>torch.quint4x2</code></td>
<td><code>torch.ByteTensor</code></td>
<td>/</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>创建</p>
</blockquote>
<p>一个张量tensor可以从Python的<code>list</code>或序列构建</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure>
<p>根据可选择的大小和数据新建一个tensor。 如果没有提供参数，将会返回一个空的零维张量。如果提供了<code>numpy.ndarray</code>,<code>torch.Tensor</code>或<code>torch.Storage</code>，将会返回一个有同样参数的tensor.如果提供了python序列，将会从序列的副本创建一个tensor</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 接口 一个空张量tensor可以通过规定其大小来构建</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.Tensor</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.Tensor(*sizes)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.Tensor(size)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.Tensor(sequence)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.Tensor(ndarray)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.Tensor(tensor)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.Tensor(storage)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化</span></span><br><span class="line">torch.IntTensor(<span class="number">2</span>, <span class="number">4</span>).zero_()</span><br></pre></td></tr></table></figure>
<p>可以用python的索引和切片来获取和修改一个张量tensor中的内容</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x = torch.FloatTensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">x[<span class="number">1</span>][<span class="number">2</span>]</span><br><span class="line">Out[<span class="number">0</span>]: tensor(<span class="number">6.</span>)</span><br><span class="line">    </span><br><span class="line">x[<span class="number">0</span>][<span class="number">1</span>] = <span class="number">8</span></span><br><span class="line">x</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">8.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>]])</span><br></pre></td></tr></table></figure>
<p>每一个张量tensor都有一个相应的<code>torch.Storage</code>用来保存其数据。类tensor提供了一个存储的多维的、横向视图，并且定义了在数值运算</p>
<p><strong>会改变tensor的函数操作会用一个下划线后缀来标示</strong>。比如，<code>torch.FloatTensor.abs_()</code>会在原地计算绝对值，并返回改变后的tensor，而<code>tensor.FloatTensor.abs()</code>将会在一个新的tensor中计算结果</p>
<blockquote>
<p>关键属性和方法</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_tensor.html#torch.Tensor.new_tensor"><code>Tensor.new_tensor</code></a></th>
<th>Returns a new Tensor with <code>data</code> as the tensor data.</th>
</tr>
</thead>
<tbody>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_full.html#torch.Tensor.new_full"><code>Tensor.new_full</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with <code>fill_value</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_empty.html#torch.Tensor.new_empty"><code>Tensor.new_empty</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with uninitialized data.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_ones.html#torch.Tensor.new_ones"><code>Tensor.new_ones</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with <code>1</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.new_zeros.html#torch.Tensor.new_zeros"><code>Tensor.new_zeros</code></a></td>
<td>Returns a Tensor of size <code>size</code> filled with <code>0</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_cuda.html#torch.Tensor.is_cuda"><code>Tensor.is_cuda</code></a></td>
<td>Is <code>True</code> if the Tensor is stored on the GPU, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_quantized.html#torch.Tensor.is_quantized"><code>Tensor.is_quantized</code></a></td>
<td>Is <code>True</code> if the Tensor is quantized, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_meta.html#torch.Tensor.is_meta"><code>Tensor.is_meta</code></a></td>
<td>Is <code>True</code> if the Tensor is a meta tensor, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.device.html#torch.Tensor.device"><code>Tensor.device</code></a></td>
<td>Is the <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/tensor_attributes.html#torch.device"><code>torch.device</code></a> where this Tensor is.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.grad.html#torch.Tensor.grad"><code>Tensor.grad</code></a></td>
<td>This attribute is <code>None</code> by default and becomes a Tensor the first time a call to <code>backward()</code> computes gradients for <code>self</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ndim.html#torch.Tensor.ndim"><code>Tensor.ndim</code></a></td>
<td>Alias for <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim"><code>dim()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.real.html#torch.Tensor.real"><code>Tensor.real</code></a></td>
<td>Returns a new tensor containing real values of the <code>self</code> tensor for a complex-valued input tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.imag.html#torch.Tensor.imag"><code>Tensor.imag</code></a></td>
<td>Returns a new tensor containing imaginary values of the <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.abs.html#torch.Tensor.abs"><code>Tensor.abs</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs"><code>torch.abs()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.abs_.html#torch.Tensor.abs_"><code>Tensor.abs_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.abs.html#torch.Tensor.abs"><code>abs()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.absolute.html#torch.Tensor.absolute"><code>Tensor.absolute</code></a></td>
<td>Alias for <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.abs.html#torch.abs"><code>abs()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.absolute_.html#torch.Tensor.absolute_"><code>Tensor.absolute_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.absolute.html#torch.Tensor.absolute"><code>absolute()</code></a> Alias for <code>abs_()</code></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.acos.html#torch.Tensor.acos"><code>Tensor.acos</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.acos.html#torch.acos"><code>torch.acos()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.acos_.html#torch.Tensor.acos_"><code>Tensor.acos_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.acos.html#torch.Tensor.acos"><code>acos()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccos.html#torch.Tensor.arccos"><code>Tensor.arccos</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.arccos.html#torch.arccos"><code>torch.arccos()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccos_.html#torch.Tensor.arccos_"><code>Tensor.arccos_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccos.html#torch.Tensor.arccos"><code>arccos()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.add.html#torch.Tensor.add"><code>Tensor.add</code></a></td>
<td>Add a scalar or tensor to <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.add_.html#torch.Tensor.add_"><code>Tensor.add_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.add.html#torch.Tensor.add"><code>add()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm"><code>Tensor.addbmm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.addbmm.html#torch.addbmm"><code>torch.addbmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addbmm_.html#torch.Tensor.addbmm_"><code>Tensor.addbmm_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addbmm.html#torch.Tensor.addbmm"><code>addbmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv"><code>Tensor.addcdiv</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.addcdiv.html#torch.addcdiv"><code>torch.addcdiv()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcdiv_.html#torch.Tensor.addcdiv_"><code>Tensor.addcdiv_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcdiv.html#torch.Tensor.addcdiv"><code>addcdiv()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul"><code>Tensor.addcmul</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.addcmul.html#torch.addcmul"><code>torch.addcmul()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcmul_.html#torch.Tensor.addcmul_"><code>Tensor.addcmul_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addcmul.html#torch.Tensor.addcmul"><code>addcmul()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmm.html#torch.Tensor.addmm"><code>Tensor.addmm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.addmm.html#torch.addmm"><code>torch.addmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmm_.html#torch.Tensor.addmm_"><code>Tensor.addmm_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmm.html#torch.Tensor.addmm"><code>addmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sspaddmm.html#torch.Tensor.sspaddmm"><code>Tensor.sspaddmm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sspaddmm.html#torch.sspaddmm"><code>torch.sspaddmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmv.html#torch.Tensor.addmv"><code>Tensor.addmv</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.addmv.html#torch.addmv"><code>torch.addmv()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmv_.html#torch.Tensor.addmv_"><code>Tensor.addmv_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addmv.html#torch.Tensor.addmv"><code>addmv()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addr.html#torch.Tensor.addr"><code>Tensor.addr</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.addr.html#torch.addr"><code>torch.addr()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addr_.html#torch.Tensor.addr_"><code>Tensor.addr_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.addr.html#torch.Tensor.addr"><code>addr()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.adjoint.html#torch.Tensor.adjoint"><code>Tensor.adjoint</code></a></td>
<td>Alias for <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.adjoint.html#torch.adjoint"><code>adjoint()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.allclose.html#torch.Tensor.allclose"><code>Tensor.allclose</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.allclose.html#torch.allclose"><code>torch.allclose()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.amax.html#torch.Tensor.amax"><code>Tensor.amax</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.amax.html#torch.amax"><code>torch.amax()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.amin.html#torch.Tensor.amin"><code>Tensor.amin</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.amin.html#torch.amin"><code>torch.amin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.aminmax.html#torch.Tensor.aminmax"><code>Tensor.aminmax</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.aminmax.html#torch.aminmax"><code>torch.aminmax()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.angle.html#torch.Tensor.angle"><code>Tensor.angle</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.angle.html#torch.angle"><code>torch.angle()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.apply_.html#torch.Tensor.apply_"><code>Tensor.apply_</code></a></td>
<td>Applies the function <code>callable</code> to each element in the tensor, replacing each element with the value returned by <code>callable</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.argmax.html#torch.Tensor.argmax"><code>Tensor.argmax</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.argmax.html#torch.argmax"><code>torch.argmax()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.argmin.html#torch.Tensor.argmin"><code>Tensor.argmin</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.argmin.html#torch.argmin"><code>torch.argmin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.argsort.html#torch.Tensor.argsort"><code>Tensor.argsort</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.argsort.html#torch.argsort"><code>torch.argsort()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.argwhere.html#torch.Tensor.argwhere"><code>Tensor.argwhere</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.argwhere.html#torch.argwhere"><code>torch.argwhere()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.asin.html#torch.Tensor.asin"><code>Tensor.asin</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.asin.html#torch.asin"><code>torch.asin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.asin_.html#torch.Tensor.asin_"><code>Tensor.asin_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.asin.html#torch.Tensor.asin"><code>asin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin"><code>Tensor.arcsin</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.arcsin.html#torch.arcsin"><code>torch.arcsin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsin_.html#torch.Tensor.arcsin_"><code>Tensor.arcsin_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsin.html#torch.Tensor.arcsin"><code>arcsin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.as_strided.html#torch.Tensor.as_strided"><code>Tensor.as_strided</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.as_strided.html#torch.as_strided"><code>torch.as_strided()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan.html#torch.Tensor.atan"><code>Tensor.atan</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.atan.html#torch.atan"><code>torch.atan()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan_.html#torch.Tensor.atan_"><code>Tensor.atan_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan.html#torch.Tensor.atan"><code>atan()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan.html#torch.Tensor.arctan"><code>Tensor.arctan</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.arctan.html#torch.arctan"><code>torch.arctan()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan_.html#torch.Tensor.arctan_"><code>Tensor.arctan_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan.html#torch.Tensor.arctan"><code>arctan()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan2.html#torch.Tensor.atan2"><code>Tensor.atan2</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.atan2.html#torch.atan2"><code>torch.atan2()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan2_.html#torch.Tensor.atan2_"><code>Tensor.atan2_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atan2.html#torch.Tensor.atan2"><code>atan2()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan2.html#torch.Tensor.arctan2"><code>Tensor.arctan2</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.arctan2.html#torch.arctan2"><code>torch.arctan2()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctan2_.html#torch.Tensor.arctan2_"><code>Tensor.arctan2_</code></a></td>
<td>atan2_(other) -&gt; Tensor</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.all.html#torch.Tensor.all"><code>Tensor.all</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.all.html#torch.all"><code>torch.all()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.any.html#torch.Tensor.any"><code>Tensor.any</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.any.html#torch.any"><code>torch.any()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.backward.html#torch.Tensor.backward"><code>Tensor.backward</code></a></td>
<td>Computes the gradient of current tensor w.r.t.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm"><code>Tensor.baddbmm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.baddbmm.html#torch.baddbmm"><code>torch.baddbmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.baddbmm_.html#torch.Tensor.baddbmm_"><code>Tensor.baddbmm_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.baddbmm.html#torch.Tensor.baddbmm"><code>baddbmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli.html#torch.Tensor.bernoulli"><code>Tensor.bernoulli</code></a></td>
<td>Returns a result tensor where each \texttt{result[i]}result[i] is independently sampled from \text{Bernoulli}(\texttt{self[i]})Bernoulli(self[i]).</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bernoulli_.html#torch.Tensor.bernoulli_"><code>Tensor.bernoulli_</code></a></td>
<td>Fills each location of <code>self</code> with an independent sample from \text{Bernoulli}(\texttt{p})Bernoulli(p).</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bfloat16.html#torch.Tensor.bfloat16"><code>Tensor.bfloat16</code></a></td>
<td><code>self.bfloat16()</code> is equivalent to <code>self.to(torch.bfloat16)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bincount.html#torch.Tensor.bincount"><code>Tensor.bincount</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bincount.html#torch.bincount"><code>torch.bincount()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not"><code>Tensor.bitwise_not</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bitwise_not.html#torch.bitwise_not"><code>torch.bitwise_not()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_not_.html#torch.Tensor.bitwise_not_"><code>Tensor.bitwise_not_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_not.html#torch.Tensor.bitwise_not"><code>bitwise_not()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and"><code>Tensor.bitwise_and</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bitwise_and.html#torch.bitwise_and"><code>torch.bitwise_and()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_and_.html#torch.Tensor.bitwise_and_"><code>Tensor.bitwise_and_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_and.html#torch.Tensor.bitwise_and"><code>bitwise_and()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or"><code>Tensor.bitwise_or</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bitwise_or.html#torch.bitwise_or"><code>torch.bitwise_or()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_or_.html#torch.Tensor.bitwise_or_"><code>Tensor.bitwise_or_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_or.html#torch.Tensor.bitwise_or"><code>bitwise_or()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor"><code>Tensor.bitwise_xor</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bitwise_xor.html#torch.bitwise_xor"><code>torch.bitwise_xor()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_xor_.html#torch.Tensor.bitwise_xor_"><code>Tensor.bitwise_xor_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_xor.html#torch.Tensor.bitwise_xor"><code>bitwise_xor()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift"><code>Tensor.bitwise_left_shift</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bitwise_left_shift.html#torch.bitwise_left_shift"><code>torch.bitwise_left_shift()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_left_shift_.html#torch.Tensor.bitwise_left_shift_"><code>Tensor.bitwise_left_shift_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_left_shift.html#torch.Tensor.bitwise_left_shift"><code>bitwise_left_shift()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift"><code>Tensor.bitwise_right_shift</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bitwise_right_shift.html#torch.bitwise_right_shift"><code>torch.bitwise_right_shift()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_right_shift_.html#torch.Tensor.bitwise_right_shift_"><code>Tensor.bitwise_right_shift_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bitwise_right_shift.html#torch.Tensor.bitwise_right_shift"><code>bitwise_right_shift()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bmm.html#torch.Tensor.bmm"><code>Tensor.bmm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.bmm.html#torch.bmm"><code>torch.bmm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.bool.html#torch.Tensor.bool"><code>Tensor.bool</code></a></td>
<td><code>self.bool()</code> is equivalent to <code>self.to(torch.bool)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.byte.html#torch.Tensor.byte"><code>Tensor.byte</code></a></td>
<td><code>self.byte()</code> is equivalent to <code>self.to(torch.uint8)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.broadcast_to.html#torch.Tensor.broadcast_to"><code>Tensor.broadcast_to</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.broadcast_to.html#torch.broadcast_to"><code>torch.broadcast_to()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cauchy_.html#torch.Tensor.cauchy_"><code>Tensor.cauchy_</code></a></td>
<td>Fills the tensor with numbers drawn from the Cauchy distribution:</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ceil.html#torch.Tensor.ceil"><code>Tensor.ceil</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.ceil.html#torch.ceil"><code>torch.ceil()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ceil_.html#torch.Tensor.ceil_"><code>Tensor.ceil_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ceil.html#torch.Tensor.ceil"><code>ceil()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.char.html#torch.Tensor.char"><code>Tensor.char</code></a></td>
<td><code>self.char()</code> is equivalent to <code>self.to(torch.int8)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cholesky.html#torch.Tensor.cholesky"><code>Tensor.cholesky</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cholesky.html#torch.cholesky"><code>torch.cholesky()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cholesky_inverse.html#torch.Tensor.cholesky_inverse"><code>Tensor.cholesky_inverse</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cholesky_inverse.html#torch.cholesky_inverse"><code>torch.cholesky_inverse()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cholesky_solve.html#torch.Tensor.cholesky_solve"><code>Tensor.cholesky_solve</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cholesky_solve.html#torch.cholesky_solve"><code>torch.cholesky_solve()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.chunk.html#torch.Tensor.chunk"><code>Tensor.chunk</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.chunk.html#torch.chunk"><code>torch.chunk()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp.html#torch.Tensor.clamp"><code>Tensor.clamp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.clamp.html#torch.clamp"><code>torch.clamp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_"><code>Tensor.clamp_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp.html#torch.Tensor.clamp"><code>clamp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clip.html#torch.Tensor.clip"><code>Tensor.clip</code></a></td>
<td>Alias for <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp.html#torch.Tensor.clamp"><code>clamp()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clip_.html#torch.Tensor.clip_"><code>Tensor.clip_</code></a></td>
<td>Alias for <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clamp_.html#torch.Tensor.clamp_"><code>clamp_()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.clone.html#torch.Tensor.clone"><code>Tensor.clone</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.clone.html#torch.clone"><code>torch.clone()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.contiguous.html#torch.Tensor.contiguous"><code>Tensor.contiguous</code></a></td>
<td>Returns a contiguous in memory tensor containing the same data as <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.copy_.html#torch.Tensor.copy_"><code>Tensor.copy_</code></a></td>
<td>Copies the elements from <code>src</code> into <code>self</code> tensor and returns <code>self</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj.html#torch.Tensor.conj"><code>Tensor.conj</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.conj.html#torch.conj"><code>torch.conj()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical"><code>Tensor.conj_physical</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.conj_physical.html#torch.conj_physical"><code>torch.conj_physical()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj_physical_.html#torch.Tensor.conj_physical_"><code>Tensor.conj_physical_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.conj_physical.html#torch.Tensor.conj_physical"><code>conj_physical()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.resolve_conj.html#torch.Tensor.resolve_conj"><code>Tensor.resolve_conj</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.resolve_conj.html#torch.resolve_conj"><code>torch.resolve_conj()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.resolve_neg.html#torch.Tensor.resolve_neg"><code>Tensor.resolve_neg</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.resolve_neg.html#torch.resolve_neg"><code>torch.resolve_neg()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.copysign.html#torch.Tensor.copysign"><code>Tensor.copysign</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.copysign.html#torch.copysign"><code>torch.copysign()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.copysign_.html#torch.Tensor.copysign_"><code>Tensor.copysign_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.copysign.html#torch.Tensor.copysign"><code>copysign()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cos.html#torch.Tensor.cos"><code>Tensor.cos</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cos.html#torch.cos"><code>torch.cos()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cos_.html#torch.Tensor.cos_"><code>Tensor.cos_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cos.html#torch.Tensor.cos"><code>cos()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cosh.html#torch.Tensor.cosh"><code>Tensor.cosh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cosh.html#torch.cosh"><code>torch.cosh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cosh_.html#torch.Tensor.cosh_"><code>Tensor.cosh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cosh.html#torch.Tensor.cosh"><code>cosh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.corrcoef.html#torch.Tensor.corrcoef"><code>Tensor.corrcoef</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.corrcoef.html#torch.corrcoef"><code>torch.corrcoef()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.count_nonzero.html#torch.Tensor.count_nonzero"><code>Tensor.count_nonzero</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.count_nonzero.html#torch.count_nonzero"><code>torch.count_nonzero()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cov.html#torch.Tensor.cov"><code>Tensor.cov</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cov.html#torch.cov"><code>torch.cov()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.acosh.html#torch.Tensor.acosh"><code>Tensor.acosh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.acosh.html#torch.acosh"><code>torch.acosh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.acosh_.html#torch.Tensor.acosh_"><code>Tensor.acosh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.acosh.html#torch.Tensor.acosh"><code>acosh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccosh.html#torch.Tensor.arccosh"><code>Tensor.arccosh</code></a></td>
<td>acosh() -&gt; Tensor</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arccosh_.html#torch.Tensor.arccosh_"><code>Tensor.arccosh_</code></a></td>
<td>acosh_() -&gt; Tensor</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cpu.html#torch.Tensor.cpu"><code>Tensor.cpu</code></a></td>
<td>Returns a copy of this object in CPU memory.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cross.html#torch.Tensor.cross"><code>Tensor.cross</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cross.html#torch.cross"><code>torch.cross()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cuda.html#torch.Tensor.cuda"><code>Tensor.cuda</code></a></td>
<td>Returns a copy of this object in CUDA memory.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logcumsumexp.html#torch.Tensor.logcumsumexp"><code>Tensor.logcumsumexp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logcumsumexp.html#torch.logcumsumexp"><code>torch.logcumsumexp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cummax.html#torch.Tensor.cummax"><code>Tensor.cummax</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cummax.html#torch.cummax"><code>torch.cummax()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cummin.html#torch.Tensor.cummin"><code>Tensor.cummin</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cummin.html#torch.cummin"><code>torch.cummin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod"><code>Tensor.cumprod</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cumprod.html#torch.cumprod"><code>torch.cumprod()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumprod_.html#torch.Tensor.cumprod_"><code>Tensor.cumprod_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumprod.html#torch.Tensor.cumprod"><code>cumprod()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum"><code>Tensor.cumsum</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.cumsum.html#torch.cumsum"><code>torch.cumsum()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumsum_.html#torch.Tensor.cumsum_"><code>Tensor.cumsum_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cumsum.html#torch.Tensor.cumsum"><code>cumsum()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.chalf.html#torch.Tensor.chalf"><code>Tensor.chalf</code></a></td>
<td><code>self.chalf()</code> is equivalent to <code>self.to(torch.complex32)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cfloat.html#torch.Tensor.cfloat"><code>Tensor.cfloat</code></a></td>
<td><code>self.cfloat()</code> is equivalent to <code>self.to(torch.complex64)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.cdouble.html#torch.Tensor.cdouble"><code>Tensor.cdouble</code></a></td>
<td><code>self.cdouble()</code> is equivalent to <code>self.to(torch.complex128)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.data_ptr.html#torch.Tensor.data_ptr"><code>Tensor.data_ptr</code></a></td>
<td>Returns the address of the first element of <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.deg2rad.html#torch.Tensor.deg2rad"><code>Tensor.deg2rad</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.deg2rad.html#torch.deg2rad"><code>torch.deg2rad()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dequantize.html#torch.Tensor.dequantize"><code>Tensor.dequantize</code></a></td>
<td>Given a quantized Tensor, dequantize it and return the dequantized float Tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.det.html#torch.Tensor.det"><code>Tensor.det</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.det.html#torch.det"><code>torch.det()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dense_dim.html#torch.Tensor.dense_dim"><code>Tensor.dense_dim</code></a></td>
<td>Return the number of dense dimensions in a <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/sparse.html#sparse-docs">sparse tensor</a> <code>self</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.detach.html#torch.Tensor.detach"><code>Tensor.detach</code></a></td>
<td>Returns a new Tensor, detached from the current graph.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.detach_.html#torch.Tensor.detach_"><code>Tensor.detach_</code></a></td>
<td>Detaches the Tensor from the graph that created it, making it a leaf.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.diag.html#torch.Tensor.diag"><code>Tensor.diag</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.diag.html#torch.diag"><code>torch.diag()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.diag_embed.html#torch.Tensor.diag_embed"><code>Tensor.diag_embed</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.diag_embed.html#torch.diag_embed"><code>torch.diag_embed()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.diagflat.html#torch.Tensor.diagflat"><code>Tensor.diagflat</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.diagflat.html#torch.diagflat"><code>torch.diagflat()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.diagonal.html#torch.Tensor.diagonal"><code>Tensor.diagonal</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.diagonal.html#torch.diagonal"><code>torch.diagonal()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.diagonal_scatter.html#torch.Tensor.diagonal_scatter"><code>Tensor.diagonal_scatter</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.diagonal_scatter.html#torch.diagonal_scatter"><code>torch.diagonal_scatter()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fill_diagonal_.html#torch.Tensor.fill_diagonal_"><code>Tensor.fill_diagonal_</code></a></td>
<td>Fill the main diagonal of a tensor that has at least 2-dimensions.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmax.html#torch.Tensor.fmax"><code>Tensor.fmax</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.fmax.html#torch.fmax"><code>torch.fmax()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmin.html#torch.Tensor.fmin"><code>Tensor.fmin</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.fmin.html#torch.fmin"><code>torch.fmin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.diff.html#torch.Tensor.diff"><code>Tensor.diff</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.diff.html#torch.diff"><code>torch.diff()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.digamma.html#torch.Tensor.digamma"><code>Tensor.digamma</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.digamma.html#torch.digamma"><code>torch.digamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.digamma_.html#torch.Tensor.digamma_"><code>Tensor.digamma_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.digamma.html#torch.Tensor.digamma"><code>digamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim"><code>Tensor.dim</code></a></td>
<td>Returns the number of dimensions of <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dist.html#torch.Tensor.dist"><code>Tensor.dist</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.dist.html#torch.dist"><code>torch.dist()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.div.html#torch.Tensor.div"><code>Tensor.div</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.div.html#torch.div"><code>torch.div()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.div_.html#torch.Tensor.div_"><code>Tensor.div_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.div.html#torch.Tensor.div"><code>div()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.divide.html#torch.Tensor.divide"><code>Tensor.divide</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.divide.html#torch.divide"><code>torch.divide()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.divide_.html#torch.Tensor.divide_"><code>Tensor.divide_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.divide.html#torch.Tensor.divide"><code>divide()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dot.html#torch.Tensor.dot"><code>Tensor.dot</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.dot.html#torch.dot"><code>torch.dot()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.double.html#torch.Tensor.double"><code>Tensor.double</code></a></td>
<td><code>self.double()</code> is equivalent to <code>self.to(torch.float64)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dsplit.html#torch.Tensor.dsplit"><code>Tensor.dsplit</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.dsplit.html#torch.dsplit"><code>torch.dsplit()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.element_size.html#torch.Tensor.element_size"><code>Tensor.element_size</code></a></td>
<td>Returns the size in bytes of an individual element.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.eq.html#torch.Tensor.eq"><code>Tensor.eq</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.eq.html#torch.eq"><code>torch.eq()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.eq_.html#torch.Tensor.eq_"><code>Tensor.eq_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.eq.html#torch.Tensor.eq"><code>eq()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.equal.html#torch.Tensor.equal"><code>Tensor.equal</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.equal.html#torch.equal"><code>torch.equal()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erf.html#torch.Tensor.erf"><code>Tensor.erf</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.erf.html#torch.erf"><code>torch.erf()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erf_.html#torch.Tensor.erf_"><code>Tensor.erf_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erf.html#torch.Tensor.erf"><code>erf()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfc.html#torch.Tensor.erfc"><code>Tensor.erfc</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.erfc.html#torch.erfc"><code>torch.erfc()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfc_.html#torch.Tensor.erfc_"><code>Tensor.erfc_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfc.html#torch.Tensor.erfc"><code>erfc()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv"><code>Tensor.erfinv</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.erfinv.html#torch.erfinv"><code>torch.erfinv()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfinv_.html#torch.Tensor.erfinv_"><code>Tensor.erfinv_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.erfinv.html#torch.Tensor.erfinv"><code>erfinv()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.exp.html#torch.Tensor.exp"><code>Tensor.exp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.exp.html#torch.exp"><code>torch.exp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.exp_.html#torch.Tensor.exp_"><code>Tensor.exp_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.exp.html#torch.Tensor.exp"><code>exp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.expm1.html#torch.Tensor.expm1"><code>Tensor.expm1</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.expm1.html#torch.expm1"><code>torch.expm1()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.expm1_.html#torch.Tensor.expm1_"><code>Tensor.expm1_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.expm1.html#torch.Tensor.expm1"><code>expm1()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html#torch.Tensor.expand"><code>Tensor.expand</code></a></td>
<td>Returns a new view of the <code>self</code> tensor with singleton dimensions expanded to a larger size.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.expand_as.html#torch.Tensor.expand_as"><code>Tensor.expand_as</code></a></td>
<td>Expand this tensor to the same size as <code>other</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.exponential_.html#torch.Tensor.exponential_"><code>Tensor.exponential_</code></a></td>
<td>Fills <code>self</code> tensor with elements drawn from the exponential distribution:</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fix.html#torch.Tensor.fix"><code>Tensor.fix</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.fix.html#torch.fix"><code>torch.fix()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fix_.html#torch.Tensor.fix_"><code>Tensor.fix_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fix.html#torch.Tensor.fix"><code>fix()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fill_.html#torch.Tensor.fill_"><code>Tensor.fill_</code></a></td>
<td>Fills <code>self</code> tensor with the specified value.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.flatten.html#torch.Tensor.flatten"><code>Tensor.flatten</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.flatten.html#torch.flatten"><code>torch.flatten()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.flip.html#torch.Tensor.flip"><code>Tensor.flip</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.flip.html#torch.flip"><code>torch.flip()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fliplr.html#torch.Tensor.fliplr"><code>Tensor.fliplr</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.fliplr.html#torch.fliplr"><code>torch.fliplr()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.flipud.html#torch.Tensor.flipud"><code>Tensor.flipud</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.flipud.html#torch.flipud"><code>torch.flipud()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.float.html#torch.Tensor.float"><code>Tensor.float</code></a></td>
<td><code>self.float()</code> is equivalent to <code>self.to(torch.float32)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.float_power.html#torch.Tensor.float_power"><code>Tensor.float_power</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.float_power.html#torch.float_power"><code>torch.float_power()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.float_power_.html#torch.Tensor.float_power_"><code>Tensor.float_power_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.float_power.html#torch.Tensor.float_power"><code>float_power()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor.html#torch.Tensor.floor"><code>Tensor.floor</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.floor.html#torch.floor"><code>torch.floor()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_.html#torch.Tensor.floor_"><code>Tensor.floor_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor.html#torch.Tensor.floor"><code>floor()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide"><code>Tensor.floor_divide</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.floor_divide.html#torch.floor_divide"><code>torch.floor_divide()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_divide_.html#torch.Tensor.floor_divide_"><code>Tensor.floor_divide_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.floor_divide.html#torch.Tensor.floor_divide"><code>floor_divide()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmod.html#torch.Tensor.fmod"><code>Tensor.fmod</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.fmod.html#torch.fmod"><code>torch.fmod()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmod_.html#torch.Tensor.fmod_"><code>Tensor.fmod_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.fmod.html#torch.Tensor.fmod"><code>fmod()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.frac.html#torch.Tensor.frac"><code>Tensor.frac</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.frac.html#torch.frac"><code>torch.frac()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.frac_.html#torch.Tensor.frac_"><code>Tensor.frac_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.frac.html#torch.Tensor.frac"><code>frac()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.frexp.html#torch.Tensor.frexp"><code>Tensor.frexp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.frexp.html#torch.frexp"><code>torch.frexp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.gather.html#torch.Tensor.gather"><code>Tensor.gather</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.gather.html#torch.gather"><code>torch.gather()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.gcd.html#torch.Tensor.gcd"><code>Tensor.gcd</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.gcd.html#torch.gcd"><code>torch.gcd()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.gcd_.html#torch.Tensor.gcd_"><code>Tensor.gcd_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.gcd.html#torch.Tensor.gcd"><code>gcd()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ge.html#torch.Tensor.ge"><code>Tensor.ge</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.ge.html#torch.ge"><code>torch.ge()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ge_.html#torch.Tensor.ge_"><code>Tensor.ge_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ge.html#torch.Tensor.ge"><code>ge()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal"><code>Tensor.greater_equal</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.greater_equal.html#torch.greater_equal"><code>torch.greater_equal()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_equal_.html#torch.Tensor.greater_equal_"><code>Tensor.greater_equal_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_equal.html#torch.Tensor.greater_equal"><code>greater_equal()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.geometric_.html#torch.Tensor.geometric_"><code>Tensor.geometric_</code></a></td>
<td>Fills <code>self</code> tensor with elements drawn from the geometric distribution:</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.geqrf.html#torch.Tensor.geqrf"><code>Tensor.geqrf</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.geqrf.html#torch.geqrf"><code>torch.geqrf()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ger.html#torch.Tensor.ger"><code>Tensor.ger</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.ger.html#torch.ger"><code>torch.ger()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.get_device.html#torch.Tensor.get_device"><code>Tensor.get_device</code></a></td>
<td>For CUDA tensors, this function returns the device ordinal of the GPU on which the tensor resides.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.gt.html#torch.Tensor.gt"><code>Tensor.gt</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.gt.html#torch.gt"><code>torch.gt()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.gt_.html#torch.Tensor.gt_"><code>Tensor.gt_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.gt.html#torch.Tensor.gt"><code>gt()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater.html#torch.Tensor.greater"><code>Tensor.greater</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.greater.html#torch.greater"><code>torch.greater()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater_.html#torch.Tensor.greater_"><code>Tensor.greater_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.greater.html#torch.Tensor.greater"><code>greater()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.half.html#torch.Tensor.half"><code>Tensor.half</code></a></td>
<td><code>self.half()</code> is equivalent to <code>self.to(torch.float16)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.hardshrink.html#torch.Tensor.hardshrink"><code>Tensor.hardshrink</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nn.functional.hardshrink.html#torch.nn.functional.hardshrink"><code>torch.nn.functional.hardshrink()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.heaviside.html#torch.Tensor.heaviside"><code>Tensor.heaviside</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.heaviside.html#torch.heaviside"><code>torch.heaviside()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.histc.html#torch.Tensor.histc"><code>Tensor.histc</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.histc.html#torch.histc"><code>torch.histc()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.histogram.html#torch.Tensor.histogram"><code>Tensor.histogram</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.histogram.html#torch.histogram"><code>torch.histogram()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.hsplit.html#torch.Tensor.hsplit"><code>Tensor.hsplit</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.hsplit.html#torch.hsplit"><code>torch.hsplit()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.hypot.html#torch.Tensor.hypot"><code>Tensor.hypot</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.hypot.html#torch.hypot"><code>torch.hypot()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.hypot_.html#torch.Tensor.hypot_"><code>Tensor.hypot_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.hypot.html#torch.Tensor.hypot"><code>hypot()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.i0.html#torch.Tensor.i0"><code>Tensor.i0</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.i0.html#torch.i0"><code>torch.i0()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.i0_.html#torch.Tensor.i0_"><code>Tensor.i0_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.i0.html#torch.Tensor.i0"><code>i0()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.igamma.html#torch.Tensor.igamma"><code>Tensor.igamma</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.igamma.html#torch.igamma"><code>torch.igamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.igamma_.html#torch.Tensor.igamma_"><code>Tensor.igamma_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.igamma.html#torch.Tensor.igamma"><code>igamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.igammac.html#torch.Tensor.igammac"><code>Tensor.igammac</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.igammac.html#torch.igammac"><code>torch.igammac()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.igammac_.html#torch.Tensor.igammac_"><code>Tensor.igammac_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.igammac.html#torch.Tensor.igammac"><code>igammac()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_"><code>Tensor.index_add_</code></a></td>
<td>Accumulate the elements of <code>alpha</code> times <code>source</code> into the <code>self</code> tensor by adding to the indices in the order given in <code>index</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_add.html#torch.Tensor.index_add"><code>Tensor.index_add</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_add_.html#torch.Tensor.index_add_"><code>torch.Tensor.index_add_()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_"><code>Tensor.index_copy_</code></a></td>
<td>Copies the elements of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"><code>tensor</code></a> into the <code>self</code> tensor by selecting the indices in the order given in <code>index</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_copy.html#torch.Tensor.index_copy"><code>Tensor.index_copy</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_copy_.html#torch.Tensor.index_copy_"><code>torch.Tensor.index_copy_()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_"><code>Tensor.index_fill_</code></a></td>
<td>Fills the elements of the <code>self</code> tensor with value <code>value</code> by selecting the indices in the order given in <code>index</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_fill.html#torch.Tensor.index_fill"><code>Tensor.index_fill</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_fill_.html#torch.Tensor.index_fill_"><code>torch.Tensor.index_fill_()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_"><code>Tensor.index_put_</code></a></td>
<td>Puts values from the tensor <code>values</code> into the tensor <code>self</code> using the indices specified in <code>indices</code> (which is a tuple of Tensors).</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_put.html#torch.Tensor.index_put"><code>Tensor.index_put</code></a></td>
<td>Out-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_put_.html#torch.Tensor.index_put_"><code>index_put_()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_reduce_.html#torch.Tensor.index_reduce_"><code>Tensor.index_reduce_</code></a></td>
<td>Accumulate the elements of <code>source</code> into the <code>self</code> tensor by accumulating to the indices in the order given in <code>index</code> using the reduction given by the <code>reduce</code> argument.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_reduce.html#torch.Tensor.index_reduce"><code>Tensor.index_reduce</code></a></td>
<td></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.index_select.html#torch.Tensor.index_select"><code>Tensor.index_select</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.index_select.html#torch.index_select"><code>torch.index_select()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.indices.html#torch.Tensor.indices"><code>Tensor.indices</code></a></td>
<td>Return the indices tensor of a <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs">sparse COO tensor</a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.inner.html#torch.Tensor.inner"><code>Tensor.inner</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.inner.html#torch.inner"><code>torch.inner()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.int.html#torch.Tensor.int"><code>Tensor.int</code></a></td>
<td><code>self.int()</code> is equivalent to <code>self.to(torch.int32)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.int_repr.html#torch.Tensor.int_repr"><code>Tensor.int_repr</code></a></td>
<td>Given a quantized Tensor, <code>self.int_repr()</code> returns a CPU Tensor with uint8_t as data type that stores the underlying uint8_t values of the given Tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.inverse.html#torch.Tensor.inverse"><code>Tensor.inverse</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.inverse.html#torch.inverse"><code>torch.inverse()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.isclose.html#torch.Tensor.isclose"><code>Tensor.isclose</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.isclose.html#torch.isclose"><code>torch.isclose()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.isfinite.html#torch.Tensor.isfinite"><code>Tensor.isfinite</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.isfinite.html#torch.isfinite"><code>torch.isfinite()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.isinf.html#torch.Tensor.isinf"><code>Tensor.isinf</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.isinf.html#torch.isinf"><code>torch.isinf()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.isposinf.html#torch.Tensor.isposinf"><code>Tensor.isposinf</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.isposinf.html#torch.isposinf"><code>torch.isposinf()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.isneginf.html#torch.Tensor.isneginf"><code>Tensor.isneginf</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.isneginf.html#torch.isneginf"><code>torch.isneginf()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.isnan.html#torch.Tensor.isnan"><code>Tensor.isnan</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.isnan.html#torch.isnan"><code>torch.isnan()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_contiguous.html#torch.Tensor.is_contiguous"><code>Tensor.is_contiguous</code></a></td>
<td>Returns True if <code>self</code> tensor is contiguous in memory in the order specified by memory format.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_complex.html#torch.Tensor.is_complex"><code>Tensor.is_complex</code></a></td>
<td>Returns True if the data type of <code>self</code> is a complex data type.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_conj.html#torch.Tensor.is_conj"><code>Tensor.is_conj</code></a></td>
<td>Returns True if the conjugate bit of <code>self</code> is set to true.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_floating_point.html#torch.Tensor.is_floating_point"><code>Tensor.is_floating_point</code></a></td>
<td>Returns True if the data type of <code>self</code> is a floating point data type.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_inference.html#torch.Tensor.is_inference"><code>Tensor.is_inference</code></a></td>
<td>See <code>torch.is_inference()</code></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_leaf.html#torch.Tensor.is_leaf"><code>Tensor.is_leaf</code></a></td>
<td>All Tensors that have <code>requires_grad</code> which is <code>False</code> will be leaf Tensors by convention.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_pinned.html#torch.Tensor.is_pinned"><code>Tensor.is_pinned</code></a></td>
<td>Returns true if this tensor resides in pinned memory.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_set_to.html#torch.Tensor.is_set_to"><code>Tensor.is_set_to</code></a></td>
<td>Returns True if both tensors are pointing to the exact same memory (same storage, offset, size and stride).</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_shared.html#torch.Tensor.is_shared"><code>Tensor.is_shared</code></a></td>
<td>Checks if tensor is in shared memory.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_signed.html#torch.Tensor.is_signed"><code>Tensor.is_signed</code></a></td>
<td>Returns True if the data type of <code>self</code> is a signed data type.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.is_sparse.html#torch.Tensor.is_sparse"><code>Tensor.is_sparse</code></a></td>
<td>Is <code>True</code> if the Tensor uses sparse storage layout, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.istft.html#torch.Tensor.istft"><code>Tensor.istft</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.istft.html#torch.istft"><code>torch.istft()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.isreal.html#torch.Tensor.isreal"><code>Tensor.isreal</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.isreal.html#torch.isreal"><code>torch.isreal()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.item.html#torch.Tensor.item"><code>Tensor.item</code></a></td>
<td>Returns the value of this tensor as a standard Python number.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.kthvalue.html#torch.Tensor.kthvalue"><code>Tensor.kthvalue</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.kthvalue.html#torch.kthvalue"><code>torch.kthvalue()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lcm.html#torch.Tensor.lcm"><code>Tensor.lcm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.lcm.html#torch.lcm"><code>torch.lcm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lcm_.html#torch.Tensor.lcm_"><code>Tensor.lcm_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lcm.html#torch.Tensor.lcm"><code>lcm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp"><code>Tensor.ldexp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.ldexp.html#torch.ldexp"><code>torch.ldexp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ldexp_.html#torch.Tensor.ldexp_"><code>Tensor.ldexp_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ldexp.html#torch.Tensor.ldexp"><code>ldexp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.le.html#torch.Tensor.le"><code>Tensor.le</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.le.html#torch.le"><code>torch.le()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.le_.html#torch.Tensor.le_"><code>Tensor.le_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.le.html#torch.Tensor.le"><code>le()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal"><code>Tensor.less_equal</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.less_equal.html#torch.less_equal"><code>torch.less_equal()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_equal_.html#torch.Tensor.less_equal_"><code>Tensor.less_equal_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_equal.html#torch.Tensor.less_equal"><code>less_equal()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lerp.html#torch.Tensor.lerp"><code>Tensor.lerp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.lerp.html#torch.lerp"><code>torch.lerp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lerp_.html#torch.Tensor.lerp_"><code>Tensor.lerp_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lerp.html#torch.Tensor.lerp"><code>lerp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma"><code>Tensor.lgamma</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.lgamma.html#torch.lgamma"><code>torch.lgamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lgamma_.html#torch.Tensor.lgamma_"><code>Tensor.lgamma_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lgamma.html#torch.Tensor.lgamma"><code>lgamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log.html#torch.Tensor.log"><code>Tensor.log</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.log.html#torch.log"><code>torch.log()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log_.html#torch.Tensor.log_"><code>Tensor.log_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log.html#torch.Tensor.log"><code>log()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logdet.html#torch.Tensor.logdet"><code>Tensor.logdet</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logdet.html#torch.logdet"><code>torch.logdet()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log10.html#torch.Tensor.log10"><code>Tensor.log10</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.log10.html#torch.log10"><code>torch.log10()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log10_.html#torch.Tensor.log10_"><code>Tensor.log10_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log10.html#torch.Tensor.log10"><code>log10()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log1p.html#torch.Tensor.log1p"><code>Tensor.log1p</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.log1p.html#torch.log1p"><code>torch.log1p()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log1p_.html#torch.Tensor.log1p_"><code>Tensor.log1p_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log1p.html#torch.Tensor.log1p"><code>log1p()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log2.html#torch.Tensor.log2"><code>Tensor.log2</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.log2.html#torch.log2"><code>torch.log2()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log2_.html#torch.Tensor.log2_"><code>Tensor.log2_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log2.html#torch.Tensor.log2"><code>log2()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.log_normal_.html#torch.Tensor.log_normal_"><code>Tensor.log_normal_</code></a></td>
<td>Fills <code>self</code> tensor with numbers samples from the log-normal distribution parameterized by the given mean \mu<em>μ</em> and standard deviation \sigma<em>σ</em>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logaddexp.html#torch.Tensor.logaddexp"><code>Tensor.logaddexp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logaddexp.html#torch.logaddexp"><code>torch.logaddexp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logaddexp2.html#torch.Tensor.logaddexp2"><code>Tensor.logaddexp2</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logaddexp2.html#torch.logaddexp2"><code>torch.logaddexp2()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logsumexp.html#torch.Tensor.logsumexp"><code>Tensor.logsumexp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logsumexp.html#torch.logsumexp"><code>torch.logsumexp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and"><code>Tensor.logical_and</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logical_and.html#torch.logical_and"><code>torch.logical_and()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_and_.html#torch.Tensor.logical_and_"><code>Tensor.logical_and_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_and.html#torch.Tensor.logical_and"><code>logical_and()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not"><code>Tensor.logical_not</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logical_not.html#torch.logical_not"><code>torch.logical_not()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_not_.html#torch.Tensor.logical_not_"><code>Tensor.logical_not_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_not.html#torch.Tensor.logical_not"><code>logical_not()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or"><code>Tensor.logical_or</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logical_or.html#torch.logical_or"><code>torch.logical_or()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_or_.html#torch.Tensor.logical_or_"><code>Tensor.logical_or_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_or.html#torch.Tensor.logical_or"><code>logical_or()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor"><code>Tensor.logical_xor</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logical_xor.html#torch.logical_xor"><code>torch.logical_xor()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_xor_.html#torch.Tensor.logical_xor_"><code>Tensor.logical_xor_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logical_xor.html#torch.Tensor.logical_xor"><code>logical_xor()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logit.html#torch.Tensor.logit"><code>Tensor.logit</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.logit.html#torch.logit"><code>torch.logit()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logit_.html#torch.Tensor.logit_"><code>Tensor.logit_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.logit.html#torch.Tensor.logit"><code>logit()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.long.html#torch.Tensor.long"><code>Tensor.long</code></a></td>
<td><code>self.long()</code> is equivalent to <code>self.to(torch.int64)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lt.html#torch.Tensor.lt"><code>Tensor.lt</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.lt.html#torch.lt"><code>torch.lt()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lt_.html#torch.Tensor.lt_"><code>Tensor.lt_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lt.html#torch.Tensor.lt"><code>lt()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.less.html#torch.Tensor.less"><code>Tensor.less</code></a></td>
<td>lt(other) -&gt; Tensor</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.less_.html#torch.Tensor.less_"><code>Tensor.less_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.less.html#torch.Tensor.less"><code>less()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lu.html#torch.Tensor.lu"><code>Tensor.lu</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.lu.html#torch.lu"><code>torch.lu()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.lu_solve.html#torch.Tensor.lu_solve"><code>Tensor.lu_solve</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.lu_solve.html#torch.lu_solve"><code>torch.lu_solve()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.as_subclass.html#torch.Tensor.as_subclass"><code>Tensor.as_subclass</code></a></td>
<td>Makes a <code>cls</code> instance with the same data pointer as <code>self</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.map_.html#torch.Tensor.map_"><code>Tensor.map_</code></a></td>
<td>Applies <code>callable</code> for each element in <code>self</code> tensor and the given <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"><code>tensor</code></a> and stores the results in <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_"><code>Tensor.masked_scatter_</code></a></td>
<td>Copies elements from <code>source</code> into <code>self</code> tensor at positions where the <code>mask</code> is True.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_scatter.html#torch.Tensor.masked_scatter"><code>Tensor.masked_scatter</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_scatter_.html#torch.Tensor.masked_scatter_"><code>torch.Tensor.masked_scatter_()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_"><code>Tensor.masked_fill_</code></a></td>
<td>Fills elements of <code>self</code> tensor with <code>value</code> where <code>mask</code> is True.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill.html#torch.Tensor.masked_fill"><code>Tensor.masked_fill</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_fill_.html#torch.Tensor.masked_fill_"><code>torch.Tensor.masked_fill_()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.masked_select.html#torch.Tensor.masked_select"><code>Tensor.masked_select</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.masked_select.html#torch.masked_select"><code>torch.masked_select()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.matmul.html#torch.Tensor.matmul"><code>Tensor.matmul</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.matmul.html#torch.matmul"><code>torch.matmul()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power"><code>Tensor.matrix_power</code></a></td>
<td>NOTE<a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.matrix_power.html#torch.Tensor.matrix_power"><code>matrix_power()</code></a> is deprecated, use <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.linalg.matrix_power.html#torch.linalg.matrix_power"><code>torch.linalg.matrix_power()</code></a> instead.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.matrix_exp.html#torch.Tensor.matrix_exp"><code>Tensor.matrix_exp</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.matrix_exp.html#torch.matrix_exp"><code>torch.matrix_exp()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.max.html#torch.Tensor.max"><code>Tensor.max</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.max.html#torch.max"><code>torch.max()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.maximum.html#torch.Tensor.maximum"><code>Tensor.maximum</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.maximum.html#torch.maximum"><code>torch.maximum()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mean.html#torch.Tensor.mean"><code>Tensor.mean</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"><code>torch.mean()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nanmean.html#torch.Tensor.nanmean"><code>Tensor.nanmean</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nanmean.html#torch.nanmean"><code>torch.nanmean()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.median.html#torch.Tensor.median"><code>Tensor.median</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.median.html#torch.median"><code>torch.median()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nanmedian.html#torch.Tensor.nanmedian"><code>Tensor.nanmedian</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nanmedian.html#torch.nanmedian"><code>torch.nanmedian()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.min.html#torch.Tensor.min"><code>Tensor.min</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.min.html#torch.min"><code>torch.min()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.minimum.html#torch.Tensor.minimum"><code>Tensor.minimum</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.minimum.html#torch.minimum"><code>torch.minimum()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mm.html#torch.Tensor.mm"><code>Tensor.mm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.mm.html#torch.mm"><code>torch.mm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.smm.html#torch.Tensor.smm"><code>Tensor.smm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.smm.html#torch.smm"><code>torch.smm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mode.html#torch.Tensor.mode"><code>Tensor.mode</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.mode.html#torch.mode"><code>torch.mode()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.movedim.html#torch.Tensor.movedim"><code>Tensor.movedim</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.movedim.html#torch.movedim"><code>torch.movedim()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.moveaxis.html#torch.Tensor.moveaxis"><code>Tensor.moveaxis</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.moveaxis.html#torch.moveaxis"><code>torch.moveaxis()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.msort.html#torch.Tensor.msort"><code>Tensor.msort</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.msort.html#torch.msort"><code>torch.msort()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mul.html#torch.Tensor.mul"><code>Tensor.mul</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.mul.html#torch.mul"><code>torch.mul()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mul_.html#torch.Tensor.mul_"><code>Tensor.mul_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mul.html#torch.Tensor.mul"><code>mul()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.multiply.html#torch.Tensor.multiply"><code>Tensor.multiply</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.multiply.html#torch.multiply"><code>torch.multiply()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.multiply_.html#torch.Tensor.multiply_"><code>Tensor.multiply_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.multiply.html#torch.Tensor.multiply"><code>multiply()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.multinomial.html#torch.Tensor.multinomial"><code>Tensor.multinomial</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.multinomial.html#torch.multinomial"><code>torch.multinomial()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mv.html#torch.Tensor.mv"><code>Tensor.mv</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.mv.html#torch.mv"><code>torch.mv()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma"><code>Tensor.mvlgamma</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.mvlgamma.html#torch.mvlgamma"><code>torch.mvlgamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mvlgamma_.html#torch.Tensor.mvlgamma_"><code>Tensor.mvlgamma_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.mvlgamma.html#torch.Tensor.mvlgamma"><code>mvlgamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nansum.html#torch.Tensor.nansum"><code>Tensor.nansum</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nansum.html#torch.nansum"><code>torch.nansum()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.narrow.html#torch.Tensor.narrow"><code>Tensor.narrow</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.narrow.html#torch.narrow"><code>torch.narrow()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.narrow_copy.html#torch.Tensor.narrow_copy"><code>Tensor.narrow_copy</code></a></td>
<td>See <code>torch.narrow_copy()</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ndimension.html#torch.Tensor.ndimension"><code>Tensor.ndimension</code></a></td>
<td>Alias for <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.dim.html#torch.Tensor.dim"><code>dim()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num"><code>Tensor.nan_to_num</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nan_to_num.html#torch.nan_to_num"><code>torch.nan_to_num()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nan_to_num_.html#torch.Tensor.nan_to_num_"><code>Tensor.nan_to_num_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nan_to_num.html#torch.Tensor.nan_to_num"><code>nan_to_num()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ne.html#torch.Tensor.ne"><code>Tensor.ne</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.ne.html#torch.ne"><code>torch.ne()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ne_.html#torch.Tensor.ne_"><code>Tensor.ne_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ne.html#torch.Tensor.ne"><code>ne()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal"><code>Tensor.not_equal</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.not_equal.html#torch.not_equal"><code>torch.not_equal()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.not_equal_.html#torch.Tensor.not_equal_"><code>Tensor.not_equal_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.not_equal.html#torch.Tensor.not_equal"><code>not_equal()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.neg.html#torch.Tensor.neg"><code>Tensor.neg</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.neg.html#torch.neg"><code>torch.neg()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.neg_.html#torch.Tensor.neg_"><code>Tensor.neg_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.neg.html#torch.Tensor.neg"><code>neg()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.negative.html#torch.Tensor.negative"><code>Tensor.negative</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.negative.html#torch.negative"><code>torch.negative()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.negative_.html#torch.Tensor.negative_"><code>Tensor.negative_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.negative.html#torch.Tensor.negative"><code>negative()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nelement.html#torch.Tensor.nelement"><code>Tensor.nelement</code></a></td>
<td>Alias for <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.numel.html#torch.Tensor.numel"><code>numel()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter"><code>Tensor.nextafter</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nextafter.html#torch.nextafter"><code>torch.nextafter()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nextafter_.html#torch.Tensor.nextafter_"><code>Tensor.nextafter_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nextafter.html#torch.Tensor.nextafter"><code>nextafter()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nonzero.html#torch.Tensor.nonzero"><code>Tensor.nonzero</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nonzero.html#torch.nonzero"><code>torch.nonzero()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.norm.html#torch.Tensor.norm"><code>Tensor.norm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.norm.html#torch.norm"><code>torch.norm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.normal_.html#torch.Tensor.normal_"><code>Tensor.normal_</code></a></td>
<td>Fills <code>self</code> tensor with elements samples from the normal distribution parameterized by <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.mean.html#torch.mean"><code>mean</code></a> and <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"><code>std</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.numel.html#torch.Tensor.numel"><code>Tensor.numel</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.numel.html#torch.numel"><code>torch.numel()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html#torch.Tensor.numpy"><code>Tensor.numpy</code></a></td>
<td>Returns the tensor as a NumPy <code>ndarray</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.orgqr.html#torch.Tensor.orgqr"><code>Tensor.orgqr</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.orgqr.html#torch.orgqr"><code>torch.orgqr()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ormqr.html#torch.Tensor.ormqr"><code>Tensor.ormqr</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.ormqr.html#torch.ormqr"><code>torch.ormqr()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.outer.html#torch.Tensor.outer"><code>Tensor.outer</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.outer.html#torch.outer"><code>torch.outer()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.permute.html#torch.Tensor.permute"><code>Tensor.permute</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.permute.html#torch.permute"><code>torch.permute()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.pin_memory.html#torch.Tensor.pin_memory"><code>Tensor.pin_memory</code></a></td>
<td>Copies the tensor to pinned memory, if it’s not already pinned.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.pinverse.html#torch.Tensor.pinverse"><code>Tensor.pinverse</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.pinverse.html#torch.pinverse"><code>torch.pinverse()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma"><code>Tensor.polygamma</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.polygamma.html#torch.polygamma"><code>torch.polygamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.polygamma_.html#torch.Tensor.polygamma_"><code>Tensor.polygamma_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.polygamma.html#torch.Tensor.polygamma"><code>polygamma()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.positive.html#torch.Tensor.positive"><code>Tensor.positive</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.positive.html#torch.positive"><code>torch.positive()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.pow.html#torch.Tensor.pow"><code>Tensor.pow</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.pow.html#torch.pow"><code>torch.pow()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.pow_.html#torch.Tensor.pow_"><code>Tensor.pow_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.pow.html#torch.Tensor.pow"><code>pow()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.prod.html#torch.Tensor.prod"><code>Tensor.prod</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.prod.html#torch.prod"><code>torch.prod()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.put_.html#torch.Tensor.put_"><code>Tensor.put_</code></a></td>
<td>Copies the elements from <code>source</code> into the positions specified by <code>index</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.qr.html#torch.Tensor.qr"><code>Tensor.qr</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.qr.html#torch.qr"><code>torch.qr()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.qscheme.html#torch.Tensor.qscheme"><code>Tensor.qscheme</code></a></td>
<td>Returns the quantization scheme of a given QTensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.quantile.html#torch.Tensor.quantile"><code>Tensor.quantile</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.quantile.html#torch.quantile"><code>torch.quantile()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.nanquantile.html#torch.Tensor.nanquantile"><code>Tensor.nanquantile</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.nanquantile.html#torch.nanquantile"><code>torch.nanquantile()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_scale.html#torch.Tensor.q_scale"><code>Tensor.q_scale</code></a></td>
<td>Given a Tensor quantized by linear(affine) quantization, returns the scale of the underlying quantizer().</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_zero_point.html#torch.Tensor.q_zero_point"><code>Tensor.q_zero_point</code></a></td>
<td>Given a Tensor quantized by linear(affine) quantization, returns the zero_point of the underlying quantizer().</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_per_channel_scales.html#torch.Tensor.q_per_channel_scales"><code>Tensor.q_per_channel_scales</code></a></td>
<td>Given a Tensor quantized by linear (affine) per-channel quantization, returns a Tensor of scales of the underlying quantizer.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_per_channel_zero_points.html#torch.Tensor.q_per_channel_zero_points"><code>Tensor.q_per_channel_zero_points</code></a></td>
<td>Given a Tensor quantized by linear (affine) per-channel quantization, returns a tensor of zero_points of the underlying quantizer.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.q_per_channel_axis.html#torch.Tensor.q_per_channel_axis"><code>Tensor.q_per_channel_axis</code></a></td>
<td>Given a Tensor quantized by linear (affine) per-channel quantization, returns the index of dimension on which per-channel quantization is applied.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.rad2deg.html#torch.Tensor.rad2deg"><code>Tensor.rad2deg</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.rad2deg.html#torch.rad2deg"><code>torch.rad2deg()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.random_.html#torch.Tensor.random_"><code>Tensor.random_</code></a></td>
<td>Fills <code>self</code> tensor with numbers sampled from the discrete uniform distribution over <code>[from, to - 1]</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.ravel.html#torch.Tensor.ravel"><code>Tensor.ravel</code></a></td>
<td>see <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.ravel.html#torch.ravel"><code>torch.ravel()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal"><code>Tensor.reciprocal</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.reciprocal.html#torch.reciprocal"><code>torch.reciprocal()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.reciprocal_.html#torch.Tensor.reciprocal_"><code>Tensor.reciprocal_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.reciprocal.html#torch.Tensor.reciprocal"><code>reciprocal()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.record_stream.html#torch.Tensor.record_stream"><code>Tensor.record_stream</code></a></td>
<td>Ensures that the tensor memory is not reused for another tensor until all current work queued on <code>stream</code> are complete.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.register_hook.html#torch.Tensor.register_hook"><code>Tensor.register_hook</code></a></td>
<td>Registers a backward hook.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.remainder.html#torch.Tensor.remainder"><code>Tensor.remainder</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.remainder.html#torch.remainder"><code>torch.remainder()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.remainder_.html#torch.Tensor.remainder_"><code>Tensor.remainder_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.remainder.html#torch.Tensor.remainder"><code>remainder()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.renorm.html#torch.Tensor.renorm"><code>Tensor.renorm</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.renorm.html#torch.renorm"><code>torch.renorm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.renorm_.html#torch.Tensor.renorm_"><code>Tensor.renorm_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.renorm.html#torch.Tensor.renorm"><code>renorm()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.repeat.html#torch.Tensor.repeat"><code>Tensor.repeat</code></a></td>
<td>Repeats this tensor along the specified dimensions.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.repeat_interleave.html#torch.Tensor.repeat_interleave"><code>Tensor.repeat_interleave</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.repeat_interleave.html#torch.repeat_interleave"><code>torch.repeat_interleave()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad.html#torch.Tensor.requires_grad"><code>Tensor.requires_grad</code></a></td>
<td>Is <code>True</code> if gradients need to be computed for this Tensor, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.requires_grad_.html#torch.Tensor.requires_grad_"><code>Tensor.requires_grad_</code></a></td>
<td>Change if autograd should record operations on this tensor: sets this tensor’s <code>requires_grad</code> attribute in-place.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.reshape.html#torch.Tensor.reshape"><code>Tensor.reshape</code></a></td>
<td>Returns a tensor with the same data and number of elements as <code>self</code> but with the specified shape.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.reshape_as.html#torch.Tensor.reshape_as"><code>Tensor.reshape_as</code></a></td>
<td>Returns this tensor as the same shape as <code>other</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.resize_.html#torch.Tensor.resize_"><code>Tensor.resize_</code></a></td>
<td>Resizes <code>self</code> tensor to the specified size.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.resize_as_.html#torch.Tensor.resize_as_"><code>Tensor.resize_as_</code></a></td>
<td>Resizes the <code>self</code> tensor to be the same size as the specified <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor"><code>tensor</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.retain_grad.html#torch.Tensor.retain_grad"><code>Tensor.retain_grad</code></a></td>
<td>Enables this Tensor to have their <code>grad</code> populated during <code>backward()</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.retains_grad.html#torch.Tensor.retains_grad"><code>Tensor.retains_grad</code></a></td>
<td>Is <code>True</code> if this Tensor is non-leaf and its <code>grad</code> is enabled to be populated during <code>backward()</code>, <code>False</code> otherwise.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.roll.html#torch.Tensor.roll"><code>Tensor.roll</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.roll.html#torch.roll"><code>torch.roll()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.rot90.html#torch.Tensor.rot90"><code>Tensor.rot90</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.rot90.html#torch.rot90"><code>torch.rot90()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round"><code>Tensor.round</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.round.html#torch.round"><code>torch.round()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.round_.html#torch.Tensor.round_"><code>Tensor.round_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.round.html#torch.Tensor.round"><code>round()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt"><code>Tensor.rsqrt</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.rsqrt.html#torch.rsqrt"><code>torch.rsqrt()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.rsqrt_.html#torch.Tensor.rsqrt_"><code>Tensor.rsqrt_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.rsqrt.html#torch.Tensor.rsqrt"><code>rsqrt()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter.html#torch.Tensor.scatter"><code>Tensor.scatter</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"><code>torch.Tensor.scatter_()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"><code>Tensor.scatter_</code></a></td>
<td>Writes all values from the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"><code>Tensor.scatter_add_</code></a></td>
<td>Adds all values from the tensor <code>src</code> into <code>self</code> at the indices specified in the <code>index</code> tensor in a similar fashion as <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_.html#torch.Tensor.scatter_"><code>scatter_()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add.html#torch.Tensor.scatter_add"><code>Tensor.scatter_add</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_add_.html#torch.Tensor.scatter_add_"><code>torch.Tensor.scatter_add_()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_"><code>Tensor.scatter_reduce_</code></a></td>
<td>Reduces all values from the <code>src</code> tensor to the indices specified in the <code>index</code> tensor in the <code>self</code> tensor using the applied reduction defined via the <code>reduce</code> argument (<code>&quot;sum&quot;</code>, <code>&quot;prod&quot;</code>, <code>&quot;mean&quot;</code>, <code>&quot;amax&quot;</code>, <code>&quot;amin&quot;</code>).</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce.html#torch.Tensor.scatter_reduce"><code>Tensor.scatter_reduce</code></a></td>
<td>Out-of-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.scatter_reduce_.html#torch.Tensor.scatter_reduce_"><code>torch.Tensor.scatter_reduce_()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.select.html#torch.Tensor.select"><code>Tensor.select</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.select.html#torch.select"><code>torch.select()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.select_scatter.html#torch.Tensor.select_scatter"><code>Tensor.select_scatter</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.select_scatter.html#torch.select_scatter"><code>torch.select_scatter()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.set_.html#torch.Tensor.set_"><code>Tensor.set_</code></a></td>
<td>Sets the underlying storage, size, and strides.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.share_memory_.html#torch.Tensor.share_memory_"><code>Tensor.share_memory_</code></a></td>
<td>Moves the underlying storage to shared memory.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.short.html#torch.Tensor.short"><code>Tensor.short</code></a></td>
<td><code>self.short()</code> is equivalent to <code>self.to(torch.int16)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid"><code>Tensor.sigmoid</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sigmoid.html#torch.sigmoid"><code>torch.sigmoid()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sigmoid_.html#torch.Tensor.sigmoid_"><code>Tensor.sigmoid_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sigmoid.html#torch.Tensor.sigmoid"><code>sigmoid()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sign.html#torch.Tensor.sign"><code>Tensor.sign</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sign.html#torch.sign"><code>torch.sign()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sign_.html#torch.Tensor.sign_"><code>Tensor.sign_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sign.html#torch.Tensor.sign"><code>sign()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.signbit.html#torch.Tensor.signbit"><code>Tensor.signbit</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.signbit.html#torch.signbit"><code>torch.signbit()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sgn.html#torch.Tensor.sgn"><code>Tensor.sgn</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sgn.html#torch.sgn"><code>torch.sgn()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sgn_.html#torch.Tensor.sgn_"><code>Tensor.sgn_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sgn.html#torch.Tensor.sgn"><code>sgn()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sin.html#torch.Tensor.sin"><code>Tensor.sin</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sin.html#torch.sin"><code>torch.sin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sin_.html#torch.Tensor.sin_"><code>Tensor.sin_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sin.html#torch.Tensor.sin"><code>sin()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinc.html#torch.Tensor.sinc"><code>Tensor.sinc</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sinc.html#torch.sinc"><code>torch.sinc()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinc_.html#torch.Tensor.sinc_"><code>Tensor.sinc_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinc.html#torch.Tensor.sinc"><code>sinc()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinh.html#torch.Tensor.sinh"><code>Tensor.sinh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sinh.html#torch.sinh"><code>torch.sinh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinh_.html#torch.Tensor.sinh_"><code>Tensor.sinh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sinh.html#torch.Tensor.sinh"><code>sinh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.asinh.html#torch.Tensor.asinh"><code>Tensor.asinh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.asinh.html#torch.asinh"><code>torch.asinh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.asinh_.html#torch.Tensor.asinh_"><code>Tensor.asinh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.asinh.html#torch.Tensor.asinh"><code>asinh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh"><code>Tensor.arcsinh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.arcsinh.html#torch.arcsinh"><code>torch.arcsinh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsinh_.html#torch.Tensor.arcsinh_"><code>Tensor.arcsinh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arcsinh.html#torch.Tensor.arcsinh"><code>arcsinh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.size.html#torch.Tensor.size"><code>Tensor.size</code></a></td>
<td>Returns the size of the <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.slogdet.html#torch.Tensor.slogdet"><code>Tensor.slogdet</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.slogdet.html#torch.slogdet"><code>torch.slogdet()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.slice_scatter.html#torch.Tensor.slice_scatter"><code>Tensor.slice_scatter</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.slice_scatter.html#torch.slice_scatter"><code>torch.slice_scatter()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sort.html#torch.Tensor.sort"><code>Tensor.sort</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sort.html#torch.sort"><code>torch.sort()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.split.html#torch.Tensor.split"><code>Tensor.split</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.split.html#torch.split"><code>torch.split()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sparse_mask.html#torch.Tensor.sparse_mask"><code>Tensor.sparse_mask</code></a></td>
<td>Returns a new <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/sparse.html#sparse-docs">sparse tensor</a> with values from a strided tensor <code>self</code> filtered by the indices of the sparse tensor <code>mask</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sparse_dim.html#torch.Tensor.sparse_dim"><code>Tensor.sparse_dim</code></a></td>
<td>Return the number of sparse dimensions in a <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/sparse.html#sparse-docs">sparse tensor</a> <code>self</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt"><code>Tensor.sqrt</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sqrt.html#torch.sqrt"><code>torch.sqrt()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sqrt_.html#torch.Tensor.sqrt_"><code>Tensor.sqrt_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sqrt.html#torch.Tensor.sqrt"><code>sqrt()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.square.html#torch.Tensor.square"><code>Tensor.square</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.square.html#torch.square"><code>torch.square()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.square_.html#torch.Tensor.square_"><code>Tensor.square_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.square.html#torch.Tensor.square"><code>square()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze"><code>Tensor.squeeze</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.squeeze.html#torch.squeeze"><code>torch.squeeze()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze_.html#torch.Tensor.squeeze_"><code>Tensor.squeeze_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.squeeze.html#torch.Tensor.squeeze"><code>squeeze()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.std.html#torch.Tensor.std"><code>Tensor.std</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.std.html#torch.std"><code>torch.std()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.stft.html#torch.Tensor.stft"><code>Tensor.stft</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.stft.html#torch.stft"><code>torch.stft()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.storage.html#torch.Tensor.storage"><code>Tensor.storage</code></a></td>
<td>Returns the underlying storage.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.storage_offset.html#torch.Tensor.storage_offset"><code>Tensor.storage_offset</code></a></td>
<td>Returns <code>self</code> tensor’s offset in the underlying storage in terms of number of storage elements (not bytes).</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.storage_type.html#torch.Tensor.storage_type"><code>Tensor.storage_type</code></a></td>
<td>Returns the type of the underlying storage.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.stride.html#torch.Tensor.stride"><code>Tensor.stride</code></a></td>
<td>Returns the stride of <code>self</code> tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sub.html#torch.Tensor.sub"><code>Tensor.sub</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sub.html#torch.sub"><code>torch.sub()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sub_.html#torch.Tensor.sub_"><code>Tensor.sub_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sub.html#torch.Tensor.sub"><code>sub()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.subtract.html#torch.Tensor.subtract"><code>Tensor.subtract</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.subtract.html#torch.subtract"><code>torch.subtract()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.subtract_.html#torch.Tensor.subtract_"><code>Tensor.subtract_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.subtract.html#torch.Tensor.subtract"><code>subtract()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sum.html#torch.Tensor.sum"><code>Tensor.sum</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.sum.html#torch.sum"><code>torch.sum()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.sum_to_size.html#torch.Tensor.sum_to_size"><code>Tensor.sum_to_size</code></a></td>
<td>Sum <code>this</code> tensor to <code>size</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.svd.html#torch.Tensor.svd"><code>Tensor.svd</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.svd.html#torch.svd"><code>torch.svd()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.swapaxes.html#torch.Tensor.swapaxes"><code>Tensor.swapaxes</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.swapaxes.html#torch.swapaxes"><code>torch.swapaxes()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.swapdims.html#torch.Tensor.swapdims"><code>Tensor.swapdims</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.swapdims.html#torch.swapdims"><code>torch.swapdims()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.symeig.html#torch.Tensor.symeig"><code>Tensor.symeig</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.symeig.html#torch.symeig"><code>torch.symeig()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.t.html#torch.Tensor.t"><code>Tensor.t</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.t.html#torch.t"><code>torch.t()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.t_.html#torch.Tensor.t_"><code>Tensor.t_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.t.html#torch.Tensor.t"><code>t()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tensor_split.html#torch.Tensor.tensor_split"><code>Tensor.tensor_split</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tensor_split.html#torch.tensor_split"><code>torch.tensor_split()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tile.html#torch.Tensor.tile"><code>Tensor.tile</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tile.html#torch.tile"><code>torch.tile()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch.Tensor.to"><code>Tensor.to</code></a></td>
<td>Performs Tensor dtype and/or device conversion.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_mkldnn.html#torch.Tensor.to_mkldnn"><code>Tensor.to_mkldnn</code></a></td>
<td>Returns a copy of the tensor in <code>torch.mkldnn</code> layout.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.take.html#torch.Tensor.take"><code>Tensor.take</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.take.html#torch.take"><code>torch.take()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.take_along_dim.html#torch.Tensor.take_along_dim"><code>Tensor.take_along_dim</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.take_along_dim.html#torch.take_along_dim"><code>torch.take_along_dim()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tan.html#torch.Tensor.tan"><code>Tensor.tan</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tan.html#torch.tan"><code>torch.tan()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tan_.html#torch.Tensor.tan_"><code>Tensor.tan_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tan.html#torch.Tensor.tan"><code>tan()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tanh.html#torch.Tensor.tanh"><code>Tensor.tanh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tanh.html#torch.tanh"><code>torch.tanh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tanh_.html#torch.Tensor.tanh_"><code>Tensor.tanh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tanh.html#torch.Tensor.tanh"><code>tanh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atanh.html#torch.Tensor.atanh"><code>Tensor.atanh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.atanh.html#torch.atanh"><code>torch.atanh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atanh_.html#torch.Tensor.atanh_"><code>Tensor.atanh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.atanh.html#torch.Tensor.atanh"><code>atanh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh"><code>Tensor.arctanh</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.arctanh.html#torch.arctanh"><code>torch.arctanh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctanh_.html#torch.Tensor.arctanh_"><code>Tensor.arctanh_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.arctanh.html#torch.Tensor.arctanh"><code>arctanh()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tolist.html#torch.Tensor.tolist"><code>Tensor.tolist</code></a></td>
<td>Returns the tensor as a (nested) list.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.topk.html#torch.Tensor.topk"><code>Tensor.topk</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.topk.html#torch.topk"><code>torch.topk()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_dense.html#torch.Tensor.to_dense"><code>Tensor.to_dense</code></a></td>
<td>Creates a strided copy of <code>self</code> if <code>self</code> is not a strided tensor, otherwise returns <code>self</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse.html#torch.Tensor.to_sparse"><code>Tensor.to_sparse</code></a></td>
<td>Returns a sparse copy of the tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_csr.html#torch.Tensor.to_sparse_csr"><code>Tensor.to_sparse_csr</code></a></td>
<td>Convert a tensor to compressed row storage format (CSR).</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_csc.html#torch.Tensor.to_sparse_csc"><code>Tensor.to_sparse_csc</code></a></td>
<td>Convert a tensor to compressed column storage (CSC) format.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_bsr.html#torch.Tensor.to_sparse_bsr"><code>Tensor.to_sparse_bsr</code></a></td>
<td>Convert a CSR tensor to a block sparse row (BSR) storage format of given blocksize.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.to_sparse_bsc.html#torch.Tensor.to_sparse_bsc"><code>Tensor.to_sparse_bsc</code></a></td>
<td>Convert a CSR tensor to a block sparse column (BSC) storage format of given blocksize.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.trace.html#torch.Tensor.trace"><code>Tensor.trace</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.trace.html#torch.trace"><code>torch.trace()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.transpose.html#torch.Tensor.transpose"><code>Tensor.transpose</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.transpose.html#torch.transpose"><code>torch.transpose()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.transpose_.html#torch.Tensor.transpose_"><code>Tensor.transpose_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.transpose.html#torch.Tensor.transpose"><code>transpose()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.triangular_solve.html#torch.Tensor.triangular_solve"><code>Tensor.triangular_solve</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.triangular_solve.html#torch.triangular_solve"><code>torch.triangular_solve()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tril.html#torch.Tensor.tril"><code>Tensor.tril</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.tril.html#torch.tril"><code>torch.tril()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tril_.html#torch.Tensor.tril_"><code>Tensor.tril_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.tril.html#torch.Tensor.tril"><code>tril()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.triu.html#torch.Tensor.triu"><code>Tensor.triu</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.triu.html#torch.triu"><code>torch.triu()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.triu_.html#torch.Tensor.triu_"><code>Tensor.triu_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.triu.html#torch.Tensor.triu"><code>triu()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.true_divide.html#torch.Tensor.true_divide"><code>Tensor.true_divide</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.true_divide.html#torch.true_divide"><code>torch.true_divide()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_"><code>Tensor.true_divide_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.true_divide_.html#torch.Tensor.true_divide_"><code>true_divide_()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.trunc.html#torch.Tensor.trunc"><code>Tensor.trunc</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.trunc.html#torch.trunc"><code>torch.trunc()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.trunc_.html#torch.Tensor.trunc_"><code>Tensor.trunc_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.trunc.html#torch.Tensor.trunc"><code>trunc()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.type.html#torch.Tensor.type"><code>Tensor.type</code></a></td>
<td>Returns the type if dtype is not provided, else casts this object to the specified type.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.type_as.html#torch.Tensor.type_as"><code>Tensor.type_as</code></a></td>
<td>Returns this tensor cast to the type of the given tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unbind.html#torch.Tensor.unbind"><code>Tensor.unbind</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.unbind.html#torch.unbind"><code>torch.unbind()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unflatten.html#torch.Tensor.unflatten"><code>Tensor.unflatten</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.unflatten.html#torch.unflatten"><code>torch.unflatten()</code></a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unfold.html#torch.Tensor.unfold"><code>Tensor.unfold</code></a></td>
<td>Returns a view of the original tensor which contains all slices of size <code>size</code> from <code>self</code> tensor in the dimension <code>dimension</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.uniform_.html#torch.Tensor.uniform_"><code>Tensor.uniform_</code></a></td>
<td>Fills <code>self</code> tensor with numbers sampled from the continuous uniform distribution:</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unique.html#torch.Tensor.unique"><code>Tensor.unique</code></a></td>
<td>Returns the unique elements of the input tensor.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unique_consecutive.html#torch.Tensor.unique_consecutive"><code>Tensor.unique_consecutive</code></a></td>
<td>Eliminates all but the first element from every consecutive group of equivalent elements.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze"><code>Tensor.unsqueeze</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.unsqueeze.html#torch.unsqueeze"><code>torch.unsqueeze()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze_.html#torch.Tensor.unsqueeze_"><code>Tensor.unsqueeze_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.unsqueeze.html#torch.Tensor.unsqueeze"><code>unsqueeze()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.values.html#torch.Tensor.values"><code>Tensor.values</code></a></td>
<td>Return the values tensor of a <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/sparse.html#sparse-coo-docs">sparse COO tensor</a>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.var.html#torch.Tensor.var"><code>Tensor.var</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.var.html#torch.var"><code>torch.var()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.vdot.html#torch.Tensor.vdot"><code>Tensor.vdot</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.vdot.html#torch.vdot"><code>torch.vdot()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.view.html#torch.Tensor.view"><code>Tensor.view</code></a></td>
<td>Returns a new tensor with the same data as the <code>self</code> tensor but of a different <code>shape</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.view_as.html#torch.Tensor.view_as"><code>Tensor.view_as</code></a></td>
<td>View this tensor as the same size as <code>other</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.vsplit.html#torch.Tensor.vsplit"><code>Tensor.vsplit</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.vsplit.html#torch.vsplit"><code>torch.vsplit()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.where.html#torch.Tensor.where"><code>Tensor.where</code></a></td>
<td><code>self.where(condition, y)</code> is equivalent to <code>torch.where(condition, self, y)</code>.</td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy"><code>Tensor.xlogy</code></a></td>
<td>See <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.xlogy.html#torch.xlogy"><code>torch.xlogy()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.xlogy_.html#torch.Tensor.xlogy_"><code>Tensor.xlogy_</code></a></td>
<td>In-place version of <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.xlogy.html#torch.Tensor.xlogy"><code>xlogy()</code></a></td>
</tr>
<tr>
<td><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/generated/torch.Tensor.zero_.html#torch.Tensor.zero_"><code>Tensor.zero_</code></a></td>
<td>Fills <code>self</code> tensor with zeros.</td>
</tr>
</tbody>
</table>
</div>
<h2 id="storage"><a href="#storage" class="headerlink" title="storage"></a>storage</h2><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.jianshu.com/p/ebd7f6395bf4">tensor的数据结构、storage()、stride()、storage_offset()</a></p>
</blockquote>
<p>pytorch中一个tensor对象分为<strong>头信息区（Tensor）</strong>和<strong>存储区（Storage）</strong>两部分</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://upload-images.jianshu.io/upload_images/14029140-4ea52cc5277ae598.png?imageMogr2/auto-orient/strip|imageView2/2/w/719/format/webp" alt="tensor结构"><br>头信息区主要保存tensor的形状（size）、步长（stride）、数据类型（type）等信息；而真正的data（数据）则以<strong>连续一维数组</strong>的形式放在存储区，由torch.Storage实例管理着</p>
<p><strong>注意：storage永远是一维数组，任何维度的tensor的实际数据都存储在一维的storage中</strong></p>
<blockquote>
<p>获取tensor的storage</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">a = torch.tensor([[<span class="number">1.0</span>, <span class="number">4.0</span>],[<span class="number">2.0</span>, <span class="number">1.0</span>],[<span class="number">3.0</span>, <span class="number">5.0</span>]])</span><br><span class="line">a.storage()</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line"> <span class="number">1.0</span></span><br><span class="line"> <span class="number">4.0</span></span><br><span class="line"> <span class="number">2.0</span></span><br><span class="line"> <span class="number">1.0</span></span><br><span class="line"> <span class="number">3.0</span></span><br><span class="line"> <span class="number">5.0</span></span><br><span class="line">[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size <span class="number">6</span>]</span><br><span class="line"></span><br><span class="line">a.storage()[<span class="number">2</span>] = <span class="number">9</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">id</span>(a.storage())</span><br><span class="line">Out[<span class="number">1</span>]: <span class="number">1343354913168</span></span><br></pre></td></tr></table></figure>
<h1 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h1><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/AccumulateMore/CV">小土堆+李沐课程笔记</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.bilibili.com/video/BV1hE411t7RN/?p=11&amp;vd_source=d741c08a55ba6a6a780b28e90920def0">PyTorch深度学习快速入门教程（绝对通俗易懂！）【小土堆】</a></p>
</blockquote>
<h2 id="Pytorch加载数据"><a href="#Pytorch加载数据" class="headerlink" title="Pytorch加载数据"></a>Pytorch加载数据</h2><p>Pytorch中加载数据需要Dataset、Dataloader。</p>
<ul>
<li>Dataset提供一种方式去获取每个数据及其对应的label，告诉我们总共有多少个数据。</li>
<li>Dataloader为后面的网络提供不同的数据形式，它将一批一批数据进行一个打包。</li>
</ul>
<h2 id="Tensorboard"><a href="#Tensorboard" class="headerlink" title="Tensorboard"></a>Tensorboard</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备的测试数据集</span></span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor())               </span><br><span class="line"><span class="comment"># batch_size=4 使得 img0, target0 = dataset[0]、img1, target1 = dataset[1]、img2, target2 = dataset[2]、img3, target3 = dataset[3]，然后这四个数据作为Dataloader的一个返回      </span></span><br><span class="line">test_loader = DataLoader(dataset=test_data,batch_size=<span class="number">64</span>,shuffle=<span class="literal">True</span>,num_workers=<span class="number">0</span>,drop_last=<span class="literal">False</span>)      </span><br><span class="line"><span class="comment"># 用for循环取出DataLoader打包好的四个数据</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line">step = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">    imgs, targets = data <span class="comment"># 每个data都是由4张图片组成，imgs.size 为 [4,3,32,32]，四张32×32图片三通道，targets由四个标签组成             </span></span><br><span class="line">    writer.add_images(<span class="string">&quot;test_data&quot;</span>,imgs,step)</span><br><span class="line">    step = step + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
<h2 id="Transforms"><a href="#Transforms" class="headerlink" title="Transforms"></a>Transforms</h2><p>① Transforms当成工具箱的话，里面的class就是不同的工具。例如像totensor、resize这些工具。</p>
<p>② Transforms拿一些特定格式的图片，经过Transforms里面的工具，获得我们想要的结果。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"></span><br><span class="line">img_path = <span class="string">&quot;Data/FirstTypeData/val/bees/10870992_eebeeb3a12.jpg&quot;</span></span><br><span class="line">img = Image.<span class="built_in">open</span>(img_path)  </span><br><span class="line"></span><br><span class="line">tensor_trans = transforms.ToTensor()  <span class="comment"># 创建 transforms.ToTensor类 的实例化对象</span></span><br><span class="line">tensor_img = tensor_trans(img)  <span class="comment"># 调用 transforms.ToTensor类 的__call__的魔术方法   </span></span><br><span class="line"><span class="built_in">print</span>(tensor_img)</span><br></pre></td></tr></table></figure>
<h2 id="torchvision数据集"><a href="#torchvision数据集" class="headerlink" title="torchvision数据集"></a>torchvision数据集</h2><p>① torchvision中有很多数据集，当我们写代码时指定相应的数据集指定一些参数，它就可以自行下载。</p>
<p>② CIFAR-10数据集包含60000张32×32的彩色图片，一共10个类别，其中50000张训练图片，10000张测试图片。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line">train_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,download=<span class="literal">True</span>) <span class="comment"># root为存放数据集的相对路线</span></span><br><span class="line">test_set = torchvision.datasets.CIFAR10(root=<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>) <span class="comment"># train=True是训练集，train=False是测试集  </span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(test_set[<span class="number">0</span>])       <span class="comment"># 输出的3是target </span></span><br><span class="line"><span class="built_in">print</span>(test_set.classes)  <span class="comment"># 测试数据集中有多少种</span></span><br><span class="line"></span><br><span class="line">img, target = test_set[<span class="number">0</span>] <span class="comment"># 分别获得图片、target</span></span><br><span class="line"><span class="built_in">print</span>(img)</span><br><span class="line"><span class="built_in">print</span>(target)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(test_set.classes[target]) <span class="comment"># 3号target对应的种类</span></span><br><span class="line">img.show()</span><br></pre></td></tr></table></figure>
<h2 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h2><p>① Loss损失函数一方面计算实际输出和目标之间的差距。</p>
<p>② Loss损失函数另一方面为我们更新输出提供一定的依据</p>
<blockquote>
<p>L1loss损失函数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> L1Loss</span><br><span class="line">inputs = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32)</span><br><span class="line">targets = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>],dtype=torch.float32)</span><br><span class="line">inputs = torch.reshape(inputs,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">targets = torch.reshape(targets,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">loss = L1Loss()  <span class="comment"># 默认为 maen</span></span><br><span class="line">result = loss(inputs,targets)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>MSE损失函数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> L1Loss</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line">inputs = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],dtype=torch.float32)</span><br><span class="line">targets = torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">5</span>],dtype=torch.float32)</span><br><span class="line">inputs = torch.reshape(inputs,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">targets = torch.reshape(targets,(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>))</span><br><span class="line">loss_mse = nn.MSELoss()</span><br><span class="line">result_mse = loss_mse(inputs,targets)</span><br><span class="line"><span class="built_in">print</span>(result_mse)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>交叉熵损失函数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> L1Loss</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">x = torch.tensor([<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.3</span>])</span><br><span class="line">y = torch.tensor([<span class="number">1</span>])</span><br><span class="line">x = torch.reshape(x,(<span class="number">1</span>,<span class="number">3</span>)) <span class="comment"># 1的 batch_size，有三类</span></span><br><span class="line">loss_cross = nn.CrossEntropyLoss()</span><br><span class="line">result_cross = loss_cross(x,y)</span><br><span class="line"><span class="built_in">print</span>(result_cross)</span><br></pre></td></tr></table></figure>
<h2 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h2><p>① 损失函数调用backward方法，就可以调用损失函数的反向传播方法，就可以求出我们需要调节的梯度，我们就可以利用我们的优化器就可以根据梯度对参数进行调整，达到整体误差降低的目的。</p>
<p>② 梯度要清零，如果梯度不清零会导致梯度累加</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss() <span class="comment"># 交叉熵    </span></span><br><span class="line">tudui = Tudui()</span><br><span class="line">optim = torch.optim.SGD(tudui.parameters(),lr=<span class="number">0.01</span>)   <span class="comment"># 随机梯度下降优化器</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">    imgs, targets = data</span><br><span class="line">    outputs = tudui(imgs)</span><br><span class="line">    result_loss = loss(outputs, targets) <span class="comment"># 计算实际输出与目标输出的差距</span></span><br><span class="line">    optim.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">    result_loss.backward() <span class="comment"># 反向传播，计算损失函数的梯度</span></span><br><span class="line">    optim.step()   <span class="comment"># 根据梯度，对网络的参数进行调优</span></span><br><span class="line">    <span class="built_in">print</span>(result_loss) <span class="comment"># 对数据只看了一遍，只看了一轮，所以loss下降不大</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>神经网络学习率优化</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn </span><br><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> Conv2d, MaxPool2d, Flatten, Linear, Sequential</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line">dataloader = DataLoader(dataset, batch_size=<span class="number">64</span>,drop_last=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui, self).__init__()        </span><br><span class="line">        self.model1 = Sequential(</span><br><span class="line">            Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,padding=<span class="number">2</span>),</span><br><span class="line">            MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            Flatten(),</span><br><span class="line">            Linear(<span class="number">1024</span>,<span class="number">64</span>),</span><br><span class="line">            Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line">loss = nn.CrossEntropyLoss() <span class="comment"># 交叉熵    </span></span><br><span class="line">tudui = Tudui()</span><br><span class="line">optim = torch.optim.SGD(tudui.parameters(),lr=<span class="number">0.01</span>)   <span class="comment"># 随机梯度下降优化器</span></span><br><span class="line">scheduler = torch.optim.lr_scheduler.StepLR(optim, step_size=<span class="number">5</span>, gamma=<span class="number">0.1</span>) <span class="comment"># 每过 step_size 更新一次优化器，更新是学习率为原来的学习率的 0.1 倍    </span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">20</span>):</span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = tudui(imgs)</span><br><span class="line">        result_loss = loss(outputs, targets) <span class="comment"># 计算实际输出与目标输出的差距</span></span><br><span class="line">        optim.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">        result_loss.backward() <span class="comment"># 反向传播，计算损失函数的梯度</span></span><br><span class="line">        optim.step()   <span class="comment"># 根据梯度，对网络的参数进行调优</span></span><br><span class="line">        scheduler.step() <span class="comment"># 学习率太小了，所以20个轮次后，相当于没走多少</span></span><br><span class="line">        running_loss = running_loss + result_loss</span><br><span class="line">    <span class="built_in">print</span>(running_loss) <span class="comment"># 对这一轮所有误差的总和</span></span><br></pre></td></tr></table></figure>
<h2 id="网络模型使用及修改"><a href="#网络模型使用及修改" class="headerlink" title="网络模型使用及修改"></a>网络模型使用及修改</h2><blockquote>
<p>网络模型添加</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">dataset = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line">vgg16_true = torchvision.models.vgg16(pretrained=<span class="literal">True</span>) <span class="comment"># 下载卷积层对应的参数是多少、池化层对应的参数时多少，这些参数时ImageNet训练好了的</span></span><br><span class="line">vgg16_true.add_module(<span class="string">&#x27;add_linear&#x27;</span>,nn.Linear(<span class="number">1000</span>,<span class="number">10</span>)) <span class="comment"># 在VGG16后面添加一个线性层，使得输出为适应CIFAR10的输出，CIFAR10需要输出10个种类</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(vgg16_true)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>网络模型修改</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line">vgg16_false = torchvision.models.vgg16(pretrained=<span class="literal">False</span>) <span class="comment"># 没有预训练的参数     </span></span><br><span class="line"><span class="built_in">print</span>(vgg16_false)</span><br><span class="line">vgg16_false.classifier[<span class="number">6</span>] = nn.Linear(<span class="number">4096</span>,<span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(vgg16_false)</span><br></pre></td></tr></table></figure>
<h2 id="网络模型保存与读取"><a href="#网络模型保存与读取" class="headerlink" title="网络模型保存与读取"></a>网络模型保存与读取</h2><blockquote>
<p>模型结构 + 模型参数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">torch.save(vgg16,<span class="string">&quot;./model/vgg16_method1.pth&quot;</span>) <span class="comment"># 保存方式一：模型结构 + 模型参数      </span></span><br><span class="line"><span class="built_in">print</span>(vgg16)</span><br><span class="line"></span><br><span class="line">model = torch.load(<span class="string">&quot;./model/vgg16_method1.pth&quot;</span>) <span class="comment"># 保存方式一对应的加载模型    </span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>模型参数（官方推荐），不保存网络模型结构</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">vgg16 = torchvision.models.vgg16(pretrained=<span class="literal">False</span>)</span><br><span class="line">torch.save(vgg16.state_dict(),<span class="string">&quot;./model/vgg16_method2.pth&quot;</span>) <span class="comment"># 保存方式二：模型参数（官方推荐）,不再保存网络模型结构  </span></span><br><span class="line"><span class="built_in">print</span>(vgg16)</span><br><span class="line"></span><br><span class="line">model = torch.load(<span class="string">&quot;./model/vgg16_method2.pth&quot;</span>) <span class="comment"># 导入模型参数   </span></span><br><span class="line"><span class="built_in">print</span>(model)</span><br></pre></td></tr></table></figure>
<h2 id="固定模型参数"><a href="#固定模型参数" class="headerlink" title="固定模型参数"></a>固定模型参数</h2><p>在训练过程中可能需要固定一部分模型的参数，只更新另一部分参数，有两种思路实现这个目标</p>
<ol>
<li>一个是设置不要更新参数的<a target="_blank" rel="noopener external nofollow noreferrer" href="https://so.csdn.net/so/search?q=网络层&amp;spm=1001.2101.3001.7020">网络层</a>为false</li>
<li>另一个就是在定义优化器时只传入要更新的参数</li>
</ol>
<p>当然最优的做法是，优化器中只传入requires_grad=True的参数，这样占用的内存会更小一点，效率也会更高</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义一个简单的网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">net</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_class=<span class="number">3</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(net, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">8</span>, <span class="number">4</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">4</span>, num_class)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> self.fc2(self.fc1(x))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = net()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 冻结fc1层的参数</span></span><br><span class="line"><span class="keyword">for</span> name, param <span class="keyword">in</span> model.named_parameters():</span><br><span class="line">    <span class="keyword">if</span> <span class="string">&quot;fc1&quot;</span> <span class="keyword">in</span> name:</span><br><span class="line">        param.requires_grad = <span class="literal">False</span></span><br><span class="line"></span><br><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 只传入requires_grad = True的参数</span></span><br><span class="line">optimizer = optim.SGD(<span class="built_in">filter</span>(<span class="keyword">lambda</span> p: p.requires_grad, net.parameters(), lr=<span class="number">1e-2</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;model.fc1.weight&quot;</span>, model.fc1.weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;model.fc2.weight&quot;</span>, model.fc2.weight)</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    x = torch.randn((<span class="number">3</span>, <span class="number">8</span>))</span><br><span class="line">    label = torch.randint(<span class="number">0</span>, <span class="number">3</span>, [<span class="number">3</span>]).long()</span><br><span class="line">    output = model(x)</span><br><span class="line"></span><br><span class="line">    loss = loss_fn(output, label)</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;model.fc1.weight&quot;</span>, model.fc1.weight)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;model.fc2.weight&quot;</span>, model.fc2.weight)</span><br></pre></td></tr></table></figure>
<h2 id="训练流程"><a href="#训练流程" class="headerlink" title="训练流程"></a>训练流程</h2><blockquote>
<p>DataLoader加载数据集</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line"></span><br><span class="line"><span class="comment"># length 长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># 如果train_data_size=10，则打印：训练数据集的长度为：10</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试数据集的长度：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用 Dataloader 来加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data_size, batch_size=<span class="number">64</span>)        </span><br><span class="line">test_dataloader = DataLoader(test_data_size, batch_size=<span class="number">64</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>测试网络正确</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 搭建神经网络</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui, self).__init__()        </span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  <span class="comment"># 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),  <span class="comment"># 展平后变成 64*4*4 了</span></span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    tudui = Tudui()</span><br><span class="line">    <span class="built_in">input</span> = torch.ones((<span class="number">64</span>,<span class="number">3</span>,<span class="number">32</span>,<span class="number">32</span>))</span><br><span class="line">    output = tudui(<span class="built_in">input</span>)</span><br><span class="line">    <span class="built_in">print</span>(output.shape)  <span class="comment"># 测试输出的尺寸是不是我们想要的</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p>网络训练数据</p>
</blockquote>
<p>① model.train()和model.eval()的区别主要在于Batch Normalization和Dropout两层。</p>
<p>② 如果模型中有BN层(Batch Normalization）和 Dropout，需要在训练时添加model.train()。model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。</p>
<p>③ 不启用 Batch Normalization 和 Dropout。 如果模型中有BN层(Batch Normalization）和Dropout，在测试时添加model.eval()。model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元。</p>
<p>④ 训练完train样本后，生成的模型model要用来测试样本。在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的性质。</p>
<p>⑤ 在做one classification的时候，训练集和测试集的样本分布是不一样的，尤其需要注意这一点</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">from</span> torch.utils.tensorboard <span class="keyword">import</span> SummaryWriter</span><br><span class="line"></span><br><span class="line"><span class="comment"># from model import * 相当于把 model中的所有内容写到这里，这里直接把 model 写在这里</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Tudui</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Tudui, self).__init__()        </span><br><span class="line">        self.model1 = nn.Sequential(</span><br><span class="line">            nn.Conv2d(<span class="number">3</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),  <span class="comment"># 输入通道3，输出通道32，卷积核尺寸5×5，步长1，填充2    </span></span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">32</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Conv2d(<span class="number">32</span>,<span class="number">64</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">2</span>),</span><br><span class="line">            nn.MaxPool2d(<span class="number">2</span>),</span><br><span class="line">            nn.Flatten(),  <span class="comment"># 展平后变成 64*4*4 了</span></span><br><span class="line">            nn.Linear(<span class="number">64</span>*<span class="number">4</span>*<span class="number">4</span>,<span class="number">64</span>),</span><br><span class="line">            nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">        )</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        x = self.model1(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备数据集</span></span><br><span class="line">train_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">True</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line">test_data = torchvision.datasets.CIFAR10(<span class="string">&quot;./dataset&quot;</span>,train=<span class="literal">False</span>,transform=torchvision.transforms.ToTensor(),download=<span class="literal">True</span>)       </span><br><span class="line"></span><br><span class="line"><span class="comment"># length 长度</span></span><br><span class="line">train_data_size = <span class="built_in">len</span>(train_data)</span><br><span class="line">test_data_size = <span class="built_in">len</span>(test_data)</span><br><span class="line"><span class="comment"># 如果train_data_size=10，则打印：训练数据集的长度为：10</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;训练数据集的长度：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(train_data_size))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;测试数据集的长度：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(test_data_size))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 利用 Dataloader 来加载数据集</span></span><br><span class="line">train_dataloader = DataLoader(train_data, batch_size=<span class="number">64</span>)        </span><br><span class="line">test_dataloader = DataLoader(test_data, batch_size=<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建网络模型</span></span><br><span class="line">tudui = Tudui() </span><br><span class="line"></span><br><span class="line"><span class="comment"># 损失函数</span></span><br><span class="line">loss_fn = nn.CrossEntropyLoss() <span class="comment"># 交叉熵，fn 是 fuction 的缩写</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line">learning = <span class="number">0.01</span>  <span class="comment"># 1e-2 就是 0.01 的意思</span></span><br><span class="line">optimizer = torch.optim.SGD(tudui.parameters(),learning)   <span class="comment"># 随机梯度下降优化器  </span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置网络的一些参数</span></span><br><span class="line"><span class="comment"># 记录训练的次数</span></span><br><span class="line">total_train_step = <span class="number">0</span></span><br><span class="line"><span class="comment"># 记录测试的次数</span></span><br><span class="line">total_test_step = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练的轮次</span></span><br><span class="line">epoch = <span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加 tensorboard</span></span><br><span class="line">writer = SummaryWriter(<span class="string">&quot;logs&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(epoch):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;-----第 &#123;&#125; 轮训练开始-----&quot;</span>.<span class="built_in">format</span>(i+<span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 训练步骤开始</span></span><br><span class="line">    tudui.train() <span class="comment"># 当网络中有dropout层、batchnorm层时，这些层能起作用</span></span><br><span class="line">    <span class="keyword">for</span> data <span class="keyword">in</span> train_dataloader:</span><br><span class="line">        imgs, targets = data</span><br><span class="line">        outputs = tudui(imgs)</span><br><span class="line">        loss = loss_fn(outputs, targets) <span class="comment"># 计算实际输出与目标输出的差距</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 优化器对模型调优</span></span><br><span class="line">        optimizer.zero_grad()  <span class="comment"># 梯度清零</span></span><br><span class="line">        loss.backward() <span class="comment"># 反向传播，计算损失函数的梯度</span></span><br><span class="line">        optimizer.step()   <span class="comment"># 根据梯度，对网络的参数进行调优</span></span><br><span class="line">        </span><br><span class="line">        total_train_step = total_train_step + <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> total_train_step % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&quot;训练次数：&#123;&#125;，Loss：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_train_step,loss.item()))  <span class="comment"># 方式二：获得loss值</span></span><br><span class="line">            writer.add_scalar(<span class="string">&quot;train_loss&quot;</span>,loss.item(),total_train_step)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 测试步骤开始（每一轮训练后都查看在测试数据集上的loss情况）</span></span><br><span class="line">    tudui.<span class="built_in">eval</span>()  <span class="comment"># 当网络中有dropout层、batchnorm层时，这些层不能起作用</span></span><br><span class="line">    total_test_loss = <span class="number">0</span></span><br><span class="line">    total_accuracy = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():  <span class="comment"># 没有梯度了</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_dataloader: <span class="comment"># 测试数据集提取数据</span></span><br><span class="line">            imgs, targets = data</span><br><span class="line">            outputs = tudui(imgs)</span><br><span class="line">            loss = loss_fn(outputs, targets) <span class="comment"># 仅data数据在网络模型上的损失</span></span><br><span class="line">            total_test_loss = total_test_loss + loss.item() <span class="comment"># 所有loss</span></span><br><span class="line">            accuracy = (outputs.argmax(<span class="number">1</span>) == targets).<span class="built_in">sum</span>()</span><br><span class="line">            total_accuracy = total_accuracy + accuracy</span><br><span class="line">            </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的Loss：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_test_loss))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;整体测试集上的正确率：&#123;&#125;&quot;</span>.<span class="built_in">format</span>(total_accuracy/test_data_size))</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_loss&quot;</span>,total_test_loss,total_test_step)</span><br><span class="line">    writer.add_scalar(<span class="string">&quot;test_accuracy&quot;</span>,total_accuracy/test_data_size,total_test_step)  </span><br><span class="line">    total_test_step = total_test_step + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    torch.save(tudui, <span class="string">&quot;./model/tudui_&#123;&#125;.pth&quot;</span>.<span class="built_in">format</span>(i)) <span class="comment"># 保存每一轮训练后的结果</span></span><br><span class="line">    <span class="comment">#torch.save(tudui.state_dict(),&quot;tudui_&#123;&#125;.path&quot;.format(i)) # 保存方式二         </span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;模型已保存&quot;</span>)</span><br><span class="line">    </span><br><span class="line">writer.close()</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/narutohyc">narutohyc</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://study.hycbook.com/article/53039.html">https://study.hycbook.com/article/53039.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://study.hycbook.com" target="_blank">兼一书虫</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post_share"><div class="social-share" data-image="https://pic.hycbook.com/i/hexo/post_cover/蕾姆2.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><link rel="stylesheet" href="/" media="defer" onload="this.media='all'"/><div class="post-reward"><button class="tip-button reward-button"><span class="tip-button__text">打赏</span><div class="coin-wrapper"><div class="coin"><div class="coin__middle"></div><div class="coin__back"></div><div class="coin__front"></div></div></div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic.hycbook.com/i//hexo/qr_codes/hyc_wechat.webp" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/qr_codes/hyc_wechat.webp" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://pic.hycbook.com/i//hexo/qr_codes/hyc_alipay.webp" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/qr_codes/hyc_alipay.webp" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></button></div><audio id="coinAudio" src="https://s1.vika.cn/space/2022/10/29/6db0ad2bccf949f09054b3b206dcc66f?attname=马里奥游戏投币叮当.mp3"></audio><script defer="defer" src="/"></script><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/article/31546.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆10.webp" onerror="onerror=null;src='https://pic.hycbook.com/i/hexo/config_imgs/404.svg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">python常用库学习</div></div></a></div><div class="next-post pull-right"><a href="/article/42221.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆3.webp" onerror="onerror=null;src='https://pic.hycbook.com/i/hexo/config_imgs/404.svg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">数据标注工具</div></div></a></div></nav><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Twikoo</span><span class="switch-btn"></span><span class="second-comment">Valine</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#torch"><span class="toc-number">1.</span> <span class="toc-text">torch</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">1.1.</span> <span class="toc-text">基本操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E6%93%8D%E4%BD%9C-Creation-Ops"><span class="toc-number">1.2.</span> <span class="toc-text">创建操作 Creation Ops</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95-%E5%88%87%E7%89%87-%E8%BF%9E%E6%8E%A5-%E6%8D%A2%E4%BD%8D"><span class="toc-number">1.3.</span> <span class="toc-text">索引|切片|连接|换位</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%8A%BD%E6%A0%B7"><span class="toc-number">1.4.</span> <span class="toc-text">随机抽样</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%8C%96Serialization"><span class="toc-number">1.5.</span> <span class="toc-text">序列化Serialization</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E5%8C%96-Parallelism"><span class="toc-number">1.6.</span> <span class="toc-text">并行化 Parallelism</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E6%93%8D%E4%BD%9CMath-operations"><span class="toc-number">1.7.</span> <span class="toc-text">数学操作Math operations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Reduction-Ops"><span class="toc-number">1.8.</span> <span class="toc-text">Reduction Ops</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%AF%94%E8%BE%83%E6%93%8D%E4%BD%9C-Comparison-Ops"><span class="toc-number">1.9.</span> <span class="toc-text">比较操作 Comparison Ops</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E5%AE%83%E6%93%8D%E4%BD%9C-Other-Operations"><span class="toc-number">1.10.</span> <span class="toc-text">其它操作 Other Operations</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#BLAS-and-LAPACK-Operations"><span class="toc-number">1.11.</span> <span class="toc-text">BLAS and LAPACK Operations</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Tensor"><span class="toc-number">2.</span> <span class="toc-text">Tensor</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#storage"><span class="toc-number">2.1.</span> <span class="toc-text">storage</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AE%9E%E4%BE%8B"><span class="toc-number">3.</span> <span class="toc-text">实例</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Pytorch%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE"><span class="toc-number">3.1.</span> <span class="toc-text">Pytorch加载数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tensorboard"><span class="toc-number">3.2.</span> <span class="toc-text">Tensorboard</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Transforms"><span class="toc-number">3.3.</span> <span class="toc-text">Transforms</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torchvision%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">3.4.</span> <span class="toc-text">torchvision数据集</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">3.5.</span> <span class="toc-text">损失函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">3.6.</span> <span class="toc-text">优化器</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%BD%BF%E7%94%A8%E5%8F%8A%E4%BF%AE%E6%94%B9"><span class="toc-number">3.7.</span> <span class="toc-text">网络模型使用及修改</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E4%B8%8E%E8%AF%BB%E5%8F%96"><span class="toc-number">3.8.</span> <span class="toc-text">网络模型保存与读取</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9B%BA%E5%AE%9A%E6%A8%A1%E5%9E%8B%E5%8F%82%E6%95%B0"><span class="toc-number">3.9.</span> <span class="toc-text">固定模型参数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="toc-number">3.10.</span> <span class="toc-text">训练流程</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://pic.hycbook.com/i/hexo/config_imgs/footer_bg.webp')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By narutohyc</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><p><a target="_blank" href="https://hexo.io/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为Hexo"></a>&nbsp;<a target="_blank" href="https://demo.jerryc.me/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用butterfly"></a>&nbsp;<a target="_blank" href="https://vercel.com/ " rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vervel-brightgreen?style=flat&logo=Vercel" title="本站采用双线部署，默认线路托管于Vercel"></a>&nbsp;<a target="_blank" href="https://github.com/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由Gtihub托管"></a>&nbsp;<a target="_blank" href="https://zixiaoyun.com" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/图床-薄荷图床-green" title="薄荷图床"></a></p><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=35020502000647" rel="external nofollow noreferrer"><img style="position:relative;top:4px" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/config_imgs//备案图标.webp" alt="ICP"/>闽公网安备35020502000647号  </a><a href="https://beian.miit.gov.cn/" rel="external nofollow noreferrer" target="_blank">闽ICP备2022013843号-1</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="fa-solid fa-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="fa-solid fa-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="fa-solid fa-arrow-rotate-right"></i></div><div class="rightMenu-item" id="menu-home"><i class="fa-solid fa-house"></i></div></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:window.open(&quot;https://www.baidu.com/s?wd=&quot;+window.getSelection().toString());window.location.reload();" rel="external nofollow noreferrer"><i class="fas fa-comment"></i><span>百度搜索</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" href="/archives/"><i class="fa-solid fa-archive"></i><span>文章归档</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="fa-solid fa-folder-open"></i><span>文章分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="fa-solid fa-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuNormal"><a class="rightMenu-item menu-link" id="menu-radompage"><i class="fa-solid fa-shoe-prints"></i><span>随便逛逛</span></a><div class="rightMenu-item" id="menu-translate"><i class="fa-solid fa-earth-asia"></i><span>繁简切换</span></div><div class="rightMenu-item" id="menu-darkmode"><i class="fa-solid fa-moon"></i><span>切换模式</span></div></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? '' : ''

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://vercel.hycbook.com',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://vercel.hycbook.com',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'ncn88uooQf0IO2rrGE7Vniwp-gzGzoHsz',
      appKey: 'Yghpzg1QfBMFJ0MxxHubVzKL',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Twikoo' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'https://vercel.hycbook.com',
        region: '',
        pageSize: 3,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script defer src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script defer data-pjax src="/js/rightMenu.js"></script><script defer data-pjax src="/js/udf_mouse.js"></script><script defer data-pjax src="/js/udf_js.js"></script><script defer data-pjax src="/zhheo/random.js"></script><script data-pjax src="/js/coin.js"></script><script defer src="https://npm.elemecdn.com/vue@2.6.11"></script><script async src="//at.alicdn.com/t/c/font_3670467_a0sijt8frxo.js"></script><script defer src="/live2d-widget/autoload.js"></script><script defer src="/js/udf_js.js"></script><script defer src="https://cdnjs.cloudflare.com/ajax/libs/toastr.js/latest/toastr.min.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><script>window.$crisp = [];
window.CRISP_WEBSITE_ID = "561b80db-3f0f-45cb-b3b1-aae7355939e6";
(function () {
  d = document;
  s = d.createElement("script");
  s.src = "https://client.crisp.chat/l.js";
  s.async = 1;
  d.getElementsByTagName("head")[0].appendChild(s);
})();
$crisp.push(["safe", true])

if (false) {
  $crisp.push(["do", "chat:hide"])
  $crisp.push(["on", "chat:closed", function() {
    $crisp.push(["do", "chat:hide"])
  }])
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      $crisp.push(["do", "chat:show"])
      $crisp.push(["do", "chat:open"])

    });
  }
  chatBtnFn()
} else {
  if (false) {
    function chatBtnHide () {
      $crisp.push(["do", "chat:hide"])
    }
    function chatBtnShow () {
      $crisp.push(["do", "chat:show"])
    }
  }
}</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="app-refresh" id="app-refresh" style="position: fixed;top: -2.2rem;left: 0;right: 0;z-index: 99999;padding: 0 1rem;font-size: 15px;height: 2.2rem;transition: all 0.3s ease;"><div class="app-refresh-wrap" style=" display: flex;color: #fff;height: 100%;align-items: center;justify-content: center;"><label>✨ 兼一书虫上新啦！ 👉</label><a href="javascript:void(0)" rel="external nofollow noreferrer" onclick="location.reload()"><span style="color: #fff;text-decoration: underline;cursor: pointer;">🍭查看新品🍬</span></a></div></div><script>if ('serviceWorker' in navigator) {
  if (navigator.serviceWorker.controller) {
    navigator.serviceWorker.addEventListener('controllerchange', function() {
      showNotification()
    })
  }
  window.addEventListener('load', function() {
    navigator.serviceWorker.register('/sw.js')
  })
}

function showNotification() {
  if (GLOBAL_CONFIG.Snackbar) {
    var snackbarBg =
      document.documentElement.getAttribute('data-theme') === 'light' ?
      GLOBAL_CONFIG.Snackbar.bgLight :
      GLOBAL_CONFIG.Snackbar.bgDark
    var snackbarPos = GLOBAL_CONFIG.Snackbar.position
    Snackbar.show({
      text: '✨ 兼一书虫上新啦！ 👉',
      backgroundColor: snackbarBg,
      duration: 500000,
      pos: snackbarPos,
      actionText: '🍭查看新品🍬',
      actionTextColor: '#fff',
      onActionClick: function(e) {
        location.reload()
      },
    })
  } else {
    var showBg =
      document.documentElement.getAttribute('data-theme') === 'light' ?
      '#49b1f5' :
      '#1f1f1f'
    var cssText = `top: 0; background: ${showBg};`
    document.getElementById('app-refresh')
      .style.cssText = cssText
  }
}</script></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>