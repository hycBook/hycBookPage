<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>pytorch学习_基础知识 | 兼一书虫</title><meta name="keywords" content="pytorch"><meta name="author" content="narutohyc"><meta name="copyright" content="narutohyc"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="pytorch学习_基础知识">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch学习_基础知识">
<meta property="og:url" content="https://study.hycbook.com/article/53039.html">
<meta property="og:site_name" content="兼一书虫">
<meta property="og:description" content="pytorch学习_基础知识">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic.hycbook.com/i/hexo/post_cover/%E8%95%BE%E5%A7%862.webp">
<meta property="article:published_time" content="2023-04-16T08:39:14.000Z">
<meta property="article:modified_time" content="2024-03-25T01:35:01.043Z">
<meta property="article:author" content="narutohyc">
<meta property="article:tag" content="pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic.hycbook.com/i/hexo/post_cover/%E8%95%BE%E5%A7%862.webp"><link rel="shortcut icon" href="https://pic.hycbook.com/i//hexo/config_imgs/网站图标.webp"><link rel="canonical" href="https://study.hycbook.com/article/53039"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="manifest" href="/manifest.json"/><meta name="msapplication-TileColor" content="#c6ff7a"/><link rel="apple-touch-icon" sizes="180x180" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/favicon-16x16.png"/><link rel="mask-icon" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/safari-pinned-tab.svg" color="#5bbad5"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?68340394dfd808cea9826e8a57f87aa6";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":120,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":200,"languages":{"author":"作者: narutohyc","link":"链接: ","source":"来源: 兼一书虫","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'pytorch学习_基础知识',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-25 09:35:01'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/mainColor/heoMainColor.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/404/404.css"><script src="https://npm.elemecdn.com/echarts@4.9.0/dist/echarts.min.js"></script><link href="https://cdn.bootcdn.net/ajax/libs/toastr.js/2.1.4/toastr.min.css" rel="stylesheet"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/categoryBar/categoryBar.css"><link rel="stylesheet" href="/css/hyc_udf.css"><link rel="stylesheet" href="/css/udf_css.css"><link rel="stylesheet" href="/css/year.css"><svg aria-hidden="true" style="position:absolute; overflow:hidden; width:0; height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248L626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/person_img/兼一头像.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">125</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">173</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fangwu"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 归档</span></a></li><li><a class="site-page child" href="/categories"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> 分类</span></a></li><li><a class="site-page child" href="/tags"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yuedu">             </use></svg><span> gitbook版</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://common.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> common</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://dl.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> deep learning</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://python.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> python</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://snooby.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> snooby</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://hycbook.flowus.cn"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang">                   </use></svg><span> flowus</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xuegao">             </use></svg><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yinle">                   </use></svg><span> 音乐</span></a></li><li><a class="site-page child" href="/bangumis"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wodezhuifan">                   </use></svg><span> 追番</span></a></li><li><a class="site-page child" href="/gallery"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xiangce">                   </use></svg><span> 相册</span></a></li><li><a class="site-page child" href="/video"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-shipin">                   </use></svg><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/charts"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xigua"></use></svg><span> 统计图</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-suannai">             </use></svg><span> 网盘</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://pan.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 私月盘</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://share.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> 共享盘</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifeiji">             </use></svg><span> 导航</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/comments"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-TIFFANYSROOM_huaban">                   </use></svg><span> 留言板</span></a></li><li><a class="site-page child" href="/link"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-friends">                   </use></svg><span> 友链</span></a></li><li><a class="site-page child" href="/about"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-aixin">                   </use></svg><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic.hycbook.com/i/hexo/post_imgs/蕾姆2.webp')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">兼一书虫</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fangwu"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 归档</span></a></li><li><a class="site-page child" href="/categories"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> 分类</span></a></li><li><a class="site-page child" href="/tags"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yuedu">             </use></svg><span> gitbook版</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://common.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> common</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://dl.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> deep learning</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://python.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> python</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://snooby.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> snooby</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://hycbook.flowus.cn"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang">                   </use></svg><span> flowus</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xuegao">             </use></svg><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yinle">                   </use></svg><span> 音乐</span></a></li><li><a class="site-page child" href="/bangumis"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wodezhuifan">                   </use></svg><span> 追番</span></a></li><li><a class="site-page child" href="/gallery"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xiangce">                   </use></svg><span> 相册</span></a></li><li><a class="site-page child" href="/video"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-shipin">                   </use></svg><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/charts"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xigua"></use></svg><span> 统计图</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-suannai">             </use></svg><span> 网盘</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://pan.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 私月盘</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://share.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> 共享盘</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifeiji">             </use></svg><span> 导航</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/comments"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-TIFFANYSROOM_huaban">                   </use></svg><span> 留言板</span></a></li><li><a class="site-page child" href="/link"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-friends">                   </use></svg><span> 友链</span></a></li><li><a class="site-page child" href="/about"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-aixin">                   </use></svg><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">pytorch学习_基础知识</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-04-16T08:39:14.000Z" title="发表于 2023-04-16 16:39:14">2023-04-16</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2024-03-25T01:35:01.043Z" title="更新于 2024-03-25 09:35:01">2024-03-25</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/deep-learning/">deep-learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">19.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>91分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title="pytorch学习_基础知识"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="twikoo_visitors"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/article/53039.html#post-comment"><span id="twikoo-count"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><hr>
<p>PyTorch 是一个开源的机器学习库，广泛应用于计算机视觉和自然语言处理等人工智能领域。由Facebook的人工智能研究团队开发，它基于之前的Torch库。PyTorch以其高度灵活和动态的计算图特性，在科研领域尤其受到青睐。下面是对PyTorch基础知识的一些介绍：</p>
<p><strong>核心特性</strong></p>
<ul>
<li><p><strong>动态计算图</strong>：PyTorch 使用动态计算图（也称为Define-by-Run方法），这意味着计算图的构建是即时的，并且可以根据运行中的数据进行改变。这为复杂的动态输入和不同长度的输出提供了便利</p>
</li>
<li><p><strong>简洁的接口</strong>：PyTorch 提供了简洁直观的API，便于快速实现和调试模型，使得研究人员可以将更多时间投入到实验设计而非代码调试上</p>
</li>
<li><p><strong>Python优先</strong>：PyTorch 设计为符合Python语言习惯，并且可以无缝集成到Python生态中，与NumPy等库协同工作</p>
</li>
</ul>
<p><strong>基础组件</strong></p>
<ul>
<li><p><strong>张量（Tensors）</strong>：张量是PyTorch中的基础数据结构，它类似于NumPy的ndarrays，但它也可以在GPU上运行以加速计算</p>
</li>
<li><p><strong>自动微分（Autograd）</strong>：PyTorch 的 <code>autograd</code> 模块提供了自动计算梯度的功能，对于实现神经网络中的反向传播算法至关重要</p>
</li>
<li><p><strong>神经网络（torch.nn）</strong>：<code>torch.nn</code> 模块包含了构建神经网络所需的所有元素。这些可重用的层（例如卷积层、线性层等）和损失函数可以帮助用户轻松构建复杂的网络结构</p>
</li>
<li><p><strong>优化（torch.optim）</strong>：PyTorch 提供了常用的优化算法，如SGD、Adam等，用于网络参数的迭代优化</p>
</li>
<li><p><strong>数据加载（torch.utils.data）</strong>：PyTorch 提供了数据加载和处理工具，方便用户创建数据加载管道，加速数据预处理和模型训练过程</p>
</li>
<li><p><strong>序列化工具（Serialization）</strong>：PyTorch 模型和张量可以通过 <code>torch.save</code> 轻松地序列化到磁盘，并通过 <code>torch.load</code> 进行反序列化</p>
</li>
</ul>
<p><strong>CUDA集成</strong></p>
<ul>
<li>PyTorch 提供了与NVIDIA CUDA的深度集成，允许张量和模型被无缝地在GPU上运行，大幅提升了计算速度</li>
</ul>
<p><strong>社区和生态</strong></p>
<ul>
<li>PyTorch 拥有活跃的社区，提供了大量预训练模型和开箱即用的工具。同时，它也是一些高级API（如FastAI）和框架（如Hugging Face的Transformers）的基础</li>
</ul>
<p>PyTorch 不仅适合于研究原型的开发，还能用于生产环境的部署</p>
<p>它提供了一系列工具来支持模型的量化、蒸馏和优化，使其在不牺牲性能的情况下运行更快、占用资源更少。随着其持续发展和完善，PyTorch 已经成为了机器学习研究者和开发者的首选工具之一</p>
<h1 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h1><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/torch.html#tensors">torch — PyTorch 2.2 documentation</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.pytorchtutorial.com/docs/package_references/torch/">pytorch中文文档</a></p>
</blockquote>
<p>以下是一些常用的方法</p>
<blockquote>
<p><strong>torch.is_tensor</strong>: 如果obj是一个pytorch张量，则返回True</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">torch.is_tensor(x)</span><br><span class="line"></span><br><span class="line">Out[<span class="number">0</span>]: <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.is_storage</strong>: 如何obj是一个pytorch storage对象，则返回True</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">torch.is_storage(x)</span><br><span class="line"></span><br><span class="line">Out[<span class="number">0</span>]: <span class="literal">False</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.numel</strong>: 返回<code>input</code> 张量中的元素个数</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line">torch.numel(a)</span><br><span class="line">Out[<span class="number">0</span>]: <span class="number">120</span></span><br><span class="line"></span><br><span class="line">a = torch.zeros(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line">torch.numel(a)</span><br><span class="line">Out[<span class="number">1</span>]: <span class="number">16</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.set_printoptions</strong>: 设置打印选项</p>
</blockquote>
<p>参数:</p>
<ul>
<li>precision – 浮点数输出的精度位数 (默认为8 )</li>
<li>threshold – 阈值，触发汇总显示而不是完全显示(repr)的数组元素的总数 （默认为1000）</li>
<li>edgeitems – 汇总显示中，每维（轴）两端显示的项数（默认值为3）</li>
<li>linewidth – 用于插入行间隔的每行字符数（默认为80）。Thresholded matricies will ignore this parameter.</li>
<li>profile – pretty打印的完全默认值。 可以覆盖上述所有选项 (默认为short, full)</li>
</ul>
<h1 id="创建操作"><a href="#创建操作" class="headerlink" title="创建操作"></a>创建操作</h1><h2 id="张量创建函数"><a href="#张量创建函数" class="headerlink" title="张量创建函数"></a>张量创建函数</h2><ul>
<li><p><code>torch.tensor()</code>: 通过复制数据创建一个具有自动求导历史的张量(如果数据是一个张量)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([[<span class="number">0.1</span>, <span class="number">1.2</span>], [<span class="number">2.2</span>, <span class="number">3.1</span>], [<span class="number">4.9</span>, <span class="number">5.2</span>]])</span><br><span class="line">tensor([[ <span class="number">0.1000</span>,  <span class="number">1.2000</span>],</span><br><span class="line">        [ <span class="number">2.2000</span>,  <span class="number">3.1000</span>],</span><br><span class="line">        [ <span class="number">4.9000</span>,  <span class="number">5.2000</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">0</span>, <span class="number">1</span>])  <span class="comment"># Type inference on data</span></span><br><span class="line">tensor([ <span class="number">0</span>,  <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([[<span class="number">0.11111</span>, <span class="number">0.222222</span>, <span class="number">0.3333333</span>]],</span><br><span class="line"><span class="meta">... </span>             dtype=torch.float64,</span><br><span class="line"><span class="meta">... </span>             device=torch.device(<span class="string">&#x27;cuda:0&#x27;</span>))  <span class="comment"># creates a double tensor on a CUDA device</span></span><br><span class="line">tensor([[ <span class="number">0.1111</span>,  <span class="number">0.2222</span>,  <span class="number">0.3333</span>]], dtype=torch.float64, device=<span class="string">&#x27;cuda:0&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">3.14159</span>)  <span class="comment"># Create a zero-dimensional (scalar) tensor</span></span><br><span class="line">tensor(<span class="number">3.1416</span>)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([])  <span class="comment"># Create an empty tensor (of size (0,))</span></span><br><span class="line">tensor([])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.sparse_coo_tensor()</code>: 通过坐标格式的索引和值构建稀疏张量</p>
<p>使用稀疏矩阵的一个主要优点是，在存储和计算上更加高效，特别是对于非常大的数据集。例如，在矩阵乘法或其他线性代数运算中，利用稀疏性可以显著减少不必要的乘法和加法计算，因为零元素与任何数相乘都是零，并且不会影响加法运算的结果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设我们有2个非零元素分别在(0, 2)和(1, 0)的位置</span></span><br><span class="line">indices = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">0</span>]])  <span class="comment"># 表示非零元素的坐标</span></span><br><span class="line">values = torch.tensor([<span class="number">3</span>, <span class="number">4</span>])             <span class="comment"># 这些非零元素的值</span></span><br><span class="line"><span class="comment"># 创建COO格式的稀疏张量</span></span><br><span class="line">sparse_coo = torch.sparse_coo_tensor(indices, values, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sparse_coo)</span><br><span class="line">tensor(indices=tensor([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">                       [<span class="number">2</span>, <span class="number">0</span>]]),</span><br><span class="line">       values=tensor([<span class="number">3</span>, <span class="number">4</span>]),</span><br><span class="line">       size=(<span class="number">2</span>, <span class="number">3</span>), nnz=<span class="number">2</span>, layout=torch.sparse_coo)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.sparse_csr_tensor()</code>: 通过压缩稀疏行格式的索引和值构建稀疏张量</p>
<p><code>torch.sparse_csc_tensor()</code>: 通过压缩稀疏列格式的索引和值构建稀疏张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义CSR格式的三个组件：行索引、列索引和值</span></span><br><span class="line">crow_indices = torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">col_indices = torch.tensor([<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">values = torch.tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="comment"># 创建CSR格式的稀疏张量</span></span><br><span class="line">sparse_csr = torch.sparse_csr_tensor(crow_indices, col_indices, values)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(sparse_csr)</span><br><span class="line">tensor(crow_indices=tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]),</span><br><span class="line">       col_indices=tensor([<span class="number">0</span>, <span class="number">1</span>]),</span><br><span class="line">       values=tensor([<span class="number">1</span>, <span class="number">2</span>]), size=(<span class="number">2</span>, <span class="number">2</span>), nnz=<span class="number">2</span>, layout=torch.sparse_csr)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.sparse_bsr_tensor()</code>: 通过块压缩稀疏行格式的索引和2维块构建稀疏张量</p>
<p><code>torch.sparse_bsc_tensor()</code>: 通过块压缩稀疏列格式的索引和2维块构建稀疏张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>crow_indices = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>col_indices = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>values = [[[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]], [[<span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>]]]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.sparse_bsr_tensor(torch.tensor(crow_indices, dtype=torch.int64),</span><br><span class="line"><span class="meta">... </span>                        torch.tensor(col_indices, dtype=torch.int64),</span><br><span class="line"><span class="meta">... </span>                        torch.tensor(values), dtype=torch.double)</span><br><span class="line">tensor(crow_indices=tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]),</span><br><span class="line">       col_indices=tensor([<span class="number">0</span>, <span class="number">1</span>]),</span><br><span class="line">       values=tensor([[[<span class="number">1.</span>, <span class="number">2.</span>],</span><br><span class="line">                       [<span class="number">3.</span>, <span class="number">4.</span>]],</span><br><span class="line">                      [[<span class="number">5.</span>, <span class="number">6.</span>],</span><br><span class="line">                       [<span class="number">7.</span>, <span class="number">8.</span>]]]), size=(<span class="number">2</span>, <span class="number">2</span>), nnz=<span class="number">2</span>, dtype=torch.float64,</span><br><span class="line">       layout=torch.sparse_bsr)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="数据类型转换函数"><a href="#数据类型转换函数" class="headerlink" title="数据类型转换函数"></a>数据类型转换函数</h2><ul>
<li><p><code>torch.asarray()</code>: 将对象转换为张量</p>
<p><code>torch.as_tensor()</code>: 将数据转换为张量，共享数据并尽可能保留自动求导历史</p>
<p><code>torch.as_strided()</code>: 创建一个具有指定大小、步长和存储偏移的现有张量的视图(<code>不好理解</code>)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将对象转换为张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Shares memory with tensor &#x27;a&#x27;</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.asarray(a)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.data_ptr() == b.data_ptr()</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># Forces memory copy</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = torch.asarray(a, copy=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.data_ptr() == c.data_ptr()</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据转换为张量，共享数据并尽可能保留自动求导历史</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = numpy.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.as_tensor(a)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">tensor([ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">0</span>] = -<span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([-<span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = numpy.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.as_tensor(a, device=torch.device(<span class="string">&#x27;cuda&#x27;</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">tensor([ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">0</span>] = -<span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([<span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个具有指定大小、步长和存储偏移的现有张量的视图</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.9039</span>,  <span class="number">0.6291</span>,  <span class="number">1.0795</span>],</span><br><span class="line">        [ <span class="number">0.1586</span>,  <span class="number">2.1939</span>, -<span class="number">0.4900</span>],</span><br><span class="line">        [-<span class="number">0.1909</span>, -<span class="number">0.7503</span>,  <span class="number">1.9355</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.as_strided(x, (<span class="number">2</span>, <span class="number">2</span>), (<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">tensor([[<span class="number">0.9039</span>, <span class="number">1.0795</span>],</span><br><span class="line">        [<span class="number">0.6291</span>, <span class="number">0.1586</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.as_strided(<span class="built_in">input</span>=x, size=(<span class="number">2</span>, <span class="number">2</span>), stride=(<span class="number">1</span>, <span class="number">2</span>), storage_offset=<span class="number">1</span>)</span><br><span class="line">tensor([[<span class="number">0.6291</span>, <span class="number">0.1586</span>],</span><br><span class="line">        [<span class="number">1.0795</span>, <span class="number">2.1939</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.from_file()</code>: 从内存映射文件创建CPU张量</p>
<p><code>torch.from_numpy()</code>: 将numpy数组转换为张量</p>
<p><code>torch.from_dlpack()</code>: 将来自外部库的张量转换为PyTorch张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从内存映射文件创建CPU张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.randn(<span class="number">2</span>, <span class="number">5</span>, dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t.numpy().tofile(<span class="string">&#x27;storage.pt&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t_mapped = torch.from_file(<span class="string">&#x27;storage.pt&#x27;</span>, shared=<span class="literal">False</span>, size=<span class="number">10</span>, dtype=torch.float64)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将numpy数组转换为张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = numpy.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.from_numpy(a)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">tensor([ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t[<span class="number">0</span>] = -<span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">array([-<span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="初始化填充函数"><a href="#初始化填充函数" class="headerlink" title="初始化填充函数"></a>初始化填充函数</h2><ul>
<li><p><code>torch.zeros()</code>: 返回一个指定形状且用0填充的张量</p>
<p><code>torch.zeros_like()</code>: 返回一个与给定张量形状相同且用0填充的张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros(<span class="number">5</span>)</span><br><span class="line">tensor([ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> = torch.empty(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros_like(<span class="built_in">input</span>)</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.ones()</code>: 返回一个指定形状且用1填充的张量</p>
<p><code>torch.ones_like()</code>: 返回一个与给定张量形状相同且用1填充的张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.ones(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.ones(<span class="number">5</span>)</span><br><span class="line">tensor([ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> = torch.empty(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.ones_like(<span class="built_in">input</span>)</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.arange()</code>: 返回一个从<code>start</code>到<code>end</code>（不包含<code>end</code>）且步长为<code>step</code>的1维张量</p>
<p><code>torch.range()</code>: (<strong>未来版本弃用</strong>)返回一个从<code>start</code>到<code>end</code>（包含<code>end</code>）且步长为<code>step</code>的1维张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">5</span>)</span><br><span class="line">tensor([ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">tensor([ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">1</span>, <span class="number">2.5</span>, <span class="number">0.5</span>)</span><br><span class="line">tensor([ <span class="number">1.0000</span>,  <span class="number">1.5000</span>,  <span class="number">2.0000</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="特定区间填充函数"><a href="#特定区间填充函数" class="headerlink" title="特定区间填充函数"></a>特定区间填充函数</h2><ul>
<li><p><code>torch.linspace()</code>: 返回一个从<code>start</code>到<code>end</code>（包括<code>end</code>）且在其中均匀分布的指定大小的1维张量</p>
<script type="math/tex; mode=display">
(start, start+\frac{end-start}{steps-1}, \dots, start+(steps-2)* \frac{end-start}{steps-1}, end)</script><p><code>torch.logspace()</code>: 返回一个在对数刻度上从<script type="math/tex">base^{start}</script>到<script type="math/tex">base^{end}</script>（包括<code>end</code>）且均匀分布的指定大小的1维张量</p>
<script type="math/tex; mode=display">
(base^{start}, base^{(start+ \frac{end-start}{steps-1})}, \dots, base^{(start+(steps-2)*\frac{end-start}{steps-1})}, base^{end})</script><p>这两个函数生成的张量常常用于数据预处理、数学模拟、绘图等需要生成规则数列的场景</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.linspace(start=<span class="number">3</span>, end=<span class="number">10</span>, steps=<span class="number">5</span>)</span><br><span class="line">tensor([  <span class="number">3.0000</span>,   <span class="number">4.7500</span>,   <span class="number">6.5000</span>,   <span class="number">8.2500</span>,  <span class="number">10.0000</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.linspace(-<span class="number">10</span>, <span class="number">10</span>, steps=<span class="number">5</span>)</span><br><span class="line">tensor([-<span class="number">10.</span>,  -<span class="number">5.</span>,   <span class="number">0.</span>,   <span class="number">5.</span>,  <span class="number">10.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.linspace(start=-<span class="number">10</span>, end=<span class="number">10</span>, steps=<span class="number">5</span>)</span><br><span class="line">tensor([-<span class="number">10.</span>,  -<span class="number">5.</span>,   <span class="number">0.</span>,   <span class="number">5.</span>,  <span class="number">10.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.linspace(start=-<span class="number">10</span>, end=<span class="number">10</span>, steps=<span class="number">1</span>)</span><br><span class="line">tensor([-<span class="number">10.</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.logspace(start=-<span class="number">10</span>, end=<span class="number">10</span>, steps=<span class="number">5</span>)</span><br><span class="line">tensor([ <span class="number">1.0000e-10</span>,  <span class="number">1.0000e-05</span>,  <span class="number">1.0000e+00</span>,  <span class="number">1.0000e+05</span>,  <span class="number">1.0000e+10</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.logspace(start=<span class="number">0.1</span>, end=<span class="number">1.0</span>, steps=<span class="number">5</span>)</span><br><span class="line">tensor([  <span class="number">1.2589</span>,   <span class="number">2.1135</span>,   <span class="number">3.5481</span>,   <span class="number">5.9566</span>,  <span class="number">10.0000</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.logspace(start=<span class="number">0.1</span>, end=<span class="number">1.0</span>, steps=<span class="number">1</span>)</span><br><span class="line">tensor([<span class="number">1.2589</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.logspace(start=<span class="number">2</span>, end=<span class="number">2</span>, steps=<span class="number">1</span>, base=<span class="number">2</span>)</span><br><span class="line">tensor([<span class="number">4.0</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="其他辅助函数"><a href="#其他辅助函数" class="headerlink" title="其他辅助函数"></a>其他辅助函数</h2><ul>
<li><p><code>torch.eye()</code>: 返回一个二维张量，对角线上为1，其他地方为0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.eye(<span class="number">3</span>)</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.eye(n=<span class="number">3</span>, m=<span class="number">2</span>)</span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.empty()</code>: 返回一个指定形状且未初始化的张量</p>
<p><code>torch.empty_like()</code>: 返回一个与给定张量形状相同且未初始化的张量</p>
<p><code>torch.empty_strided()</code>: 创建一个具有指定大小和跨度且未初始化的张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.empty((<span class="number">2</span>,<span class="number">3</span>), dtype=torch.int64)</span><br><span class="line">tensor([[ <span class="number">9.4064e+13</span>,  <span class="number">2.8000e+01</span>,  <span class="number">9.3493e+13</span>],</span><br><span class="line">        [ <span class="number">7.5751e+18</span>,  <span class="number">7.1428e+18</span>,  <span class="number">7.5955e+18</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a=torch.empty((<span class="number">2</span>,<span class="number">3</span>), dtype=torch.int32, device = <span class="string">&#x27;cuda&#x27;</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.empty_like(a)</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]], device=<span class="string">&#x27;cuda:0&#x27;</span>, dtype=torch.int32)</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.empty_strided((<span class="number">2</span>, <span class="number">3</span>), (<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">8.9683e-44</span>, <span class="number">4.4842e-44</span>, <span class="number">5.1239e+07</span>],</span><br><span class="line">        [<span class="number">0.0000e+00</span>, <span class="number">0.0000e+00</span>, <span class="number">3.0705e-41</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.stride()</span><br><span class="line">(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.full()</code>: 返回一个指定形状且用给定值填充的张量</p>
<p><code>torch.full_like()</code>: 返回一个与给定张量形状相同且用给定值填充的张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.full((<span class="number">2</span>, <span class="number">3</span>), fill_value=<span class="number">3.141592</span>)</span><br><span class="line">tensor([[ <span class="number">3.1416</span>,  <span class="number">3.1416</span>,  <span class="number">3.1416</span>],</span><br><span class="line">        [ <span class="number">3.1416</span>,  <span class="number">3.1416</span>,  <span class="number">3.1416</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.full_like(torch.empty((<span class="number">2</span>,<span class="number">3</span>)), fill_value=<span class="number">9</span>)</span><br><span class="line">tensor([[<span class="number">9.</span>, <span class="number">9.</span>, <span class="number">9.</span>],</span><br><span class="line">        [<span class="number">9.</span>, <span class="number">9.</span>, <span class="number">9.</span>]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="量化函数"><a href="#量化函数" class="headerlink" title="量化函数"></a>量化函数</h2><ul>
<li><p><code>torch.quantize_per_tensor()</code>: 将浮点张量转换为给定比例和零点的量化张量</p>
<p><code>torch.quantize_per_channel()</code>: 将浮点张量转换为按通道给定比例和零点的量化张量</p>
</li>
<li><p><code>torch.dequantize()</code>: 通过去量化量化张量来返回一个fp32张量</p>
</li>
</ul>
<h2 id="复数和其他特殊类型函数"><a href="#复数和其他特殊类型函数" class="headerlink" title="复数和其他特殊类型函数"></a>复数和其他特殊类型函数</h2><ul>
<li><p><code>torch.complex()</code>: 构造一个其实部等于<code>real</code>、虚部等于<code>imag</code>的复数张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>real = torch.tensor([<span class="number">1</span>, <span class="number">2</span>], dtype=torch.float32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>imag = torch.tensor([<span class="number">3</span>, <span class="number">4</span>], dtype=torch.float32)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = torch.<span class="built_in">complex</span>(real, imag)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z</span><br><span class="line">tensor([(<span class="number">1.</span>+<span class="number">3.j</span>), (<span class="number">2.</span>+<span class="number">4.j</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.dtype</span><br><span class="line">torch.complex64</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.polar()</code>: 根据极坐标的绝对值<code>abs</code>和角度<code>angle</code>构造复数张量的笛卡尔坐标</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">abs</span> = torch.tensor([<span class="number">1</span>, <span class="number">2</span>], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>angle = torch.tensor([np.pi / <span class="number">2</span>, <span class="number">5</span> * np.pi / <span class="number">4</span>], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = torch.polar(<span class="built_in">abs</span>, angle)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z</span><br><span class="line">tensor([(<span class="number">0.0000</span>+<span class="number">1.0000j</span>), (-<span class="number">1.4142</span>-<span class="number">1.4142j</span>)], dtype=torch.complex128)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.heaviside()</code>: 计算输入张量每个元素的Heaviside阶跃函数</p>
<script type="math/tex; mode=display">
heaviside(input, values) =
 \begin{cases} 
0, & \text{if } input \lt 0 \\
values, & \text{if } input == 0 \\
\text{1}, & \text{if } input \gt 0 \\
\end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> = torch.tensor([-<span class="number">1.5</span>, <span class="number">0</span>, <span class="number">2.0</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>values = torch.tensor([<span class="number">0.5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.heaviside(<span class="built_in">input</span>, values)</span><br><span class="line">tensor([<span class="number">0.0000</span>, <span class="number">0.5000</span>, <span class="number">1.0000</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>values = torch.tensor([<span class="number">1.2</span>, -<span class="number">2.0</span>, <span class="number">3.5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.heaviside(<span class="built_in">input</span>, values)</span><br><span class="line">tensor([<span class="number">0.</span>, -<span class="number">2.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>这些函数在数据预处理、模型初始化和其他计算任务中非常有用。通过这些函数，你可以创建大小、形状、种类各异的张量来满足不同的需求</p>
<h1 id="索引-切片-连接-换位"><a href="#索引-切片-连接-换位" class="headerlink" title="索引|切片|连接|换位"></a>索引|切片|连接|换位</h1><p>这部分主要分为索引和切片、合并和拼接、变换和重塑、元素添加与替换、搜索和条件操作、扩展与重复操作</p>
<h2 id="索引和切片"><a href="#索引和切片" class="headerlink" title="索引和切片"></a>索引和切片</h2><ul>
<li><p><code>argwhere</code>: 返回非零元素的索引</p>
<p><code>nonzero</code>:  返回非零元素的索引</p>
<p><code>argwhere</code> 和 <code>nonzero</code> 函数都用于查找非零元素的索引，但它们返回的格式略有不同。在某些编程库中，<code>argwhere</code> 通常返回一个二维数组，其中每一行都是输入中非零元素的索引坐标；而 <code>nonzero</code> 返回的是一个元组，每个元素是一个一维数组，表示非零元素在各个维度上的位置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># argwhere</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.argwhere(t)</span><br><span class="line">tensor([[<span class="number">0</span>],</span><br><span class="line">        [<span class="number">2</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>], [<span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.argwhere(t)</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># nonzero</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.nonzero(torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]))</span><br><span class="line">tensor([[ <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.nonzero(torch.tensor([[<span class="number">0.6</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],</span><br><span class="line"><span class="meta">... </span>                            [<span class="number">0.0</span>, <span class="number">0.4</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],</span><br><span class="line"><span class="meta">... </span>                            [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.2</span>, <span class="number">0.0</span>],</span><br><span class="line"><span class="meta">... </span>                            [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>,-<span class="number">0.4</span>]]))</span><br><span class="line">tensor([[ <span class="number">0</span>,  <span class="number">0</span>],</span><br><span class="line">        [ <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">2</span>,  <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>,  <span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.nonzero(torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]), as_tuple=<span class="literal">True</span>)</span><br><span class="line">(tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>]),)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.nonzero(torch.tensor([[<span class="number">0.6</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],</span><br><span class="line"><span class="meta">... </span>                            [<span class="number">0.0</span>, <span class="number">0.4</span>, <span class="number">0.0</span>, <span class="number">0.0</span>],</span><br><span class="line"><span class="meta">... </span>                            [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.2</span>, <span class="number">0.0</span>],</span><br><span class="line"><span class="meta">... </span>                            [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>,-<span class="number">0.4</span>]]), as_tuple=<span class="literal">True</span>)</span><br><span class="line">(tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]), tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.nonzero(torch.tensor(<span class="number">5</span>), as_tuple=<span class="literal">True</span>)</span><br><span class="line">(tensor([<span class="number">0</span>]),)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>select</code>: 在特定维度进行索引</p>
<p><code>index_select</code>: 根据索引选择数据</p>
<ul>
<li><strong>input</strong> (<a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em></a>) – the input tensor.</li>
<li><strong>dim</strong> (<a target="_blank" rel="noopener external nofollow noreferrer" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – the dimension in which we index</li>
<li><strong>index</strong> (<em>IntTensor</em> <em>or</em> <em>LongTensor</em>) – the 1-D tensor containing the indices to index</li>
</ul>
<p><code>masked_select</code>: 根据布尔掩码选择数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在特定维度进行索引</span></span><br><span class="line">tensor = [[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">6</span>]]</span><br><span class="line">selected_row = select(tensor, dim=<span class="number">0</span>, index=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(selected_row) <span class="comment"># 输出: [3, 4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据索引选择数据</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.1427</span>,  <span class="number">0.0231</span>, -<span class="number">0.5414</span>, -<span class="number">1.0009</span>],</span><br><span class="line">        [-<span class="number">0.4664</span>,  <span class="number">0.2647</span>, -<span class="number">0.1228</span>, -<span class="number">1.1068</span>],</span><br><span class="line">        [-<span class="number">1.1734</span>, -<span class="number">0.6571</span>,  <span class="number">0.7230</span>, -<span class="number">0.6004</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>indices = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.index_select(x, <span class="number">0</span>, indices)</span><br><span class="line">tensor([[ <span class="number">0.1427</span>,  <span class="number">0.0231</span>, -<span class="number">0.5414</span>, -<span class="number">1.0009</span>],</span><br><span class="line">        [-<span class="number">1.1734</span>, -<span class="number">0.6571</span>,  <span class="number">0.7230</span>, -<span class="number">0.6004</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.index_select(x, <span class="number">1</span>, indices)</span><br><span class="line">tensor([[ <span class="number">0.1427</span>, -<span class="number">0.5414</span>],</span><br><span class="line">        [-<span class="number">0.4664</span>, -<span class="number">0.1228</span>],</span><br><span class="line">        [-<span class="number">1.1734</span>,  <span class="number">0.7230</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 根据布尔掩码选择数据</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.3552</span>, -<span class="number">2.3825</span>, -<span class="number">0.8297</span>,  <span class="number">0.3477</span>],</span><br><span class="line">        [-<span class="number">1.2035</span>,  <span class="number">1.2252</span>,  <span class="number">0.5002</span>,  <span class="number">0.6248</span>],</span><br><span class="line">        [ <span class="number">0.1307</span>, -<span class="number">2.0608</span>,  <span class="number">0.1244</span>,  <span class="number">2.0139</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask = x.ge(<span class="number">0.5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mask</span><br><span class="line">tensor([[<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">False</span>, <span class="literal">True</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.masked_select(x, mask)</span><br><span class="line">tensor([ <span class="number">1.2252</span>,  <span class="number">0.5002</span>,  <span class="number">0.6248</span>,  <span class="number">2.0139</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>narrow</code>: 缩小张量的一个维度</p>
<ul>
<li><strong>input</strong> (<a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em></a>) – the tensor to narrow</li>
<li><strong>dim</strong> (<a target="_blank" rel="noopener external nofollow noreferrer" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – the dimension along which to narrow</li>
<li><strong>start</strong> (<a target="_blank" rel="noopener external nofollow noreferrer" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a> <em>or</em> <a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/tensors.html#torch.Tensor"><em>Tensor</em></a>) – index of the element to start the narrowed dimension from. Can be negative, which means indexing from the end of dim. If Tensor, it must be an 0-dim integral Tensor (bools not allowed)</li>
<li><strong>length</strong> (<a target="_blank" rel="noopener external nofollow noreferrer" href="https://docs.python.org/3/library/functions.html#int"><em>int</em></a>) – length of the narrowed dimension, must be weakly positive</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.narrow(x, <span class="number">0</span>, <span class="number">0</span>, <span class="number">2</span>)</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.narrow(x, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">tensor([[ <span class="number">2</span>,  <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">5</span>,  <span class="number">6</span>],</span><br><span class="line">        [ <span class="number">8</span>,  <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.narrow(x, -<span class="number">1</span>, torch.tensor(-<span class="number">1</span>), <span class="number">1</span>)</span><br><span class="line">tensor([[<span class="number">3</span>],</span><br><span class="line">        [<span class="number">6</span>],</span><br><span class="line">        [<span class="number">9</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>narrow_copy</code>: <code>narrow</code>操作的复制版本</p>
</li>
<li><p><code>take</code>: 根据索引从输入张量中取元素</p>
<p><code>take_along_dim</code>: 沿指定维度根据索引取元素</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 根据索引从输入张量中取元素</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>src = torch.tensor([[<span class="number">4</span>, <span class="number">3</span>, <span class="number">5</span>],</span><br><span class="line"><span class="meta">... </span>                    [<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.take(src, torch.tensor([<span class="number">0</span>, <span class="number">2</span>, <span class="number">5</span>]))</span><br><span class="line">tensor([ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">8</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 沿指定维度根据索引取元素</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([[<span class="number">10</span>, <span class="number">30</span>, <span class="number">20</span>], [<span class="number">60</span>, <span class="number">40</span>, <span class="number">50</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>max_idx = torch.argmax(t)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.take_along_dim(t, max_idx)</span><br><span class="line">tensor([<span class="number">60</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sorted_idx = torch.argsort(t, dim=<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.take_along_dim(t, sorted_idx, dim=<span class="number">1</span>)</span><br><span class="line">tensor([[<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>],</span><br><span class="line">        [<span class="number">40</span>, <span class="number">50</span>, <span class="number">60</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>unbind</code>: 按维度解绑张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unbind(torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>                           [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>                           [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]))</span><br><span class="line">(tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]), tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]), tensor([<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]))</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>unravel_index</code>: 将平面索引转换为坐标索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unravel_index(torch.tensor(<span class="number">4</span>), (<span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line">(tensor(<span class="number">2</span>),</span><br><span class="line"> tensor(<span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unravel_index(torch.tensor([<span class="number">4</span>, <span class="number">1</span>]), (<span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line">(tensor([<span class="number">2</span>, <span class="number">0</span>]),</span><br><span class="line"> tensor([<span class="number">0</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unravel_index(torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]), (<span class="number">3</span>, <span class="number">2</span>))</span><br><span class="line">(tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>]),</span><br><span class="line"> tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unravel_index(torch.tensor([<span class="number">1234</span>, <span class="number">5678</span>]), (<span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">(tensor([<span class="number">1</span>, <span class="number">5</span>]),</span><br><span class="line"> tensor([<span class="number">2</span>, <span class="number">6</span>]),</span><br><span class="line"> tensor([<span class="number">3</span>, <span class="number">7</span>]),</span><br><span class="line"> tensor([<span class="number">4</span>, <span class="number">8</span>]))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unravel_index(torch.tensor([[<span class="number">1234</span>], [<span class="number">5678</span>]]), (<span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>, <span class="number">10</span>))</span><br><span class="line">(tensor([[<span class="number">1</span>], [<span class="number">5</span>]]),</span><br><span class="line"> tensor([[<span class="number">2</span>], [<span class="number">6</span>]]),</span><br><span class="line"> tensor([[<span class="number">3</span>], [<span class="number">7</span>]]),</span><br><span class="line"> tensor([[<span class="number">4</span>], [<span class="number">8</span>]]))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unravel_index(torch.tensor([[<span class="number">1234</span>], [<span class="number">5678</span>]]), (<span class="number">100</span>, <span class="number">100</span>))</span><br><span class="line">(tensor([[<span class="number">12</span>], [<span class="number">56</span>]]),</span><br><span class="line"> tensor([[<span class="number">34</span>], [<span class="number">78</span>]]))</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>squeeze</code>: 去除大小为1的维度</p>
<p><code>unsqueeze</code>: 在指定位置添加大小为1的维度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 压缩维度</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.zeros(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.squeeze(x)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.squeeze(x, <span class="number">0</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.squeeze(x, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.squeeze(x, (<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增加维度</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unsqueeze(x, <span class="number">0</span>)</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.unsqueeze(x, <span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>],</span><br><span class="line">        [ <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="合并和拼接"><a href="#合并和拼接" class="headerlink" title="合并和拼接"></a>合并和拼接</h2><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://mp.weixin.qq.com/s?__biz=MzI1MzY0MzE4Mg==&amp;mid=2247484138&amp;idx=2&amp;sn=f1dca4b3790284371fe103b2108d92a6&amp;chksm=e9d0122bdea79b3d832612ae41a68e120a74764d0b557ffc286da1c4163f397c4e780a77a5b9&amp;mpshare=1&amp;scene=1&amp;srcid=0501xRYNtB00dNBcW8UlLzml#rd">numpy中的hstack()、vstack()、stack()、concatenate()函数详解</a></p>
</blockquote>
<ul>
<li><p><code>cat</code>, <code>concat</code>, <code>concatenate</code>: 将序列的张量在指定维度连接(<code>concat</code>和<code>concatenate</code>是<code>cat</code>的别名)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">0</span>)</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>],</span><br><span class="line">        [ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.cat((x, x, x), <span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>,  <span class="number">0.6580</span>, -<span class="number">1.0969</span>, -<span class="number">0.4614</span>,  <span class="number">0.6580</span>,</span><br><span class="line">         -<span class="number">1.0969</span>, -<span class="number">0.4614</span>],</span><br><span class="line">        [-<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>, -<span class="number">0.1034</span>, -<span class="number">0.5790</span>,  <span class="number">0.1497</span>, -<span class="number">0.1034</span>,</span><br><span class="line">         -<span class="number">0.5790</span>,  <span class="number">0.1497</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>chunk</code>: 把张量分成指定数量的块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.arange(<span class="number">11</span>).chunk(<span class="number">6</span>)</span><br><span class="line">(tensor([<span class="number">0</span>, <span class="number">1</span>]),</span><br><span class="line"> tensor([<span class="number">2</span>, <span class="number">3</span>]),</span><br><span class="line"> tensor([<span class="number">4</span>, <span class="number">5</span>]),</span><br><span class="line"> tensor([<span class="number">6</span>, <span class="number">7</span>]),</span><br><span class="line"> tensor([<span class="number">8</span>, <span class="number">9</span>]),</span><br><span class="line"> tensor([<span class="number">10</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个张量</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"><span class="comment"># 将这个张量分割成3个块</span></span><br><span class="line">chunks = torch.chunk(x, chunks=<span class="number">3</span>, dim=<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 输出分割后的块</span></span><br><span class="line"><span class="keyword">for</span> i, chunk <span class="keyword">in</span> <span class="built_in">enumerate</span>(chunks):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Chunk <span class="subst">&#123;i&#125;</span>: <span class="subst">&#123;chunk&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">Chunk <span class="number">0</span>: tensor([<span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line">Chunk <span class="number">1</span>: tensor([<span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Chunk <span class="number">2</span>: tensor([<span class="number">5</span>])    </span><br></pre></td></tr></table></figure>
</li>
<li><p><code>column_stack</code>: 按列堆叠张量创建新张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.column_stack((a, b))</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">4</span>],</span><br><span class="line">    [<span class="number">2</span>, <span class="number">5</span>],</span><br><span class="line">    [<span class="number">3</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.arange(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.arange(<span class="number">10</span>).reshape(<span class="number">5</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.column_stack((a, b, b))</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>hstack</code>: 水平方向堆叠张量</p>
<p><code>vstack</code>(别名<code>row_stack</code>): 垂直方向堆叠张量</p>
<p><code>dstack</code>: 深度方向堆叠张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 水平方向堆叠张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.hstack((a,b))</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.tensor([[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.hstack((a,b))</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">2</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 垂直方向堆叠张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.vstack((a,b))</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.tensor([[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.vstack((a,b))</span><br><span class="line">tensor([[<span class="number">1</span>],</span><br><span class="line">        [<span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>],</span><br><span class="line">        [<span class="number">6</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 深度方向堆叠张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.tensor([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.dstack((a,b))</span><br><span class="line">tensor([[[<span class="number">1</span>, <span class="number">4</span>],</span><br><span class="line">         [<span class="number">2</span>, <span class="number">5</span>],</span><br><span class="line">         [<span class="number">3</span>, <span class="number">6</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([[<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.tensor([[<span class="number">4</span>],[<span class="number">5</span>],[<span class="number">6</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.dstack((a,b))</span><br><span class="line">tensor([[[<span class="number">1</span>, <span class="number">4</span>]],</span><br><span class="line">        [[<span class="number">2</span>, <span class="number">5</span>]],</span><br><span class="line">        [[<span class="number">3</span>, <span class="number">6</span>]]])</span><br></pre></td></tr></table></figure>
<p><code>torch.dstack</code> 和 <code>torch.column_stack</code> 函数都是用于堆叠张量的函数，但它们在堆叠的细节上有所不同</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">A = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                  [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>]])</span><br><span class="line">B = torch.tensor([[<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>],</span><br><span class="line">                  [<span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">A.shape</span><br><span class="line">Out[<span class="number">27</span>]: torch.Size([<span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># dstack是一整个对象堆叠</span></span><br><span class="line">dstack_result = torch.dstack((A, B)) </span><br><span class="line">dstack_result</span><br><span class="line">Out[<span class="number">30</span>]: </span><br><span class="line">tensor([[[ <span class="number">1</span>,  <span class="number">7</span>],</span><br><span class="line">         [ <span class="number">2</span>,  <span class="number">8</span>],</span><br><span class="line">         [ <span class="number">3</span>,  <span class="number">9</span>]],</span><br><span class="line">        [[ <span class="number">4</span>, <span class="number">10</span>],</span><br><span class="line">         [ <span class="number">5</span>, <span class="number">11</span>],</span><br><span class="line">         [ <span class="number">6</span>, <span class="number">12</span>]]])</span><br><span class="line">dstack_result.shape</span><br><span class="line">Out[<span class="number">31</span>]: torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># column_stack专门用于二维张量（矩阵），它会将这些矩阵堆叠成一个更宽的矩阵（即增加列）</span></span><br><span class="line">column_stack_result = torch.column_stack((A, B))</span><br><span class="line">column_stack_result</span><br><span class="line">Out[<span class="number">33</span>]: </span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>,  <span class="number">7</span>,  <span class="number">8</span>,  <span class="number">9</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">5</span>,  <span class="number">6</span>, <span class="number">10</span>, <span class="number">11</span>, <span class="number">12</span>]])</span><br><span class="line">column_stack_result.shape</span><br><span class="line">Out[<span class="number">34</span>]: torch.Size([<span class="number">2</span>, <span class="number">6</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>stack</code>: 在新维度上连接张量序列</p>
<blockquote>
<p>pytorch的hstack、vstack、dstack、column_stack以及stack函数之间的区别和联系</p>
</blockquote>
<p>这些堆叠函数之间的联系在于它们的核心目的：将多个张量组合成一个新的、更大的张量</p>
<p>不同的函数根据堆叠的方向（尺寸或维度）和具体的操作细节来区分，下面是它们之间联系的一个概览：</p>
<p><strong>维度方向的联系</strong>：</p>
<ul>
<li><code>hstack</code>（水平堆叠）通常用于增加列数，适用于1D和2D张量，对于1D张量会先将其视作列向量</li>
<li><code>vstack</code>（垂直堆叠）常用于增加行数，也适用于1D和2D张量，对于1D张量会先将其视作行向量</li>
<li><code>dstack</code>（深度堆叠）是在第三个维度上进行堆叠，适用于创建或扩展为3D张量的情况</li>
<li><code>column_stack</code>与<code>hstack</code>相似，但它是专门设计来处理1D张量，将它们作为列向量来堆叠成2D张量的；对于2D张量，它的行为与<code>hstack</code>相同</li>
<li><code>stack</code>是一个更通用的函数，可以在指定的任何维度上进行堆叠，而不局限于特定的堆叠方向。它总是增加一个新的维度来堆叠张量</li>
</ul>
<p><strong>操作联系</strong>：</p>
<ul>
<li>所有这些函数都是用来组合张量的，但是<code>stack</code>函数会创建一个新的维度，而其他函数（<code>hstack</code>, <code>vstack</code>, <code>dstack</code>, <code>column_stack</code>）则在现有的维度上进行操作</li>
<li><code>hstack</code>, <code>vstack</code>, <code>dstack</code>, <code>column_stack</code>可以看作是<code>stack</code>的特例，它们在指定的一个特定的现有维度上进行操作（<code>hstack</code>在最后一个维度，<code>vstack</code>在第一个维度，<code>dstack</code>在第三个维度，<code>column_stack</code>针对1D张量在第二个新建维度，对2D张量在最后一个维度）</li>
</ul>
<p><strong>使用场景联系</strong>：</p>
<ul>
<li>当你想要在特定的轴方向上组合数据，而不想增加新的维度时，你会选择使用<code>hstack</code>, <code>vstack</code>, <code>dstack</code>, 或 <code>column_stack</code></li>
<li>当你需要在新的维度上堆叠张量时（例如，在时间序列数据或不同样本之间），你会选择使用<code>stack</code></li>
</ul>
<p>在实际使用中，选择哪一个函数取决于你的具体需求以及你要操作的张量的维度。这些函数提供了方便的方式来对数据进行重构和整合，这是在准备数据集、构建深度学习模型等场景中非常常见的需求</p>
</li>
<li><p><code>hsplit</code>: 水平方向分割张量</p>
<p><code>vsplit</code>: 垂直方向分割张量</p>
<p><code>dsplit</code>: 深度方向分割张量</p>
<p><code>split</code>: 分割张量成多个块，函数将张量分割成特定大小的块。你可以指定每个块的大小，或者传递一个包含每个块大小的列表。如果张量不能均匀分割，最后一块的大小将小于前面的块</p>
<p><code>tensor_split</code>: 沿特定维度分割张量，基于索引来分割张量的。你可以指定一个分割点的索引列表，函数会在这些索引处分割张量。这些索引指的是分割后每个新张量的第一个元素的索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.arange(<span class="number">16.0</span>).reshape(<span class="number">4</span>,<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">        [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">        [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 水平方向分割张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.hsplit(t, <span class="number">2</span>)</span><br><span class="line">(tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>],</span><br><span class="line">         [ <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">13.</span>]]),</span><br><span class="line"> tensor([[ <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">         [<span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [<span class="number">14.</span>, <span class="number">15.</span>]]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.hsplit(t, [<span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line">(tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>],</span><br><span class="line">         [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>]]),</span><br><span class="line"> tensor([[ <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">7.</span>],</span><br><span class="line">         [<span class="number">11.</span>],</span><br><span class="line">         [<span class="number">15.</span>]]),</span><br><span class="line"> tensor([], size=(<span class="number">4</span>, <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 垂直方向分割张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.vsplit(t, <span class="number">2</span>)</span><br><span class="line">(tensor([[<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">         [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">7.</span>]]),</span><br><span class="line"> tensor([[ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.vsplit(t, [<span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line">(tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">         [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>]]),</span><br><span class="line"> tensor([[<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]]),</span><br><span class="line"> tensor([], size=(<span class="number">0</span>, <span class="number">4</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 深度方向分割张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.arange(<span class="number">16.0</span>).reshape(<span class="number">2</span>, <span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">tensor([[[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">         [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>]],</span><br><span class="line">        [[ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.dsplit(t, <span class="number">2</span>)</span><br><span class="line">(tensor([[[ <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">4.</span>,  <span class="number">5.</span>]],</span><br><span class="line">       [[ <span class="number">8.</span>,  <span class="number">9.</span>],</span><br><span class="line">        [<span class="number">12.</span>, <span class="number">13.</span>]]]),</span><br><span class="line"> tensor([[[ <span class="number">2.</span>,  <span class="number">3.</span>],</span><br><span class="line">          [ <span class="number">6.</span>,  <span class="number">7.</span>]],</span><br><span class="line">         [[<span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">          [<span class="number">14.</span>, <span class="number">15.</span>]]]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.dsplit(t, [<span class="number">3</span>, <span class="number">6</span>])</span><br><span class="line">(tensor([[[ <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line">          [ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>]],</span><br><span class="line">         [[ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>],</span><br><span class="line">          [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>]]]),</span><br><span class="line"> tensor([[[ <span class="number">3.</span>],</span><br><span class="line">          [ <span class="number">7.</span>]],</span><br><span class="line">         [[<span class="number">11.</span>],</span><br><span class="line">          [<span class="number">15.</span>]]]),</span><br><span class="line"> tensor([], size=(<span class="number">2</span>, <span class="number">2</span>, <span class="number">0</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分割张量成多个块</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.split(t, <span class="number">2</span>, dim=<span class="number">0</span>)</span><br><span class="line">(tensor([[<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>],</span><br><span class="line">         [<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">7.</span>]]),</span><br><span class="line"> tensor([[ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.split(t, [<span class="number">1</span>, <span class="number">3</span>], dim=<span class="number">0</span>)</span><br><span class="line">(tensor([[<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>]]),</span><br><span class="line"> tensor([[ <span class="number">4.</span>,  <span class="number">5.</span>,  <span class="number">6.</span>,  <span class="number">7.</span>],</span><br><span class="line">         [ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 沿特定维度分割张量</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor_split(t, [<span class="number">1</span>, <span class="number">2</span>], dim=<span class="number">0</span>)</span><br><span class="line">(tensor([[<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>]]),</span><br><span class="line"> tensor([[<span class="number">4.</span>, <span class="number">5.</span>, <span class="number">6.</span>, <span class="number">7.</span>]]),</span><br><span class="line"> tensor([[ <span class="number">8.</span>,  <span class="number">9.</span>, <span class="number">10.</span>, <span class="number">11.</span>],</span><br><span class="line">         [<span class="number">12.</span>, <span class="number">13.</span>, <span class="number">14.</span>, <span class="number">15.</span>]]))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="变换和重塑"><a href="#变换和重塑" class="headerlink" title="变换和重塑"></a>变换和重塑</h2><ul>
<li><p><code>adjoint</code>: 返回共轭的张量，并交换最后两维</p>
</li>
<li><p><code>conj</code>: 返回共轭位翻转的张量视图</p>
</li>
<li><p><code>gather</code>: 沿指定维度聚集值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.gather(<span class="built_in">input</span>, dim, index, *, sparse_grad=<span class="literal">False</span>) -&gt; Tensor</span><br></pre></td></tr></table></figure>
<p>其中参数的意义如下：</p>
<ul>
<li><code>input</code> 是要从中提取数据的张量</li>
<li><code>dim</code> 是要沿着哪个维度进行提取</li>
<li><code>index</code> 是与 <code>input</code> 张量在除了 <code>dim</code> 指定的维度外具有相同大小的张量，包含了要提取的元素的索引</li>
<li><code>sparse_grad</code> 是布尔值，用于指示是否进行稀疏梯度的计算；通常用于高级用途</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个 3x3 的矩阵</span></span><br><span class="line">input_tensor = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">                             [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">                             [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个索引，用于选择每一行的第二个元素</span></span><br><span class="line">index = torch.tensor([[<span class="number">1</span>], [<span class="number">1</span>], [<span class="number">1</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 gather 来提取元素，dim=1 表示沿着列的方向进行操作</span></span><br><span class="line">torch.gather(input_tensor, <span class="number">1</span>, index)</span><br><span class="line"><span class="comment"># 输出：[[2], [5], [8]]</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.gather(t, <span class="number">1</span>, torch.tensor([[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">0</span>]]))</span><br><span class="line">tensor([[ <span class="number">1</span>,  <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">4</span>,  <span class="number">3</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>movedim</code>(别名<code>moveaxis</code>): 移动张量维度位置的函数，这个操作可以让你指定某个维度（或多个维度）从它的原始位置移动到一个新的位置</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.randn(<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">tensor([[[-<span class="number">0.3362</span>],</span><br><span class="line">        [-<span class="number">0.8437</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">0.9627</span>],</span><br><span class="line">        [ <span class="number">0.1727</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.5173</span>],</span><br><span class="line">        [-<span class="number">0.1398</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.moveaxis(t, <span class="number">1</span>, <span class="number">0</span>).shape</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.moveaxis(t, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line">tensor([[[-<span class="number">0.3362</span>],</span><br><span class="line">        [-<span class="number">0.9627</span>],</span><br><span class="line">        [ <span class="number">0.5173</span>]],</span><br><span class="line"></span><br><span class="line">        [[-<span class="number">0.8437</span>],</span><br><span class="line">        [ <span class="number">0.1727</span>],</span><br><span class="line">        [-<span class="number">0.1398</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.moveaxis(t, (<span class="number">1</span>, <span class="number">2</span>), (<span class="number">0</span>, <span class="number">1</span>)).shape</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.moveaxis(t, (<span class="number">1</span>, <span class="number">2</span>), (<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">tensor([[[-<span class="number">0.3362</span>, -<span class="number">0.9627</span>,  <span class="number">0.5173</span>]],</span><br><span class="line">        [[-<span class="number">0.8437</span>,  <span class="number">0.1727</span>, -<span class="number">0.1398</span>]]])</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">tensor = torch.randn(<span class="number">10</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line">torch.movedim(tensor, <span class="number">1</span>, <span class="number">0</span>).size()</span><br><span class="line">Out[<span class="number">78</span>]: torch.Size([<span class="number">3</span>, <span class="number">10</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>permute</code>: 重新排列张量的维度，重组tensor维度，支持高维操作，tensor.permute(dim0, dim1, … dimn)，表示原本的dim0放在第0维度，dim1放在第1维度，…, dimn放在第n维度，必须将所有维度写上</p>
<p><code>reshape</code>: 改变张量的形状，需要指定最终的形状</p>
<p><code>transpose</code>(等价于<code>swapaxes</code>、<code>swapdims</code>): 转置张量的维度</p>
<blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/nuohuang3371/article/details/113403755">捋清pytorch的transpose、permute、view、reshape、contiguous</a></p>
</blockquote>
<p>permute可以完全替代transpose，transpose不能替代permute</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># permute重新排列张量的维度</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.size()</span><br><span class="line">torch.Size([<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.permute(x, (<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>)).size()</span><br><span class="line">torch.Size([<span class="number">5</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># reshape改变张量的形状</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.arange(<span class="number">4.</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.reshape(a, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">2.</span>,  <span class="number">3.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.tensor([[<span class="number">0</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.reshape(b, (-<span class="number">1</span>,))</span><br><span class="line">tensor([ <span class="number">0</span>,  <span class="number">1</span>,  <span class="number">2</span>,  <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># transpose转置张量的维度</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">1.0028</span>, -<span class="number">0.9893</span>,  <span class="number">0.5809</span>],</span><br><span class="line">        [-<span class="number">0.1669</span>,  <span class="number">0.7299</span>,  <span class="number">0.4942</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.transpose(x, dim0=<span class="number">0</span>, dim1=<span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">1.0028</span>, -<span class="number">0.1669</span>],</span><br><span class="line">        [-<span class="number">0.9893</span>,  <span class="number">0.7299</span>],</span><br><span class="line">        [ <span class="number">0.5809</span>,  <span class="number">0.4942</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>t</code>: 转置二维张量的维度</p>
<p>期望输入是一个二维或二维以下的张量，并交换维度0和1</p>
<p>当输入是一个零维或一维张量时，返回的张量保持不变。当输入是一个二维张量时，这相当于 transpose(input, 0, 1)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(())</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor(<span class="number">0.1995</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.t(x)</span><br><span class="line">tensor(<span class="number">0.1995</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([ <span class="number">2.4320</span>, -<span class="number">0.4608</span>,  <span class="number">0.7702</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.t(x)</span><br><span class="line">tensor([ <span class="number">2.4320</span>, -<span class="number">0.4608</span>,  <span class="number">0.7702</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">0.4875</span>,  <span class="number">0.9158</span>, -<span class="number">0.5872</span>],</span><br><span class="line">        [ <span class="number">0.3938</span>, -<span class="number">0.6929</span>,  <span class="number">0.6932</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.t(x)</span><br><span class="line">tensor([[ <span class="number">0.4875</span>,  <span class="number">0.3938</span>],</span><br><span class="line">        [ <span class="number">0.9158</span>, -<span class="number">0.6929</span>],</span><br><span class="line">        [-<span class="number">0.5872</span>,  <span class="number">0.6932</span>]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="元素添加与替换"><a href="#元素添加与替换" class="headerlink" title="元素添加与替换"></a>元素添加与替换</h2><ul>
<li><p><code>index_add</code>: 根据索引向张量添加元素</p>
<p><code>index_copy</code>: 根据索引复制元素到张量</p>
<p><code>index_reduce</code>: 在指定维度上，根据索引减少元素</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.ones(<span class="number">5</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>index = torch.tensor([<span class="number">0</span>, <span class="number">4</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.index_add(<span class="number">0</span>, index=index, source=t)</span><br><span class="line">tensor([[  <span class="number">2.</span>,   <span class="number">3.</span>,   <span class="number">4.</span>],</span><br><span class="line">        [  <span class="number">1.</span>,   <span class="number">1.</span>,   <span class="number">1.</span>],</span><br><span class="line">        [  <span class="number">8.</span>,   <span class="number">9.</span>,  <span class="number">10.</span>],</span><br><span class="line">        [  <span class="number">1.</span>,   <span class="number">1.</span>,   <span class="number">1.</span>],</span><br><span class="line">        [  <span class="number">5.</span>,   <span class="number">6.</span>,   <span class="number">7.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.index_add(<span class="number">0</span>, index, t, alpha=-<span class="number">1</span>)</span><br><span class="line">tensor([[ <span class="number">0.</span>, -<span class="number">1.</span>, -<span class="number">2.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [-<span class="number">6.</span>, -<span class="number">7.</span>, -<span class="number">8.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [-<span class="number">3.</span>, -<span class="number">4.</span>, -<span class="number">5.</span>]])</span><br><span class="line"></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>scatter</code>, <code>scatter_add</code>, <code>scatter_reduce</code>: 根据索引分散和添加元素</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scatter(output, dim, index, src) </span><br></pre></td></tr></table></figure>
<p>catter函数就是把src数组中的数据重新分配到output数组当中，index数组中表示了要把src数组中的数据分配到output数组中的位置，若未指定，则填充0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">input</span> = torch.randn(<span class="number">2</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(<span class="built_in">input</span>)</span><br><span class="line">tensor([[ <span class="number">1.4782</span>, -<span class="number">1.1345</span>, -<span class="number">1.1457</span>, -<span class="number">0.6050</span>],</span><br><span class="line">        [-<span class="number">0.4183</span>, -<span class="number">0.0229</span>,  <span class="number">1.2361</span>, -<span class="number">1.7747</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = torch.zeros(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>index = torch.tensor([[<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = output.scatter(dim=<span class="number">1</span>, index=index, src=<span class="built_in">input</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(output)</span><br><span class="line">tensor([[-<span class="number">0.6050</span>, -<span class="number">1.1345</span>, -<span class="number">1.1457</span>,  <span class="number">1.4782</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">1.2361</span>, -<span class="number">0.4183</span>, -<span class="number">0.0229</span>, -<span class="number">1.7747</span>,  <span class="number">0.0000</span>]])</span><br></pre></td></tr></table></figure>
<p>一般scatter用于生成onehot向量，如下所示</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>index = torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">0</span>], [<span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>onehot = torch.zeros(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>onehot.scatter_(<span class="number">1</span>, index, <span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span>(onehot)</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>diagonal_scatter</code>: 沿对角线分散元素</p>
</li>
<li><p><code>select_scatter</code>: 在给定索引处分散元素</p>
</li>
<li><p><code>slice_scatter</code>: 在给定维度上分散元素</p>
</li>
</ul>
<h2 id="搜索和条件操作"><a href="#搜索和条件操作" class="headerlink" title="搜索和条件操作"></a>搜索和条件操作</h2><ul>
<li><p><code>where</code>: 根据条件从两个张量中选择元素</p>
<script type="math/tex; mode=display">
out_i =
 \begin{cases} 
input_i, & \text{if } condition_i \\
other_i, & \text{otherwise}
\end{cases}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.ones(<span class="number">3</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[-<span class="number">0.4620</span>,  <span class="number">0.3139</span>],</span><br><span class="line">        [ <span class="number">0.3898</span>, -<span class="number">0.7197</span>],</span><br><span class="line">        [ <span class="number">0.0478</span>, -<span class="number">0.1657</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.where(condition=x &gt; <span class="number">0</span>, <span class="built_in">input</span>=<span class="number">1.0</span>, other=<span class="number">0.0</span>)</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">0.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.where(condition=x &gt; <span class="number">0</span>, <span class="built_in">input</span>=x, other=y)</span><br><span class="line">tensor([[ <span class="number">1.0000</span>,  <span class="number">0.3139</span>],</span><br><span class="line">        [ <span class="number">0.3898</span>,  <span class="number">1.0000</span>],</span><br><span class="line">        [ <span class="number">0.0478</span>,  <span class="number">1.0000</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">2</span>, <span class="number">2</span>, dtype=torch.double)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[ <span class="number">1.0779</span>,  <span class="number">0.0383</span>],</span><br><span class="line">        [-<span class="number">0.8785</span>, -<span class="number">1.1089</span>]], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.where(condition=x &gt; <span class="number">0</span>, <span class="built_in">input</span>=x, other=<span class="number">0.</span>)</span><br><span class="line">tensor([[<span class="number">1.0779</span>, <span class="number">0.0383</span>],</span><br><span class="line">        [<span class="number">0.0000</span>, <span class="number">0.0000</span>]], dtype=torch.float64)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="扩展与重复操作"><a href="#扩展与重复操作" class="headerlink" title="扩展与重复操作"></a>扩展与重复操作</h2><ul>
<li><p><code>tile</code>: 通过重复张量的元素来构建新张量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.tile((<span class="number">2</span>,))</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tile(y, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>]])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="随机抽样"><a href="#随机抽样" class="headerlink" title="随机抽样"></a>随机抽样</h1><h2 id="随机种子"><a href="#随机种子" class="headerlink" title="随机种子"></a>随机种子</h2><ul>
<li><p><strong>torch.seed</strong>: 设置torch cpu随机数种子</p>
<p><strong>torch.manual_seed</strong>: 设置torch cpu随机数种子，torch.manual_seed(seed)</p>
<p><strong>torch.cuda.manual_seed</strong>: 设置torch cuda随机数种子</p>
<p><strong>torch.initial_seed</strong>: 查看设置的种子值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">torch.seed()</span><br><span class="line">Out[<span class="number">112</span>]: <span class="number">2362131181677400</span></span><br><span class="line">torch.initial_seed()</span><br><span class="line">Out[<span class="number">113</span>]: <span class="number">2362131181677400</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(<span class="number">101</span>)</span><br><span class="line">Out[<span class="number">114</span>]: &lt;torch._C.Generator at <span class="number">0x137cd248b10</span>&gt;</span><br><span class="line">torch.initial_seed()</span><br><span class="line">Out[<span class="number">115</span>]: <span class="number">101</span></span><br><span class="line">torch.cuda.manual_seed(<span class="number">0</span>)</span><br><span class="line">torch.initial_seed()</span><br><span class="line">Out[<span class="number">116</span>]: <span class="number">0</span></span><br></pre></td></tr></table></figure>
</li>
<li><p><code>get_rng_state()</code>: 返回当前随机数生成器的状态。这个状态是一个<code>torch.ByteTensor</code>，它包含了RNG内部的所有状态信息，使得RNG可以在这个状态下继续生成随机数序列。这允许你在某个特定点“保存”RNG的状态，然后在需要的时候恢复到这个状态</p>
</li>
<li><p><code>set_rng_state(state)</code>: 设置随机数生成器的状态。<code>state</code>应该是通过<code>get_rng_state()</code>函数获取的状态张量。这个函数用于恢复RNG到一个特定的状态，这样可以从那个状态开始重新生成相同的随机数序列</p>
</li>
</ul>
<h2 id="随机采样函数"><a href="#随机采样函数" class="headerlink" title="随机采样函数"></a>随机采样函数</h2><blockquote>
<p>常见的概率分布参考 — <a href="https://study.hycbook.com/article/8271.html">兼一书虫-机器学习概率论(1)</a></p>
</blockquote>
<ul>
<li><p><code>torch.rand()</code>: 创建一个具有给定形状的张量，并用区间[0, 1)内的<code>均匀分布</code>的随机数填充</p>
<p><code>torch.rand_like()</code>: 返回一个与给定张量形状相同的张量，并用区间[0, 1)内的<code>均匀分布</code>的随机数填充</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand(<span class="number">4</span>)</span><br><span class="line">tensor([ <span class="number">0.5204</span>,  <span class="number">0.2503</span>,  <span class="number">0.3525</span>,  <span class="number">0.5673</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">tensor([[ <span class="number">0.8237</span>,  <span class="number">0.5781</span>,  <span class="number">0.6879</span>],</span><br><span class="line">        [ <span class="number">0.3816</span>,  <span class="number">0.7249</span>,  <span class="number">0.0998</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.rand_like(torch.rand(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">tensor([[<span class="number">0.3885</span>, <span class="number">0.9888</span>, <span class="number">0.4838</span>],</span><br><span class="line">        [<span class="number">0.8154</span>, <span class="number">0.6068</span>, <span class="number">0.6895</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.randn()</code>: 返回一个具有给定形状的张量，并用<code>标准正态分布</code>的随机数填充</p>
<p><code>torch.randn_like()</code>: 返回一个与给定张量形状相同的张量，并用<code>标准正态分布</code>的随机数填充</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randn(<span class="number">4</span>)</span><br><span class="line">tensor([-<span class="number">2.1436</span>,  <span class="number">0.9966</span>,  <span class="number">2.3426</span>, -<span class="number">0.6366</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">tensor([[ <span class="number">1.5954</span>,  <span class="number">2.8929</span>, -<span class="number">1.0923</span>],</span><br><span class="line">        [ <span class="number">1.1719</span>, -<span class="number">0.4709</span>, -<span class="number">0.1996</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randn_like(torch.randn(<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">tensor([[ <span class="number">0.9979</span>,  <span class="number">0.0471</span>, -<span class="number">1.1305</span>],</span><br><span class="line">        [ <span class="number">0.7216</span>, -<span class="number">0.0747</span>,  <span class="number">0.0610</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.randint()</code>: 返回一个具有给定形状的张量，并用区间[low, high)内的随机整数填充</p>
<p><code>torch.randint_like()</code>: 返回一个与给定张量形状相同的张量，并用区间[low, high)内的随机整数填充</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randint(low=<span class="number">3</span>, high=<span class="number">5</span>, size=(<span class="number">3</span>,))</span><br><span class="line">tensor([<span class="number">4</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randint(<span class="number">10</span>, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">5</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randint(<span class="number">3</span>, <span class="number">10</span>, (<span class="number">2</span>, <span class="number">2</span>))</span><br><span class="line">tensor([[<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randint_like(<span class="built_in">input</span>=torch.randint(low=<span class="number">3</span>, high=<span class="number">5</span>, size=(<span class="number">3</span>,)), low=<span class="number">6</span>, high=<span class="number">10</span>)</span><br><span class="line">tensor([<span class="number">8</span>, <span class="number">9</span>, <span class="number">9</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>torch.randperm()</code>: 返回一个从0到给定参数<code>n - 1</code>的整数的随机排列</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.randperm(<span class="number">4</span>)</span><br><span class="line">tensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>torch.bernoulli</strong>: 从伯努利分布中提取二进制随机数（0或1），输入张量应为包含用于绘制二进制随机数的概率的张量</p>
<p>因此，输入中的所有值都必须在以下范围内(0,1)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.empty(<span class="number">3</span>, <span class="number">3</span>).uniform_(<span class="number">0</span>, <span class="number">1</span>)  <span class="comment"># generate a uniform random matrix with range [0, 1]</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">0.1737</span>,  <span class="number">0.0950</span>,  <span class="number">0.3609</span>],</span><br><span class="line">        [ <span class="number">0.7148</span>,  <span class="number">0.0289</span>,  <span class="number">0.2676</span>],</span><br><span class="line">        [ <span class="number">0.9456</span>,  <span class="number">0.8937</span>,  <span class="number">0.7202</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.bernoulli(a)</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.ones(<span class="number">3</span>, <span class="number">3</span>) <span class="comment"># probability of drawing &quot;1&quot; is 1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.bernoulli(a)</span><br><span class="line">tensor([[ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">1.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.zeros(<span class="number">3</span>, <span class="number">3</span>) <span class="comment"># probability of drawing &quot;1&quot; is 0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.bernoulli(a)</span><br><span class="line">tensor([[ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line">        [ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>torch.poisson</strong>: 泊松分布用于计算一个事件在平均价值率(时间)的一定时间内发生的可能性。泊松分布是一个离散的概率分布</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rates = torch.rand(<span class="number">4</span>, <span class="number">4</span>) * <span class="number">5</span>  <span class="comment"># rate parameter between 0 and 5</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.poisson(rates)</span><br><span class="line">tensor([[<span class="number">9.</span>, <span class="number">1.</span>, <span class="number">3.</span>, <span class="number">5.</span>],</span><br><span class="line">        [<span class="number">8.</span>, <span class="number">6.</span>, <span class="number">6.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">4.</span>, <span class="number">5.</span>, <span class="number">3.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">1.</span>, <span class="number">4.</span>, <span class="number">2.</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>torch.multinomial</strong>: 对input的每一行做n_samples次取值，输出的张量是每一次取值时input张量对应行的下标</p>
<ul>
<li>input (Tensor) – 包含概率值的张量</li>
<li>num_samples (int) – 抽取的样本数</li>
<li>replacement (bool, optional) – 布尔值，决定是否能重复抽取</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">weights = torch.Tensor([<span class="number">0</span>, <span class="number">10</span>, <span class="number">3</span>, <span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">torch.multinomial(weights, <span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># replacement=True时 概率为0的没机会被取到</span></span><br><span class="line">torch.multinomial(weights, <span class="number">4</span>, replacement=<span class="literal">True</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>torch.normal</strong>: 返回一个张量，包含从给定参数<code>means</code>,<code>std</code>的离散正态分布中抽取随机数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">torch.normal(mean=torch.arange(<span class="number">1.</span>, <span class="number">11.</span>), std=torch.arange(<span class="number">1</span>, <span class="number">0</span>, -<span class="number">0.1</span>))</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([ <span class="number">0.9732</span>,  <span class="number">2.0833</span>,  <span class="number">2.5282</span>,  <span class="number">4.3588</span>,  <span class="number">5.4837</span>,  <span class="number">5.1150</span>,  <span class="number">7.0366</span>,  <span class="number">7.9774</span>,</span><br><span class="line">         <span class="number">9.1679</span>, <span class="number">10.0248</span>])</span><br><span class="line"></span><br><span class="line">torch.normal(mean=<span class="number">0.5</span>, std=torch.arange(<span class="number">1.</span>, <span class="number">6.</span>))</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">0.7067</span>,  <span class="number">2.4856</span>, -<span class="number">2.1957</span>, -<span class="number">4.3114</span>, <span class="number">16.2506</span>])</span><br><span class="line"></span><br><span class="line">torch.normal(mean=torch.arange(<span class="number">1.</span>, <span class="number">6.</span>))</span><br><span class="line">Out[<span class="number">2</span>]: tensor([<span class="number">0.7835</span>, <span class="number">4.6096</span>, <span class="number">2.7244</span>, <span class="number">5.2810</span>, <span class="number">4.8413</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h1><ul>
<li><p><strong>torch.save</strong>: 保存一个对象到一个硬盘文件上 参考: <a target="_blank" rel="noopener external nofollow noreferrer" href="http://pytorch.org/docs/notes/serialization.html#recommend-saving-models">Recommended approach for saving a model</a> </p>
<p>torch.save(<em>obj</em>, <em>f</em>, <em>pickle_module=pickle</em>, <em>pickle_protocol=DEFAULT_PROTOCOL</em>, <em>_use_new_zipfile_serialization=True</em>)</p>
<ul>
<li>obj – 保存对象</li>
<li>f － 类文件对象 (返回文件描述符)或一个保存文件名的字符串</li>
<li>pickle_module – 用于pickling元数据和对象的模块</li>
<li>pickle_protocol – 指定pickle protocal 可以覆盖默认参数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to file</span></span><br><span class="line">torch.save(x, <span class="string">&#x27;tensor.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save to io.BytesIO buffer</span></span><br><span class="line">buffer = io.BytesIO()</span><br><span class="line">torch.save(x, buffer)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>torch.load</strong>: 从磁盘文件中读取一个通过<code>torch.save()</code>保存的对象</p>
<p>torch.load(<em>f</em>, <em>map_location=None</em>, <em>pickle_module=pickle</em>, <strong><em>, </em>weights_only=False<em>, </em></strong>pickle_load_args*)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">torch.load(<span class="string">&#x27;tensors.pt&#x27;</span>, encoding=<span class="string">&#x27;ascii&#x27;</span>)</span><br><span class="line">torch.load(<span class="string">&#x27;tensors.pt&#x27;</span>, map_location=torch.device(<span class="string">&#x27;cpu&#x27;</span>))</span><br><span class="line">torch.load(<span class="string">&#x27;tensors.pt&#x27;</span>, map_location=&#123;<span class="string">&#x27;cuda:1&#x27;</span>:<span class="string">&#x27;cuda:0&#x27;</span>&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Load from io.BytesIO buffer</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;tensor.pt&#x27;</span>, <span class="string">&#x27;rb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    buffer = io.BytesIO(f.read())</span><br><span class="line">torch.load(buffer)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="并行化"><a href="#并行化" class="headerlink" title="并行化"></a>并行化</h1><p>在PyTorch中，CPU操作可以通过多线程来并行化，以此提高性能。这里涉及到两种形式的并行化：intra-op并行和inter-op并行。下面是关于这些函数的解释：</p>
<ol>
<li><p><code>get_num_threads()</code>: 这个函数返回用于在CPU上并行执行操作（即intra-op并行）的线程数。Intra-op并行是指单个操作（如矩阵乘法）内部的并行执行。PyTorch会尝试使用所有可用的CPU核心来加速这些操作</p>
</li>
<li><p><code>set_num_threads(int)</code>: 这个函数用来设置在CPU上进行intra-op并行操作时使用的线程数。如果你想限制PyTorch使用的CPU核心数量，可以调用这个函数</p>
</li>
<li><p><code>get_num_interop_threads()</code>: 这个函数返回用于CPU上的inter-op并行的线程数。Inter-op并行是指不同操作之间的并行执行。例如，如果你有多个不依赖于彼此的操作，PyTorch可以同时执行它们以提高效率</p>
</li>
<li><p><code>set_num_interop_threads(int)</code>: 这个函数用来设置用于inter-op并行的线程数。通过设定线程数，可以控制同时进行的不同操作的数量</p>
</li>
</ol>
<p>在多核CPU上，适当地设置这些值可以帮助你更好地利用系统资源，提高程序的运行效率。然而，如果设置的线程数太多，可能会导致线程竞争和上下文切换的开销，反而降低性能</p>
<p>通常默认设置是已经针对性能进行了优化，但是在特定的系统和应用场景下，手动调整这些值可以获得更佳的性能表现</p>
<h1 id="梯度管理"><a href="#梯度管理" class="headerlink" title="梯度管理"></a>梯度管理</h1><p>在PyTorch中，梯度计算对于训练神经网络是必要的，因为它们用于优化模型的参数。然而，在某些情况下，比如在模型评估或应用阶段，你可能不需要计算梯度。梯度计算会占用额外的内存和计算资源，禁用它们可以提高效率。为了方便地开启和关闭梯度计算，PyTorch提供了几个上下文管理器：</p>
<ol>
<li><p><code>torch.no_grad()</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># 在这个代码块中，所有的操作都不会跟踪梯度</span></span><br><span class="line">    predictions = model(inputs)</span><br></pre></td></tr></table></figure>
<p>在这个例子中，<code>model(inputs)</code> 的执行不会计算梯度，这对于模型推断（inference）阶段非常有用，因为它减少了内存消耗并提高了计算速度</p>
</li>
<li><p><code>torch.enable_grad()</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.enable_grad():</span><br><span class="line">    <span class="comment"># 在这个代码块中，梯度计算是启用的</span></span><br><span class="line">    predictions = model(inputs)</span><br><span class="line">    loss = loss_fn(predictions, targets)</span><br><span class="line">    loss.backward()</span><br></pre></td></tr></table></figure>
<p>这里，即使全局梯度计算被禁用，<code>torch.enable_grad()</code> 仍可以在其作用域内启用梯度计算，以便计算损失函数的梯度</p>
</li>
<li><p><code>torch.set_grad_enabled()</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.set_grad_enabled(mode=<span class="literal">True</span>) <span class="comment"># 启用梯度计算</span></span><br><span class="line"><span class="comment"># 后续操作将会跟踪梯度</span></span><br><span class="line">predictions = model(inputs)</span><br><span class="line">loss = loss_fn(predictions, targets)</span><br><span class="line">loss.backward()</span><br><span class="line">torch.set_grad_enabled(mode=<span class="literal">False</span>) <span class="comment"># 禁用梯度计算</span></span><br></pre></td></tr></table></figure>
<p>在这里，使用<code>torch.set_grad_enabled()</code>函数来全局地控制是否计算梯度。传递<code>True</code>或<code>False</code>可以分别开启或关闭梯度计算</p>
</li>
<li><p><code>torch.is_grad_enabled()</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(torch.is_grad_enabled()) <span class="comment"># 打印当前是否启用了梯度计算</span></span><br></pre></td></tr></table></figure>
<p>这个函数用来检查当前是否启用了梯度计算</p>
</li>
<li><p><code>torch.inference_mode()</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    <span class="comment"># 在这个代码块中，所有的操作都不会跟踪梯度，且某些优化会被应用以加速推断</span></span><br><span class="line">    predictions = model(inputs)</span><br></pre></td></tr></table></figure>
<p><code>torch.inference_mode()</code> 更适合用在推断阶段，相比<code>torch.no_grad()</code>，它会启用额外的优化，比如禁用自动求导引擎和解除对操作immutable的限制，从而实现更高效的模型推断</p>
</li>
<li><p><code>torch.is_inference_mode_enabled()</code>:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.inference_mode():</span><br><span class="line">    <span class="built_in">print</span>(torch.is_inference_mode_enabled()) <span class="comment"># 在 inference mode 中，这将输出 True</span></span><br><span class="line"><span class="built_in">print</span>(torch.is_inference_mode_enabled()) <span class="comment"># 在 inference mode 外部，这将输出 False</span></span><br></pre></td></tr></table></figure>
<p>这个函数用来检查当前是否启用了推断模式。在<code>torch.inference_mode()</code>上下文管理器的内部，它会返回<code>True</code></p>
</li>
</ol>
<p>每个上下文管理器和函数都有其用途，根据需要进行梯度计算的控制，可以优化您的PyTorch 程序的性能</p>
<h1 id="数学操作"><a href="#数学操作" class="headerlink" title="数学操作"></a>数学操作</h1><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/torch.html#math-operations">torch — PyTorch 2.2 documentation-数学操作</a></p>
</blockquote>
<h2 id="基础操作"><a href="#基础操作" class="headerlink" title="基础操作"></a>基础操作</h2><ul>
<li><p><strong>torch.add</strong>: 对输入张量<code>input</code>逐元素加上标量值<code>value</code>，并返回结果到一个新的张量</p>
<p><strong>torch.addcdiv</strong>: 用<code>tensor2</code>对<code>tensor1</code>逐元素相除，然后乘以标量值<code>value</code> 并加到<code>tensor</code>，张量的形状不需要匹配，但元素数量必须一致</p>
<ul>
<li>tensor (Tensor) – 张量，对 tensor1 ./ tensor 进行相加</li>
<li>value (Number, optional) – 标量，对 tensor1 ./ tensor2 进行相乘</li>
<li>tensor1 (Tensor) – 张量，作为被除数(分子)</li>
<li>tensor2 (Tensor) –张量，作为除数(分母)</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<script type="math/tex; mode=display">
out _{i}=\operatorname{input}_{i}+ value \times \frac{\text { tensor } 1_{i}}{\text { tensor 2}_{i}}</script><p><strong>torch.addcmul</strong>: 用<code>tensor2</code>对<code>tensor1</code>逐元素相乘，并对结果乘以标量值<code>value</code>然后加到<code>tensor</code></p>
<ul>
<li>tensor (Tensor) – 张量，对tensor1 ./ tensor 进行相加</li>
<li>value (Number, optional) – 标量，对 tensor1 . tensor2 进行相乘</li>
<li>tensor1 (Tensor) – 张量，作为乘子1</li>
<li>tensor2 (Tensor) –张量，作为乘子2</li>
<li>out (Tensor, optional) – 输出张量</li>
</ul>
<script type="math/tex; mode=display">
out _{i}= input _{i}+ value \times tensor 1_{i} \times tensor 2_{i}</script><p>以上两个可以用于正则化操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对输入张量`input`逐元素加上标量值`value`，并返回结果到一个新的张量</span></span><br><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0.3510</span>, -<span class="number">0.2226</span>, -<span class="number">0.7971</span>, -<span class="number">0.2564</span>])</span><br><span class="line">torch.add(a, <span class="number">20</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">20.3510</span>, <span class="number">19.7774</span>, <span class="number">19.2029</span>, <span class="number">19.7436</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用`tensor2`对`tensor1`逐元素相除，然后乘以标量值`value` 并加到`tensor`</span></span><br><span class="line">t = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">t1 = torch.randn(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">t2 = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">t, t1, t2</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">(tensor([[-<span class="number">1.2863</span>,  <span class="number">1.1267</span>, -<span class="number">1.7120</span>]]),</span><br><span class="line"> tensor([[-<span class="number">0.4294</span>],</span><br><span class="line">         [-<span class="number">0.5328</span>],</span><br><span class="line">         [-<span class="number">0.5373</span>]]),</span><br><span class="line"> tensor([[-<span class="number">0.0876</span>,  <span class="number">0.4398</span>,  <span class="number">1.3583</span>]]))</span><br><span class="line"></span><br><span class="line">torch.addcdiv(t, t1, t2, value=<span class="number">0.1</span>)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[-<span class="number">0.7958</span>,  <span class="number">1.0291</span>, -<span class="number">1.7436</span>],</span><br><span class="line">        [-<span class="number">0.6778</span>,  <span class="number">1.0056</span>, -<span class="number">1.7512</span>],</span><br><span class="line">        [-<span class="number">0.6727</span>,  <span class="number">1.0046</span>, -<span class="number">1.7515</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用`tensor2`对`tensor1`逐元素相乘，并对结果乘以标量值`value`然后加到`tensor`</span></span><br><span class="line">t = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">t1 = torch.randn(<span class="number">3</span>, <span class="number">1</span>)</span><br><span class="line">t2 = torch.randn(<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">t, t1, t2</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">(tensor([[-<span class="number">1.2863</span>,  <span class="number">1.1267</span>, -<span class="number">1.7120</span>]]),</span><br><span class="line"> tensor([[-<span class="number">0.4294</span>],</span><br><span class="line">         [-<span class="number">0.5328</span>],</span><br><span class="line">         [-<span class="number">0.5373</span>]]),</span><br><span class="line"> tensor([[-<span class="number">0.0876</span>,  <span class="number">0.4398</span>,  <span class="number">1.3583</span>]]))</span><br><span class="line"></span><br><span class="line">torch.addcmul(t, t1, t2, value=<span class="number">0.1</span>)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[-<span class="number">1.2825</span>,  <span class="number">1.1078</span>, -<span class="number">1.7703</span>],</span><br><span class="line">        [-<span class="number">1.2816</span>,  <span class="number">1.1033</span>, -<span class="number">1.7844</span>],</span><br><span class="line">        [-<span class="number">1.2816</span>,  <span class="number">1.1031</span>, -<span class="number">1.7850</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>torch.ceil</strong>: 对输入<code>input</code>张量每个元素向上取整, 即取不小于每个元素的最小整数</p>
<p><strong>torch.clamp</strong>(别名<strong>torch.clip</strong>): 将输入<code>input</code>张量每个元素的夹紧到区间<script type="math/tex">[min, max]</script>，并返回结果到一个新张量</p>
<script type="math/tex; mode=display">
y_i = \begin{cases} 
\text{min}, & \text{if } x_i < \text{min} \\
x_i, & \text{if } \text{min} \leq x_i \leq \text{max} \\
\text{max}, & \text{if } x_i > \text{max}
\end{cases}</script><p><strong>torch.floor</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的floor，即不小于元素的最大整数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># torch.ceil</span></span><br><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.9105</span>, -<span class="number">0.7277</span>,  <span class="number">0.9516</span>, -<span class="number">0.1081</span>])</span><br><span class="line">torch.ceil(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([-<span class="number">0.</span>, -<span class="number">0.</span>, <span class="number">1.</span>, -<span class="number">0.</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.floor</span></span><br><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.5661</span>, -<span class="number">0.9135</span>,  <span class="number">1.1018</span>, -<span class="number">0.2633</span>])</span><br><span class="line">torch.floor(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([-<span class="number">1.</span>, -<span class="number">1.</span>,  <span class="number">1.</span>, -<span class="number">1.</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.clamp</span></span><br><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.9105</span>, -<span class="number">0.7277</span>,  <span class="number">0.9516</span>, -<span class="number">0.1081</span>])</span><br><span class="line">torch.clamp(a, <span class="built_in">min</span>=-<span class="number">0.5</span>, <span class="built_in">max</span>=<span class="number">0.5</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([-<span class="number">0.5000</span>, -<span class="number">0.5000</span>,  <span class="number">0.5000</span>, -<span class="number">0.1081</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>torch.div</strong>(别名<strong>torch.divide</strong>): 将<code>input</code>逐元素除以标量值<code>value</code>，并返回结果到输出张量<code>out</code>，torch.div(input, value, out=None)</p>
<p>两张量<code>input</code>和<code>other</code>逐元素相除，并将结果返回到输出，torch.div(<em>input</em>, <em>other</em>, <em>**, </em>rounding_mode=None<em>, </em>out=None*) → Tensor</p>
<p><strong>torch.mul</strong>(别米<strong>torch.multiply</strong>): 用标量值<code>value</code>乘以输入<code>input</code>的每个元素，并返回一个新的结果张量，torch.mul(input, value, out=None)</p>
<p>两个张量<code>input</code>,<code>other</code>按元素进行相乘，并返回到输出张量，torch.mul(input, other, out=None)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 元素除</span></span><br><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.9105</span>, -<span class="number">0.7277</span>,  <span class="number">0.9516</span>, -<span class="number">0.1081</span>])</span><br><span class="line">torch.div(a, <span class="number">0.5</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([-<span class="number">1.8210</span>, -<span class="number">1.4554</span>,  <span class="number">1.9032</span>, -<span class="number">0.2162</span>])</span><br><span class="line">a = torch.tensor([[-<span class="number">0.3711</span>, -<span class="number">1.9353</span>, -<span class="number">0.4605</span>, -<span class="number">0.2917</span>],</span><br><span class="line">                  [ <span class="number">0.1815</span>, -<span class="number">1.0111</span>,  <span class="number">0.9805</span>, -<span class="number">1.5923</span>]])</span><br><span class="line">b = torch.tensor([ <span class="number">0.8032</span>,  <span class="number">0.2930</span>, -<span class="number">0.8113</span>, -<span class="number">0.2308</span>])</span><br><span class="line">torch.div(a, b, rounding_mode=<span class="string">&#x27;trunc&#x27;</span>)</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">tensor([[-<span class="number">0.</span>, -<span class="number">6.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">0.</span>, -<span class="number">3.</span>, -<span class="number">1.</span>,  <span class="number">6.</span>]])</span><br><span class="line">torch.div(a, b, rounding_mode=<span class="string">&#x27;floor&#x27;</span>)</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">tensor([[-<span class="number">1.</span>, -<span class="number">7.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>],</span><br><span class="line">        [ <span class="number">0.</span>, -<span class="number">4.</span>, -<span class="number">2.</span>,  <span class="number">6.</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 元素乘</span></span><br><span class="line">a = torch.randn(<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0.0603</span>, -<span class="number">0.5258</span>, -<span class="number">0.3810</span>])</span><br><span class="line">b = torch.randn(<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">1.2408</span>, -<span class="number">1.3506</span>,  <span class="number">0.9296</span>])</span><br><span class="line">torch.mul(a, <span class="number">100</span>)</span><br><span class="line">Out[<span class="number">2</span>]: tensor([  <span class="number">6.0299</span>, -<span class="number">52.5785</span>, -<span class="number">38.0989</span>])</span><br><span class="line">torch.mul(a, b)</span><br><span class="line">Out[<span class="number">3</span>]: tensor([ <span class="number">0.0748</span>,  <span class="number">0.7101</span>, -<span class="number">0.3542</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>torch.exp</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的指数</p>
<p><strong>torch.frac</strong>: 返回每个元素的分数部分</p>
<p><strong>torch.log</strong>: 计算<code>input</code> 的自然对数</p>
<p><strong>torch.log1p</strong>: 计算<script type="math/tex">input + 1</script>的自然对数<script type="math/tex">y_i = log(x_i+1)</script>，对值比较小的输入，此函数比<code>torch.log()</code>更准确</p>
<p><strong>torch.neg</strong>(别名<strong>torch.negative</strong>): 返回一个新张量，包含输入<code>input</code> 张量按元素取负</p>
<p><strong>torch.pow</strong>: 对输入<code>input</code>的按元素求<code>exponent</code>次幂值，并返回结果张量，幂值<code>exponent</code> 可以为单一 <code>float</code> 数或者与<code>input</code>相同元素数的张量</p>
<p><strong>torch.round</strong>: (<code>四舍五入</code>)返回一个新张量，将输入<code>input</code>张量每个元素舍入到最近的整数</p>
<p><strong>torch.trunc</strong>: (<code>四舍五入(去尾法)</code>)返回一个新张量，包含输入<code>input</code>张量每个元素的截断值(标量x的截断值是最接近其的整数)，简而言之，有符号数的小数部分被舍弃</p>
<p><strong>torch.rsqrt</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的平方根倒数</p>
<p><strong>torch.fmod</strong>: 计算逐元素余数，<code>保留正负号</code></p>
<p><strong>torch.remainder</strong>: 计算逐元素余数， 相当于python中的%操作符，<code>不保留正负号</code></p>
<p><strong>torch.reciprocal</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的倒数，即 1.0/x</p>
<p><strong>torch.sqrt</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的平方根</p>
<p><strong>torch.abs</strong>(别名为<strong>torch.absolute</strong>): 计算输入张量的每个元素绝对值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回一个新张量，包含输入`input`张量每个元素的指数</span></span><br><span class="line">torch.exp(torch.Tensor([<span class="number">0</span>, math.log(<span class="number">2</span>)]))</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回每个元素的分数部分</span></span><br><span class="line">torch.frac(torch.Tensor([<span class="number">1</span>, <span class="number">2.5</span>, -<span class="number">3.2</span>]))</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0.0000</span>,  <span class="number">0.5000</span>, -<span class="number">0.2000</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算`input` 的自然对数</span></span><br><span class="line">a = torch.randn(<span class="number">5</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.3466</span>,  <span class="number">2.3803</span>, -<span class="number">0.0423</span>, -<span class="number">0.9744</span>,  <span class="number">0.4976</span>])</span><br><span class="line">torch.log(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([    nan,  <span class="number">0.8672</span>,     nan,     nan, -<span class="number">0.6980</span>])</span><br><span class="line"><span class="comment"># 计算input + 1的自然对数</span></span><br><span class="line">torch.log1p(a)</span><br><span class="line">Out[<span class="number">2</span>]: tensor([-<span class="number">0.4256</span>,  <span class="number">1.2180</span>, -<span class="number">0.0432</span>, -<span class="number">3.6633</span>,  <span class="number">0.4039</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回一个新张量，包含输入`input` 张量按元素取负</span></span><br><span class="line">a = torch.randn(<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0.0603</span>, -<span class="number">0.5258</span>, -<span class="number">0.3810</span>])</span><br><span class="line">torch.neg(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([-<span class="number">0.0603</span>,  <span class="number">0.5258</span>,  <span class="number">0.3810</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求指数</span></span><br><span class="line">a = torch.arange(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">exp = torch.arange(<span class="number">1</span>, <span class="number">5</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">torch.<span class="built_in">pow</span>(a, <span class="number">2</span>)</span><br><span class="line">Out[<span class="number">2</span>]: tensor([ <span class="number">1</span>,  <span class="number">4</span>,  <span class="number">9</span>, <span class="number">16</span>])</span><br><span class="line">torch.<span class="built_in">pow</span>(a, exp)</span><br><span class="line">Out[<span class="number">3</span>]: tensor([  <span class="number">1</span>,   <span class="number">4</span>,  <span class="number">27</span>, <span class="number">256</span>])</span><br><span class="line">torch.<span class="built_in">pow</span>(<span class="number">2</span>, exp)</span><br><span class="line">Out[<span class="number">4</span>]: tensor([ <span class="number">2</span>,  <span class="number">4</span>,  <span class="number">8</span>, <span class="number">16</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 四舍五入</span></span><br><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0.7995</span>, -<span class="number">2.0975</span>,  <span class="number">0.7273</span>,  <span class="number">0.7539</span>])</span><br><span class="line">torch.<span class="built_in">round</span>(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">1.</span>, -<span class="number">2.</span>,  <span class="number">1.</span>,  <span class="number">1.</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 四舍五入(去尾法)</span></span><br><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">2.1647</span>, -<span class="number">0.2294</span>,  <span class="number">0.4943</span>,  <span class="number">1.5146</span>])</span><br><span class="line">torch.trunc(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([-<span class="number">2.</span>, -<span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求平方根倒数</span></span><br><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line">torch.rsqrt(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">1.0000</span>, <span class="number">0.7071</span>, <span class="number">0.5774</span>, <span class="number">0.5000</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算逐元素余数， 保留正负号</span></span><br><span class="line">t = torch.tensor([<span class="number">10</span>, -<span class="number">22</span>, <span class="number">31</span>, -<span class="number">47</span>])</span><br><span class="line">torch.fmod(t, <span class="number">5</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0</span>, -<span class="number">2</span>,  <span class="number">1</span>, -<span class="number">2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算逐元素余数， 相当于python中的%操作符</span></span><br><span class="line">torch.remainder(t, <span class="number">5</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>])</span><br><span class="line">np.mod(np.array([<span class="number">10</span>, -<span class="number">22</span>, <span class="number">31</span>, -<span class="number">47</span>]), <span class="number">5</span>)</span><br><span class="line">Out[<span class="number">2</span>]: array([<span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>], dtype=int32)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求1/x</span></span><br><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line">torch.reciprocal(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">1.0000</span>, <span class="number">0.5000</span>, <span class="number">0.3333</span>, <span class="number">0.2500</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求平方根</span></span><br><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line">torch.sqrt(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">1.0000</span>, <span class="number">1.4142</span>, <span class="number">1.7321</span>, <span class="number">2.0000</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 求绝对值</span></span><br><span class="line">torch.<span class="built_in">abs</span>(torch.FloatTensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>]))</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="三角函数"><a href="#三角函数" class="headerlink" title="三角函数"></a>三角函数</h2><ul>
<li><p><strong>torch.asin</strong>(别名<strong>torch.arcsin</strong>): 返回一个新张量，包含输入<code>input</code>张量每个元素的<code>反正弦函数</code></p>
<p><strong>torch.atan</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的<code>反正切函数</code></p>
<p><strong>torch.atan2</strong>: 返回一个新张量，包含两个输入张量<code>input1</code>和<code>input2</code>的<code>反正切函数</code></p>
<p><strong>torch.cos</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的<code>余弦</code></p>
<p><strong>torch.acos</strong>(别名<strong>torch.arccos</strong>): 返回一个新张量，包含输入张量每个元素的<code>反余弦</code></p>
<p><strong>torch.cosh</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的<code>双曲余弦</code></p>
<p><strong>torch.sin</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的<code>正弦</code></p>
<p><strong>torch.sinh</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的<code>双曲正弦</code></p>
<p><strong>torch.tan</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的<code>正切</code></p>
<p><strong>torch.tanh</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的<code>双曲正切</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 反正弦函数</span></span><br><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0.2583</span>, -<span class="number">0.5285</span>,  <span class="number">0.8979</span>,  <span class="number">1.0104</span>])</span><br><span class="line">torch.asin(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">0.2613</span>, -<span class="number">0.5569</span>,  <span class="number">1.1149</span>,     nan])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 反正切函数</span></span><br><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([ <span class="number">0.2583</span>, -<span class="number">0.5285</span>,  <span class="number">0.8979</span>,  <span class="number">1.0104</span>])</span><br><span class="line">b = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">2</span>]: tensor([<span class="number">0.1100</span>, <span class="number">1.4311</span>, <span class="number">1.9536</span>, <span class="number">0.7652</span>])</span><br><span class="line">torch.atan(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">0.2528</span>, -<span class="number">0.4862</span>,  <span class="number">0.7316</span>,  <span class="number">0.7906</span>])</span><br><span class="line">torch.atan2(a, b)</span><br><span class="line">Out[<span class="number">3</span>]: tensor([ <span class="number">1.1681</span>, -<span class="number">0.3538</span>,  <span class="number">0.4308</span>,  <span class="number">0.9226</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 余弦</span></span><br><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.9105</span>, -<span class="number">0.7277</span>,  <span class="number">0.9516</span>, -<span class="number">0.1081</span>])</span><br><span class="line">torch.cos(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">0.6133</span>, <span class="number">0.7467</span>, <span class="number">0.5804</span>, <span class="number">0.9942</span>])</span><br><span class="line"><span class="comment"># 反余弦</span></span><br><span class="line">torch.acos(torch.FloatTensor([-<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>]))</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">3.1416</span>, <span class="number">0.0000</span>, <span class="number">1.5708</span>])</span><br><span class="line"><span class="comment"># 双曲余弦</span></span><br><span class="line">torch.cosh(a)</span><br><span class="line">Out[<span class="number">2</span>]: tensor([<span class="number">1.4439</span>, <span class="number">1.2766</span>, <span class="number">1.4880</span>, <span class="number">1.0058</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正弦</span></span><br><span class="line">a = torch.randn(<span class="number">4</span>)</span><br><span class="line">torch.sin(a)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.9215</span>,  <span class="number">0.2650</span>,  <span class="number">0.8285</span>,  <span class="number">0.5914</span>])</span><br><span class="line"><span class="comment"># 双曲正弦</span></span><br><span class="line">torch.sinh(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([-<span class="number">1.4591</span>,  <span class="number">0.2714</span>,  <span class="number">1.1392</span>,  <span class="number">0.6759</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正切</span></span><br><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line">torch.tan(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">1.5574</span>, -<span class="number">2.1850</span>, -<span class="number">0.1425</span>,  <span class="number">1.1578</span>])</span><br><span class="line"><span class="comment"># 双曲正切</span></span><br><span class="line">torch.tanh(a)</span><br><span class="line">Out[<span class="number">2</span>]: tensor([<span class="number">0.7616</span>, <span class="number">0.9640</span>, <span class="number">0.9951</span>, <span class="number">0.9993</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="位操作"><a href="#位操作" class="headerlink" title="位操作"></a>位操作</h2><ul>
<li><p><strong>bitwise_not</strong> – 按位非: 计算给定输入张量的<code>按位非</code>，这个操作会将输入张量中的每个位反转，即将所有的1变成0，将所有的0变成1。在整数数据类型中，这通常意味着进行二进制补码的运算</p>
<p><strong>bitwise_and</strong> – 按位与: 计算两个输入张量的<code>按位与</code>，只有当两个张量在同一位置的位都是1时，结果张量在该位置的位才是1，否则是0</p>
<p><strong>bitwise_or</strong> – 按位或: 计算两个输入张量的<code>按位或</code>，只要两个张量在同一位置的位中有一个是1，结果张量在该位置的位就是1。如果都是0，结果就是0</p>
<p><strong>bitwise_xor</strong> – 按位异或: 计算两个输入张量的<code>按位异或</code>，这个操作在两个张量在同一位置的位不同的时候返回1，相同的时候返回0</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按位非</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.bitwise_not(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8))</span><br><span class="line">tensor([ <span class="number">0</span>,  <span class="number">1</span>, -<span class="number">4</span>], dtype=torch.int8)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按位与</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.bitwise_and(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8), torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int8))</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">0</span>,  <span class="number">3</span>], dtype=torch.int8)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.bitwise_and(torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>]), torch.tensor([<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>]))</span><br><span class="line">tensor([ <span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按位或</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.bitwise_or(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8), torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int8))</span><br><span class="line">tensor([-<span class="number">1</span>, -<span class="number">2</span>,  <span class="number">3</span>], dtype=torch.int8)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.bitwise_or(torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>]), torch.tensor([<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>]))</span><br><span class="line">tensor([ <span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按位异或</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.bitwise_xor(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8), torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int8))</span><br><span class="line">tensor([-<span class="number">2</span>, -<span class="number">2</span>,  <span class="number">0</span>], dtype=torch.int8)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.bitwise_xor(torch.tensor([<span class="literal">True</span>, <span class="literal">True</span>, <span class="literal">False</span>]), torch.tensor([<span class="literal">False</span>, <span class="literal">True</span>, <span class="literal">False</span>]))</span><br><span class="line">tensor([ <span class="literal">True</span>, <span class="literal">False</span>, <span class="literal">False</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>bitwise_left_shift</strong> – 按位左移: 计算给定输入张量与另一张量(表示位移数量)的按位左移。这个操作将输入张量的每个位向左移动<code>other</code>指定的位数，左边溢出的位被丢弃，而右边则填充0</p>
<p><strong>bitwise_right_shift</strong> – 按位右移: 计算给定输入张量与另一张量(表示位移数量)的按位右移。这个操作将输入张量的每个位向右移动<code>other</code>指定的位数，右边溢出的位被丢弃，对于无符号数据类型，左边填充0；对于有符号数据类型，一般会进行算术右移，填充的是最高位的值，即符号位</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按位左移</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.bitwise_left_shift(torch.tensor([-<span class="number">1</span>, -<span class="number">2</span>, <span class="number">3</span>], dtype=torch.int8), torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int8))</span><br><span class="line">tensor([-<span class="number">2</span>, -<span class="number">2</span>, <span class="number">24</span>], dtype=torch.int8)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按位右移</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.bitwise_right_shift(torch.tensor([-<span class="number">2</span>, -<span class="number">7</span>, <span class="number">31</span>], dtype=torch.int8), torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">3</span>], dtype=torch.int8))</span><br><span class="line">tensor([-<span class="number">1</span>, -<span class="number">7</span>,  <span class="number">3</span>], dtype=torch.int8)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="其他操作"><a href="#其他操作" class="headerlink" title="其他操作"></a>其他操作</h2><ul>
<li><p><strong>torch.sigmoid</strong>: 返回一个新张量，包含输入<code>input</code>张量每个元素的sigmoid值</p>
<p><strong>torch.sign</strong>: 符号函数，返回一个新张量，包含输入<code>input</code>张量每个元素的正负</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 求sigmoid值</span></span><br><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line">torch.sigmoid(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">0.7311</span>, <span class="number">0.8808</span>, <span class="number">0.9526</span>, <span class="number">0.9820</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 符号函数</span></span><br><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line">torch.sign(a)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>torch.lerp</strong>: 对两个张量以<code>start</code>，<code>end</code>做线性插值， 将结果返回到输出张量</p>
<ul>
<li>start (Tensor) – 起始点张量</li>
<li>end (Tensor) – 终止点张量</li>
<li>weight (float) – 插值公式的weight</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
<script type="math/tex; mode=display">
out_i=start_i+weight_i∗(end_i−start_i)</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">start = torch.arange(<span class="number">1.</span>, <span class="number">5.</span>)</span><br><span class="line">end = torch.empty(<span class="number">4</span>).fill_(<span class="number">10</span>)</span><br><span class="line">start, end</span><br><span class="line">Out[<span class="number">0</span>]: (tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>]), tensor([<span class="number">10.</span>, <span class="number">10.</span>, <span class="number">10.</span>, <span class="number">10.</span>]))</span><br><span class="line"></span><br><span class="line">torch.lerp(start, end, <span class="number">0.5</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">5.5000</span>, <span class="number">6.0000</span>, <span class="number">6.5000</span>, <span class="number">7.0000</span>])</span><br><span class="line">torch.lerp(start, end, torch.full_like(start, <span class="number">0.5</span>))</span><br><span class="line">Out[<span class="number">2</span>]: tensor([<span class="number">5.5000</span>, <span class="number">6.0000</span>, <span class="number">6.5000</span>, <span class="number">7.0000</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="归约操作"><a href="#归约操作" class="headerlink" title="归约操作"></a>归约操作</h1><h2 id="极值操作"><a href="#极值操作" class="headerlink" title="极值操作"></a>极值操作</h2><ul>
<li><p><code>argmax</code>: 返回张量中最大值的索引</p>
<p><code>argmin</code>: 返回张量中最小值的索引</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回张量中最大值的索引</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">1.3398</span>,  <span class="number">0.2663</span>, -<span class="number">0.2686</span>,  <span class="number">0.2450</span>],</span><br><span class="line">        [-<span class="number">0.7401</span>, -<span class="number">0.8805</span>, -<span class="number">0.3402</span>, -<span class="number">1.1936</span>],</span><br><span class="line">        [ <span class="number">0.4907</span>, -<span class="number">1.3948</span>, -<span class="number">1.0691</span>, -<span class="number">0.3132</span>],</span><br><span class="line">        [-<span class="number">1.6092</span>,  <span class="number">0.5419</span>, -<span class="number">0.2993</span>,  <span class="number">0.3195</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.argmax(a, dim=<span class="number">1</span>)</span><br><span class="line">tensor([ <span class="number">0</span>,  <span class="number">2</span>,  <span class="number">0</span>,  <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回张量中最小值的索引</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">0.1139</span>,  <span class="number">0.2254</span>, -<span class="number">0.1381</span>,  <span class="number">0.3687</span>],</span><br><span class="line">        [ <span class="number">1.0100</span>, -<span class="number">1.1975</span>, -<span class="number">0.0102</span>, -<span class="number">0.4732</span>],</span><br><span class="line">        [-<span class="number">0.9240</span>,  <span class="number">0.1207</span>, -<span class="number">0.7506</span>, -<span class="number">1.0213</span>],</span><br><span class="line">        [ <span class="number">1.7809</span>, -<span class="number">1.2960</span>,  <span class="number">0.9384</span>,  <span class="number">0.1438</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.argmin(a)</span><br><span class="line">tensor(<span class="number">13</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.argmin(a, dim=<span class="number">1</span>)</span><br><span class="line">tensor([ <span class="number">2</span>,  <span class="number">1</span>,  <span class="number">3</span>,  <span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.argmin(a, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">tensor([[<span class="number">2</span>],</span><br><span class="line">        [<span class="number">1</span>],</span><br><span class="line">        [<span class="number">3</span>],</span><br><span class="line">        [<span class="number">1</span>]])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>amax</code>: 返回指定维度上每个切片的最大值</p>
<p><code>amin</code>: 返回指定维度上每个切片的最小值</p>
<p><code>max</code>: 返回张量中所有元素的最大值</p>
<p><code>min</code>: 返回张量中所有元素的最小值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回指定维度上每个切片的最大值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">0.8177</span>,  <span class="number">1.4878</span>, -<span class="number">0.2491</span>,  <span class="number">0.9130</span>],</span><br><span class="line">        [-<span class="number">0.7158</span>,  <span class="number">1.1775</span>,  <span class="number">2.0992</span>,  <span class="number">0.4817</span>],</span><br><span class="line">        [-<span class="number">0.0053</span>,  <span class="number">0.0164</span>, -<span class="number">1.3738</span>, -<span class="number">0.0507</span>],</span><br><span class="line">        [ <span class="number">1.9700</span>,  <span class="number">1.1106</span>, -<span class="number">1.0318</span>, -<span class="number">1.0816</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.amax(a, dim=<span class="number">1</span>)</span><br><span class="line">tensor([<span class="number">1.4878</span>, <span class="number">2.0992</span>, <span class="number">0.0164</span>, <span class="number">1.9700</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回指定维度上每个切片的最小值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">0.6451</span>, -<span class="number">0.4866</span>,  <span class="number">0.2987</span>, -<span class="number">1.3312</span>],</span><br><span class="line">        [-<span class="number">0.5744</span>,  <span class="number">1.2980</span>,  <span class="number">1.8397</span>, -<span class="number">0.2713</span>],</span><br><span class="line">        [ <span class="number">0.9128</span>,  <span class="number">0.9214</span>, -<span class="number">1.7268</span>, -<span class="number">0.2995</span>],</span><br><span class="line">        [ <span class="number">0.9023</span>,  <span class="number">0.4853</span>,  <span class="number">0.9075</span>, -<span class="number">1.6165</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.amin(a, dim=<span class="number">1</span>)</span><br><span class="line">tensor([-<span class="number">1.3312</span>, -<span class="number">0.5744</span>, -<span class="number">1.7268</span>, -<span class="number">1.6165</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回张量中所有元素的最大值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[-<span class="number">1.2360</span>, -<span class="number">0.2942</span>, -<span class="number">0.1222</span>,  <span class="number">0.8475</span>],</span><br><span class="line">        [ <span class="number">1.1949</span>, -<span class="number">1.1127</span>, -<span class="number">2.2379</span>, -<span class="number">0.6702</span>],</span><br><span class="line">        [ <span class="number">1.5717</span>, -<span class="number">0.9207</span>,  <span class="number">0.1297</span>, -<span class="number">1.8768</span>],</span><br><span class="line">        [-<span class="number">0.6172</span>,  <span class="number">1.0036</span>, -<span class="number">0.6060</span>, -<span class="number">0.2432</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.<span class="built_in">max</span>(a, <span class="number">1</span>)</span><br><span class="line">torch.return_types.<span class="built_in">max</span>(values=tensor([<span class="number">0.8475</span>, <span class="number">1.1949</span>, <span class="number">1.5717</span>, <span class="number">1.0036</span>]), indices=tensor([<span class="number">3</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回张量中所有元素的最小值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[-<span class="number">0.6248</span>,  <span class="number">1.1334</span>, -<span class="number">1.1899</span>, -<span class="number">0.2803</span>],</span><br><span class="line">        [-<span class="number">1.4644</span>, -<span class="number">0.2635</span>, -<span class="number">0.3651</span>,  <span class="number">0.6134</span>],</span><br><span class="line">        [ <span class="number">0.2457</span>,  <span class="number">0.0384</span>,  <span class="number">1.0128</span>,  <span class="number">0.7015</span>],</span><br><span class="line">        [-<span class="number">0.1153</span>,  <span class="number">2.9849</span>,  <span class="number">2.1458</span>,  <span class="number">0.5788</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.<span class="built_in">min</span>(a, <span class="number">1</span>)</span><br><span class="line">torch.return_types.<span class="built_in">min</span>(values=tensor([-<span class="number">1.1899</span>, -<span class="number">1.4644</span>,  <span class="number">0.0384</span>, -<span class="number">0.1153</span>]), indices=tensor([<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>aminmax</code>: 同时计算最小值和最大值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.aminmax(torch.tensor([<span class="number">1</span>, -<span class="number">3</span>, <span class="number">5</span>]))</span><br><span class="line">torch.return_types.aminmax(<span class="built_in">min</span>=tensor(-<span class="number">3</span>), <span class="built_in">max</span>=tensor(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment"># aminmax propagates NaNs</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.aminmax(torch.tensor([<span class="number">1</span>, -<span class="number">3</span>, <span class="number">5</span>, torch.nan]))</span><br><span class="line">torch.return_types.aminmax(<span class="built_in">min</span>=tensor(nan),<span class="built_in">max</span>=tensor(nan))</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.arange(<span class="number">10</span>).view(<span class="number">2</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>],</span><br><span class="line">        [<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t.aminmax(dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">torch.return_types.aminmax(<span class="built_in">min</span>=tensor([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]]), <span class="built_in">max</span>=tensor([[<span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]]))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="统计操作"><a href="#统计操作" class="headerlink" title="统计操作"></a>统计操作</h2><ul>
<li><p><code>mean</code>: 返回张量中所有元素的均值</p>
<p><code>nanmean</code>: 返回张量中所有非NaN元素的均值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回张量中所有元素的均值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[-<span class="number">0.3841</span>,  <span class="number">0.6320</span>,  <span class="number">0.4254</span>, -<span class="number">0.7384</span>],</span><br><span class="line">        [-<span class="number">0.9644</span>,  <span class="number">1.0131</span>, -<span class="number">0.6549</span>, -<span class="number">1.4279</span>],</span><br><span class="line">        [-<span class="number">0.2951</span>, -<span class="number">1.3350</span>, -<span class="number">0.7694</span>,  <span class="number">0.5600</span>],</span><br><span class="line">        [ <span class="number">1.0842</span>, -<span class="number">0.9580</span>,  <span class="number">0.3623</span>,  <span class="number">0.2343</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mean(a, <span class="number">1</span>)</span><br><span class="line">tensor([-<span class="number">0.0163</span>, -<span class="number">0.5085</span>, -<span class="number">0.4599</span>,  <span class="number">0.1807</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.mean(a, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">tensor([[-<span class="number">0.0163</span>],</span><br><span class="line">        [-<span class="number">0.5085</span>],</span><br><span class="line">        [-<span class="number">0.4599</span>],</span><br><span class="line">        [ <span class="number">0.1807</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回张量中所有非NaN元素的均值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([[torch.nan, <span class="number">1</span>, <span class="number">2</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.mean()</span><br><span class="line">tensor(nan)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.nanmean()</span><br><span class="line">tensor(<span class="number">1.8000</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.mean(dim=<span class="number">0</span>)</span><br><span class="line">tensor([   nan, <span class="number">1.5000</span>, <span class="number">2.5000</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.nanmean(dim=<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">1.0000</span>, <span class="number">1.5000</span>, <span class="number">2.5000</span>])</span><br><span class="line"><span class="comment"># If all elements in the reduced dimensions are NaN then the result is NaN</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([torch.nan]).nanmean()</span><br><span class="line">tensor(nan)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>median</code>: 返回张量中所有元素的中值</p>
<p><code>nanmedian</code>: 返回张量中所有非NaN元素的中值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回张量中所有元素的中值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">0.2505</span>, -<span class="number">0.3982</span>, -<span class="number">0.9948</span>,  <span class="number">0.3518</span>, -<span class="number">1.3131</span>],</span><br><span class="line">        [ <span class="number">0.3180</span>, -<span class="number">0.6993</span>,  <span class="number">1.0436</span>,  <span class="number">0.0438</span>,  <span class="number">0.2270</span>],</span><br><span class="line">        [-<span class="number">0.2751</span>,  <span class="number">0.7303</span>,  <span class="number">0.2192</span>,  <span class="number">0.3321</span>,  <span class="number">0.2488</span>],</span><br><span class="line">        [ <span class="number">1.0778</span>, -<span class="number">1.9510</span>,  <span class="number">0.7048</span>,  <span class="number">0.4742</span>, -<span class="number">0.7125</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.median(a, <span class="number">1</span>)</span><br><span class="line">torch.return_types.median(values=tensor([-<span class="number">0.3982</span>,  <span class="number">0.2270</span>,  <span class="number">0.2488</span>,  <span class="number">0.4742</span>]), indices=tensor([<span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>, <span class="number">3</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回张量中所有非NaN元素的中值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([[<span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>], [<span class="built_in">float</span>(<span class="string">&#x27;nan&#x27;</span>), <span class="number">1</span>, <span class="built_in">float</span>(<span class="string">&#x27;nan&#x27;</span>)]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[<span class="number">2.</span>, <span class="number">3.</span>, <span class="number">1.</span>],</span><br><span class="line">        [nan, <span class="number">1.</span>, nan]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.median(<span class="number">0</span>)</span><br><span class="line">torch.return_types.median(values=tensor([nan, <span class="number">1.</span>, nan]), indices=tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a.nanmedian(<span class="number">0</span>)</span><br><span class="line">torch.return_types.nanmedian(values=tensor([<span class="number">2.</span>, <span class="number">1.</span>, <span class="number">1.</span>]), indices=tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]))</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>mode</code>: 返回张量在指定维度上每行的众数及其索引</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.randint(10, (5,))</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([6, 5, 1, 0, 2])</span><br><span class="line">&gt;&gt;&gt; b = a + (torch.randn(50, 1) * 5).long()</span><br><span class="line">&gt;&gt;&gt; torch.mode(b, 0)</span><br><span class="line">torch.return_types.mode(values=tensor([6, 5, 1, 0, 2]), indices=tensor([2, 2, 2, 2, 2]))</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>sum</code>: 返回张量中所有元素的总和</p>
<p><code>nansum</code>: 返回张量中所有元素的和，将NaN视为零</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回张量中所有元素的总和</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">0.0569</span>, -<span class="number">0.2475</span>,  <span class="number">0.0737</span>, -<span class="number">0.3429</span>],</span><br><span class="line">        [-<span class="number">0.2993</span>,  <span class="number">0.9138</span>,  <span class="number">0.9337</span>, -<span class="number">1.6864</span>],</span><br><span class="line">        [ <span class="number">0.1132</span>,  <span class="number">0.7892</span>, -<span class="number">0.1003</span>,  <span class="number">0.5688</span>],</span><br><span class="line">        [ <span class="number">0.3637</span>, -<span class="number">0.9906</span>, -<span class="number">0.4752</span>, -<span class="number">1.5197</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.<span class="built_in">sum</span>(a, <span class="number">1</span>)</span><br><span class="line">tensor([-<span class="number">0.4598</span>, -<span class="number">0.1381</span>,  <span class="number">1.3708</span>, -<span class="number">2.6217</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = torch.arange(<span class="number">4</span> * <span class="number">5</span> * <span class="number">6</span>).view(<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.<span class="built_in">sum</span>(b, dim=(<span class="number">2</span>, <span class="number">1</span>))</span><br><span class="line">tensor([  <span class="number">435.</span>,  <span class="number">1335.</span>,  <span class="number">2235.</span>,  <span class="number">3135.</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 返回张量中所有元素的和，将NaN视为零</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.nansum(torch.tensor([<span class="number">1.</span>, <span class="built_in">float</span>(<span class="string">&quot;nan&quot;</span>)]))</span><br><span class="line"><span class="number">1.0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3.</span>, <span class="built_in">float</span>(<span class="string">&quot;nan&quot;</span>)]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.nansum(a)</span><br><span class="line">tensor(<span class="number">6.</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.nansum(a, dim=<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">4.</span>, <span class="number">2.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.nansum(a, dim=<span class="number">1</span>)</span><br><span class="line">tensor([<span class="number">3.</span>, <span class="number">3.</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>prod</code>: 返回张量中所有元素的乘积</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">0.5261</span>, -<span class="number">0.3837</span>],</span><br><span class="line">        [ <span class="number">1.1857</span>, -<span class="number">0.2498</span>],</span><br><span class="line">        [-<span class="number">1.1646</span>,  <span class="number">0.0705</span>],</span><br><span class="line">        [ <span class="number">1.1131</span>, -<span class="number">1.0629</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.prod(a, dim=<span class="number">1</span>)</span><br><span class="line">tensor([-<span class="number">0.2018</span>, -<span class="number">0.2962</span>, -<span class="number">0.0821</span>, -<span class="number">1.1831</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>quantile</code>: 计算张量在指定维度上每行的第q个百分位数</p>
<p><code>nanquantile</code>: 类似于quantile，但在计算百分位数时忽略NaN值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算张量在指定维度上每行的第q个百分位数</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="number">0.0795</span>, -<span class="number">1.2117</span>,  <span class="number">0.9765</span>],</span><br><span class="line">        [ <span class="number">1.1707</span>,  <span class="number">0.6706</span>,  <span class="number">0.4884</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>q = torch.tensor([<span class="number">0.25</span>, <span class="number">0.5</span>, <span class="number">0.75</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.quantile(a, q, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">tensor([[[-<span class="number">0.5661</span>],</span><br><span class="line">        [ <span class="number">0.5795</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.0795</span>],</span><br><span class="line">        [ <span class="number">0.6706</span>]],</span><br><span class="line"></span><br><span class="line">        [[ <span class="number">0.5280</span>],</span><br><span class="line">        [ <span class="number">0.9206</span>]]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.quantile(a, q, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>).shape</span><br><span class="line">torch.Size([<span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.arange(<span class="number">4.</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.quantile(a, <span class="number">0.6</span>, interpolation=<span class="string">&#x27;linear&#x27;</span>)</span><br><span class="line">tensor(<span class="number">1.8000</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.quantile(a, <span class="number">0.6</span>, interpolation=<span class="string">&#x27;lower&#x27;</span>)</span><br><span class="line">tensor(<span class="number">1.</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.quantile(a, <span class="number">0.6</span>, interpolation=<span class="string">&#x27;higher&#x27;</span>)</span><br><span class="line">tensor(<span class="number">2.</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.quantile(a, <span class="number">0.6</span>, interpolation=<span class="string">&#x27;midpoint&#x27;</span>)</span><br><span class="line">tensor(<span class="number">1.5000</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.quantile(a, <span class="number">0.6</span>, interpolation=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">tensor(<span class="number">2.</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.quantile(a, <span class="number">0.4</span>, interpolation=<span class="string">&#x27;nearest&#x27;</span>)</span><br><span class="line">tensor(<span class="number">1.</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 类似于quantile，但在计算百分位数时忽略NaN值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([<span class="built_in">float</span>(<span class="string">&#x27;nan&#x27;</span>), <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t.quantile(<span class="number">0.5</span>)</span><br><span class="line">tensor(nan)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t.nanquantile(<span class="number">0.5</span>)</span><br><span class="line">tensor(<span class="number">1.5000</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = torch.tensor([[<span class="built_in">float</span>(<span class="string">&#x27;nan&#x27;</span>), <span class="built_in">float</span>(<span class="string">&#x27;nan&#x27;</span>)], [<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">tensor([[nan, nan],</span><br><span class="line">        [<span class="number">1.</span>, <span class="number">2.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t.nanquantile(<span class="number">0.5</span>, dim=<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">1.</span>, <span class="number">2.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t.nanquantile(<span class="number">0.5</span>, dim=<span class="number">1</span>)</span><br><span class="line">tensor([   nan, <span class="number">1.5000</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="距离范数"><a href="#距离范数" class="headerlink" title="距离范数"></a>距离范数</h2><ul>
<li><p><code>norm</code>: 返回给定张量的矩阵范数或向量范数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">import</span> torch</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.arange(<span class="number">9</span>, dtype= torch.<span class="built_in">float</span>) - <span class="number">4</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>b = a.reshape((<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.norm(a)</span><br><span class="line">tensor(<span class="number">7.7460</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.norm(b)</span><br><span class="line">tensor(<span class="number">7.7460</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.norm(a, <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line">tensor(<span class="number">4.</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.norm(b, <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>))</span><br><span class="line">tensor(<span class="number">4.</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>c = torch.tensor([[ <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [-<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>]] , dtype=torch.<span class="built_in">float</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.norm(c, dim=<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">1.4142</span>, <span class="number">2.2361</span>, <span class="number">5.0000</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.norm(c, dim=<span class="number">1</span>)</span><br><span class="line">tensor([<span class="number">3.7417</span>, <span class="number">4.2426</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.norm(c, p=<span class="number">1</span>, dim=<span class="number">1</span>)</span><br><span class="line">tensor([<span class="number">6.</span>, <span class="number">6.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>d = torch.arange(<span class="number">8</span>, dtype=torch.<span class="built_in">float</span>).reshape(<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.norm(d, dim=(<span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">tensor([ <span class="number">3.7417</span>, <span class="number">11.2250</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.norm(d[<span class="number">0</span>, :, :]), torch.norm(d[<span class="number">1</span>, :, :])</span><br><span class="line">(tensor(<span class="number">3.7417</span>), tensor(<span class="number">11.2250</span>))</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>dist</code>: 返回两个张量差的p-范数</p>
<script type="math/tex; mode=display">
||x||_p = (\sum _{i=1}^{n}{|x_i|^p})^{\frac {1}{p}}</script><p>p-范数（p-norm）是向量空间中的一种度量，它用于衡量向量的大小或长度</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([-<span class="number">1.5393</span>, -<span class="number">0.8675</span>,  <span class="number">0.5916</span>,  <span class="number">1.6321</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.randn(<span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y</span><br><span class="line">tensor([ <span class="number">0.0967</span>, -<span class="number">1.0511</span>,  <span class="number">0.6295</span>,  <span class="number">0.8360</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.dist(x, other=y, p=<span class="number">3.5</span>)</span><br><span class="line">tensor(<span class="number">1.6727</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.dist(x, y, <span class="number">3</span>)</span><br><span class="line">tensor(<span class="number">1.6973</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.dist(x, y, <span class="number">0</span>)</span><br><span class="line">tensor(<span class="number">4.</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.dist(x, y, <span class="number">1</span>)</span><br><span class="line">tensor(<span class="number">2.6537</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="均值与方差"><a href="#均值与方差" class="headerlink" title="均值与方差"></a>均值与方差</h2><ul>
<li><p><code>std</code>: 计算指定维度上的标准差</p>
<p><code>var</code>: 计算指定维度上的方差</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算指定维度上的标准差</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor(</span><br><span class="line"><span class="meta">... </span>    [[ <span class="number">0.2035</span>,  <span class="number">1.2959</span>,  <span class="number">1.8101</span>, -<span class="number">0.4644</span>],</span><br><span class="line"><span class="meta">... </span>     [ <span class="number">1.5027</span>, -<span class="number">0.3270</span>,  <span class="number">0.5905</span>,  <span class="number">0.6538</span>],</span><br><span class="line"><span class="meta">... </span>     [-<span class="number">1.5745</span>,  <span class="number">1.3330</span>, -<span class="number">0.5596</span>, -<span class="number">0.6548</span>],</span><br><span class="line"><span class="meta">... </span>     [ <span class="number">0.1264</span>, -<span class="number">0.5080</span>,  <span class="number">1.6420</span>,  <span class="number">0.1992</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.std(a, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">tensor([[<span class="number">1.0311</span>],</span><br><span class="line">        [<span class="number">0.7477</span>],</span><br><span class="line">        [<span class="number">1.2204</span>],</span><br><span class="line">        [<span class="number">0.9087</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算指定维度上的方差</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor(</span><br><span class="line"><span class="meta">... </span>    [[ <span class="number">0.2035</span>,  <span class="number">1.2959</span>,  <span class="number">1.8101</span>, -<span class="number">0.4644</span>],</span><br><span class="line"><span class="meta">... </span>     [ <span class="number">1.5027</span>, -<span class="number">0.3270</span>,  <span class="number">0.5905</span>,  <span class="number">0.6538</span>],</span><br><span class="line"><span class="meta">... </span>     [-<span class="number">1.5745</span>,  <span class="number">1.3330</span>, -<span class="number">0.5596</span>, -<span class="number">0.6548</span>],</span><br><span class="line"><span class="meta">... </span>     [ <span class="number">0.1264</span>, -<span class="number">0.5080</span>,  <span class="number">1.6420</span>,  <span class="number">0.1992</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.std_mean(a, dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">(tensor([[<span class="number">1.2620</span>, <span class="number">1.0028</span>, <span class="number">1.0957</span>, <span class="number">0.6038</span>]]),</span><br><span class="line"> tensor([[ <span class="number">0.0645</span>,  <span class="number">0.4485</span>,  <span class="number">0.8707</span>, -<span class="number">0.0665</span>]]))</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>std_mean</code>: 同时计算指定维度上的标准差和均值</p>
<p><code>var_mean</code>: 同时计算指定维度上的方差和均值</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 同时计算指定维度上的标准差和均值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor(</span><br><span class="line"><span class="meta">... </span>    [[ <span class="number">0.2035</span>,  <span class="number">1.2959</span>,  <span class="number">1.8101</span>, -<span class="number">0.4644</span>],</span><br><span class="line"><span class="meta">... </span>     [ <span class="number">1.5027</span>, -<span class="number">0.3270</span>,  <span class="number">0.5905</span>,  <span class="number">0.6538</span>],</span><br><span class="line"><span class="meta">... </span>     [-<span class="number">1.5745</span>,  <span class="number">1.3330</span>, -<span class="number">0.5596</span>, -<span class="number">0.6548</span>],</span><br><span class="line"><span class="meta">... </span>     [ <span class="number">0.1264</span>, -<span class="number">0.5080</span>,  <span class="number">1.6420</span>,  <span class="number">0.1992</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.var(a, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">tensor([[<span class="number">1.0631</span>],</span><br><span class="line">        [<span class="number">0.5590</span>],</span><br><span class="line">        [<span class="number">1.4893</span>],</span><br><span class="line">        [<span class="number">0.8258</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 同时计算指定维度上的方差和均值</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.tensor(</span><br><span class="line"><span class="meta">... </span>    [[ <span class="number">0.2035</span>,  <span class="number">1.2959</span>,  <span class="number">1.8101</span>, -<span class="number">0.4644</span>],</span><br><span class="line"><span class="meta">... </span>     [ <span class="number">1.5027</span>, -<span class="number">0.3270</span>,  <span class="number">0.5905</span>,  <span class="number">0.6538</span>],</span><br><span class="line"><span class="meta">... </span>     [-<span class="number">1.5745</span>,  <span class="number">1.3330</span>, -<span class="number">0.5596</span>, -<span class="number">0.6548</span>],</span><br><span class="line"><span class="meta">... </span>     [ <span class="number">0.1264</span>, -<span class="number">0.5080</span>,  <span class="number">1.6420</span>,  <span class="number">0.1992</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.var_mean(a, dim=<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">(tensor([[<span class="number">1.5926</span>, <span class="number">1.0056</span>, <span class="number">1.2005</span>, <span class="number">0.3646</span>]]),</span><br><span class="line"> tensor([[ <span class="number">0.0645</span>,  <span class="number">0.4485</span>,  <span class="number">0.8707</span>, -<span class="number">0.0665</span>]]))</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="逻辑操作"><a href="#逻辑操作" class="headerlink" title="逻辑操作"></a>逻辑操作</h2><ul>
<li><p><code>all</code>: 检查张量中的所有元素是否都满足某个条件(例如是否都为True)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; a = torch.rand(4, 2).bool()</span><br><span class="line">&gt;&gt;&gt; a</span><br><span class="line">tensor([[True, True],</span><br><span class="line">        [True, False],</span><br><span class="line">        [True, True],</span><br><span class="line">        [True, True]], dtype=torch.bool)</span><br><span class="line">&gt;&gt;&gt; torch.all(a, dim=1)</span><br><span class="line">tensor([ True, False,  True,  True], dtype=torch.bool)</span><br><span class="line">&gt;&gt;&gt; torch.all(a, dim=0)</span><br><span class="line">tensor([ True, False], dtype=torch.bool)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>any</code>: 检查张量中是否有任何元素满足某个条件(例如是否为True)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">2</span>) &lt; <span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[ <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">        [ <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.<span class="built_in">any</span>(a, <span class="number">1</span>)</span><br><span class="line">tensor([ <span class="literal">True</span>,  <span class="literal">True</span>,  <span class="literal">True</span>, <span class="literal">False</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.<span class="built_in">any</span>(a, <span class="number">0</span>)</span><br><span class="line">tensor([<span class="literal">True</span>, <span class="literal">True</span>])</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h2 id="特殊操作"><a href="#特殊操作" class="headerlink" title="特殊操作"></a>特殊操作</h2><ul>
<li><p><code>unique</code>: 返回张量中的唯一元素</p>
<p><code>unique_consecutive</code>: 在每组连续相等的元素中只保留第一个元素</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回张量中的唯一元素</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = torch.unique(torch.tensor([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.long))</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, inverse_indices = torch.unique(</span><br><span class="line"><span class="meta">... </span>    torch.tensor([<span class="number">1</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.long), <span class="built_in">sorted</span>=<span class="literal">True</span>, return_inverse=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>inverse_indices</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, inverse_indices = torch.unique(</span><br><span class="line"><span class="meta">... </span>    torch.tensor([[<span class="number">1</span>, <span class="number">3</span>], [<span class="number">2</span>, <span class="number">3</span>]], dtype=torch.long), <span class="built_in">sorted</span>=<span class="literal">True</span>, return_inverse=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>inverse_indices</span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在每组连续相等的元素中只保留第一个元素</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.tensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output = torch.unique_consecutive(x)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, inverse_indices = torch.unique_consecutive(x, return_inverse=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>inverse_indices</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output, counts = torch.unique_consecutive(x, return_counts=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>output</span><br><span class="line">tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>counts</span><br><span class="line">tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>count_nonzero</code>: 计算给定维度上非零元素的数量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.zeros(<span class="number">3</span>,<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x[torch.randn(<span class="number">3</span>,<span class="number">3</span>) &gt; <span class="number">0.5</span>] = <span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x</span><br><span class="line">tensor([[<span class="number">0.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">0.</span>],</span><br><span class="line">        [<span class="number">0.</span>, <span class="number">0.</span>, <span class="number">1.</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.count_nonzero(x)</span><br><span class="line">tensor(<span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.count_nonzero(x, dim=<span class="number">0</span>)</span><br><span class="line">tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>])</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>logsumexp</code>: 返回在指定维度上，张量所有元素指数的对数和</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.logsumexp(a, <span class="number">1</span>)</span><br><span class="line">tensor([<span class="number">1.4907</span>, <span class="number">1.0593</span>, <span class="number">1.5696</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.dist(torch.logsumexp(a, <span class="number">1</span>), torch.log(torch.<span class="built_in">sum</span>(torch.exp(a), <span class="number">1</span>)))</span><br><span class="line">tensor(<span class="number">1.6859e-07</span>)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="比较操作"><a href="#比较操作" class="headerlink" title="比较操作"></a>比较操作</h1><blockquote>
<p><strong>torch.eq</strong>: torch.eq(input, other, out=None) → Tensor</p>
<p>比较元素相等性。第二个参数可为一个数或与第一个参数同类型形状的张量</p>
<p><strong>torch.ge</strong>: torch.ge(input, other, out=None) → Tensor</p>
<p>逐元素比较<code>input</code>和<code>other</code>，即是否 input&gt;=other</p>
<p><strong>torch.gt</strong>: torch.gt(input, other, out=None) → Tensor</p>
<p>逐元素比较<code>input</code>和<code>other</code> ， 即是否input&gt;otherinput&gt;other </p>
<p><strong>torch.le</strong>: torch.le(input, other, out=None) → Tensor</p>
<p>逐元素比较<code>input</code>和<code>other</code> ， 即是否input&lt;=other</p>
<p><strong>torch.lt</strong>: torch.lt(input, other, out=None) → Tensor</p>
<p>逐元素比较<code>input</code>和<code>other</code> ， 即是否 input&lt;other</p>
<p><strong>torch.ne</strong>: torch.ne(input, other, out=None) → Tensor</p>
<p>逐元素比较<code>input</code>和<code>other</code>，即是否 input!=other</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">torch.eq(torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.Tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">4</span>]]))</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[ <span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">        [<span class="literal">False</span>,  <span class="literal">True</span>]])</span><br><span class="line"></span><br><span class="line">torch.ge(torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.Tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">4</span>]]))</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[ <span class="literal">True</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>,  <span class="literal">True</span>]])</span><br><span class="line"></span><br><span class="line">torch.gt(torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.Tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">4</span>]]))</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">tensor([[<span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">        [<span class="literal">False</span>, <span class="literal">False</span>]])</span><br><span class="line"></span><br><span class="line">torch.le(torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.Tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">4</span>]]))</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">tensor([[ <span class="literal">True</span>, <span class="literal">False</span>],</span><br><span class="line">        [ <span class="literal">True</span>,  <span class="literal">True</span>]])</span><br><span class="line"></span><br><span class="line">torch.lt(torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.Tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">4</span>]]))</span><br><span class="line">Out[<span class="number">4</span>]: </span><br><span class="line">tensor([[<span class="literal">False</span>, <span class="literal">False</span>],</span><br><span class="line">        [ <span class="literal">True</span>, <span class="literal">False</span>]])</span><br><span class="line"></span><br><span class="line">torch.ne(torch.Tensor([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">4</span>]]), torch.Tensor([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">4</span>, <span class="number">4</span>]]))</span><br><span class="line">Out[<span class="number">5</span>]: </span><br><span class="line">tensor([[<span class="literal">False</span>,  <span class="literal">True</span>],</span><br><span class="line">        [ <span class="literal">True</span>, <span class="literal">False</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.equal</strong>: torch.equal(tensor1, tensor2) → bool</p>
<p>如果两个张量有相同的形状和元素值，则返回<code>True</code> ，否则 <code>False</code></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.equal(torch.Tensor([<span class="number">1</span>, <span class="number">2</span>]), torch.Tensor([<span class="number">1</span>, <span class="number">2</span>]))</span><br><span class="line">Out[<span class="number">0</span>]: <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.kthvalue</strong>: torch.kthvalue(input, k, dim=None, out=None) -&gt; (Tensor, LongTensor)</p>
<p>取输入张量<code>input</code>指定维上第k 个最小值。如果不指定<code>dim</code>，则默认为<code>input</code>的最后一维</p>
<p><strong>torch.topk</strong>: 沿给定<code>dim</code>维度返回输入张量<code>input</code>中 <code>k</code> 个最大值。 如果不指定<code>dim</code>，则默认为<code>input</code>的最后一维。 如果为<code>largest</code>为 <code>False</code> ，则返回最小的 <code>k</code> 个值</p>
<p>返回一个元组 <em>(values,indices)</em>，其中<code>indices</code>是原始输入张量<code>input</code>中测元素下标。 如果设定布尔值<code>sorted</code> 为<em>True</em>，将会确保返回的 <code>k</code> 个值被排序</p>
<p>torch.topk(input, k, dim=None, largest=True, sorted=True, out=None) -&gt; (Tensor, LongTensor)</p>
<p>参数:</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>k (int) – “top-k”中的<code>k</code></li>
<li>dim (int, optional) – 排序的维</li>
<li>largest (bool, optional) – 布尔值，控制返回最大或最小值</li>
<li>sorted (bool, optional) – 布尔值，控制返回值是否排序</li>
<li>out (tuple, optional) – 可选输出张量 (Tensor, LongTensor) output buffers</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">6</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.kthvalue</span></span><br><span class="line">torch.kthvalue(x, <span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]:</span><br><span class="line">torch.return_types.kthvalue(</span><br><span class="line">values=tensor(<span class="number">4</span>),</span><br><span class="line">indices=tensor(<span class="number">3</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># torch.topk</span></span><br><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">6</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br><span class="line"></span><br><span class="line">torch.topk(x, <span class="number">3</span>)</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">torch.return_types.topk(</span><br><span class="line">values=tensor([<span class="number">5</span>, <span class="number">4</span>, <span class="number">3</span>]),</span><br><span class="line">indices=tensor([<span class="number">4</span>, <span class="number">3</span>, <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">torch.topk(x, <span class="number">3</span>, <span class="number">0</span>, largest=<span class="literal">False</span>)</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">torch.return_types.topk(</span><br><span class="line">values=tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]),</span><br><span class="line">indices=tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>]))</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.max</strong>: 返回输入张量所有元素的最大值</p>
<p>torch.max()</p>
<p>返回输入张量给定维度上每行的最大值，并同时返回每个最大值的位置索引</p>
<p>torch.max(input, dim, max=None, max_indices=None) -&gt; (Tensor, LongTensor)</p>
<p><code>input</code>中<strong>逐元素</strong>与<code>other</code>相应位置的元素对比，返回最大值到输出张量</p>
<p>torch.max(input, other, out=None) → Tensor</p>
<p><strong>torch.min</strong>: 返回输入张量所有元素的最小值</p>
<p>torch.min(input) → float</p>
<p>返回输入张量给定维度上每行的最小值，并同时返回每个最小值的位置索引</p>
<p>torch.min(input, dim, min=None, min_indices=None) -&gt; (Tensor, LongTensor)</p>
<p><code>input</code>中<strong>逐元素</strong>与<code>other</code>相应位置的元素对比，返回最小值到输出张量</p>
<p>torch.min(input, other, out=None) → Tensor</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[-<span class="number">0.1204</span>, -<span class="number">0.5016</span>],</span><br><span class="line">        [ <span class="number">1.2717</span>,  <span class="number">0.7351</span>]])</span><br><span class="line">b = torch.randn(<span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[-<span class="number">1.4497</span>,  <span class="number">0.7534</span>],</span><br><span class="line">        [ <span class="number">0.5994</span>, -<span class="number">0.1490</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大值</span></span><br><span class="line">torch.<span class="built_in">max</span>(torch.arange(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">Out[<span class="number">2</span>]: tensor(<span class="number">4</span>)</span><br><span class="line">torch.<span class="built_in">max</span>(a, <span class="number">1</span>)</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">torch.return_types.<span class="built_in">max</span>(</span><br><span class="line">values=tensor([-<span class="number">0.1204</span>,  <span class="number">1.2717</span>]),</span><br><span class="line">indices=tensor([<span class="number">0</span>, <span class="number">0</span>]))</span><br><span class="line">torch.<span class="built_in">max</span>(a, b)</span><br><span class="line">Out[<span class="number">4</span>]: </span><br><span class="line">tensor([[-<span class="number">0.1204</span>,  <span class="number">0.7534</span>],</span><br><span class="line">        [ <span class="number">1.2717</span>,  <span class="number">0.7351</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最小值</span></span><br><span class="line">torch.<span class="built_in">min</span>(torch.arange(<span class="number">1</span>, <span class="number">5</span>))</span><br><span class="line">Out[<span class="number">5</span>]: tensor(<span class="number">1</span>)</span><br><span class="line">torch.<span class="built_in">min</span>(a, <span class="number">1</span>)</span><br><span class="line">Out[<span class="number">6</span>]: </span><br><span class="line">torch.return_types.<span class="built_in">min</span>(</span><br><span class="line">values=tensor([-<span class="number">0.5016</span>,  <span class="number">0.7351</span>]),</span><br><span class="line">indices=tensor([<span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">torch.<span class="built_in">min</span>(a, b)</span><br><span class="line">Out[<span class="number">7</span>]: </span><br><span class="line">tensor([[-<span class="number">1.4497</span>, -<span class="number">0.5016</span>],</span><br><span class="line">        [ <span class="number">0.5994</span>, -<span class="number">0.1490</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.sort</strong>: torch.sort(input, dim=None, descending=False, out=None) -&gt; (Tensor, LongTensor)</p>
<p>对输入张量<code>input</code>沿着指定维按升序排序。如果不给定<code>dim</code>，则默认为输入的最后一维。如果指定参数<code>descending</code>为<code>True</code>，则按降序排序</p>
<p>返回元组 (sorted_tensor, sorted_indices) ， <code>sorted_indices</code> 为原始输入中的下标</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[-<span class="number">2.3460</span>,  <span class="number">1.3734</span>,  <span class="number">1.1444</span>, -<span class="number">0.4736</span>],</span><br><span class="line">        [-<span class="number">1.1785</span>,  <span class="number">0.8436</span>, -<span class="number">1.4403</span>, -<span class="number">0.1073</span>],</span><br><span class="line">        [-<span class="number">0.1198</span>,  <span class="number">0.7067</span>, -<span class="number">0.0734</span>, -<span class="number">1.6181</span>]])</span><br><span class="line"></span><br><span class="line"><span class="built_in">sorted</span>, indices = torch.sort(x)</span><br><span class="line"><span class="built_in">sorted</span>, indices</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">(tensor([[-<span class="number">2.3460</span>, -<span class="number">0.4736</span>,  <span class="number">1.1444</span>,  <span class="number">1.3734</span>],</span><br><span class="line">         [-<span class="number">1.4403</span>, -<span class="number">1.1785</span>, -<span class="number">0.1073</span>,  <span class="number">0.8436</span>],</span><br><span class="line">         [-<span class="number">1.6181</span>, -<span class="number">0.1198</span>, -<span class="number">0.0734</span>,  <span class="number">0.7067</span>]]),</span><br><span class="line"> tensor([[<span class="number">0</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>, <span class="number">1</span>],</span><br><span class="line">         [<span class="number">3</span>, <span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>]]))</span><br></pre></td></tr></table></figure>
<h1 id="其它操作"><a href="#其它操作" class="headerlink" title="其它操作"></a>其它操作</h1><blockquote>
<p><strong>torch.cross</strong>: 返回沿着维度<code>dim</code>上，两个张量<code>input</code>和<code>other</code>的向量积（叉积）。 <code>input</code>和<code>other</code> 必须有相同的形状，且指定的<code>dim</code>维上size必须为<code>3</code></p>
<p>如果不指定<code>dim</code>，则默认为第一个尺度为<code>3</code>的维</p>
<p>torch.cross(input, other, dim=-1, out=None) → Tensor</p>
</blockquote>
<script type="math/tex; mode=display">
\left[\begin{array}{l}a_{1} \\ a_{2} \\ a_{3}\end{array}\right] \times\left[\begin{array}{l}b_{1} \\ b_{2} \\ b_{3}\end{array}\right]=\left[\begin{array}{c}a_{2} b_{3}-a_{3} b_{2} \\ a_{3} b_{1}-a_{1} b_{3} \\ a_{1} b_{2}-a_{2} b_{1}\end{array}\right]</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randint(<span class="number">1</span>, <span class="number">6</span>, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">2</span>, <span class="number">3</span>]])</span><br><span class="line">b = torch.randint(<span class="number">1</span>, <span class="number">6</span>, (<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">3</span>, <span class="number">4</span>, <span class="number">2</span>]])</span><br><span class="line"></span><br><span class="line">torch.cross(a, a)</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br><span class="line"></span><br><span class="line">torch.cross(a, b)</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">tensor([[ <span class="number">3</span>, -<span class="number">5</span>,  <span class="number">1</span>],</span><br><span class="line">        [-<span class="number">8</span>,  <span class="number">1</span>, <span class="number">10</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.diag</strong>: torch.diag(input, diagonal=0, out=None) → Tensor</p>
<ul>
<li>如果输入是一个向量(1D 张量)，则返回一个以<code>input</code>为对角线元素的2D方阵</li>
<li>如果输入是一个矩阵(2D 张量)，则返回一个包含<code>input</code>对角线元素的1D张量</li>
</ul>
<p>参数<code>diagonal</code>指定对角线:</p>
<ul>
<li><code>diagonal</code> = 0, 主对角线</li>
<li><code>diagonal</code> &gt; 0, 主对角线之上</li>
<li><code>diagonal</code> &lt; 0, 主对角线之下</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 如果输入是一个向量(1D 张量)，则返回一个以`input`为对角线元素的2D方阵</span></span><br><span class="line">a = torch.randn(<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([-<span class="number">0.3509</span>,  <span class="number">0.6176</span>, -<span class="number">1.4976</span>])</span><br><span class="line">torch.diag(a)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[-<span class="number">0.3509</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.0000</span>,  <span class="number">0.6176</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>, -<span class="number">1.4976</span>]])</span><br><span class="line">torch.diag(a, <span class="number">1</span>)</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">tensor([[ <span class="number">0.0000</span>, -<span class="number">0.3509</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.6176</span>,  <span class="number">0.0000</span>],</span><br><span class="line">        [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>, -<span class="number">1.4976</span>],</span><br><span class="line">        [ <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>,  <span class="number">0.0000</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果输入是一个矩阵(2D 张量)，则返回一个包含`input`对角线元素的1D张量</span></span><br><span class="line"><span class="comment"># 取得给定矩阵第k个对角线:</span></span><br><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">Out[<span class="number">3</span>]: </span><br><span class="line">tensor([[ <span class="number">0.8224</span>,  <span class="number">0.7792</span>,  <span class="number">0.2605</span>],</span><br><span class="line">        [-<span class="number">0.8646</span>,  <span class="number">0.2568</span>, -<span class="number">0.8189</span>],</span><br><span class="line">        [ <span class="number">1.1693</span>,  <span class="number">0.8108</span>, -<span class="number">1.9662</span>]])</span><br><span class="line">torch.diag(a, <span class="number">0</span>)</span><br><span class="line">Out[<span class="number">4</span>]: tensor([ <span class="number">0.8224</span>,  <span class="number">0.2568</span>, -<span class="number">1.9662</span>])</span><br><span class="line">torch.diag(a, <span class="number">1</span>)</span><br><span class="line">Out[<span class="number">5</span>]: tensor([ <span class="number">0.7792</span>, -<span class="number">0.8189</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.histc</strong>: torch.histc(input, bins=100, min=0, max=0, out=None) → Tensor</p>
<p>计算输入张量的直方图。以<code>min</code>和<code>max</code>为range边界，将其均分成<code>bins</code>个直条，然后将排序好的数据划分到各个直条(bins)中</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>bins (int) – 直方图 bins(直条)的个数(默认100个)</li>
<li>min (int) – range的下边界(包含)</li>
<li>max (int) – range的上边界(包含)</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">torch.histc(torch.FloatTensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]), bins=<span class="number">4</span>, <span class="built_in">min</span>=<span class="number">0</span>, <span class="built_in">max</span>=<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">0.</span>, <span class="number">2.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br><span class="line"></span><br><span class="line">torch.histc(torch.FloatTensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>]), bins=<span class="number">4</span>, <span class="built_in">min</span>=<span class="number">0</span>, <span class="built_in">max</span>=<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([<span class="number">0.</span>, <span class="number">3.</span>, <span class="number">1.</span>, <span class="number">0.</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.renorm</strong>: torch.renorm(input, p, dim, maxnorm, out=None) → Tensor</p>
<p>返回一个张量，包含规范化后的各个子张量，使得沿着<code>dim</code>维划分的各子张量的p范数小于<code>maxnorm</code></p>
<p>如果p范数的值小于<code>maxnorm</code>，则当前子张量不需要修改</p>
<p>参数：</p>
<ul>
<li>input (Tensor) – 输入张量</li>
<li>p (float) – 范数的p</li>
<li>dim (int) – 沿着此维切片，得到张量子集</li>
<li>maxnorm (float) – 每个子张量的范数的最大值</li>
<li>out (Tensor, optional) – 结果张量</li>
</ul>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">x[<span class="number">1</span>].fill_(<span class="number">2</span>)</span><br><span class="line">x[<span class="number">2</span>].fill_(<span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">1.</span>, <span class="number">1.</span>, <span class="number">1.</span>],</span><br><span class="line">        [<span class="number">2.</span>, <span class="number">2.</span>, <span class="number">2.</span>],</span><br><span class="line">        [<span class="number">3.</span>, <span class="number">3.</span>, <span class="number">3.</span>]])</span><br><span class="line"></span><br><span class="line">torch.renorm(x, p=<span class="number">1</span>, dim=<span class="number">0</span>, maxnorm=<span class="number">5</span>)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[<span class="number">1.0000</span>, <span class="number">1.0000</span>, <span class="number">1.0000</span>],</span><br><span class="line">        [<span class="number">1.6667</span>, <span class="number">1.6667</span>, <span class="number">1.6667</span>],</span><br><span class="line">        [<span class="number">1.6667</span>, <span class="number">1.6667</span>, <span class="number">1.6667</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.trace</strong>: 返回输入2维矩阵对角线元素的和(迹)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">10</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">torch.trace(x)</span><br><span class="line">Out[<span class="number">1</span>]: tensor(<span class="number">15</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.tril</strong>: torch.tril(input, diagonal=0, out=None) → Tensor</p>
<p>返回一个张量<code>out</code>，包含输入矩阵(2D张量)的下三角部分，<code>out</code>其余部分被设为<code>0</code></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">10</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">torch.tril(x)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">torch.tril(x, diagonal=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.triu</strong>: torch.triu(input, k=0, out=None) → Tensor</p>
<p>返回一个张量，包含输入矩阵(2D张量)的上三角部分，其余部分被设为<code>0</code>。这里所说的上三角部分为矩阵指定对角线<code>diagonal</code>之上的元素。</p>
<p>参数<code>k</code>控制对角线: - <code>k</code> = 0, 主对角线 - <code>k</code> &gt; 0, 主对角线之上 - <code>k</code> &lt; 0, 主对角线之下</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">x = torch.arange(<span class="number">1</span>, <span class="number">10</span>).view(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">torch.triu(x)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">5</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">9</span>]])</span><br><span class="line"></span><br><span class="line">torch.triu(x, diagonal=<span class="number">1</span>)</span><br><span class="line">Out[<span class="number">2</span>]: </span><br><span class="line">tensor([[<span class="number">0</span>, <span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">6</span>],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.cumprod</strong>: torch.cumprod(input, dim, out=None) → Tensor</p>
<p>返回输入沿指定维度的累积，例如，如果输入是一个N 元向量，则结果也是一个N 元向量，<script type="math/tex">y_i= \prod _{i}{x_i}</script></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line"></span><br><span class="line">torch.cumprod(a, dim=<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">1.</span>,  <span class="number">2.</span>,  <span class="number">6.</span>, <span class="number">24.</span>])</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.cumsum</strong>: torch.cumsum(input, dim, out=None) → Tensor</p>
<p>返回输入沿指定维度的累加，例如，如果输入是一个N 元向量，则结果也是一个N 元向量，<script type="math/tex">y_i= \sum _{i}{x_i}</script></p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br><span class="line">Out[<span class="number">0</span>]: tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">3.</span>, <span class="number">4.</span>])</span><br><span class="line"></span><br><span class="line">torch.cumsum(a, dim=<span class="number">0</span>)</span><br><span class="line">Out[<span class="number">1</span>]: tensor([ <span class="number">1.</span>,  <span class="number">3.</span>,  <span class="number">6.</span>, <span class="number">10.</span>])</span><br></pre></td></tr></table></figure>
<h1 id="BLAS和LAPACK操作"><a href="#BLAS和LAPACK操作" class="headerlink" title="BLAS和LAPACK操作"></a>BLAS和LAPACK操作</h1><p>BLAS（Basic Linear Algebra Subprograms）和LAPACK（Linear Algebra Package）是两个广泛使用的数学库，它们提供了一系列的数学运算，这些运算是高性能线性代数计算的基础</p>
<ul>
<li><strong>BLAS</strong> 提供了基本的线性代数运算，它主要关注向量与向量之间（Level 1 BLAS）、矩阵与向量之间（Level 2 BLAS）以及矩阵与矩阵之间（Level 3 BLAS）的运算。BLAS 的这些操作是高度优化的，旨在提供高效率的计算，这对于任何需要大量线性代数计算的程序都是非常重要的。例如，BLAS 提供了矩阵乘法、向量加法、标量与向量的乘法等基础操作</li>
<li><strong>LAPACK</strong> 构建于 BLAS 之上，提供了更复杂的线性代数运算，如求解线性方程组、计算矩阵特征值和特征向量、奇异值分解、LU分解、QR分解等。LAPACK 是为了解决更大规模的线性代数问题而设计的，它能够利用 BLAS 提供的基础操作来实现更高级的数学运算</li>
</ul>
<p>在很多现代的数值计算环境或科学计算库中，例如 NumPy、SciPy、MATLAB 和 R，底层都直接或间接地使用了 BLAS 和 LAPACK 的实现。这些库通常会链接到特定硬件优化版本的 BLAS 和 LAPACK，如 MKL（Intel Math Kernel Library）或 OpenBLAS，以获得更好的性能</p>
<p>BLAS 和 LAPACK 是高性能数值计算领域的标准构件，它们为复杂的线性代数运算提供了强大的支持</p>
<blockquote>
<p>torch.addbmm</p>
<p>torch.addmm</p>
<p>torch.addmv</p>
<p>torch.addr</p>
<p>torch.baddbmm</p>
<p>torch.bmm</p>
<p>torch.btrifact</p>
<p>torch.btrisolve</p>
<p><strong>torch.dot</strong>: 计算两个张量的点乘(内乘),两个张量都为1-D 向量</p>
<p>torch.dot(tensor1, tensor2) → float</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.dot(torch.Tensor([<span class="number">2</span>, <span class="number">3</span>]), torch.Tensor([<span class="number">2</span>, <span class="number">1</span>]))</span><br><span class="line">Out[<span class="number">0</span>]: tensor(<span class="number">7.</span>)</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>torch.linalg.eig</strong>: 计算实方阵<code>a</code> 的特征值和特征向量</p>
<p>torch.linalg.eig(A, * , out=None)</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">A = torch.randn(<span class="number">2</span>, <span class="number">2</span>, dtype=torch.complex128)</span><br><span class="line">Out[<span class="number">0</span>]: </span><br><span class="line">tensor([[-<span class="number">0.2029</span>-<span class="number">0.0673j</span>, -<span class="number">0.5188</span>-<span class="number">0.6723j</span>],</span><br><span class="line">        [-<span class="number">1.1984</span>+<span class="number">0.0585j</span>,  <span class="number">0.5786</span>-<span class="number">0.1849j</span>]], dtype=torch.complex128)</span><br><span class="line"></span><br><span class="line">L, V = torch.linalg.eig(A)</span><br><span class="line">Out[<span class="number">1</span>]: </span><br><span class="line">(tensor([-<span class="number">0.7870</span>-<span class="number">0.5003j</span>,  <span class="number">1.1626</span>+<span class="number">0.2481j</span>], dtype=torch.complex128),</span><br><span class="line"> tensor([[ <span class="number">0.7596</span>+<span class="number">0.0000j</span>, -<span class="number">0.4008</span>-<span class="number">0.3285j</span>],</span><br><span class="line">         [ <span class="number">0.6258</span>-<span class="number">0.1770j</span>,  <span class="number">0.8552</span>+<span class="number">0.0000j</span>]], dtype=torch.complex128))</span><br></pre></td></tr></table></figure>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/narutohyc">narutohyc</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://study.hycbook.com/article/53039.html">https://study.hycbook.com/article/53039.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://study.hycbook.com" target="_blank">兼一书虫</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/pytorch/">pytorch</a></div><div class="post_share"><div class="social-share" data-image="https://pic.hycbook.com/i/hexo/post_cover/蕾姆2.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><link rel="stylesheet" href="/" media="defer" onload="this.media='all'"/><div class="post-reward"><button class="tip-button reward-button"><span class="tip-button__text">打赏</span><div class="coin-wrapper"><div class="coin"><div class="coin__middle"></div><div class="coin__back"></div><div class="coin__front"></div></div></div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic.hycbook.com/i//hexo/qr_codes/hyc_wechat.webp" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/qr_codes/hyc_wechat.webp" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://pic.hycbook.com/i//hexo/qr_codes/hyc_alipay.webp" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/qr_codes/hyc_alipay.webp" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></button></div><audio id="coinAudio" src="https://s1.vika.cn/space/2022/10/29/6db0ad2bccf949f09054b3b206dcc66f?attname=马里奥游戏投币叮当.mp3"></audio><script defer="defer" src="/"></script><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/article/31546.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆10.webp" onerror="onerror=null;src='https://pic.hycbook.com/i/hexo/config_imgs/404.svg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">python常用库学习</div></div></a></div><div class="next-post pull-right"><a href="/article/42221.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆3.webp" onerror="onerror=null;src='https://pic.hycbook.com/i/hexo/config_imgs/404.svg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">数据标注工具</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/article/53040.html" title="pytorch学习_进阶知识"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆5.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-02-20</div><div class="title">pytorch学习_进阶知识</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Twikoo</span><span class="switch-btn"></span><span class="second-comment">Valine</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C"><span class="toc-number">1.</span> <span class="toc-text">基本操作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%88%9B%E5%BB%BA%E6%93%8D%E4%BD%9C"><span class="toc-number">2.</span> <span class="toc-text">创建操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%A0%E9%87%8F%E5%88%9B%E5%BB%BA%E5%87%BD%E6%95%B0"><span class="toc-number">2.1.</span> <span class="toc-text">张量创建函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2%E5%87%BD%E6%95%B0"><span class="toc-number">2.2.</span> <span class="toc-text">数据类型转换函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9D%E5%A7%8B%E5%8C%96%E5%A1%AB%E5%85%85%E5%87%BD%E6%95%B0"><span class="toc-number">2.3.</span> <span class="toc-text">初始化填充函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E5%AE%9A%E5%8C%BA%E9%97%B4%E5%A1%AB%E5%85%85%E5%87%BD%E6%95%B0"><span class="toc-number">2.4.</span> <span class="toc-text">特定区间填充函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E8%BE%85%E5%8A%A9%E5%87%BD%E6%95%B0"><span class="toc-number">2.5.</span> <span class="toc-text">其他辅助函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%87%8F%E5%8C%96%E5%87%BD%E6%95%B0"><span class="toc-number">2.6.</span> <span class="toc-text">量化函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A4%8D%E6%95%B0%E5%92%8C%E5%85%B6%E4%BB%96%E7%89%B9%E6%AE%8A%E7%B1%BB%E5%9E%8B%E5%87%BD%E6%95%B0"><span class="toc-number">2.7.</span> <span class="toc-text">复数和其他特殊类型函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95-%E5%88%87%E7%89%87-%E8%BF%9E%E6%8E%A5-%E6%8D%A2%E4%BD%8D"><span class="toc-number">3.</span> <span class="toc-text">索引|切片|连接|换位</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%B4%A2%E5%BC%95%E5%92%8C%E5%88%87%E7%89%87"><span class="toc-number">3.1.</span> <span class="toc-text">索引和切片</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%90%88%E5%B9%B6%E5%92%8C%E6%8B%BC%E6%8E%A5"><span class="toc-number">3.2.</span> <span class="toc-text">合并和拼接</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%98%E6%8D%A2%E5%92%8C%E9%87%8D%E5%A1%91"><span class="toc-number">3.3.</span> <span class="toc-text">变换和重塑</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%83%E7%B4%A0%E6%B7%BB%E5%8A%A0%E4%B8%8E%E6%9B%BF%E6%8D%A2"><span class="toc-number">3.4.</span> <span class="toc-text">元素添加与替换</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%90%9C%E7%B4%A2%E5%92%8C%E6%9D%A1%E4%BB%B6%E6%93%8D%E4%BD%9C"><span class="toc-number">3.5.</span> <span class="toc-text">搜索和条件操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%A9%E5%B1%95%E4%B8%8E%E9%87%8D%E5%A4%8D%E6%93%8D%E4%BD%9C"><span class="toc-number">3.6.</span> <span class="toc-text">扩展与重复操作</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%8A%BD%E6%A0%B7"><span class="toc-number">4.</span> <span class="toc-text">随机抽样</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E7%A7%8D%E5%AD%90"><span class="toc-number">4.1.</span> <span class="toc-text">随机种子</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E9%87%87%E6%A0%B7%E5%87%BD%E6%95%B0"><span class="toc-number">4.2.</span> <span class="toc-text">随机采样函数</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BA%8F%E5%88%97%E5%8C%96"><span class="toc-number">5.</span> <span class="toc-text">序列化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%B9%B6%E8%A1%8C%E5%8C%96"><span class="toc-number">6.</span> <span class="toc-text">并行化</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%AE%A1%E7%90%86"><span class="toc-number">7.</span> <span class="toc-text">梯度管理</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E6%93%8D%E4%BD%9C"><span class="toc-number">8.</span> <span class="toc-text">数学操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E6%93%8D%E4%BD%9C"><span class="toc-number">8.1.</span> <span class="toc-text">基础操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%89%E8%A7%92%E5%87%BD%E6%95%B0"><span class="toc-number">8.2.</span> <span class="toc-text">三角函数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%8D%E6%93%8D%E4%BD%9C"><span class="toc-number">8.3.</span> <span class="toc-text">位操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%93%8D%E4%BD%9C"><span class="toc-number">8.4.</span> <span class="toc-text">其他操作</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%BD%92%E7%BA%A6%E6%93%8D%E4%BD%9C"><span class="toc-number">9.</span> <span class="toc-text">归约操作</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%9E%81%E5%80%BC%E6%93%8D%E4%BD%9C"><span class="toc-number">9.1.</span> <span class="toc-text">极值操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%BB%9F%E8%AE%A1%E6%93%8D%E4%BD%9C"><span class="toc-number">9.2.</span> <span class="toc-text">统计操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%B7%9D%E7%A6%BB%E8%8C%83%E6%95%B0"><span class="toc-number">9.3.</span> <span class="toc-text">距离范数</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9D%87%E5%80%BC%E4%B8%8E%E6%96%B9%E5%B7%AE"><span class="toc-number">9.4.</span> <span class="toc-text">均值与方差</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E6%93%8D%E4%BD%9C"><span class="toc-number">9.5.</span> <span class="toc-text">逻辑操作</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%89%B9%E6%AE%8A%E6%93%8D%E4%BD%9C"><span class="toc-number">9.6.</span> <span class="toc-text">特殊操作</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%AF%94%E8%BE%83%E6%93%8D%E4%BD%9C"><span class="toc-number">10.</span> <span class="toc-text">比较操作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%85%B6%E5%AE%83%E6%93%8D%E4%BD%9C"><span class="toc-number">11.</span> <span class="toc-text">其它操作</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#BLAS%E5%92%8CLAPACK%E6%93%8D%E4%BD%9C"><span class="toc-number">12.</span> <span class="toc-text">BLAS和LAPACK操作</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://pic.hycbook.com/i/hexo/config_imgs/footer_bg.webp')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2024 By narutohyc</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><p><a target="_blank" href="https://hexo.io/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为Hexo"></a>&nbsp;<a target="_blank" href="https://demo.jerryc.me/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用butterfly"></a>&nbsp;<a target="_blank" href="https://vercel.com/ " rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vervel-brightgreen?style=flat&logo=Vercel" title="本站采用双线部署，默认线路托管于Vercel"></a>&nbsp;<a target="_blank" href="https://github.com/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由Gtihub托管"></a>&nbsp;<a target="_blank" href="https://zixiaoyun.com" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/图床-薄荷图床-green" title="薄荷图床"></a></p><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=35020502000647" rel="external nofollow noreferrer"><img style="position:relative;top:4px" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/config_imgs//备案图标.webp" alt="ICP"/>闽公网安备35020502000647号  </a><a href="https://beian.miit.gov.cn/" rel="external nofollow noreferrer" target="_blank">闽ICP备2023021562号-1</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><!-- button#darkmode(type="button" title=_p('rightside.night_mode_title'))--><!--  i.fas.fa-adjust--><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="fa-solid fa-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="fa-solid fa-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="fa-solid fa-arrow-rotate-right"></i></div><div class="rightMenu-item" id="menu-home"><i class="fa-solid fa-house"></i></div></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:window.open(&quot;https://www.baidu.com/s?wd=&quot;+window.getSelection().toString());window.location.reload();" rel="external nofollow noreferrer"><i class="fas fa-comment"></i><span>百度搜索</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" href="/archives/"><i class="fa-solid fa-archive"></i><span>文章归档</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="fa-solid fa-folder-open"></i><span>文章分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="fa-solid fa-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuNormal"><a class="rightMenu-item menu-link" id="menu-radompage"><i class="fa-solid fa-shoe-prints"></i><span>随便逛逛</span></a><div class="rightMenu-item" id="menu-translate"><i class="fa-solid fa-earth-asia"></i><span>繁简切换</span></div><div class="rightMenu-item" id="menu-darkmode"><i class="fa-solid fa-moon"></i><span>切换模式</span></div></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? '' : ''

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://vercel.hycbook.com',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://vercel.hycbook.com',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'ncn88uooQf0IO2rrGE7Vniwp-gzGzoHsz',
      appKey: 'Yghpzg1QfBMFJ0MxxHubVzKL',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Twikoo' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'https://vercel.hycbook.com',
        region: '',
        pageSize: 3,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script defer src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script defer data-pjax src="/zhheo/random.js"></script><script data-pjax src="/js/coin.js"></script><script defer src="https://npm.elemecdn.com/vue@2.6.11"></script><script async src="//at.alicdn.com/t/c/font_3670467_a0sijt8frxo.js"></script><script defer src="/live2d-widget/autoload.js"></script><script defer src="https://cdnjs.cloudflare.com/ajax/libs/toastr.js/latest/toastr.min.js"></script><script data-pjax defer src="https://npm.elemecdn.com/tzy-blog/lib/js/theme/chocolate.js"></script><script defer data-pjax src="/js/rightMenu.js"></script><script defer data-pjax src="/js/udf_mouse.js"></script><script defer data-pjax src="/js/udf_js.js"></script><script defer src="/js/udf_js.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><script>window.$crisp = [];
window.CRISP_WEBSITE_ID = "561b80db-3f0f-45cb-b3b1-aae7355939e6";
(function () {
  d = document;
  s = d.createElement("script");
  s.src = "https://client.crisp.chat/l.js";
  s.async = 1;
  d.getElementsByTagName("head")[0].appendChild(s);
})();
$crisp.push(["safe", true])

if (false) {
  $crisp.push(["do", "chat:hide"])
  $crisp.push(["on", "chat:closed", function() {
    $crisp.push(["do", "chat:hide"])
  }])
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      $crisp.push(["do", "chat:show"])
      $crisp.push(["do", "chat:open"])

    });
  }
  chatBtnFn()
} else {
  if (false) {
    function chatBtnHide () {
      $crisp.push(["do", "chat:hide"])
    }
    function chatBtnShow () {
      $crisp.push(["do", "chat:show"])
    }
  }
}</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="app-refresh" id="app-refresh" style="position: fixed;top: -2.2rem;left: 0;right: 0;z-index: 99999;padding: 0 1rem;font-size: 15px;height: 2.2rem;transition: all 0.3s ease;"><div class="app-refresh-wrap" style=" display: flex;color: #fff;height: 100%;align-items: center;justify-content: center;"><label>✨ 兼一书虫上新啦！ 👉</label><a href="javascript:void(0)" rel="external nofollow noreferrer" onclick="location.reload()"><span style="color: #fff;text-decoration: underline;cursor: pointer;">🍭查看新品🍬</span></a></div></div><script>if ('serviceWorker' in navigator) {
  if (navigator.serviceWorker.controller) {
    navigator.serviceWorker.addEventListener('controllerchange', function() {
      showNotification()
    })
  }
  window.addEventListener('load', function() {
    navigator.serviceWorker.register('/sw.js')
  })
}

function showNotification() {
  if (GLOBAL_CONFIG.Snackbar) {
    var snackbarBg =
      document.documentElement.getAttribute('data-theme') === 'light' ?
      GLOBAL_CONFIG.Snackbar.bgLight :
      GLOBAL_CONFIG.Snackbar.bgDark
    var snackbarPos = GLOBAL_CONFIG.Snackbar.position
    Snackbar.show({
      text: '✨ 兼一书虫上新啦！ 👉',
      backgroundColor: snackbarBg,
      duration: 500000,
      pos: snackbarPos,
      actionText: '🍭查看新品🍬',
      actionTextColor: '#fff',
      onActionClick: function(e) {
        location.reload()
      },
    })
  } else {
    var showBg =
      document.documentElement.getAttribute('data-theme') === 'light' ?
      '#49b1f5' :
      '#1f1f1f'
    var cssText = `top: 0; background: ${showBg};`
    document.getElementById('app-refresh')
      .style.cssText = cssText
  }
}</script></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>