<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"><title>深度学习核心之优化器 | 兼一书虫</title><meta name="keywords" content="深度学习,神经网络,优化算法"><meta name="author" content="narutohyc"><meta name="copyright" content="narutohyc"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="深度学习核心之优化器">
<meta property="og:type" content="article">
<meta property="og:title" content="深度学习核心之优化器">
<meta property="og:url" content="https://study.hycbook.com/article/36065.html">
<meta property="og:site_name" content="兼一书虫">
<meta property="og:description" content="深度学习核心之优化器">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic.hycbook.com/i/hexo/post_cover/%E8%95%BE%E5%A7%869.webp">
<meta property="article:published_time" content="2023-05-26T08:31:14.000Z">
<meta property="article:modified_time" content="2023-09-20T13:51:29.490Z">
<meta property="article:author" content="narutohyc">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="神经网络">
<meta property="article:tag" content="优化算法">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic.hycbook.com/i/hexo/post_cover/%E8%95%BE%E5%A7%869.webp"><link rel="shortcut icon" href="https://pic.hycbook.com/i//hexo/config_imgs/网站图标.webp"><link rel="canonical" href="https://study.hycbook.com/article/36065"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//hm.baidu.com"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="manifest" href="/manifest.json"/><meta name="msapplication-TileColor" content="#c6ff7a"/><link rel="apple-touch-icon" sizes="180x180" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/apple-touch-icon.png"/><link rel="icon" type="image/png" sizes="32x32" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/favicon-32x32.png"/><link rel="icon" type="image/png" sizes="16x16" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/favicon-16x16.png"/><link rel="mask-icon" href="https://pic.hycbook.com/i//hexo/source/img/siteicon/safari-pinned-tab.svg" color="#5bbad5"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.min.css" media="print" onload="this.media='all'"><script>var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?68340394dfd808cea9826e8a57f87aa6";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();
</script><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"/search.xml","preload":true,"languages":{"hits_empty":"找不到您查询的内容：${query}"}},
  translate: {"defaultEncoding":1,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":120,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":400},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '天',
  date_suffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: {"limitCount":200,"languages":{"author":"作者: narutohyc","link":"链接: ","source":"来源: 兼一书虫","info":"著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。"}},
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: true,
  isAnchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '深度学习核心之优化器',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2023-09-20 21:51:29'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/mainColor/heoMainColor.css"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/404/404.css"><script src="https://npm.elemecdn.com/echarts@4.9.0/dist/echarts.min.js"></script><link href="https://cdn.bootcdn.net/ajax/libs/toastr.js/2.1.4/toastr.min.css" rel="stylesheet"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/js-heo@1.0.11/categoryBar/categoryBar.css"><link rel="stylesheet" href="/css/hyc_udf.css"><link rel="stylesheet" href="/css/udf_css.css"><link rel="stylesheet" href="/css/year.css"><svg aria-hidden="true" style="position:absolute; overflow:hidden; width:0; height:0"><symbol id="icon-sun" viewBox="0 0 1024 1024"><path d="M960 512l-128 128v192h-192l-128 128-128-128H192v-192l-128-128 128-128V192h192l128-128 128 128h192v192z" fill="#FFD878" p-id="8420"></path><path d="M736 512a224 224 0 1 0-448 0 224 224 0 1 0 448 0z" fill="#FFE4A9" p-id="8421"></path><path d="M512 109.248L626.752 224H800v173.248L914.752 512 800 626.752V800h-173.248L512 914.752 397.248 800H224v-173.248L109.248 512 224 397.248V224h173.248L512 109.248M512 64l-128 128H192v192l-128 128 128 128v192h192l128 128 128-128h192v-192l128-128-128-128V192h-192l-128-128z" fill="#4D5152" p-id="8422"></path><path d="M512 320c105.888 0 192 86.112 192 192s-86.112 192-192 192-192-86.112-192-192 86.112-192 192-192m0-32a224 224 0 1 0 0 448 224 224 0 0 0 0-448z" fill="#4D5152" p-id="8423"></path></symbol><symbol id="icon-moon" viewBox="0 0 1024 1024"><path d="M611.370667 167.082667a445.013333 445.013333 0 0 1-38.4 161.834666 477.824 477.824 0 0 1-244.736 244.394667 445.141333 445.141333 0 0 1-161.109334 38.058667 85.077333 85.077333 0 0 0-65.066666 135.722666A462.08 462.08 0 1 0 747.093333 102.058667a85.077333 85.077333 0 0 0-135.722666 65.024z" fill="#FFB531" p-id="11345"></path><path d="M329.728 274.133333l35.157333-35.157333a21.333333 21.333333 0 1 0-30.165333-30.165333l-35.157333 35.157333-35.114667-35.157333a21.333333 21.333333 0 0 0-30.165333 30.165333l35.114666 35.157333-35.114666 35.157334a21.333333 21.333333 0 1 0 30.165333 30.165333l35.114667-35.157333 35.157333 35.157333a21.333333 21.333333 0 1 0 30.165333-30.165333z" fill="#030835" p-id="11346"></path></symbol></svg><!-- hexo injector head_end start --><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/font-awesome-animation.min.css" media="defer" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/tag_plugins.css" media="defer" onload="this.media='all'"><script src="https://cdn.cbd.int/hexo-butterfly-tag-plugins-plus@latest/lib/assets/carousel-touch.js"></script><!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.3.0"></head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/person_img/兼一头像.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">120</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">169</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">9</div></a></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fangwu"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 归档</span></a></li><li><a class="site-page child" href="/categories"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> 分类</span></a></li><li><a class="site-page child" href="/tags"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> gitbook版</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://common.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> common</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://python.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> python</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://dl.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 深度学习</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://snooby.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> snooby</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xuegao">             </use></svg><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yinle">                   </use></svg><span> 音乐</span></a></li><li><a class="site-page child" href="/bangumis"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wodezhuifan">                   </use></svg><span> 追番</span></a></li><li><a class="site-page child" href="/gallery"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xiangce">                   </use></svg><span> 相册</span></a></li><li><a class="site-page child" href="/video"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-shipin">                   </use></svg><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/charts"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xigua"></use></svg><span> 统计图</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-suannai">             </use></svg><span> 网盘</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://pan.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 私月盘</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://share.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> 共享盘</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifeiji">             </use></svg><span> 导航</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/comments"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-TIFFANYSROOM_huaban">                   </use></svg><span> 留言板</span></a></li><li><a class="site-page child" href="/link"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-changyonglianjie">                   </use></svg><span> 友链</span></a></li><li><a class="site-page child" href="/about"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-aixin">                   </use></svg><span> 关于</span></a></li></ul></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic.hycbook.com/i/hexo/post_imgs/蕾姆9.webp')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">兼一书虫</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> 搜索</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fangwu"></use></svg><span> 首页</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> 文章</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/archives"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 归档</span></a></li><li><a class="site-page child" href="/categories"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> 分类</span></a></li><li><a class="site-page child" href="/tags"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 标签</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wenzhang1">             </use></svg><span> gitbook版</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://common.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> common</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://python.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-fenlei">                   </use></svg><span> python</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://dl.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-biaoqian">                   </use></svg><span> 深度学习</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://snooby.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> snooby</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xuegao">             </use></svg><span> 娱乐</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/music"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-yinle">                   </use></svg><span> 音乐</span></a></li><li><a class="site-page child" href="/bangumis"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-wodezhuifan">                   </use></svg><span> 追番</span></a></li><li><a class="site-page child" href="/gallery"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xiangce">                   </use></svg><span> 相册</span></a></li><li><a class="site-page child" href="/video"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-shipin">                   </use></svg><span> 视频</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/charts"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-xigua"></use></svg><span> 统计图</span></a></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-suannai">             </use></svg><span> 网盘</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://pan.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-guidang">                   </use></svg><span> 私月盘</span></a></li><li><a class="site-page child" target="_blank" rel="noopener external nofollow noreferrer" href="https://share.hycbook.com"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifengche">                   </use></svg><span> 共享盘</span></a></li></ul></div><div class="menus_item"><a class="site-page group" href="javascript:void(0);" rel="external nofollow noreferrer"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-zhifeiji">             </use></svg><span> 导航</span><i class="fas fa-chevron-down"></i></a><ul class="menus_item_child"><li><a class="site-page child" href="/comments"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-TIFFANYSROOM_huaban">                   </use></svg><span> 留言板</span></a></li><li><a class="site-page child" href="/link"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-changyonglianjie">                   </use></svg><span> 友链</span></a></li><li><a class="site-page child" href="/about"><svg class="icon" aria-hidden="true"><use xlink:href="#icon-aixin">                   </use></svg><span> 关于</span></a></li></ul></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">深度学习核心之优化器</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2023-05-26T08:31:14.000Z" title="发表于 2023-05-26 16:31:14">2023-05-26</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2023-09-20T13:51:29.490Z" title="更新于 2023-09-20 21:51:29">2023-09-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/deep-learning/">deep_learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">字数总计:</span><span class="word-count">8.1k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">阅读时长:</span><span>27分钟</span></span><span class="post-meta-separator">|</span><span id="" data-flag-title="深度学习核心之优化器"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="twikoo_visitors"><i class="fa-solid fa-spinner fa-spin"></i></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">评论数:</span><a href="/article/36065.html#post-comment"><span id="twikoo-count"><i class="fa-solid fa-spinner fa-spin"></i></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><hr>
<h1 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h1><div class="note primary modern"><p>最优化是指<code>非线性最优化</code>，解非线性最优化的方法有很多</p>
<p>比如<code>梯度下降法</code>、<code>共轭梯度法</code>、<code>变尺度法</code>和<code>步长加速</code>法等</p>
<p>参考本站链接<a target="_blank" rel="noopener external nofollow noreferrer" href="https://hycbook.com/article/58730.html">机器学习_最优化方法</a></p>
</div>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之优化器/优化器之间的关系脉络.svg" alt="优化器之间的关系脉络"></p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://pytorch.org/docs/stable/optim.html">pytorch优化器</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/deep_learning/optimizers/gd.html">飞浆官方文档，总结到位</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.hanspub.org/journal/PaperInformation.aspx?paperID=27016&amp;btwaf=50696508">深度学习优化器方法及学习率衰减方式综述</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/pdf/1609.04747.pdf">An overview of gradient descent optimization algorithms</a></p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>1847</th>
<th>1951</th>
<th>1983</th>
<th>2011</th>
<th>2012</th>
</tr>
</thead>
<tbody>
<tr>
<td>GD(BGD)</td>
<td>SGD</td>
<td>SGDM(Momentum)、NAG</td>
<td>AdaGrad</td>
<td>Adadelta、RMSprop</td>
</tr>
</tbody>
</table>
</div>
<div class="table-container">
<table>
<thead>
<tr>
<th>2015</th>
<th>2016</th>
<th>2018</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>Adam、AdaMax</td>
<td>Nadam</td>
<td>AMSGrad</td>
</tr>
</tbody>
</table>
</div>
<blockquote>
<p>相对应的论文</p>
</blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="http://ieeexplore.ieee.org/iel5/21/4308307/04308316.pdf">A Stochastic Approximation Method SGD 1951</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="http://www.iro.umontreal.ca/~pift6266/A06/refs/backprop_old.pdf">Learning representations by back-propagating errors Momentum 1983</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.researchgate.net/publication/260365606_A_method_for_unconstrained_convex_minimization_problem_with_the_rate_of_convergence">A method for unconstrained convex minimization problem with the rate of convergence  o(1/k2) NAG 1983</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://citeseerx.ist.psu.edu/viewdoc/download;jsessionid=02923576939D50A9A51E4BE7500CF493?doi=10.1.1.232.4000&amp;rep=rep1&amp;type=pdf">Adaptive subgradient methods for online learning and stochastic optimization AdaGrad 2011</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/pdf/1212.5701.pdf">ADADELTA: an adaptive learning rate method. Adadelta 2012</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">Neural Networks for Machine Learning Lecture 6a Overview of mini-batch gradient descent RMSprop 2012</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://arxiv.org/pdf/1412.6980.pdf">Adam: A method for stochastic optimization Adam &amp; AdaMax 2014</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://openreview.net/pdf?id=OM0jvwB8jIp57ZJjtNEZ">Incorporating Nesterov Momentum into Adam NAdam 2016</a></p>
<p>以下是优化器的发展脉络，按照时间顺序列出了一些重要的优化器及其年份：</p>
<ol>
<li><p>Gradient Descent (GD)：最早的优化器之一，用于求解无约束优化问题。没有特定的年份，但早在20世纪50年代就开始被广泛应用</p>
</li>
<li><p>Stochastic Gradient Descent (SGD)：引入随机性来估计梯度的优化器，用于大规模数据集和深度学习模型。没有特定的年份，但在深度学习的早期就被广泛使用</p>
</li>
<li><p>Momentum（1983）：提出了动量概念，通过累积梯度的指数加权平均来加速收敛</p>
</li>
<li><p>AdaGrad（2011）：自适应梯度算法，通过对梯度进行归一化和调整学习率，适应不同参数的更新需求</p>
</li>
<li><p>Adadelta（2012）：改进了AdaGrad的缺点，通过考虑历史梯度的平均值来调整学习率</p>
<p>RMSprop（2012）：引入了指数加权移动平均的概念，用于调整学习率以平衡历史梯度信息</p>
</li>
<li><p>Adam（2014）：结合了动量和自适应学习率的优点，通过自适应调整学习率和梯度的一阶矩估计和二阶矩估计来进行参数更新</p>
<p>AdaMax（2014）：基于Adam算法，通过替换二阶矩估计的范数为无穷范数，提供了更稳定的更新规则</p>
</li>
<li><p>Nadam（2016）：结合了Nesterov动量和Adam算法，利用动量来加速收敛</p>
</li>
<li><p>AMSGrad（2018）：对Adam算法进行了改进，解决了Adam算法学习率下降不稳定的问题</p>
</li>
</ol>
<p>这些是一些比较重要的优化器，并且按照时间顺序列出。然而，需要注意的是，并非所有的优化器都是线性发展的，而是相互借鉴、改进和结合的结果。优化器的发展是一个活跃的研究领域，仍然有许多新的优化算法被提出和改进</p>
<blockquote>
<p>什么是优化器</p>
</blockquote>
<p>深度学习的目标是通过不断改变网络参数，使得参数能够对输入做各种非线性变换拟合输出，本质上就是一个函数去寻找最优解，所以<code>如何去更新参数</code>是深度学习研究的重点</p>
<p>通常将更新参数的算法称为<code>优化器</code>，字面理解就是通过什么算法去优化网络模型的参数</p>
<blockquote>
<p>梯度下降核心点</p>
</blockquote>
<ol>
<li><strong>方向</strong>: 确定优化的方向，一般通过求导便可以求得</li>
<li><strong>步长</strong>: 步子就是决定当前走多大，如果学习率设的过大，梯度会在最优点来回跳动，设的过小需要很久的训练才能达到最优点</li>
</ol>
<blockquote>
<p>优化器的主要作用</p>
</blockquote>
<ol>
<li><strong>参数更新</strong>：优化器根据损失函数的梯度信息，计算出每个参数的更新量，并将更新量应用于参数，从而更新模型的参数。这样，模型的参数就可以朝着能够更好地拟合训练数据的方向进行调整</li>
<li><strong>学习率调整</strong>：优化器通常会自动调整学习率，以控制参数更新的步幅。学习率决定了每次参数更新的幅度，过大的学习率可能导致参数更新过快而错过最优解，而过小的学习率可能导致收敛速度缓慢。优化器根据当前训练的进度和参数的变化情况，动态地调整学习率，以获得更好的训练效果</li>
<li><strong>优化算法选择</strong>：优化器提供了多种不同的优化算法，如梯度下降、动量优化、自适应学习率等。这些算法在参数更新的方式、学习率调整策略等方面有所不同，可以根据具体任务的需求选择合适的优化算法</li>
</ol>
<p>通过合适的优化器选择和参数调整，可以提高神经网络的训练效率和性能，加速收敛过程，使得模型能够更好地拟合训练数据，并在测试数据上取得较好的泛化能力</p>
<blockquote>
<p>优化器分类</p>
</blockquote>
<ol>
<li><strong>梯度下降优化器(Gradient Descent Optimizers)</strong>：基于梯度信息来更新参数的优化器，包括批量梯度下降(BGD)、随机梯度下降(SGD)和小批量梯度下降(Mini-Batch Gradient Descent，MBGD)等</li>
<li><strong>基于动量的优化器(Momentum-based Optimizers)</strong>：在梯度下降的基础上引入动量的概念，旨在加速收敛过程并减少震荡，常见的包括动量优化器(Momentum Optimizer)、牛顿加速度动量优化法Nesterov Accelerated Gradient(NAG)等</li>
<li><strong>自适应学习率优化器(Adaptive Learning Rate Optimizers)</strong>：根据参数更新的情况动态地调整学习率，以提高收敛速度和效果，常见的包括AdaGrad、RMSprop、Adam、AdaDelta、Adamax等</li>
<li><strong>学习率衰减优化器(Learning Rate Decay Optimizers)</strong>：在训练过程中逐渐减小学习率的优化器，常见的包括Step Decay、Exponential Decay、Piecewise Decay等</li>
<li><strong>正则化优化器(Regularization Optimizers)</strong>：结合正则化技术，通过对损失函数添加正则化项来控制模型的复杂度，常见的包括L1正则化、L2正则化等</li>
<li><strong>二阶优化器(Second-Order Optimizers)</strong>：考虑参数二阶信息的优化器，如牛顿法(Newton’s Method)、共轭梯度法(Conjugate Gradient)等</li>
</ol>
<p>不同类型的优化器在更新参数的方式、学习率调整策略、收敛速度、对噪声和局部最优的鲁棒性等方面有所区别，选择合适的优化器取决于具体的问题和数据集特征</p>
<h2 id="基本的梯度下降法"><a href="#基本的梯度下降法" class="headerlink" title="基本的梯度下降法"></a>基本的梯度下降法</h2><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/584485233">优化器综述</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.cnblogs.com/guoyaohua/p/8542554.html">深度学习——优化器算法Optimizer详解（BGD、SGD、MBGD、Momentum、NAG、Adagrad、Adadelta、RMSprop、Adam）</a></p>
</blockquote>
<p>优化器的存在就是确定优化的方向和面对当前的情况动态的调整步子</p>
<blockquote>
<p>BGD、SGD和MBGD的区别</p>
</blockquote>
<div class="table-container">
<table>
<thead>
<tr>
<th>优化器</th>
<th>BGD</th>
<th>SGD</th>
<th>MBGD</th>
</tr>
</thead>
<tbody>
<tr>
<td>样本数</td>
<td>N(所有)</td>
<td>1</td>
<td>batch_size</td>
</tr>
</tbody>
</table>
</div>
<h3 id="BGD"><a href="#BGD" class="headerlink" title="BGD"></a>BGD</h3><p><code>BGD(Batch Gradient Descent)</code>采用<strong>整个训练集</strong>的数据来计算cost function对参数的梯度</p>
<script type="math/tex; mode=display">
w_{t+1}=w_{t}-\alpha \Delta L\left(w_{t}\right)</script><p>其中<script type="math/tex">\alpha</script>为学习率，而<script type="math/tex">\Delta L\left(w_{t}\right)</script>为损失函数的一阶导数</p>
<blockquote>
<p>BGD在计算梯度时会出现冗余</p>
</blockquote>
<p>因为BGD在每一次迭代中都使用了整个训练集，而且在梯度计算过程中并没有考虑样本之间的相关性</p>
<p>因此，对于样本中的某些部分，其梯度计算可能会与其他样本的梯度计算重复，这种冗余计算可能会导致计算效率的降低，特别是在训练集很大的情况下</p>
<blockquote>
<p>优点</p>
</blockquote>
<ol>
<li><strong>收敛稳定</strong>：由于每次迭代使用整个训练集的所有样本进行参数更新，收敛过程相对稳定</li>
<li><strong>参数更新准确</strong>：使用全局梯度来更新参数，对于<strong>凸优化问题，可以达到全局最优解</strong></li>
</ol>
<blockquote>
<p>缺点</p>
</blockquote>
<ol>
<li><strong>训练速度慢</strong>：BGD 是一种批量梯度下降算法，每次更新模型参数时使用整个训练数据集</li>
<li><strong>计算开销大</strong>：需要计算整个训练集的梯度，对于大规模数据集或复杂模型，计算开销较高</li>
<li><strong>内存占用高</strong>：需要存储整个训练集的数据和梯度信息</li>
</ol>
<h3 id="SGD"><a href="#SGD" class="headerlink" title="SGD"></a>SGD</h3><p><code>SGD(Stochastic Gradient Descent)</code>是一种随机梯度下降算法，每次更新模型参数时使用<strong>单个样本</strong>或<strong>一小批样本</strong>(通常称为mini-batch，也称MBGD)</p>
<script type="math/tex; mode=display">
w_{t+1}=w_{t}-\alpha \frac{1}{m} \sum_{i=1}^{m} \Delta L\left(w_{i}\right)</script><p>其中<script type="math/tex">\alpha</script>为学习率，而<script type="math/tex">\Delta L\left(w_{t}\right)</script>为损失函数的一阶导数，<script type="math/tex">m</script>为batch_size，当<script type="math/tex">m=1</script>就是SGD，否则就是MBGD</p>
<blockquote>
<p>优点</p>
</blockquote>
<ol>
<li><strong>计算开销小</strong>：每次迭代只使用<strong>一个样本</strong>进行参数更新，计算开销较小</li>
<li><strong>适用于大规模数据集</strong>：由于样本的随机选择，可以处理大规模数据集，且易于并行处理</li>
</ol>
<blockquote>
<p>缺点</p>
</blockquote>
<ol>
<li><strong>参数更新不稳定</strong>：由于单个样本的梯度计算可能<strong>存在噪声</strong>，参数更新不稳定，可能引起参数在最优点附近震荡</li>
<li><strong>收敛速度较慢</strong>：由于参数更新的不稳定性，收敛速度相对较慢</li>
</ol>
<h3 id="MBGD"><a href="#MBGD" class="headerlink" title="MBGD"></a>MBGD</h3><p><code>MBGD(Mini-Batch Gradient Descent)</code>每一次利用一小批样本，即batch_size个样本进行计算，这样它可以降低参数更新时的方差，收敛更稳定，另一方面可以充分地利用深度学习库中高度优化的矩阵操作来进行更有效的梯度计算</p>
<script type="math/tex; mode=display">
w_{t+1}=w_{t}-\alpha \frac{1}{m} \sum_{i=1}^{m} \Delta L\left(w_{i}\right)</script><p>其中<script type="math/tex">\alpha</script>为学习率，而<script type="math/tex">\Delta L\left(w_{t}\right)</script>为损失函数的一阶导数，<script type="math/tex">m</script>为batch_size，当<script type="math/tex">m=1</script>就是SGD，否则就是MBGD</p>
<blockquote>
<p>优点</p>
</blockquote>
<ol>
<li><strong>平衡了开销和参数稳定性</strong>：使用一小批样本进行参数更新，综合了全局梯度和随机梯度的信息，计算开销和参数更新的稳定性得到一定的平衡</li>
<li><strong>收敛速度较快</strong>：相对于BGD，使用较小的批量样本更新参数，收敛速度更快</li>
</ol>
<blockquote>
<p>缺点</p>
</blockquote>
<ol>
<li><strong>批量大小需调优</strong>：批量大小的选择可能会影响模型的性能，需要进行调优</li>
<li><strong>可能导致局部最优</strong>：较小的批量样本可能会引入一定的随机性，可能导致陷入局部最优而无法达到全局最优</li>
</ol>
<h2 id="动量优化法"><a href="#动量优化法" class="headerlink" title="动量优化法"></a>动量优化法</h2><h3 id="SGDM"><a href="#SGDM" class="headerlink" title="SGDM"></a>SGDM</h3><p>随机梯度下降法虽然有效，但容易陷入局部最小值点，甚至在驻点附近以及梯度值非常小的点附近时参数更新极为缓慢</p>
<p>为了抑制SGD的震荡，<code>SGDM(Stochastic Gradient Descent Momentum)</code>认为梯度下降过程可以加入惯性</p>
<p>主要思想是下降过程中，如果发现是陡坡，那就利用惯性跑的快一些。因此，其在SGD基础上引入了一阶动量</p>
<p>在坡度比较陡的地方，会有较大的惯性，这是下降的多。坡度平缓的地方，惯性较小，下降的会比较慢</p>
<script type="math/tex; mode=display">
\begin{array}{c}
m_{t}=\lambda m_{t-1}+ \alpha \Delta L(w_t) \\
w_{t+1}=w_{t}-m_{t}
\end{array}</script><p>其中<script type="math/tex">\alpha</script>为学习率，<script type="math/tex">\Delta L(w_t)</script>表示当前t时刻梯度，<script type="math/tex">m_t</script>表示当前时刻的加权后的梯度，<script type="math/tex">\lambda</script>是<code>动量系数</code></p>
<p>而<script type="math/tex">\lambda</script>的经验值为0.9(表示最大速度10倍于SGD)，这就意味着下降方向主要是此前累积的下降方向，并略微偏向当前时刻的下降方向</p>
<p>一阶动量是各个时刻梯度方向的指数移动平均值，也就是说，<script type="math/tex">t</script>时刻的下降方向，不仅由当前点的梯度方向<script type="math/tex">\Delta L(w_t)</script>决定，而且由此前累积的下降方向<script type="math/tex">m_{t-1}</script>决定</p>
<blockquote>
<p>梯度是如何累积的</p>
</blockquote>
<p>这里将<script type="math/tex">m_t</script>展开，可以看到<strong>当前时刻的梯度是对历史梯度进行加权得到的</strong>，其中，<script type="math/tex">\lambda</script>是一个介于0和1之间的参数，控制了历史梯度对当前动量的贡献程度</p>
<ul>
<li>较大的<script type="math/tex">\lambda</script>值会使历史梯度的贡献更大，从而使动量更加平滑</li>
<li>较小的<script type="math/tex">\lambda</script>值会使当前梯度的贡献更大，从而对变化更为敏感</li>
</ul>
<p>这种权重衰减的方式使得历史梯度的贡献逐渐减小，更加关注近期的梯度变化，有助于适应变化的数据和模型参数</p>
<script type="math/tex; mode=display">
\begin{aligned}
m_{t}= & -\alpha \Delta L(w_t) -\alpha \lambda \Delta L(w_{t-1})-\alpha \lambda^{2} \Delta L(w_{t-2})-\alpha \lambda^{3} \Delta L(w_{t-3}) \cdots
\end{aligned}</script><blockquote>
<p>优点</p>
</blockquote>
<ol>
<li><p><strong>加速收敛</strong>：SGDM引入了动量的概念，通过累积之前的动量信息，有助于加速模型的收敛速度，特别是在存在平坦区域的情况下更为明显</p>
</li>
<li><p><strong>减少震荡</strong>：动量的累积作用可以减少参数更新时的震荡现象，有助于更稳定地更新模型参数</p>
</li>
<li><p><strong>尽可能跳出局部最优</strong>：动量的引入可以帮助模型跳出局部最优点，以便更好地搜索全局最优点</p>
<p>当局部沟壑比较深，动量加持用完了，依然会困在局部最优里来回振荡</p>
</li>
</ol>
<blockquote>
<p>缺点</p>
</blockquote>
<ol>
<li><strong>需要调整超参数</strong>：SGDM中的动量系数需要手动设置，选择合适的动量系数对于模型的性能影响较大，需要进行调试和调优</li>
<li><strong>可能导致过拟合</strong>：当动量系数较大时，SGDM可能在优化过程中过度依赖之前的动量信息，导致模型过拟合</li>
<li><strong>难以处理非平稳数据</strong>：对于非平稳数据，SGDM的动量累积可能会导致模型在变化快速的方向上过度追踪，而无法及时适应变化</li>
</ol>
<blockquote>
<p>改进方法</p>
</blockquote>
<ol>
<li><strong>自适应调整动量系数</strong>：可以采用自适应的方式来调整动量系数，例如使用自适应的动量方法(如Adam)来根据梯度的变化自动调整动量系数</li>
<li><strong>学习率调度策略</strong>：结合学习率调度策略，如<strong>学习率衰减</strong>或<strong>自适应学习率方法</strong>，可以更好地控制模型的学习速度和方向</li>
<li><strong>正则化技术</strong>：使用正则化技术，如L1正则化或L2正则化，可以缓解过拟合问题，使模型更具泛化能力</li>
</ol>
<p>因为加入了动量因素，SGDM缓解了SGD在局部最优点梯度为0，无法持续更新的问题和振荡幅度过大的问题，但是并没有完全解决，当局部沟壑比较深，动量加持用完了，依然会困在局部最优里来回振荡</p>
<h3 id="NAG"><a href="#NAG" class="headerlink" title="NAG"></a>NAG</h3><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://blog.csdn.net/qq_38156104/article/details/106739700">深度学习优化函数详解— Nesterov accelerated gradient (NAG)</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/22810533/">比Momentum更快：揭开Nesterov Accelerated Gradient的真面目</a></p>
</blockquote>
<p>动量法每下降一步都是由前面下降方向的一个累积和当前点的梯度方向组合而成。于是一位大神(Nesterov)就开始思考，既然每一步都要将两个梯度方向(历史梯度、当前梯度)做一个合并再下降，那为什么不先按照历史梯度往前走那么一小步，按照前面一小步位置的<strong>超前梯度</strong>来做梯度合并呢</p>
<p>如此一来，小球就可以先不管三七二十一先往前走一步，在靠前一点的位置看到梯度，然后按照那个位置再来修正这一步的梯度方向，同SGDM比较的差一点如下公式所示</p>
<script type="math/tex; mode=display">
\Delta L(w_{t-1})  \longrightarrow  \Delta L(w_{t-1}-\lambda m_{t-1})</script><div class="note info modern"><p>既然知道会走<script type="math/tex">\lambda m_{t-1}</script>，就不需要还用当前位置的梯度，可以直接走到<script type="math/tex">\lambda m_{t-1}</script>的位置计算梯度，这样子就有了<code>超前眼光</code></p>
</div>
<p>有了超前的眼光，小球就会更加<strong>聪明</strong>, 这种方法被命名为<code>牛顿加速梯度(Nesterov accelerated gradient)</code>简称<code>NAG</code>，下图是SGDM下降法与NAG下降法的可视化比较</p>
<p><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/bk_resources/deep_learning/深度学习核心之优化器/momentum下降法与NAG下降法比较.webp" alt="momentum下降法与NAG下降法比较"></p>
<p>NAG算法公式表达如下：</p>
<script type="math/tex; mode=display">
\begin{array}{c}
m_{t}=\lambda m_{t-1}+ \alpha \Delta L(w_{t-1}-\lambda m_{t-1}) \\
w_{t+1}=w_{t}-m_{t}
\end{array}</script><blockquote>
<p>为什么NAG比SGDM快</p>
</blockquote>
<p>对NAG原来的更新公式进行变换，得到这样的等效形式(<a target="_blank" rel="noopener external nofollow noreferrer" href="https://zhuanlan.zhihu.com/p/22810533/">具体推导过程</a>)</p>
<script type="math/tex; mode=display">
\begin{array}{c}
m_{t}=\lambda m_{t-1}+ \alpha \Delta L(w_{t-1}) + \lambda ( \Delta L(w_{t-1}) - \Delta L(w_{t-2}) )  \\
w_{t+1}=w_{t}-m_{t}
\end{array}</script><p>与Momentum的区别在于，本次更新方向多加了一个<script type="math/tex">\lambda ( \Delta L(w_{t-1}) - \Delta L(w_{t-2}) )</script></p>
<p>直观含义就很明显了：如果这次的梯度比上次的梯度变大了，那么有可能会继续变大，可以把预计增大的部分提前加进来；变小的情况类似</p>
<p>这个多加上去的项就是在近似目标函数的二阶导嘛，因此，<code>NAG本质上是多考虑了目标函数的二阶导信息</code>，其实所谓<strong>往前看</strong>的说法，在牛顿法这样的二阶方法中也是经常提到的，从数学角度上看，则是<strong>利用了目标函数的二阶导信息</strong></p>
<h2 id="自适应学习率优化器"><a href="#自适应学习率优化器" class="headerlink" title="自适应学习率优化器"></a>自适应学习率优化器</h2><h3 id="AdaGrad"><a href="#AdaGrad" class="headerlink" title="AdaGrad"></a>AdaGrad</h3><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.cnblogs.com/wry789/p/12988629.html">李宏毅深度学习笔记-Adagrad算法</a></p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="http://faculty.bicmr.pku.edu.cn/~wenzw/optbook/pages/stograd/Adagrad.html">AdaGrad 算法</a></p>
<p>优化的变量对于目标函数的依赖是各不相同</p>
</blockquote>
<p>在基本的梯度下降法优化中，有一个常见的问题是，<code>要优化的变量对于目标函数的依赖是各不相同的</code></p>
<ul>
<li><p>对于某些变量，已经优化到了极小值附近，但是有的变量仍然在梯度很大的地方，这时候一个统一的全局学习率是可能出现问题的</p>
</li>
<li><p>如果学习率太小，则梯度很大的变量会收敛很慢，如果梯度太大，已经优化差不多的变量就可能会不稳定</p>
</li>
</ul>
<p>现实世界的数据集中，一些特征是稀疏的(大部分特征为零，所以它是稀疏的)，而另一些则是密集的(dense，大部分特征是非零的)，因此为所有权值保持相同的学习率不利于优化</p>
<p>针对这个问题，当时在伯克利加州大学读博士的Jhon Duchi，2011年提出了<code>AdaGrad(Adaptive Gradient)</code>，也就是<code>自适应学习率</code></p>
<blockquote>
<p>基本思想</p>
</blockquote>
<p>AdaGrad的基本思想是对每个变量用不同的学习率，设置了全局学习率之后，每次通过，<strong>全局学习率逐参数的除以历史梯度平方和的平方根，使得每个参数的学习率不同</strong></p>
<p>这个学习率在一开始会比较大，用于快速梯度下降。随着优化过程的进行，对于<strong>已经下降很多的变量，则减缓学习率</strong>，对于<strong>还没怎么下降的变量，则保持一个较大的学习率</strong></p>
<blockquote>
<p>公式</p>
</blockquote>
<script type="math/tex; mode=display">
w_{t+1} =w_{t}-\frac{\alpha}{\sqrt{G_{t}+\epsilon }} \odot \eta \Delta L(w_t)</script><p>其中<script type="math/tex">\eta</script>为学习率，而<script type="math/tex">\Delta L\left(w_{t}\right)</script>为损失函数的一阶导数，<script type="math/tex">\epsilon</script>是一个平滑项，避免了除以零(通常取值在<script type="math/tex">1e-8</script>左右)，<script type="math/tex">\odot</script>表示元素逐元素相乘操作</p>
<script type="math/tex; mode=display">G_{t+1}$$可以写成下式，每个参数的所有偏微分的平方和，$$g_i$$是对梯度的缩写</script><p>G<em>{t+1} =G</em>{t}+g<em>{t+1} \odot g</em>{t+1} = \sum _{i=0}^{t} { {g_i} ^2 }</p>
<script type="math/tex; mode=display">


> 优缺点

**优点**：

* 自适应的学习率，无需人工调节，AdaGrad在迭代过程中不断调整学习率，并让目标函数中的每个参数都分别拥有自己的学习率，学习率默认值为0.01
* 有效地处理稀疏特征，因为它能够自动调整每个特征的学习率，使得稀疏特征的更新更少

**缺点**：

- **全局学习率**: 仍需要手工设置一个全局学习率$$\eta$$, 如果$$\eta$$设置过大的话，会使regularizer过于敏感，对梯度的调节太大
- **训练停止**: 由于梯度平方和的累积，**学习率会不断衰减**，可能导致在训练后期学习率过小，造成收敛速度过慢或者提前停止训练的问题(`Adadelta`算法解决)

> 附上别人写的代码

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sgd_adagrad</span>(<span class="params">parameters, sqrs, lr</span>):</span><br><span class="line">    eps = <span class="number">1e-10</span></span><br><span class="line">    <span class="keyword">for</span> param, sqr <span class="keyword">in</span> <span class="built_in">zip</span>(parameters, sqrs):</span><br><span class="line">        sqr[:] = sqr + param.grad.data ** <span class="number">2</span></span><br><span class="line">        div = lr / torch.sqrt(sqr + eps) * param.grad.data</span><br><span class="line">        param.data = param.data - div</span><br></pre></td></tr></table></figure>

### RMSProp

> [An overview of gradient descent optimization algorithms 2017](https://arxiv.org/pdf/1609.04747.pdf)

`RMSProp(Root Mean Square Propagation)`是Hinton大神于2012年在一门叫[Neural Networks for Machine Learning](https://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf)的在线课程中提出(并未正式发表)，是梯度下降优化算法的扩展

`RMSProp实际上是Adagrad引入了Momentum`，公式表达如下所示</script><p>\begin{array}{c}<br>G<em>{t+1} = \beta G_t + (1-\beta) \Delta L (w</em>{t})^2<br>\<br>w<em>{t+1} = w_t -  \frac {\alpha}{\sqrt{ G</em>{t} + \epsilon }} \odot \Delta L (w_t)<br>\end{array}</p>
<script type="math/tex; mode=display">
$$\alpha$$是学习率，$$\beta$$则类似于动量梯度下降法中的衰减因子，代表过去梯度对当前梯度的影响，一般取值0.9，$$\epsilon$$是一个平滑项，避免了除以零(通常取值在$$1e-8$$左右)，$$\odot$$表示元素逐元素相乘操作(也可以省略不写)

公式里的`累积梯度平方和`$$G_{t+1}$$可以展开写成下面的形式，$$g_i$$是对梯度的缩写(同AdaGrad)</script><p>G<em>{t+1} = \beta G_t + (1-\beta) \sum </em>{i=0}^{t} { {g_i} ^2 }</p>
<script type="math/tex; mode=display">

> 优点



> 缺点



> 代码([参考文档](https://towardsdatascience.com/understanding-rmsprop-faster-neural-network-learning-62e116fcf29a))

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">drad_squared = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> num_iterations:</span><br><span class="line">    dw = compute_gradients(x, y)</span><br><span class="line">    grad_squared = <span class="number">0.9</span> * grads_squared + <span class="number">0.1</span> * dx * dx</span><br><span class="line">    w = w - (lr / np.sqrt(grad_squared)) * dw</span><br></pre></td></tr></table></figure>

### Adadelta

> [优化器(AdaGrad,AdaDelta,RmsProp,Adam,Nadam,Nesterovs,Sgd,momentum)](https://betheme.net/news/txtlist_i150611v.html?action=onClick)
>
> [AdaDelta算法两种解决方案](https://blog.csdn.net/XiangJiaoJun_/article/details/83960136)
>
> [Adadelta 优化器](https://gmis.jiqizhixin.com/graph/technologies/173c1ba6-0a13-45f6-9374-ec0389124832)

由于AdaGrad调整学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：**不累积全部历史梯度，而只关注过去一段时间窗口的下降梯度**

即Adadelta只累加固定大小的项，并且也不直接存储这些项，仅仅是近似计算对应的平均值(指数移动平均值)，这就避免了二阶动量持续累积、导致训练过程提前结束的问题了

论文中提到了两种实现策略

> 方法一: Accumulate Over Window

从全部历史梯度变为当前时间向前的一个窗口期内的累积，计算定义为</script><p>\mathrm{E}\left[\mathrm{g}^{2}\right]<em>{\mathrm{t}}=\rho * \mathrm{E}\left[\mathrm{g}^{2}\right]</em>{\mathrm{t}-1}+(1-\rho) * \mathrm{~g}_{\mathrm{t}}^{2}</p>
<script type="math/tex; mode=display">
相当于历史梯度信息的累计乘上一个衰减系数$$\rho$$，然后用$$\rho$$作为当前梯度的平方加权系数相加

梯度更新公式为</script><p>\mathrm{w}<em>{\mathrm{t+1}}=\mathrm{w}</em>{\mathrm{t}}-\frac{\eta}{\sqrt{\mathrm{E}\left[\mathrm{g}^{2}\right]<em>{\mathrm{t}}+\epsilon}} * \mathrm{~g}</em>{\mathrm{t}}</p>
<script type="math/tex; mode=display">
解决了对历史梯度一直累加而导致学习率一直下降的问题

> 方法二: [Correct Units with Hessian Approximation](https://arxiv.org/pdf/1212.5701.pdf)

在1988年LeCun等人曾经提出一种用矩阵对角线元素来近似逆矩阵</script><p>\Delta x<em>{t}=-\frac{1}{\left|\operatorname{diag}\left(H</em>{t}\right)\right|+\mu} g_{t}</p>
<script type="math/tex; mode=display">
diag函数指的是构造Hessian矩阵的对角矩阵，$$\mu$$是常数项，防止分母为0

如果学过数值分析的同学应该知道，**牛顿法用Hessian矩阵替代人工设置的学习率**，在梯度下降的时候，可以完美的找出下降方向，不会陷入局部最小值当中，是理想的方法，但是**Hessian矩阵的逆在数据很大的情况下根本没办法求**

2012年，[Schaul&S. Zhang&LeCun]借鉴了AdaGrad的做法，提出了更精确的近似</script><p>\Delta x<em>{t}=-\frac{1}{\left|\operatorname{diag}\left(H</em>{t}\right)\right|} \frac{E\left[g<em>{t-w: t}\right]^{2}}{E\left[g</em>{t-w: t}^{2}\right]} g_{t}</p>
<script type="math/tex; mode=display">
$$E\left[g_{t-w: t}\right]$$指的是从当前t开始的前w个梯度状态的期望值

$$E\left[g_{t-w: t}^{2}\right]$$指的是从当前t开始的前w个梯度状态的**平方**的期望值

同样是基于Gradient的Regularizer，不过**只取最近的w个状态**，这样不会让梯度被惩罚至0

这里如果求期望的话，非常的麻烦，所以采取了移动平均法来计算。这里作者在论文中也给出了近似的证明</script><p>\Delta \mathrm{x} \propto \mathrm{g} \propto \frac{\mathrm{df}}{\mathrm{dx}} \propto \frac{1}{x}</p>
<script type="math/tex; mode=display">
这里是当为指数型函数, 最后一个近似成立。 对于牛顿法：</script><p>\Delta \mathrm{x} \propto H^{-1} \mathrm{~g} \propto \frac{\frac{\mathrm{df}}{\mathrm{dx}}}{\frac{\partial^{2} f}{\partial x^{2}}}</p>
<script type="math/tex; mode=display">
由上式可得：</script><p>\frac{\frac{d f}{d x}}{\frac{\partial^{2} f}{\partial x^{2}}}=\frac{1}{\frac{\partial^{2} f}{\partial x^{2}}} g_{t}</p>
<script type="math/tex; mode=display">
基中:</script><p>\frac{\frac{d f}{d x}}{\frac{\partial^{2} f}{\partial x^{2}}}=\frac{1}{\frac{\partial^{2} f}{\partial x^{2}}} g_{t}</p>
<script type="math/tex; mode=display">
这里可以用局部的加权指数平滑来替代，即：</script><p>\frac{\Delta x}{\frac{\partial f}{\partial x}} \approx-\frac{R M S[\Delta x]<em>{t-1}}{R M S[\Delta g]</em>{t}}</p>
<script type="math/tex; mode=display">
这里的RMS表示均方:</script><p>\operatorname{RMS}[g]<em>{t}=\sqrt{E\left[g^{2}\right]</em>{t}+\epsilon}</p>
<script type="math/tex; mode=display">
可以得到:</script><p>\Delta x<em>{t}=-\frac{\operatorname{RMS}[\Delta x]</em>{t-1}}{\operatorname{RMS}[g]<em>{t}} g</em>{t}</p>
<script type="math/tex; mode=display">
最后的更新公式为</script><p>\mathrm{x}<em>{\mathrm{t+1}}=\mathrm{x}</em>{\mathrm{t}} + \Delta x_{t}</p>
<script type="math/tex; mode=display">

> 优点

1. **无需手动设置学习率**：Adadelta能够自适应地调整学习率，无需手动设置
2. **解决了学习率衰减问题**：由于采用了衰减平均的方式，Adadelta能够解决学习率随时间衰减过快的问题，使得模型能够更好地收敛
3. **不依赖全局学习率**：Adadelta不需要设置全局学习率，因此可以适应不同参数的学习速度需求
4. **对初始学习率不敏感**：Adadelta相对于其他优化器对初始学习率的选择并不敏感，使得模型更具鲁棒性

> 缺点

1. **存储额外的状态信息**：Adadelta需要保存额外的状态信息(如梯度平方的累积)，增加了存储的开销
2. **算法参数要调整**：Adadelta中的衰减系数$$\rho$$需要进行适当的调整，不同任务可能需要不同的设置

### Adam

`Adam(Adaptive Moment Estimation)`自适应矩估计，是另一种**自适应学习率**的算法，**本质上是带有动量项的Adadelta或RMSprop**

是Diederik P. Kingma等人在2014年提出的优化算法，引入了两个参数$$\beta 1$$和$$\beta 2</script><blockquote>
<p>思路</p>
</blockquote>
<p>Adam不仅如RMSProp算法那样基于<strong>一阶矩均值</strong>计算适应性参数学习率，它同时还充分利用了<strong>梯度的二阶矩均值</strong>(即<strong>有偏方差</strong>)，适合解决含大规模的数据和参数的优化目标，也适合解决包含高噪声或稀疏梯度的问题，让参数更新时保持稳定</p>
<script type="math/tex; mode=display">
\begin{aligned} 

m_{t} & =\beta_{1} m_{t-1}+\left(1-\beta_{1}\right) g_{t} \\ \\

v_{t} & =\beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2} \\ \\

\hat{m}_{t} & =\frac{m_{t}}{1-\beta_{1}^{t}} \\ \\

\hat{v}_{t} & =\frac{v_{t}}{1-\beta_{2}^{t}} \\ \\

\end{aligned}</script><p>其中<script type="math/tex">\beta _1</script>控制一阶动量，<script type="math/tex">\beta _2</script>控制二阶动量</p>
<p>最终的参数更新公式为</p>
<script type="math/tex; mode=display">
w_{t+1} = w_{t}- \eta \frac{\hat{m}_{t}}{\sqrt{\hat{v}_{t}}+\epsilon}</script><p>默认值设置<script type="math/tex">\alpha=0.001</script>，<script type="math/tex">\beta 1=0.9</script>，<script type="math/tex">\beta 2=0.999</script>，<script type="math/tex">\varepsilon=10-8</script> </p>
<blockquote>
<p>优点</p>
</blockquote>
<ul>
<li><strong>自适应学习率</strong>：Adam通过<strong>自适应地调整每个参数的学习率</strong>，可以有效地应对不同参数的梯度变化情况。这使得它在训练过程中更容易收敛，并且对于大多数任务具有较好的性能，但是需要注意的是它的效果有时候不如SGDM</li>
<li><strong>速度快</strong>：Adam结合了动量方法，能够在训练过程中积累梯度的动量，从而加速参数更新的速度，尤其在具有平坦或稀疏梯度的情况下更加明显</li>
<li>结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点</li>
<li><strong>适用性广泛</strong>：也适用于大多非凸优化，适用于大数据集和高维空间</li>
</ul>
<blockquote>
<p>缺点</p>
</blockquote>
<ul>
<li><strong>内存消耗较大</strong>：Adam需要存储每个参数的动量和平方梯度估计，这会占用较大的内存空间，特别是在具有大量参数的深度神经网络中</li>
</ul>
<h3 id="AdaMax"><a href="#AdaMax" class="headerlink" title="AdaMax"></a>AdaMax</h3><p><code>AdaMax</code>是一种<strong>自适应学习率</strong>优化算法，是Adam优化器的一种<strong>变体</strong></p>
<p>AdaMax使用了梯度的无穷范数来估计梯度的大小，而Adam使用了梯度的二范数(<strong>核心区别</strong>)，变化如下所示</p>
<script type="math/tex; mode=display">
v_{t} = \beta_{2} v_{t-1}+\left(1-\beta_{2}\right) g_{t}^{2}  \longrightarrow v_{t} = \beta_{2} v_{t-1}+\left(1-\beta_{2}\right) |g_{t}|^{p}</script><p>论文中AdaMax的<script type="math/tex">p= \infty</script></p>
<blockquote>
<p>为什么是选择了无穷范数</p>
</blockquote>
<p>AdaMax选择了无穷范数(<script type="math/tex">\infty</script>范数)是因为在大多数情况下，<script type="math/tex">\infty</script>范数具有稳定的行为</p>
<p>对于一些问题，特别是在深度学习中，<script type="math/tex">\infty</script>范数可以提供更好的数值稳定性和收敛性</p>
<p>相比于其他范数，<script type="math/tex">\infty</script>范数能够更好地控制梯度的最大值，从而减少参数更新的不稳定性</p>
<p>因此，AdaMax选择了<script type="math/tex">\infty</script>范数作为其更新规则的一部分，以提高优化算法的稳定性和效果</p>
<h3 id="Nadam"><a href="#Nadam" class="headerlink" title="Nadam"></a>Nadam</h3><blockquote>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://paddlepedia.readthedocs.io/en/latest/tutorials/deep_learning/optimizers/nadam.html">深度学习优化策略Nadam</a></p>
</blockquote>
<p><code>Nadam(Nesterov-accelerated Adaptive Moment Estimation)</code>是将Adam与Nesterov加速梯度结合在一起，它对学习率的约束将更强，具备二者的优势，使得此算法在某些问题上的效果更好</p>
<p>Nadam的更新规则与Adam类似，但在计算梯度更新时引入了Nesterov动量项。具体而言，Nadam在计算梯度的移动平均和梯度更新时，使用了Nesterov动量的修正梯度来更新模型参数。这使得Nadam在处理凸优化问题时能够更好地逼近最优解，并且在处理非凸问题时能够更快地收敛</p>
<h3 id="其他优化器"><a href="#其他优化器" class="headerlink" title="其他优化器"></a>其他优化器</h3><p>AdamW（Adam with Weight Decay）： AdamW是对Adam优化器的改进，通过添加权重衰减（Weight Decay）的正则化项来解决权重衰减对Adam优化器的影响。传统的Adam优化器在计算梯度更新时，会将权重衰减项也纳入梯度计算中，导致权重衰减效果不准确。而AdamW在计算梯度更新时将权重衰减项单独处理，使得权重衰减的效果更加准确和稳定</p>
<p>ASGD（Average Stochastic Gradient Descent）： ASGD是一种随机梯度下降法的变体，它通过计算一定数量的随机梯度的平均值来更新模型参数。ASGD使用一个平均模型参数的历史记录，以减小训练过程中参数更新的方差。这样可以使模型的收敛速度更稳定，并且能够在训练过程中逐渐减小学习率，使得模型在训练后期更加趋于收敛</p>
<p>LBFGS（Limited-memory Broyden-Fletcher-Goldfarb-Shanno）： LBFGS是一种基于拟牛顿法的优化算法，用于解决无约束非线性优化问题。它利用函数的一阶导数和二阶导数信息来逼近目标函数的局部二次模型，并通过迭代更新参数来寻找最优解。LBFGS使用有限的内存来存储历史信息，以减少内存消耗。由于它不需要显式计算二阶导数矩阵，LBFGS适用于参数较多的问题，并且通常具有较好的收敛性能</p>
<p>总结：</p>
<ul>
<li>AdamW是对Adam优化器的改进，解决了权重衰减对Adam优化器的影响</li>
<li>ASGD是一种随机梯度下降法的变体，通过平均随机梯度来减小参数更新的方差，提高收敛速度和稳定性</li>
<li>LBFGS是一种基于拟牛顿法的优化算法，通过逼近目标函数的局部二次模型来寻找最优解，具有较好的收敛性能和适用性</li>
</ul>
<h1 id="学习率衰减"><a href="#学习率衰减" class="headerlink" title="学习率衰减"></a>学习率衰减</h1><p>在模型优化中，常用到的几种学习率衰减方法有：分段常数衰减、多项式衰减、指数衰减、自然指数衰减、余弦衰减、线性余弦衰减、噪声线性余弦衰减</p>
<p><a target="_blank" rel="noopener external nofollow noreferrer" href="https://www.hanspub.org/journal/PaperInformation.aspx?paperID=27016&amp;btwaf=50696508">深度学习优化器方法及学习率衰减方式综述</a></p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/narutohyc">narutohyc</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://study.hycbook.com/article/36065.html">https://study.hycbook.com/article/36065.html</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="external nofollow noreferrer" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://study.hycbook.com" target="_blank">兼一书虫</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/">神经网络</a><a class="post-meta__tags" href="/tags/%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95/">优化算法</a></div><div class="post_share"><div class="social-share" data-image="https://pic.hycbook.com/i/hexo/post_cover/蕾姆9.webp" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><link rel="stylesheet" href="/" media="defer" onload="this.media='all'"/><div class="post-reward"><button class="tip-button reward-button"><span class="tip-button__text">打赏</span><div class="coin-wrapper"><div class="coin"><div class="coin__middle"></div><div class="coin__back"></div><div class="coin__front"></div></div></div><div class="reward-main"><ul class="reward-all"><li class="reward-item"><a href="https://pic.hycbook.com/i//hexo/qr_codes/hyc_wechat.webp" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/qr_codes/hyc_wechat.webp" alt="wechat"/></a><div class="post-qr-code-desc">wechat</div></li><li class="reward-item"><a href="https://pic.hycbook.com/i//hexo/qr_codes/hyc_alipay.webp" rel="external nofollow noreferrer" target="_blank"><img class="post-qr-code-img" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/qr_codes/hyc_alipay.webp" alt="alipay"/></a><div class="post-qr-code-desc">alipay</div></li></ul></div></button></div><audio id="coinAudio" src="https://s1.vika.cn/space/2022/10/29/6db0ad2bccf949f09054b3b206dcc66f?attname=马里奥游戏投币叮当.mp3"></audio><script defer="defer" src="/"></script><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/article/46832.html"><img class="prev-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆8.webp" onerror="onerror=null;src='https://pic.hycbook.com/i/hexo/config_imgs/404.svg'" alt="cover of previous post"><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">深度学习核心之损失函数</div></div></a></div><div class="next-post pull-right"><a href="/article/54081.html"><img class="next-cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆10.webp" onerror="onerror=null;src='https://pic.hycbook.com/i/hexo/config_imgs/404.svg'" alt="cover of next post"><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">图像分割算法</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/article/46832.html" title="深度学习核心之损失函数"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆8.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-26</div><div class="title">深度学习核心之损失函数</div></div></a></div><div><a href="/article/22410.html" title="深度学习核心之激活函数"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆7.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-26</div><div class="title">深度学习核心之激活函数</div></div></a></div><div><a href="/article/451.html" title="图像分类算法"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆5.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-18</div><div class="title">图像分类算法</div></div></a></div><div><a href="/article/6384.html" title="深度学习核心基础知识点"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆6.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-20</div><div class="title">深度学习核心基础知识点</div></div></a></div><div><a href="/article/47450.html" title="图神经网络"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆2.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-08-16</div><div class="title">图神经网络</div></div></a></div><div><a href="/article/53377.html" title="深度学习模型压缩技术"><img class="cover" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i/hexo/post_cover/蕾姆11.webp" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-06-04</div><div class="title">深度学习模型压缩技术</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div><div id="comment-switch"><span class="first-comment">Twikoo</span><span class="switch-btn"></span><span class="second-comment">Valine</span></div></div><div class="comment-wrap"><div><div id="twikoo-wrap"></div></div><div><div class="vcomment" id="vcomment"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content is-expand"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E7%AE%97%E6%B3%95"><span class="toc-number">1.</span> <span class="toc-text">优化算法</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A6%82%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text">概述</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%9A%84%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95"><span class="toc-number">1.2.</span> <span class="toc-text">基本的梯度下降法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BGD"><span class="toc-number">1.2.1.</span> <span class="toc-text">BGD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SGD"><span class="toc-number">1.2.2.</span> <span class="toc-text">SGD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MBGD"><span class="toc-number">1.2.3.</span> <span class="toc-text">MBGD</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8A%A8%E9%87%8F%E4%BC%98%E5%8C%96%E6%B3%95"><span class="toc-number">1.3.</span> <span class="toc-text">动量优化法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SGDM"><span class="toc-number">1.3.1.</span> <span class="toc-text">SGDM</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#NAG"><span class="toc-number">1.3.2.</span> <span class="toc-text">NAG</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E9%80%82%E5%BA%94%E5%AD%A6%E4%B9%A0%E7%8E%87%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.4.</span> <span class="toc-text">自适应学习率优化器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#AdaGrad"><span class="toc-number">1.4.1.</span> <span class="toc-text">AdaGrad</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#AdaMax"><span class="toc-number">1.4.2.</span> <span class="toc-text">AdaMax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Nadam"><span class="toc-number">1.4.3.</span> <span class="toc-text">Nadam</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E4%BC%98%E5%8C%96%E5%99%A8"><span class="toc-number">1.4.4.</span> <span class="toc-text">其他优化器</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%AD%A6%E4%B9%A0%E7%8E%87%E8%A1%B0%E5%87%8F"><span class="toc-number">2.</span> <span class="toc-text">学习率衰减</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('https://pic.hycbook.com/i/hexo/config_imgs/footer_bg.webp')"><div id="footer-wrap"><div class="copyright">&copy;2022 - 2023 By narutohyc</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener external nofollow noreferrer" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text"><p><a target="_blank" href="https://hexo.io/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Frame-Hexo-blue?style=flat&logo=hexo" title="博客框架为Hexo"></a>&nbsp;<a target="_blank" href="https://demo.jerryc.me/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Theme-Butterfly-6513df?style=flat&logo=bitdefender" title="主题采用butterfly"></a>&nbsp;<a target="_blank" href="https://vercel.com/ " rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Hosted-Vervel-brightgreen?style=flat&logo=Vercel" title="本站采用双线部署，默认线路托管于Vercel"></a>&nbsp;<a target="_blank" href="https://github.com/" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/Source-Github-d021d6?style=flat&logo=GitHub" title="本站项目由Gtihub托管"></a>&nbsp;<a target="_blank" href="https://zixiaoyun.com" rel="external nofollow noreferrer"><img src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://img.shields.io/badge/图床-薄荷图床-green" title="薄荷图床"></a></p><a target="_blank" href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=35020502000647" rel="external nofollow noreferrer"><img style="position:relative;top:4px" src= "data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7" data-lazy-src="https://pic.hycbook.com/i//hexo/config_imgs//备案图标.webp" alt="ICP"/>闽公网安备35020502000647号  </a><a href="https://beian.miit.gov.cn/" rel="external nofollow noreferrer" target="_blank">闽ICP备2022013843号-1</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="translateLink" type="button" title="简繁转换">简</button><!-- button#darkmode(type="button" title=_p('rightside.night_mode_title'))--><!--  i.fas.fa-adjust--><a class="icon-V hidden" onclick="switchNightMode()" title="浅色和深色模式转换"><svg width="25" height="25" viewBox="0 0 1024 1024"><use id="modeicon" xlink:href="#icon-moon"></use></svg></a><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><nav class="search-nav"><span class="search-dialog-title">搜索</span><span id="loading-status"></span><button class="search-close-button"><i class="fas fa-times"></i></button></nav><div class="is-center" id="loading-database"><i class="fas fa-spinner fa-pulse"></i><span>  数据库加载中</span></div><div class="search-wrap"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="搜索文章" type="text"/></div></div><hr/><div id="local-search-results"></div></div></div><div id="search-mask"></div></div><div id="rightMenu"><div class="rightMenu-group rightMenu-small"><div class="rightMenu-item" id="menu-backward"><i class="fa-solid fa-arrow-left"></i></div><div class="rightMenu-item" id="menu-forward"><i class="fa-solid fa-arrow-right"></i></div><div class="rightMenu-item" id="menu-refresh"><i class="fa-solid fa-arrow-rotate-right"></i></div><div class="rightMenu-item" id="menu-home"><i class="fa-solid fa-house"></i></div></div><div class="rightMenu-group rightMenu-line hide" id="menu-text"><a class="rightMenu-item" href="javascript:window.open(&quot;https://www.baidu.com/s?wd=&quot;+window.getSelection().toString());window.location.reload();" rel="external nofollow noreferrer"><i class="fas fa-comment"></i><span>百度搜索</span></a></div><div class="rightMenu-group rightMenu-line rightMenuOther"><a class="rightMenu-item menu-link" href="/archives/"><i class="fa-solid fa-archive"></i><span>文章归档</span></a><a class="rightMenu-item menu-link" href="/categories/"><i class="fa-solid fa-folder-open"></i><span>文章分类</span></a><a class="rightMenu-item menu-link" href="/tags/"><i class="fa-solid fa-tags"></i><span>文章标签</span></a></div><div class="rightMenu-group rightMenu-line rightMenuNormal"><a class="rightMenu-item menu-link" id="menu-radompage"><i class="fa-solid fa-shoe-prints"></i><span>随便逛逛</span></a><div class="rightMenu-item" id="menu-translate"><i class="fa-solid fa-earth-asia"></i><span>繁简切换</span></div><div class="rightMenu-item" id="menu-darkmode"><i class="fa-solid fa-moon"></i><span>切换模式</span></div></div></div><div id="rightmenu-mask"></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox.umd.min.js"></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>(() => {
  const $mermaidWrap = document.querySelectorAll('#article-container .mermaid-wrap')
  if ($mermaidWrap.length) {
    window.runMermaid = () => {
      window.loadMermaid = true
      const theme = document.documentElement.getAttribute('data-theme') === 'dark' ? '' : ''

      Array.from($mermaidWrap).forEach((item, index) => {
        const mermaidSrc = item.firstElementChild
        const mermaidThemeConfig = '%%{init:{ \'theme\':\'' + theme + '\'}}%%\n'
        const mermaidID = 'mermaid-' + index
        const mermaidDefinition = mermaidThemeConfig + mermaidSrc.textContent
        mermaid.mermaidAPI.render(mermaidID, mermaidDefinition, (svgCode) => {
          mermaidSrc.insertAdjacentHTML('afterend', svgCode)
        })
      })
    }

    const loadMermaid = () => {
      window.loadMermaid ? runMermaid() : getScript('https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js').then(runMermaid)
    }

    window.pjax ? loadMermaid() : document.addEventListener('DOMContentLoaded', loadMermaid)
  }
})()</script><script>(()=>{
  const init = () => {
    twikoo.init(Object.assign({
      el: '#twikoo-wrap',
      envId: 'https://vercel.hycbook.com',
      region: '',
      onCommentLoaded: function () {
        btf.loadLightbox(document.querySelectorAll('#twikoo .tk-content img:not(.tk-owo-emotion)'))
      }
    }, null))
  }

  const getCount = () => {
    const countELement = document.getElementById('twikoo-count')
    if(!countELement) return
    twikoo.getCommentsCount({
      envId: 'https://vercel.hycbook.com',
      region: '',
      urls: [window.location.pathname],
      includeReply: false
    }).then(function (res) {
      countELement.innerText = res[0].count
    }).catch(function (err) {
      console.error(err);
    });
  }

  const runFn = () => {
    init()
    GLOBAL_CONFIG_SITE.isPost && getCount()
  }

  const loadTwikoo = () => {
    if (typeof twikoo === 'object') {
      setTimeout(runFn,0)
      return
    } 
    getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runFn)
  }

  if ('Twikoo' === 'Twikoo' || !false) {
    if (false) btf.loadComment(document.getElementById('twikoo-wrap'), loadTwikoo)
    else loadTwikoo()
  } else {
    window.loadOtherComment = () => {
      loadTwikoo()
    }
  }
})()</script><script>function loadValine () {
  function initValine () {
    const valine = new Valine(Object.assign({
      el: '#vcomment',
      appId: 'ncn88uooQf0IO2rrGE7Vniwp-gzGzoHsz',
      appKey: 'Yghpzg1QfBMFJ0MxxHubVzKL',
      avatar: 'monsterid',
      serverURLs: '',
      emojiMaps: "",
      path: window.location.pathname,
      visitor: true
    }, null))
  }

  if (typeof Valine === 'function') initValine() 
  else getScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js').then(initValine)
}

if ('Twikoo' === 'Valine' || !false) {
  if (false) btf.loadComment(document.getElementById('vcomment'),loadValine)
  else setTimeout(loadValine, 0)
} else {
  function loadOtherComment () {
    loadValine()
  }
}</script></div><script>window.addEventListener('load', () => {
  const changeContent = (content) => {
    if (content === '') return content

    content = content.replace(/<img.*?src="(.*?)"?[^\>]+>/ig, '[图片]') // replace image link
    content = content.replace(/<a[^>]+?href=["']?([^"']+)["']?[^>]*>([^<]+)<\/a>/gi, '[链接]') // replace url
    content = content.replace(/<pre><code>.*?<\/pre>/gi, '[代码]') // replace code
    content = content.replace(/<[^>]+>/g,"") // remove html tag

    if (content.length > 150) {
      content = content.substring(0,150) + '...'
    }
    return content
  }

  const getComment = () => {
    const runTwikoo = () => {
      twikoo.getRecentComments({
        envId: 'https://vercel.hycbook.com',
        region: '',
        pageSize: 3,
        includeReply: true
      }).then(function (res) {
        const twikooArray = res.map(e => {
          return {
            'content': changeContent(e.comment),
            'avatar': e.avatar,
            'nick': e.nick,
            'url': e.url + '#' + e.id,
            'date': new Date(e.created).toISOString()
          }
        })

        saveToLocal.set('twikoo-newest-comments', JSON.stringify(twikooArray), 10/(60*24))
        generateHtml(twikooArray)
      }).catch(function (err) {
        const $dom = document.querySelector('#card-newest-comments .aside-list')
        $dom.innerHTML= "无法获取评论，请确认相关配置是否正确"
      })
    }

    if (typeof twikoo === 'object') {
      runTwikoo()
    } else {
      getScript('https://cdn.jsdelivr.net/npm/twikoo/dist/twikoo.all.min.js').then(runTwikoo)
    }
  }

  const generateHtml = array => {
    let result = ''

    if (array.length) {
      for (let i = 0; i < array.length; i++) {
        result += '<div class=\'aside-list-item\'>'

        if (true) {
          const name = 'data-lazy-src'
          result += `<a href='${array[i].url}' class='thumbnail'><img ${name}='${array[i].avatar}' alt='${array[i].nick}'></a>`
        }
        
        result += `<div class='content'>
        <a class='comment' href='${array[i].url}' title='${array[i].content}'>${array[i].content}</a>
        <div class='name'><span>${array[i].nick} / </span><time datetime="${array[i].date}">${btf.diffDate(array[i].date, true)}</time></div>
        </div></div>`
      }
    } else {
      result += '没有评论'
    }

    let $dom = document.querySelector('#card-newest-comments .aside-list')
    $dom.innerHTML= result
    window.lazyLoadInstance && window.lazyLoadInstance.update()
    window.pjax && window.pjax.refresh($dom)
  }

  const newestCommentInit = () => {
    if (document.querySelector('#card-newest-comments .aside-list')) {
      const data = saveToLocal.get('twikoo-newest-comments')
      if (data) {
        generateHtml(JSON.parse(data))
      } else {
        getComment()
      }
    }
  }

  newestCommentInit()
  document.addEventListener('pjax:complete', newestCommentInit)
})</script><script defer src="https://npm.elemecdn.com/jquery@latest/dist/jquery.min.js"></script><script defer data-pjax src="/zhheo/random.js"></script><script data-pjax src="/js/coin.js"></script><script defer src="https://npm.elemecdn.com/vue@2.6.11"></script><script async src="//at.alicdn.com/t/c/font_3670467_a0sijt8frxo.js"></script><script defer src="/live2d-widget/autoload.js"></script><script defer src="https://cdnjs.cloudflare.com/ajax/libs/toastr.js/latest/toastr.min.js"></script><script data-pjax defer src="https://npm.elemecdn.com/tzy-blog/lib/js/theme/chocolate.js"></script><script defer data-pjax src="/js/rightMenu.js"></script><script defer data-pjax src="/js/udf_mouse.js"></script><script defer data-pjax src="/js/udf_js.js"></script><script defer src="/js/udf_js.js"></script><script defer="defer" id="fluttering_ribbon" mobile="false" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/canvas-fluttering-ribbon.min.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = false;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/dist/click-heart.min.js" async="async" mobile="true"></script><script>window.$crisp = [];
window.CRISP_WEBSITE_ID = "561b80db-3f0f-45cb-b3b1-aae7355939e6";
(function () {
  d = document;
  s = d.createElement("script");
  s.src = "https://client.crisp.chat/l.js";
  s.async = 1;
  d.getElementsByTagName("head")[0].appendChild(s);
})();
$crisp.push(["safe", true])

if (false) {
  $crisp.push(["do", "chat:hide"])
  $crisp.push(["on", "chat:closed", function() {
    $crisp.push(["do", "chat:hide"])
  }])
  var chatBtnFn = () => {
    var chatBtn = document.getElementById("chat_btn")
    chatBtn.addEventListener("click", function(){
      $crisp.push(["do", "chat:show"])
      $crisp.push(["do", "chat:open"])

    });
  }
  chatBtnFn()
} else {
  if (false) {
    function chatBtnHide () {
      $crisp.push(["do", "chat:hide"])
    }
    function chatBtnShow () {
      $crisp.push(["do", "chat:show"])
    }
  }
}</script><script src="https://cdn.jsdelivr.net/npm/pjax/pjax.min.js"></script><script>let pjaxSelectors = ["head > title","#config-diff","#body-wrap","#rightside-config-hide","#rightside-config-show",".js-pjax"]

var pjax = new Pjax({
  elements: 'a:not([target="_blank"])',
  selectors: pjaxSelectors,
  cacheBust: false,
  analytics: false,
  scrollRestoration: false
})

document.addEventListener('pjax:send', function () {

  // removeEventListener scroll 
  window.tocScrollFn && window.removeEventListener('scroll', window.tocScrollFn)
  window.scrollCollect && window.removeEventListener('scroll', scrollCollect)

  typeof preloader === 'object' && preloader.initLoading()
  document.getElementById('rightside').style.cssText = "opacity: ''; transform: ''"
  
  if (window.aplayers) {
    for (let i = 0; i < window.aplayers.length; i++) {
      if (!window.aplayers[i].options.fixed) {
        window.aplayers[i].destroy()
      }
    }
  }

  typeof typed === 'object' && typed.destroy()

  //reset readmode
  const $bodyClassList = document.body.classList
  $bodyClassList.contains('read-mode') && $bodyClassList.remove('read-mode')

  typeof disqusjs === 'object' && disqusjs.destroy()
})

document.addEventListener('pjax:complete', function () {
  window.refreshFn()

  document.querySelectorAll('script[data-pjax]').forEach(item => {
    const newScript = document.createElement('script')
    const content = item.text || item.textContent || item.innerHTML || ""
    Array.from(item.attributes).forEach(attr => newScript.setAttribute(attr.name, attr.value))
    newScript.appendChild(document.createTextNode(content))
    item.parentNode.replaceChild(newScript, item)
  })

  GLOBAL_CONFIG.islazyload && window.lazyLoadInstance.update()

  typeof chatBtnFn === 'function' && chatBtnFn()
  typeof panguInit === 'function' && panguInit()

  // google analytics
  typeof gtag === 'function' && gtag('config', '', {'page_path': window.location.pathname});

  // baidu analytics
  typeof _hmt === 'object' && _hmt.push(['_trackPageview',window.location.pathname]);

  typeof loadMeting === 'function' && document.getElementsByClassName('aplayer').length && loadMeting()

  // prismjs
  typeof Prism === 'object' && Prism.highlightAll()

  typeof preloader === 'object' && preloader.endLoading()
})

document.addEventListener('pjax:error', (e) => {
  if (e.request.status === 404) {
    pjax.loadUrl('/404.html')
  }
})</script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><div class="app-refresh" id="app-refresh" style="position: fixed;top: -2.2rem;left: 0;right: 0;z-index: 99999;padding: 0 1rem;font-size: 15px;height: 2.2rem;transition: all 0.3s ease;"><div class="app-refresh-wrap" style=" display: flex;color: #fff;height: 100%;align-items: center;justify-content: center;"><label>✨ 兼一书虫上新啦！ 👉</label><a href="javascript:void(0)" rel="external nofollow noreferrer" onclick="location.reload()"><span style="color: #fff;text-decoration: underline;cursor: pointer;">🍭查看新品🍬</span></a></div></div><script>if ('serviceWorker' in navigator) {
  if (navigator.serviceWorker.controller) {
    navigator.serviceWorker.addEventListener('controllerchange', function() {
      showNotification()
    })
  }
  window.addEventListener('load', function() {
    navigator.serviceWorker.register('/sw.js')
  })
}

function showNotification() {
  if (GLOBAL_CONFIG.Snackbar) {
    var snackbarBg =
      document.documentElement.getAttribute('data-theme') === 'light' ?
      GLOBAL_CONFIG.Snackbar.bgLight :
      GLOBAL_CONFIG.Snackbar.bgDark
    var snackbarPos = GLOBAL_CONFIG.Snackbar.position
    Snackbar.show({
      text: '✨ 兼一书虫上新啦！ 👉',
      backgroundColor: snackbarBg,
      duration: 500000,
      pos: snackbarPos,
      actionText: '🍭查看新品🍬',
      actionTextColor: '#fff',
      onActionClick: function(e) {
        location.reload()
      },
    })
  } else {
    var showBg =
      document.documentElement.getAttribute('data-theme') === 'light' ?
      '#49b1f5' :
      '#1f1f1f'
    var cssText = `top: 0; background: ${showBg};`
    document.getElementById('app-refresh')
      .style.cssText = cssText
  }
}</script></div><!-- hexo injector body_end start --><script async src="//at.alicdn.com/t/font_2032782_8d5kxvn09md.js"></script><!-- hexo injector body_end end --></body></html>